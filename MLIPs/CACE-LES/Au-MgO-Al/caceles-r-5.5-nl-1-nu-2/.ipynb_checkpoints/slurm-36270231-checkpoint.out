2025-06-25 04:10:50.800 INFO: reading data
2025-06-25 04:10:53.507 INFO: Loaded 5000 training configurations from '../Au-MgO-Al.xyz'
2025-06-25 04:10:53.507 INFO: Using random 10.0% of training set for validation
2025-06-25 04:11:11.343 INFO: CUDA version: 12.1, CUDA device: 0
2025-06-25 04:11:11.343 INFO: device: cuda
2025-06-25 04:11:11.343 INFO: building CACE representation
2025-06-25 04:11:11.538 INFO: Representation: Cace(
  (node_onehot): NodeEncoder(num_classes=4)
  (node_embedding_sender): NodeEmbedding(num_classes=4, embedding_dim=4)
  (node_embedding_receiver): NodeEmbedding(num_classes=4, embedding_dim=4)
  (edge_coding): EdgeEncoder(directed=True)
  (radial_basis): BesselRBF(cutoff=5.5, n_rbf=6, trainable=True)
  (cutoff_fn): PolynomialCutoff(p=6.0, cutoff=5.5)
  (angular_basis): AngularComponent(l_max=2)
  (radial_transform): SharedRadialLinearTransform(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 6x12x16 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 6x12x16 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 6x12x16 (cuda:0)]
    )
  )
  (symmetrizer): Symmetrizer()
  (message_passing_list): ModuleList()
)
2025-06-25 04:11:11.538 INFO: building CACE NNP
2025-06-25 04:11:11.539 INFO: First train loop:
2025-06-25 04:11:11.539 INFO: creating training task
2025-06-25 04:11:11.540 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-25 04:11:58.373 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 67.0643, Val Loss: 20.3343
2025-06-25 04:11:58.374 INFO: Epoch 1, Train Loss: 67.0643, Val Loss: 20.3343
train_e/atom_mae: 0.016240
2025-06-25 04:11:58.376 INFO: train_e/atom_mae: 0.016240
train_e/atom_rmse: 0.026471
2025-06-25 04:11:58.377 INFO: train_e/atom_rmse: 0.026471
train_f_mae: 0.125112
2025-06-25 04:11:58.381 INFO: train_f_mae: 0.125112
train_f_rmse: 0.257326
2025-06-25 04:11:58.381 INFO: train_f_rmse: 0.257326
val_e/atom_mae: 0.012712
2025-06-25 04:11:58.384 INFO: val_e/atom_mae: 0.012712
val_e/atom_rmse: 0.013209
2025-06-25 04:11:58.384 INFO: val_e/atom_rmse: 0.013209
val_f_mae: 0.075861
2025-06-25 04:11:58.385 INFO: val_f_mae: 0.075861
val_f_rmse: 0.141856
2025-06-25 04:11:58.385 INFO: val_f_rmse: 0.141856
##### Step: 1 Learning rate: 0.004 #####
2025-06-25 04:12:43.546 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 11.7299, Val Loss: 7.1372
2025-06-25 04:12:43.546 INFO: Epoch 2, Train Loss: 11.7299, Val Loss: 7.1372
train_e/atom_mae: 0.012659
2025-06-25 04:12:43.547 INFO: train_e/atom_mae: 0.012659
train_e/atom_rmse: 0.016118
2025-06-25 04:12:43.547 INFO: train_e/atom_rmse: 0.016118
train_f_mae: 0.061753
2025-06-25 04:12:43.551 INFO: train_f_mae: 0.061753
train_f_rmse: 0.106844
2025-06-25 04:12:43.551 INFO: train_f_rmse: 0.106844
val_e/atom_mae: 0.009485
2025-06-25 04:12:43.554 INFO: val_e/atom_mae: 0.009485
val_e/atom_rmse: 0.010307
2025-06-25 04:12:43.554 INFO: val_e/atom_rmse: 0.010307
val_f_mae: 0.052298
2025-06-25 04:12:43.554 INFO: val_f_mae: 0.052298
val_f_rmse: 0.083717
2025-06-25 04:12:43.555 INFO: val_f_rmse: 0.083717
##### Step: 2 Learning rate: 0.006 #####
2025-06-25 04:13:28.725 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 6.5948, Val Loss: 5.6414
2025-06-25 04:13:28.726 INFO: Epoch 3, Train Loss: 6.5948, Val Loss: 5.6414
train_e/atom_mae: 0.009581
2025-06-25 04:13:28.727 INFO: train_e/atom_mae: 0.009581
train_e/atom_rmse: 0.012033
2025-06-25 04:13:28.727 INFO: train_e/atom_rmse: 0.012033
train_f_mae: 0.050415
2025-06-25 04:13:28.730 INFO: train_f_mae: 0.050415
train_f_rmse: 0.080122
2025-06-25 04:13:28.731 INFO: train_f_rmse: 0.080122
val_e/atom_mae: 0.003000
2025-06-25 04:13:28.734 INFO: val_e/atom_mae: 0.003000
val_e/atom_rmse: 0.004196
2025-06-25 04:13:28.734 INFO: val_e/atom_rmse: 0.004196
val_f_mae: 0.047977
2025-06-25 04:13:28.734 INFO: val_f_mae: 0.047977
val_f_rmse: 0.074967
2025-06-25 04:13:28.734 INFO: val_f_rmse: 0.074967
##### Step: 3 Learning rate: 0.008 #####
2025-06-25 04:14:13.833 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 5.2701, Val Loss: 4.8923
2025-06-25 04:14:13.833 INFO: Epoch 4, Train Loss: 5.2701, Val Loss: 4.8923
train_e/atom_mae: 0.008846
2025-06-25 04:14:13.834 INFO: train_e/atom_mae: 0.008846
train_e/atom_rmse: 0.011356
2025-06-25 04:14:13.834 INFO: train_e/atom_rmse: 0.011356
train_f_mae: 0.044910
2025-06-25 04:14:13.838 INFO: train_f_mae: 0.044910
train_f_rmse: 0.071513
2025-06-25 04:14:13.838 INFO: train_f_rmse: 0.071513
val_e/atom_mae: 0.015532
2025-06-25 04:14:13.841 INFO: val_e/atom_mae: 0.015532
val_e/atom_rmse: 0.016211
2025-06-25 04:14:13.841 INFO: val_e/atom_rmse: 0.016211
val_f_mae: 0.043192
2025-06-25 04:14:13.842 INFO: val_f_mae: 0.043192
val_f_rmse: 0.067634
2025-06-25 04:14:13.842 INFO: val_f_rmse: 0.067634
##### Step: 4 Learning rate: 0.01 #####
2025-06-25 04:14:59.017 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 4.7039, Val Loss: 5.0702
2025-06-25 04:14:59.017 INFO: Epoch 5, Train Loss: 4.7039, Val Loss: 5.0702
train_e/atom_mae: 0.010363
2025-06-25 04:14:59.018 INFO: train_e/atom_mae: 0.010363
train_e/atom_rmse: 0.012969
2025-06-25 04:14:59.018 INFO: train_e/atom_rmse: 0.012969
train_f_mae: 0.042558
2025-06-25 04:14:59.022 INFO: train_f_mae: 0.042558
train_f_rmse: 0.067085
2025-06-25 04:14:59.022 INFO: train_f_rmse: 0.067085
val_e/atom_mae: 0.010170
2025-06-25 04:14:59.025 INFO: val_e/atom_mae: 0.010170
val_e/atom_rmse: 0.011002
2025-06-25 04:14:59.025 INFO: val_e/atom_rmse: 0.011002
val_f_mae: 0.044968
2025-06-25 04:14:59.026 INFO: val_f_mae: 0.044968
val_f_rmse: 0.070169
2025-06-25 04:14:59.026 INFO: val_f_rmse: 0.070169
##### Step: 5 Learning rate: 0.01 #####
2025-06-25 04:15:44.121 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 4.2410, Val Loss: 4.0297
2025-06-25 04:15:44.121 INFO: Epoch 6, Train Loss: 4.2410, Val Loss: 4.0297
train_e/atom_mae: 0.009543
2025-06-25 04:15:44.122 INFO: train_e/atom_mae: 0.009543
train_e/atom_rmse: 0.011877
2025-06-25 04:15:44.122 INFO: train_e/atom_rmse: 0.011877
train_f_mae: 0.040613
2025-06-25 04:15:44.126 INFO: train_f_mae: 0.040613
train_f_rmse: 0.063799
2025-06-25 04:15:44.126 INFO: train_f_rmse: 0.063799
val_e/atom_mae: 0.003965
2025-06-25 04:15:44.128 INFO: val_e/atom_mae: 0.003965
val_e/atom_rmse: 0.004864
2025-06-25 04:15:44.129 INFO: val_e/atom_rmse: 0.004864
val_f_mae: 0.041074
2025-06-25 04:15:44.129 INFO: val_f_mae: 0.041074
val_f_rmse: 0.063254
2025-06-25 04:15:44.130 INFO: val_f_rmse: 0.063254
##### Step: 6 Learning rate: 0.01 #####
2025-06-25 04:16:29.166 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 3.6118, Val Loss: 3.9248
2025-06-25 04:16:29.166 INFO: Epoch 7, Train Loss: 3.6118, Val Loss: 3.9248
train_e/atom_mae: 0.011252
2025-06-25 04:16:29.167 INFO: train_e/atom_mae: 0.011252
train_e/atom_rmse: 0.013622
2025-06-25 04:16:29.167 INFO: train_e/atom_rmse: 0.013622
train_f_mae: 0.037682
2025-06-25 04:16:29.171 INFO: train_f_mae: 0.037682
train_f_rmse: 0.058201
2025-06-25 04:16:29.171 INFO: train_f_rmse: 0.058201
val_e/atom_mae: 0.013402
2025-06-25 04:16:29.174 INFO: val_e/atom_mae: 0.013402
val_e/atom_rmse: 0.013846
2025-06-25 04:16:29.174 INFO: val_e/atom_rmse: 0.013846
val_f_mae: 0.039443
2025-06-25 04:16:29.174 INFO: val_f_mae: 0.039443
val_f_rmse: 0.060769
2025-06-25 04:16:29.175 INFO: val_f_rmse: 0.060769
##### Step: 7 Learning rate: 0.01 #####
2025-06-25 04:17:14.365 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 3.4583, Val Loss: 5.6918
2025-06-25 04:17:14.365 INFO: Epoch 8, Train Loss: 3.4583, Val Loss: 5.6918
train_e/atom_mae: 0.012172
2025-06-25 04:17:14.366 INFO: train_e/atom_mae: 0.012172
train_e/atom_rmse: 0.016166
2025-06-25 04:17:14.366 INFO: train_e/atom_rmse: 0.016166
train_f_mae: 0.036746
2025-06-25 04:17:14.370 INFO: train_f_mae: 0.036746
train_f_rmse: 0.056054
2025-06-25 04:17:14.370 INFO: train_f_rmse: 0.056054
val_e/atom_mae: 0.046682
2025-06-25 04:17:14.373 INFO: val_e/atom_mae: 0.046682
val_e/atom_rmse: 0.046916
2025-06-25 04:17:14.373 INFO: val_e/atom_rmse: 0.046916
val_f_mae: 0.037552
2025-06-25 04:17:14.374 INFO: val_f_mae: 0.037552
val_f_rmse: 0.055032
2025-06-25 04:17:14.374 INFO: val_f_rmse: 0.055032
##### Step: 8 Learning rate: 0.01 #####
2025-06-25 04:17:59.406 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 3.4724, Val Loss: 3.0990
2025-06-25 04:17:59.407 INFO: Epoch 9, Train Loss: 3.4724, Val Loss: 3.0990
train_e/atom_mae: 0.013481
2025-06-25 04:17:59.408 INFO: train_e/atom_mae: 0.013481
train_e/atom_rmse: 0.017831
2025-06-25 04:17:59.408 INFO: train_e/atom_rmse: 0.017831
train_f_mae: 0.036688
2025-06-25 04:17:59.411 INFO: train_f_mae: 0.036688
train_f_rmse: 0.055567
2025-06-25 04:17:59.412 INFO: train_f_rmse: 0.055567
val_e/atom_mae: 0.012980
2025-06-25 04:17:59.414 INFO: val_e/atom_mae: 0.012980
val_e/atom_rmse: 0.013472
2025-06-25 04:17:59.415 INFO: val_e/atom_rmse: 0.013472
val_f_mae: 0.035756
2025-06-25 04:17:59.415 INFO: val_f_mae: 0.035756
val_f_rmse: 0.053659
2025-06-25 04:17:59.415 INFO: val_f_rmse: 0.053659
##### Step: 9 Learning rate: 0.01 #####
2025-06-25 04:18:44.548 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 2.9923, Val Loss: 2.6993
2025-06-25 04:18:44.549 INFO: Epoch 10, Train Loss: 2.9923, Val Loss: 2.6993
train_e/atom_mae: 0.008124
2025-06-25 04:18:44.550 INFO: train_e/atom_mae: 0.008124
train_e/atom_rmse: 0.010241
2025-06-25 04:18:44.550 INFO: train_e/atom_rmse: 0.010241
train_f_mae: 0.035146
2025-06-25 04:18:44.554 INFO: train_f_mae: 0.035146
train_f_rmse: 0.053529
2025-06-25 04:18:44.554 INFO: train_f_rmse: 0.053529
val_e/atom_mae: 0.003206
2025-06-25 04:18:44.556 INFO: val_e/atom_mae: 0.003206
val_e/atom_rmse: 0.003858
2025-06-25 04:18:44.557 INFO: val_e/atom_rmse: 0.003858
val_f_mae: 0.034584
2025-06-25 04:18:44.557 INFO: val_f_mae: 0.034584
val_f_rmse: 0.051781
2025-06-25 04:18:44.557 INFO: val_f_rmse: 0.051781
##### Step: 10 Learning rate: 0.01 #####
2025-06-25 04:19:29.623 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 2.9024, Val Loss: 3.2475
2025-06-25 04:19:29.623 INFO: Epoch 11, Train Loss: 2.9024, Val Loss: 3.2475
train_e/atom_mae: 0.008242
2025-06-25 04:19:29.624 INFO: train_e/atom_mae: 0.008242
train_e/atom_rmse: 0.010571
2025-06-25 04:19:29.624 INFO: train_e/atom_rmse: 0.010571
train_f_mae: 0.034675
2025-06-25 04:19:29.628 INFO: train_f_mae: 0.034675
train_f_rmse: 0.052604
2025-06-25 04:19:29.628 INFO: train_f_rmse: 0.052604
val_e/atom_mae: 0.004571
2025-06-25 04:19:29.631 INFO: val_e/atom_mae: 0.004571
val_e/atom_rmse: 0.005719
2025-06-25 04:19:29.631 INFO: val_e/atom_rmse: 0.005719
val_f_mae: 0.036965
2025-06-25 04:19:29.631 INFO: val_f_mae: 0.036965
val_f_rmse: 0.056638
2025-06-25 04:19:29.632 INFO: val_f_rmse: 0.056638
##### Step: 11 Learning rate: 0.01 #####
2025-06-25 04:20:14.743 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 2.7394, Val Loss: 2.6007
2025-06-25 04:20:14.743 INFO: Epoch 12, Train Loss: 2.7394, Val Loss: 2.6007
train_e/atom_mae: 0.007651
2025-06-25 04:20:14.744 INFO: train_e/atom_mae: 0.007651
train_e/atom_rmse: 0.009536
2025-06-25 04:20:14.744 INFO: train_e/atom_rmse: 0.009536
train_f_mae: 0.033734
2025-06-25 04:20:14.748 INFO: train_f_mae: 0.033734
train_f_rmse: 0.051277
2025-06-25 04:20:14.748 INFO: train_f_rmse: 0.051277
val_e/atom_mae: 0.007862
2025-06-25 04:20:14.751 INFO: val_e/atom_mae: 0.007862
val_e/atom_rmse: 0.008946
2025-06-25 04:20:14.751 INFO: val_e/atom_rmse: 0.008946
val_f_mae: 0.033518
2025-06-25 04:20:14.752 INFO: val_f_mae: 0.033518
val_f_rmse: 0.050039
2025-06-25 04:20:14.752 INFO: val_f_rmse: 0.050039
##### Step: 12 Learning rate: 0.01 #####
2025-06-25 04:20:59.923 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 2.6273, Val Loss: 2.6554
2025-06-25 04:20:59.923 INFO: Epoch 13, Train Loss: 2.6273, Val Loss: 2.6554
train_e/atom_mae: 0.007504
2025-06-25 04:20:59.924 INFO: train_e/atom_mae: 0.007504
train_e/atom_rmse: 0.009381
2025-06-25 04:20:59.925 INFO: train_e/atom_rmse: 0.009381
train_f_mae: 0.032711
2025-06-25 04:20:59.928 INFO: train_f_mae: 0.032711
train_f_rmse: 0.050208
2025-06-25 04:20:59.928 INFO: train_f_rmse: 0.050208
val_e/atom_mae: 0.003502
2025-06-25 04:20:59.931 INFO: val_e/atom_mae: 0.003502
val_e/atom_rmse: 0.003695
2025-06-25 04:20:59.932 INFO: val_e/atom_rmse: 0.003695
val_f_mae: 0.033651
2025-06-25 04:20:59.932 INFO: val_f_mae: 0.033651
val_f_rmse: 0.051370
2025-06-25 04:20:59.932 INFO: val_f_rmse: 0.051370
##### Step: 13 Learning rate: 0.01 #####
2025-06-25 04:21:44.948 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 2.8333, Val Loss: 2.5333
2025-06-25 04:21:44.948 INFO: Epoch 14, Train Loss: 2.8333, Val Loss: 2.5333
train_e/atom_mae: 0.008143
2025-06-25 04:21:44.949 INFO: train_e/atom_mae: 0.008143
train_e/atom_rmse: 0.010193
2025-06-25 04:21:44.950 INFO: train_e/atom_rmse: 0.010193
train_f_mae: 0.034012
2025-06-25 04:21:44.953 INFO: train_f_mae: 0.034012
train_f_rmse: 0.052035
2025-06-25 04:21:44.953 INFO: train_f_rmse: 0.052035
val_e/atom_mae: 0.007606
2025-06-25 04:21:44.956 INFO: val_e/atom_mae: 0.007606
val_e/atom_rmse: 0.008513
2025-06-25 04:21:44.956 INFO: val_e/atom_rmse: 0.008513
val_f_mae: 0.033211
2025-06-25 04:21:44.957 INFO: val_f_mae: 0.033211
val_f_rmse: 0.049453
2025-06-25 04:21:44.957 INFO: val_f_rmse: 0.049453
##### Step: 14 Learning rate: 0.01 #####
2025-06-25 04:22:30.064 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 2.7546, Val Loss: 2.4145
2025-06-25 04:22:30.064 INFO: Epoch 15, Train Loss: 2.7546, Val Loss: 2.4145
train_e/atom_mae: 0.009748
2025-06-25 04:22:30.065 INFO: train_e/atom_mae: 0.009748
train_e/atom_rmse: 0.011886
2025-06-25 04:22:30.065 INFO: train_e/atom_rmse: 0.011886
train_f_mae: 0.033142
2025-06-25 04:22:30.069 INFO: train_f_mae: 0.033142
train_f_rmse: 0.050829
2025-06-25 04:22:30.069 INFO: train_f_rmse: 0.050829
val_e/atom_mae: 0.016227
2025-06-25 04:22:30.072 INFO: val_e/atom_mae: 0.016227
val_e/atom_rmse: 0.016623
2025-06-25 04:22:30.072 INFO: val_e/atom_rmse: 0.016623
val_f_mae: 0.030688
2025-06-25 04:22:30.073 INFO: val_f_mae: 0.030688
val_f_rmse: 0.045608
2025-06-25 04:22:30.073 INFO: val_f_rmse: 0.045608
##### Step: 15 Learning rate: 0.01 #####
2025-06-25 04:23:15.047 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 2.6303, Val Loss: 2.8244
2025-06-25 04:23:15.048 INFO: Epoch 16, Train Loss: 2.6303, Val Loss: 2.8244
train_e/atom_mae: 0.007785
2025-06-25 04:23:15.049 INFO: train_e/atom_mae: 0.007785
train_e/atom_rmse: 0.010069
2025-06-25 04:23:15.049 INFO: train_e/atom_rmse: 0.010069
train_f_mae: 0.032754
2025-06-25 04:23:15.052 INFO: train_f_mae: 0.032754
train_f_rmse: 0.050076
2025-06-25 04:23:15.053 INFO: train_f_rmse: 0.050076
val_e/atom_mae: 0.003430
2025-06-25 04:23:15.055 INFO: val_e/atom_mae: 0.003430
val_e/atom_rmse: 0.004348
2025-06-25 04:23:15.056 INFO: val_e/atom_rmse: 0.004348
val_f_mae: 0.035526
2025-06-25 04:23:15.056 INFO: val_f_mae: 0.035526
val_f_rmse: 0.052930
2025-06-25 04:23:15.056 INFO: val_f_rmse: 0.052930
##### Step: 16 Learning rate: 0.01 #####
2025-06-25 04:24:00.133 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 2.7245, Val Loss: 2.3591
2025-06-25 04:24:00.133 INFO: Epoch 17, Train Loss: 2.7245, Val Loss: 2.3591
train_e/atom_mae: 0.009541
2025-06-25 04:24:00.134 INFO: train_e/atom_mae: 0.009541
train_e/atom_rmse: 0.011828
2025-06-25 04:24:00.134 INFO: train_e/atom_rmse: 0.011828
train_f_mae: 0.032870
2025-06-25 04:24:00.138 INFO: train_f_mae: 0.032870
train_f_rmse: 0.050549
2025-06-25 04:24:00.138 INFO: train_f_rmse: 0.050549
val_e/atom_mae: 0.010846
2025-06-25 04:24:00.141 INFO: val_e/atom_mae: 0.010846
val_e/atom_rmse: 0.011385
2025-06-25 04:24:00.141 INFO: val_e/atom_rmse: 0.011385
val_f_mae: 0.030774
2025-06-25 04:24:00.142 INFO: val_f_mae: 0.030774
val_f_rmse: 0.046928
2025-06-25 04:24:00.142 INFO: val_f_rmse: 0.046928
##### Step: 17 Learning rate: 0.01 #####
2025-06-25 04:24:45.183 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 2.2829, Val Loss: 2.3927
2025-06-25 04:24:45.183 INFO: Epoch 18, Train Loss: 2.2829, Val Loss: 2.3927
train_e/atom_mae: 0.006848
2025-06-25 04:24:45.184 INFO: train_e/atom_mae: 0.006848
train_e/atom_rmse: 0.008674
2025-06-25 04:24:45.184 INFO: train_e/atom_rmse: 0.008674
train_f_mae: 0.030818
2025-06-25 04:24:45.188 INFO: train_f_mae: 0.030818
train_f_rmse: 0.046817
2025-06-25 04:24:45.188 INFO: train_f_rmse: 0.046817
val_e/atom_mae: 0.005968
2025-06-25 04:24:45.191 INFO: val_e/atom_mae: 0.005968
val_e/atom_rmse: 0.007206
2025-06-25 04:24:45.191 INFO: val_e/atom_rmse: 0.007206
val_f_mae: 0.031949
2025-06-25 04:24:45.192 INFO: val_f_mae: 0.031949
val_f_rmse: 0.048268
2025-06-25 04:24:45.192 INFO: val_f_rmse: 0.048268
##### Step: 18 Learning rate: 0.01 #####
2025-06-25 04:25:30.258 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 2.4002, Val Loss: 2.0811
2025-06-25 04:25:30.258 INFO: Epoch 19, Train Loss: 2.4002, Val Loss: 2.0811
train_e/atom_mae: 0.009600
2025-06-25 04:25:30.259 INFO: train_e/atom_mae: 0.009600
train_e/atom_rmse: 0.012333
2025-06-25 04:25:30.259 INFO: train_e/atom_rmse: 0.012333
train_f_mae: 0.031136
2025-06-25 04:25:30.263 INFO: train_f_mae: 0.031136
train_f_rmse: 0.047076
2025-06-25 04:25:30.263 INFO: train_f_rmse: 0.047076
val_e/atom_mae: 0.006372
2025-06-25 04:25:30.266 INFO: val_e/atom_mae: 0.006372
val_e/atom_rmse: 0.007829
2025-06-25 04:25:30.266 INFO: val_e/atom_rmse: 0.007829
val_f_mae: 0.030412
2025-06-25 04:25:30.267 INFO: val_f_mae: 0.030412
val_f_rmse: 0.044799
2025-06-25 04:25:30.267 INFO: val_f_rmse: 0.044799
##### Step: 19 Learning rate: 0.01 #####
2025-06-25 04:26:15.384 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 2.3136, Val Loss: 2.5876
2025-06-25 04:26:15.384 INFO: Epoch 20, Train Loss: 2.3136, Val Loss: 2.5876
train_e/atom_mae: 0.005767
2025-06-25 04:26:15.385 INFO: train_e/atom_mae: 0.005767
train_e/atom_rmse: 0.007217
2025-06-25 04:26:15.385 INFO: train_e/atom_rmse: 0.007217
train_f_mae: 0.030954
2025-06-25 04:26:15.389 INFO: train_f_mae: 0.030954
train_f_rmse: 0.047440
2025-06-25 04:26:15.389 INFO: train_f_rmse: 0.047440
val_e/atom_mae: 0.003969
2025-06-25 04:26:15.392 INFO: val_e/atom_mae: 0.003969
val_e/atom_rmse: 0.004901
2025-06-25 04:26:15.392 INFO: val_e/atom_rmse: 0.004901
val_f_mae: 0.033204
2025-06-25 04:26:15.393 INFO: val_f_mae: 0.033204
val_f_rmse: 0.050582
2025-06-25 04:26:15.393 INFO: val_f_rmse: 0.050582
##### Step: 20 Learning rate: 0.005 #####
2025-06-25 04:27:00.376 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 2.0309, Val Loss: 2.0651
2025-06-25 04:27:00.376 INFO: Epoch 21, Train Loss: 2.0309, Val Loss: 2.0651
train_e/atom_mae: 0.007175
2025-06-25 04:27:00.377 INFO: train_e/atom_mae: 0.007175
train_e/atom_rmse: 0.008965
2025-06-25 04:27:00.377 INFO: train_e/atom_rmse: 0.008965
train_f_mae: 0.029115
2025-06-25 04:27:00.381 INFO: train_f_mae: 0.029115
train_f_rmse: 0.043973
2025-06-25 04:27:00.381 INFO: train_f_rmse: 0.043973
val_e/atom_mae: 0.012160
2025-06-25 04:27:00.384 INFO: val_e/atom_mae: 0.012160
val_e/atom_rmse: 0.012889
2025-06-25 04:27:00.384 INFO: val_e/atom_rmse: 0.012889
val_f_mae: 0.028531
2025-06-25 04:27:00.385 INFO: val_f_mae: 0.028531
val_f_rmse: 0.043175
2025-06-25 04:27:00.385 INFO: val_f_rmse: 0.043175
##### Step: 21 Learning rate: 0.005 #####
2025-06-25 04:27:45.530 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 1.9007, Val Loss: 1.8632
2025-06-25 04:27:45.530 INFO: Epoch 22, Train Loss: 1.9007, Val Loss: 1.8632
train_e/atom_mae: 0.004173
2025-06-25 04:27:45.531 INFO: train_e/atom_mae: 0.004173
train_e/atom_rmse: 0.005097
2025-06-25 04:27:45.531 INFO: train_e/atom_rmse: 0.005097
train_f_mae: 0.028407
2025-06-25 04:27:45.535 INFO: train_f_mae: 0.028407
train_f_rmse: 0.043235
2025-06-25 04:27:45.535 INFO: train_f_rmse: 0.043235
val_e/atom_mae: 0.005144
2025-06-25 04:27:45.537 INFO: val_e/atom_mae: 0.005144
val_e/atom_rmse: 0.006327
2025-06-25 04:27:45.538 INFO: val_e/atom_rmse: 0.006327
val_f_mae: 0.027872
2025-06-25 04:27:45.538 INFO: val_f_mae: 0.027872
val_f_rmse: 0.042600
2025-06-25 04:27:45.538 INFO: val_f_rmse: 0.042600
##### Step: 22 Learning rate: 0.005 #####
2025-06-25 04:28:30.527 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 1.8563, Val Loss: 1.9119
2025-06-25 04:28:30.527 INFO: Epoch 23, Train Loss: 1.8563, Val Loss: 1.9119
train_e/atom_mae: 0.005610
2025-06-25 04:28:30.528 INFO: train_e/atom_mae: 0.005610
train_e/atom_rmse: 0.006889
2025-06-25 04:28:30.528 INFO: train_e/atom_rmse: 0.006889
train_f_mae: 0.028012
2025-06-25 04:28:30.532 INFO: train_f_mae: 0.028012
train_f_rmse: 0.042413
2025-06-25 04:28:30.532 INFO: train_f_rmse: 0.042413
val_e/atom_mae: 0.009736
2025-06-25 04:28:30.535 INFO: val_e/atom_mae: 0.009736
val_e/atom_rmse: 0.010587
2025-06-25 04:28:30.535 INFO: val_e/atom_rmse: 0.010587
val_f_mae: 0.028340
2025-06-25 04:28:30.535 INFO: val_f_mae: 0.028340
val_f_rmse: 0.042146
2025-06-25 04:28:30.536 INFO: val_f_rmse: 0.042146
##### Step: 23 Learning rate: 0.005 #####
2025-06-25 04:29:15.624 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 2.0211, Val Loss: 2.2707
2025-06-25 04:29:15.624 INFO: Epoch 24, Train Loss: 2.0211, Val Loss: 2.2707
train_e/atom_mae: 0.007748
2025-06-25 04:29:15.625 INFO: train_e/atom_mae: 0.007748
train_e/atom_rmse: 0.009648
2025-06-25 04:29:15.625 INFO: train_e/atom_rmse: 0.009648
train_f_mae: 0.028899
2025-06-25 04:29:15.629 INFO: train_f_mae: 0.028899
train_f_rmse: 0.043686
2025-06-25 04:29:15.629 INFO: train_f_rmse: 0.043686
val_e/atom_mae: 0.012963
2025-06-25 04:29:15.632 INFO: val_e/atom_mae: 0.012963
val_e/atom_rmse: 0.013996
2025-06-25 04:29:15.633 INFO: val_e/atom_rmse: 0.013996
val_f_mae: 0.030338
2025-06-25 04:29:15.633 INFO: val_f_mae: 0.030338
val_f_rmse: 0.045096
2025-06-25 04:29:15.633 INFO: val_f_rmse: 0.045096
##### Step: 24 Learning rate: 0.005 #####
2025-06-25 04:30:00.670 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 1.8723, Val Loss: 1.6597
2025-06-25 04:30:00.670 INFO: Epoch 25, Train Loss: 1.8723, Val Loss: 1.6597
train_e/atom_mae: 0.006243
2025-06-25 04:30:00.671 INFO: train_e/atom_mae: 0.006243
train_e/atom_rmse: 0.007796
2025-06-25 04:30:00.671 INFO: train_e/atom_rmse: 0.007796
train_f_mae: 0.028096
2025-06-25 04:30:00.675 INFO: train_f_mae: 0.028096
train_f_rmse: 0.042411
2025-06-25 04:30:00.675 INFO: train_f_rmse: 0.042411
val_e/atom_mae: 0.005034
2025-06-25 04:30:00.678 INFO: val_e/atom_mae: 0.005034
val_e/atom_rmse: 0.006069
2025-06-25 04:30:00.678 INFO: val_e/atom_rmse: 0.006069
val_f_mae: 0.026921
2025-06-25 04:30:00.679 INFO: val_f_mae: 0.026921
val_f_rmse: 0.040189
2025-06-25 04:30:00.679 INFO: val_f_rmse: 0.040189
##### Step: 25 Learning rate: 0.005 #####
2025-06-25 04:30:45.708 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 1.7883, Val Loss: 1.5634
2025-06-25 04:30:45.708 INFO: Epoch 26, Train Loss: 1.7883, Val Loss: 1.5634
train_e/atom_mae: 0.005781
2025-06-25 04:30:45.709 INFO: train_e/atom_mae: 0.005781
train_e/atom_rmse: 0.007304
2025-06-25 04:30:45.709 INFO: train_e/atom_rmse: 0.007304
train_f_mae: 0.027464
2025-06-25 04:30:45.713 INFO: train_f_mae: 0.027464
train_f_rmse: 0.041518
2025-06-25 04:30:45.713 INFO: train_f_rmse: 0.041518
val_e/atom_mae: 0.003789
2025-06-25 04:30:45.716 INFO: val_e/atom_mae: 0.003789
val_e/atom_rmse: 0.004341
2025-06-25 04:30:45.716 INFO: val_e/atom_rmse: 0.004341
val_f_mae: 0.026424
2025-06-25 04:30:45.717 INFO: val_f_mae: 0.026424
val_f_rmse: 0.039251
2025-06-25 04:30:45.717 INFO: val_f_rmse: 0.039251
##### Step: 26 Learning rate: 0.005 #####
2025-06-25 04:31:30.836 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 1.9164, Val Loss: 2.0864
2025-06-25 04:31:30.837 INFO: Epoch 27, Train Loss: 1.9164, Val Loss: 2.0864
train_e/atom_mae: 0.007219
2025-06-25 04:31:30.837 INFO: train_e/atom_mae: 0.007219
train_e/atom_rmse: 0.009005
2025-06-25 04:31:30.838 INFO: train_e/atom_rmse: 0.009005
train_f_mae: 0.028175
2025-06-25 04:31:30.841 INFO: train_f_mae: 0.028175
train_f_rmse: 0.042641
2025-06-25 04:31:30.841 INFO: train_f_rmse: 0.042641
val_e/atom_mae: 0.008591
2025-06-25 04:31:30.844 INFO: val_e/atom_mae: 0.008591
val_e/atom_rmse: 0.010155
2025-06-25 04:31:30.845 INFO: val_e/atom_rmse: 0.010155
val_f_mae: 0.029142
2025-06-25 04:31:30.845 INFO: val_f_mae: 0.029142
val_f_rmse: 0.044290
2025-06-25 04:31:30.845 INFO: val_f_rmse: 0.044290
##### Step: 27 Learning rate: 0.005 #####
2025-06-25 04:32:15.841 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 1.8370, Val Loss: 1.8308
2025-06-25 04:32:15.841 INFO: Epoch 28, Train Loss: 1.8370, Val Loss: 1.8308
train_e/atom_mae: 0.005181
2025-06-25 04:32:15.842 INFO: train_e/atom_mae: 0.005181
train_e/atom_rmse: 0.006387
2025-06-25 04:32:15.842 INFO: train_e/atom_rmse: 0.006387
train_f_mae: 0.027896
2025-06-25 04:32:15.846 INFO: train_f_mae: 0.027896
train_f_rmse: 0.042280
2025-06-25 04:32:15.846 INFO: train_f_rmse: 0.042280
val_e/atom_mae: 0.003766
2025-06-25 04:32:15.848 INFO: val_e/atom_mae: 0.003766
val_e/atom_rmse: 0.004380
2025-06-25 04:32:15.849 INFO: val_e/atom_rmse: 0.004380
val_f_mae: 0.028179
2025-06-25 04:32:15.849 INFO: val_f_mae: 0.028179
val_f_rmse: 0.042516
2025-06-25 04:32:15.849 INFO: val_f_rmse: 0.042516
##### Step: 28 Learning rate: 0.005 #####
2025-06-25 04:33:00.970 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 1.7746, Val Loss: 1.6643
2025-06-25 04:33:00.970 INFO: Epoch 29, Train Loss: 1.7746, Val Loss: 1.6643
train_e/atom_mae: 0.005065
2025-06-25 04:33:00.971 INFO: train_e/atom_mae: 0.005065
train_e/atom_rmse: 0.006271
2025-06-25 04:33:00.971 INFO: train_e/atom_rmse: 0.006271
train_f_mae: 0.027476
2025-06-25 04:33:00.975 INFO: train_f_mae: 0.027476
train_f_rmse: 0.041558
2025-06-25 04:33:00.975 INFO: train_f_rmse: 0.041558
val_e/atom_mae: 0.003842
2025-06-25 04:33:00.978 INFO: val_e/atom_mae: 0.003842
val_e/atom_rmse: 0.004227
2025-06-25 04:33:00.978 INFO: val_e/atom_rmse: 0.004227
val_f_mae: 0.026558
2025-06-25 04:33:00.979 INFO: val_f_mae: 0.026558
val_f_rmse: 0.040530
2025-06-25 04:33:00.979 INFO: val_f_rmse: 0.040530
##### Step: 29 Learning rate: 0.005 #####
2025-06-25 04:33:45.966 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 1.7531, Val Loss: 1.6749
2025-06-25 04:33:45.966 INFO: Epoch 30, Train Loss: 1.7531, Val Loss: 1.6749
train_e/atom_mae: 0.005386
2025-06-25 04:33:45.967 INFO: train_e/atom_mae: 0.005386
train_e/atom_rmse: 0.006645
2025-06-25 04:33:45.967 INFO: train_e/atom_rmse: 0.006645
train_f_mae: 0.027270
2025-06-25 04:33:45.970 INFO: train_f_mae: 0.027270
train_f_rmse: 0.041227
2025-06-25 04:33:45.971 INFO: train_f_rmse: 0.041227
val_e/atom_mae: 0.005080
2025-06-25 04:33:45.973 INFO: val_e/atom_mae: 0.005080
val_e/atom_rmse: 0.006022
2025-06-25 04:33:45.974 INFO: val_e/atom_rmse: 0.006022
val_f_mae: 0.027328
2025-06-25 04:33:45.974 INFO: val_f_mae: 0.027328
val_f_rmse: 0.040386
2025-06-25 04:33:45.974 INFO: val_f_rmse: 0.040386
##### Step: 30 Learning rate: 0.005 #####
2025-06-25 04:34:31.009 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 1.7366, Val Loss: 1.7849
2025-06-25 04:34:31.009 INFO: Epoch 31, Train Loss: 1.7366, Val Loss: 1.7849
train_e/atom_mae: 0.006297
2025-06-25 04:34:31.010 INFO: train_e/atom_mae: 0.006297
train_e/atom_rmse: 0.007820
2025-06-25 04:34:31.010 INFO: train_e/atom_rmse: 0.007820
train_f_mae: 0.027085
2025-06-25 04:34:31.014 INFO: train_f_mae: 0.027085
train_f_rmse: 0.040775
2025-06-25 04:34:31.014 INFO: train_f_rmse: 0.040775
val_e/atom_mae: 0.010077
2025-06-25 04:34:31.017 INFO: val_e/atom_mae: 0.010077
val_e/atom_rmse: 0.010794
2025-06-25 04:34:31.017 INFO: val_e/atom_rmse: 0.010794
val_f_mae: 0.026923
2025-06-25 04:34:31.018 INFO: val_f_mae: 0.026923
val_f_rmse: 0.040545
2025-06-25 04:34:31.018 INFO: val_f_rmse: 0.040545
##### Step: 31 Learning rate: 0.005 #####
2025-06-25 04:35:16.077 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 1.7958, Val Loss: 1.6392
2025-06-25 04:35:16.077 INFO: Epoch 32, Train Loss: 1.7958, Val Loss: 1.6392
train_e/atom_mae: 0.006598
2025-06-25 04:35:16.078 INFO: train_e/atom_mae: 0.006598
train_e/atom_rmse: 0.008128
2025-06-25 04:35:16.078 INFO: train_e/atom_rmse: 0.008128
train_f_mae: 0.027365
2025-06-25 04:35:16.082 INFO: train_f_mae: 0.027365
train_f_rmse: 0.041423
2025-06-25 04:35:16.082 INFO: train_f_rmse: 0.041423
val_e/atom_mae: 0.003935
2025-06-25 04:35:16.085 INFO: val_e/atom_mae: 0.003935
val_e/atom_rmse: 0.004318
2025-06-25 04:35:16.085 INFO: val_e/atom_rmse: 0.004318
val_f_mae: 0.027030
2025-06-25 04:35:16.085 INFO: val_f_mae: 0.027030
val_f_rmse: 0.040207
2025-06-25 04:35:16.085 INFO: val_f_rmse: 0.040207
##### Step: 32 Learning rate: 0.005 #####
2025-06-25 04:36:01.070 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 1.7843, Val Loss: 1.7507
2025-06-25 04:36:01.071 INFO: Epoch 33, Train Loss: 1.7843, Val Loss: 1.7507
train_e/atom_mae: 0.005638
2025-06-25 04:36:01.072 INFO: train_e/atom_mae: 0.005638
train_e/atom_rmse: 0.007048
2025-06-25 04:36:01.072 INFO: train_e/atom_rmse: 0.007048
train_f_mae: 0.027557
2025-06-25 04:36:01.076 INFO: train_f_mae: 0.027557
train_f_rmse: 0.041523
2025-06-25 04:36:01.076 INFO: train_f_rmse: 0.041523
val_e/atom_mae: 0.007777
2025-06-25 04:36:01.079 INFO: val_e/atom_mae: 0.007777
val_e/atom_rmse: 0.008894
2025-06-25 04:36:01.079 INFO: val_e/atom_rmse: 0.008894
val_f_mae: 0.026773
2025-06-25 04:36:01.079 INFO: val_f_mae: 0.026773
val_f_rmse: 0.040682
2025-06-25 04:36:01.079 INFO: val_f_rmse: 0.040682
##### Step: 33 Learning rate: 0.005 #####
2025-06-25 04:36:46.196 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 1.6859, Val Loss: 1.6299
2025-06-25 04:36:46.196 INFO: Epoch 34, Train Loss: 1.6859, Val Loss: 1.6299
train_e/atom_mae: 0.005036
2025-06-25 04:36:46.197 INFO: train_e/atom_mae: 0.005036
train_e/atom_rmse: 0.006189
2025-06-25 04:36:46.197 INFO: train_e/atom_rmse: 0.006189
train_f_mae: 0.026802
2025-06-25 04:36:46.201 INFO: train_f_mae: 0.026802
train_f_rmse: 0.040492
2025-06-25 04:36:46.201 INFO: train_f_rmse: 0.040492
val_e/atom_mae: 0.003815
2025-06-25 04:36:46.204 INFO: val_e/atom_mae: 0.003815
val_e/atom_rmse: 0.004265
2025-06-25 04:36:46.204 INFO: val_e/atom_rmse: 0.004265
val_f_mae: 0.026680
2025-06-25 04:36:46.205 INFO: val_f_mae: 0.026680
val_f_rmse: 0.040099
2025-06-25 04:36:46.205 INFO: val_f_rmse: 0.040099
##### Step: 34 Learning rate: 0.005 #####
2025-06-25 04:37:31.181 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 1.7069, Val Loss: 1.5220
2025-06-25 04:37:31.182 INFO: Epoch 35, Train Loss: 1.7069, Val Loss: 1.5220
train_e/atom_mae: 0.005836
2025-06-25 04:37:31.183 INFO: train_e/atom_mae: 0.005836
train_e/atom_rmse: 0.007252
2025-06-25 04:37:31.183 INFO: train_e/atom_rmse: 0.007252
train_f_mae: 0.026892
2025-06-25 04:37:31.186 INFO: train_f_mae: 0.026892
train_f_rmse: 0.040537
2025-06-25 04:37:31.187 INFO: train_f_rmse: 0.040537
val_e/atom_mae: 0.003847
2025-06-25 04:37:31.189 INFO: val_e/atom_mae: 0.003847
val_e/atom_rmse: 0.004767
2025-06-25 04:37:31.190 INFO: val_e/atom_rmse: 0.004767
val_f_mae: 0.026089
2025-06-25 04:37:31.190 INFO: val_f_mae: 0.026089
val_f_rmse: 0.038659
2025-06-25 04:37:31.190 INFO: val_f_rmse: 0.038659
##### Step: 35 Learning rate: 0.005 #####
2025-06-25 04:38:16.347 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 1.6825, Val Loss: 1.6224
2025-06-25 04:38:16.347 INFO: Epoch 36, Train Loss: 1.6825, Val Loss: 1.6224
train_e/atom_mae: 0.005239
2025-06-25 04:38:16.348 INFO: train_e/atom_mae: 0.005239
train_e/atom_rmse: 0.006476
2025-06-25 04:38:16.348 INFO: train_e/atom_rmse: 0.006476
train_f_mae: 0.026679
2025-06-25 04:38:16.352 INFO: train_f_mae: 0.026679
train_f_rmse: 0.040395
2025-06-25 04:38:16.352 INFO: train_f_rmse: 0.040395
val_e/atom_mae: 0.003979
2025-06-25 04:38:16.355 INFO: val_e/atom_mae: 0.003979
val_e/atom_rmse: 0.004071
2025-06-25 04:38:16.356 INFO: val_e/atom_rmse: 0.004071
val_f_mae: 0.026943
2025-06-25 04:38:16.356 INFO: val_f_mae: 0.026943
val_f_rmse: 0.040029
2025-06-25 04:38:16.356 INFO: val_f_rmse: 0.040029
##### Step: 36 Learning rate: 0.005 #####
2025-06-25 04:39:01.377 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 1.9744, Val Loss: 2.2018
2025-06-25 04:39:01.377 INFO: Epoch 37, Train Loss: 1.9744, Val Loss: 2.2018
train_e/atom_mae: 0.009091
2025-06-25 04:39:01.378 INFO: train_e/atom_mae: 0.009091
train_e/atom_rmse: 0.011122
2025-06-25 04:39:01.378 INFO: train_e/atom_rmse: 0.011122
train_f_mae: 0.028144
2025-06-25 04:39:01.382 INFO: train_f_mae: 0.028144
train_f_rmse: 0.042717
2025-06-25 04:39:01.382 INFO: train_f_rmse: 0.042717
val_e/atom_mae: 0.012376
2025-06-25 04:39:01.385 INFO: val_e/atom_mae: 0.012376
val_e/atom_rmse: 0.013640
2025-06-25 04:39:01.385 INFO: val_e/atom_rmse: 0.013640
val_f_mae: 0.030133
2025-06-25 04:39:01.386 INFO: val_f_mae: 0.030133
val_f_rmse: 0.044460
2025-06-25 04:39:01.386 INFO: val_f_rmse: 0.044460
##### Step: 37 Learning rate: 0.005 #####
2025-06-25 04:39:46.418 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 1.8109, Val Loss: 1.5389
2025-06-25 04:39:46.419 INFO: Epoch 38, Train Loss: 1.8109, Val Loss: 1.5389
train_e/atom_mae: 0.006732
2025-06-25 04:39:46.420 INFO: train_e/atom_mae: 0.006732
train_e/atom_rmse: 0.008525
2025-06-25 04:39:46.420 INFO: train_e/atom_rmse: 0.008525
train_f_mae: 0.027265
2025-06-25 04:39:46.424 INFO: train_f_mae: 0.027265
train_f_rmse: 0.041508
2025-06-25 04:39:46.424 INFO: train_f_rmse: 0.041508
val_e/atom_mae: 0.004122
2025-06-25 04:39:46.427 INFO: val_e/atom_mae: 0.004122
val_e/atom_rmse: 0.005198
2025-06-25 04:39:46.427 INFO: val_e/atom_rmse: 0.005198
val_f_mae: 0.025844
2025-06-25 04:39:46.427 INFO: val_f_mae: 0.025844
val_f_rmse: 0.038810
2025-06-25 04:39:46.428 INFO: val_f_rmse: 0.038810
##### Step: 38 Learning rate: 0.005 #####
2025-06-25 04:40:31.507 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 1.6677, Val Loss: 1.8676
2025-06-25 04:40:31.507 INFO: Epoch 39, Train Loss: 1.6677, Val Loss: 1.8676
train_e/atom_mae: 0.005465
2025-06-25 04:40:31.508 INFO: train_e/atom_mae: 0.005465
train_e/atom_rmse: 0.006739
2025-06-25 04:40:31.508 INFO: train_e/atom_rmse: 0.006739
train_f_mae: 0.026645
2025-06-25 04:40:31.512 INFO: train_f_mae: 0.026645
train_f_rmse: 0.040160
2025-06-25 04:40:31.512 INFO: train_f_rmse: 0.040160
val_e/atom_mae: 0.004039
2025-06-25 04:40:31.515 INFO: val_e/atom_mae: 0.004039
val_e/atom_rmse: 0.005218
2025-06-25 04:40:31.515 INFO: val_e/atom_rmse: 0.005218
val_f_mae: 0.028341
2025-06-25 04:40:31.515 INFO: val_f_mae: 0.028341
val_f_rmse: 0.042832
2025-06-25 04:40:31.516 INFO: val_f_rmse: 0.042832
##### Step: 39 Learning rate: 0.005 #####
2025-06-25 04:41:16.487 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 1.6729, Val Loss: 1.6371
2025-06-25 04:41:16.487 INFO: Epoch 40, Train Loss: 1.6729, Val Loss: 1.6371
train_e/atom_mae: 0.005083
2025-06-25 04:41:16.488 INFO: train_e/atom_mae: 0.005083
train_e/atom_rmse: 0.006307
2025-06-25 04:41:16.488 INFO: train_e/atom_rmse: 0.006307
train_f_mae: 0.026736
2025-06-25 04:41:16.492 INFO: train_f_mae: 0.026736
train_f_rmse: 0.040308
2025-06-25 04:41:16.492 INFO: train_f_rmse: 0.040308
val_e/atom_mae: 0.003928
2025-06-25 04:41:16.495 INFO: val_e/atom_mae: 0.003928
val_e/atom_rmse: 0.004243
2025-06-25 04:41:16.495 INFO: val_e/atom_rmse: 0.004243
val_f_mae: 0.026199
2025-06-25 04:41:16.496 INFO: val_f_mae: 0.026199
val_f_rmse: 0.040192
2025-06-25 04:41:16.496 INFO: val_f_rmse: 0.040192
2025-06-25 04:41:16.501 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-25 04:42:01.698 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 9.9740, Val Loss: 2.3234
2025-06-25 04:42:01.698 INFO: Epoch 1, Train Loss: 9.9740, Val Loss: 2.3234
train_e/atom_mae: 0.018108
2025-06-25 04:42:01.699 INFO: train_e/atom_mae: 0.018108
train_e/atom_rmse: 0.035982
2025-06-25 04:42:01.699 INFO: train_e/atom_rmse: 0.035982
train_f_mae: 0.044791
2025-06-25 04:42:01.703 INFO: train_f_mae: 0.044791
train_f_rmse: 0.091692
2025-06-25 04:42:01.703 INFO: train_f_rmse: 0.091692
val_e/atom_mae: 0.006605
2025-06-25 04:42:01.706 INFO: val_e/atom_mae: 0.006605
val_e/atom_rmse: 0.007706
2025-06-25 04:42:01.706 INFO: val_e/atom_rmse: 0.007706
val_f_mae: 0.031323
2025-06-25 04:42:01.707 INFO: val_f_mae: 0.031323
val_f_rmse: 0.047450
2025-06-25 04:42:01.707 INFO: val_f_rmse: 0.047450
##### Step: 1 Learning rate: 0.004 #####
2025-06-25 04:42:46.758 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 3.7647, Val Loss: 2.9846
2025-06-25 04:42:46.758 INFO: Epoch 2, Train Loss: 3.7647, Val Loss: 2.9846
train_e/atom_mae: 0.014553
2025-06-25 04:42:46.759 INFO: train_e/atom_mae: 0.014553
train_e/atom_rmse: 0.017751
2025-06-25 04:42:46.759 INFO: train_e/atom_rmse: 0.017751
train_f_mae: 0.035420
2025-06-25 04:42:46.763 INFO: train_f_mae: 0.035420
train_f_rmse: 0.058167
2025-06-25 04:42:46.763 INFO: train_f_rmse: 0.058167
val_e/atom_mae: 0.009267
2025-06-25 04:42:46.766 INFO: val_e/atom_mae: 0.009267
val_e/atom_rmse: 0.009835
2025-06-25 04:42:46.766 INFO: val_e/atom_rmse: 0.009835
val_f_mae: 0.032842
2025-06-25 04:42:46.767 INFO: val_f_mae: 0.032842
val_f_rmse: 0.053549
2025-06-25 04:42:46.767 INFO: val_f_rmse: 0.053549
##### Step: 2 Learning rate: 0.006 #####
2025-06-25 04:43:31.945 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 3.3134, Val Loss: 2.7275
2025-06-25 04:43:31.946 INFO: Epoch 3, Train Loss: 3.3134, Val Loss: 2.7275
train_e/atom_mae: 0.008534
2025-06-25 04:43:31.947 INFO: train_e/atom_mae: 0.008534
train_e/atom_rmse: 0.010462
2025-06-25 04:43:31.947 INFO: train_e/atom_rmse: 0.010462
train_f_mae: 0.034893
2025-06-25 04:43:31.951 INFO: train_f_mae: 0.034893
train_f_rmse: 0.056400
2025-06-25 04:43:31.951 INFO: train_f_rmse: 0.056400
val_e/atom_mae: 0.008521
2025-06-25 04:43:31.954 INFO: val_e/atom_mae: 0.008521
val_e/atom_rmse: 0.009678
2025-06-25 04:43:31.954 INFO: val_e/atom_rmse: 0.009678
val_f_mae: 0.033542
2025-06-25 04:43:31.954 INFO: val_f_mae: 0.033542
val_f_rmse: 0.051129
2025-06-25 04:43:31.955 INFO: val_f_rmse: 0.051129
##### Step: 3 Learning rate: 0.008 #####
2025-06-25 04:44:17.041 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 2.3161, Val Loss: 1.5175
2025-06-25 04:44:17.041 INFO: Epoch 4, Train Loss: 2.3161, Val Loss: 1.5175
train_e/atom_mae: 0.006293
2025-06-25 04:44:17.042 INFO: train_e/atom_mae: 0.006293
train_e/atom_rmse: 0.008133
2025-06-25 04:44:17.042 INFO: train_e/atom_rmse: 0.008133
train_f_mae: 0.029813
2025-06-25 04:44:17.046 INFO: train_f_mae: 0.029813
train_f_rmse: 0.047288
2025-06-25 04:44:17.046 INFO: train_f_rmse: 0.047288
val_e/atom_mae: 0.003251
2025-06-25 04:44:17.049 INFO: val_e/atom_mae: 0.003251
val_e/atom_rmse: 0.003846
2025-06-25 04:44:17.049 INFO: val_e/atom_rmse: 0.003846
val_f_mae: 0.025801
2025-06-25 04:44:17.049 INFO: val_f_mae: 0.025801
val_f_rmse: 0.038724
2025-06-25 04:44:17.050 INFO: val_f_rmse: 0.038724
##### Step: 4 Learning rate: 0.01 #####
2025-06-25 04:45:02.166 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 2.0074, Val Loss: 2.9388
2025-06-25 04:45:02.167 INFO: Epoch 5, Train Loss: 2.0074, Val Loss: 2.9388
train_e/atom_mae: 0.006743
2025-06-25 04:45:02.168 INFO: train_e/atom_mae: 0.006743
train_e/atom_rmse: 0.008622
2025-06-25 04:45:02.168 INFO: train_e/atom_rmse: 0.008622
train_f_mae: 0.028045
2025-06-25 04:45:02.171 INFO: train_f_mae: 0.028045
train_f_rmse: 0.043789
2025-06-25 04:45:02.172 INFO: train_f_rmse: 0.043789
val_e/atom_mae: 0.011578
2025-06-25 04:45:02.174 INFO: val_e/atom_mae: 0.011578
val_e/atom_rmse: 0.013592
2025-06-25 04:45:02.175 INFO: val_e/atom_rmse: 0.013592
val_f_mae: 0.033004
2025-06-25 04:45:02.175 INFO: val_f_mae: 0.033004
val_f_rmse: 0.052108
2025-06-25 04:45:02.175 INFO: val_f_rmse: 0.052108
##### Step: 5 Learning rate: 0.01 #####
2025-06-25 04:45:47.271 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 1.8159, Val Loss: 2.1282
2025-06-25 04:45:47.271 INFO: Epoch 6, Train Loss: 1.8159, Val Loss: 2.1282
train_e/atom_mae: 0.006377
2025-06-25 04:45:47.272 INFO: train_e/atom_mae: 0.006377
train_e/atom_rmse: 0.007950
2025-06-25 04:45:47.272 INFO: train_e/atom_rmse: 0.007950
train_f_mae: 0.026799
2025-06-25 04:45:47.275 INFO: train_f_mae: 0.026799
train_f_rmse: 0.041706
2025-06-25 04:45:47.276 INFO: train_f_rmse: 0.041706
val_e/atom_mae: 0.012472
2025-06-25 04:45:47.278 INFO: val_e/atom_mae: 0.012472
val_e/atom_rmse: 0.013620
2025-06-25 04:45:47.279 INFO: val_e/atom_rmse: 0.013620
val_f_mae: 0.028444
2025-06-25 04:45:47.279 INFO: val_f_mae: 0.028444
val_f_rmse: 0.043632
2025-06-25 04:45:47.279 INFO: val_f_rmse: 0.043632
##### Step: 6 Learning rate: 0.01 #####
2025-06-25 04:46:32.395 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 1.8951, Val Loss: 3.2673
2025-06-25 04:46:32.395 INFO: Epoch 7, Train Loss: 1.8951, Val Loss: 3.2673
train_e/atom_mae: 0.006472
2025-06-25 04:46:32.396 INFO: train_e/atom_mae: 0.006472
train_e/atom_rmse: 0.008232
2025-06-25 04:46:32.396 INFO: train_e/atom_rmse: 0.008232
train_f_mae: 0.026942
2025-06-25 04:46:32.400 INFO: train_f_mae: 0.026942
train_f_rmse: 0.042580
2025-06-25 04:46:32.400 INFO: train_f_rmse: 0.042580
val_e/atom_mae: 0.016188
2025-06-25 04:46:32.403 INFO: val_e/atom_mae: 0.016188
val_e/atom_rmse: 0.016933
2025-06-25 04:46:32.403 INFO: val_e/atom_rmse: 0.016933
val_f_mae: 0.034316
2025-06-25 04:46:32.403 INFO: val_f_mae: 0.034316
val_f_rmse: 0.054041
2025-06-25 04:46:32.404 INFO: val_f_rmse: 0.054041
##### Step: 7 Learning rate: 0.01 #####
2025-06-25 04:47:17.437 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 1.4973, Val Loss: 1.3559
2025-06-25 04:47:17.437 INFO: Epoch 8, Train Loss: 1.4973, Val Loss: 1.3559
train_e/atom_mae: 0.006782
2025-06-25 04:47:17.438 INFO: train_e/atom_mae: 0.006782
train_e/atom_rmse: 0.008391
2025-06-25 04:47:17.438 INFO: train_e/atom_rmse: 0.008391
train_f_mae: 0.024333
2025-06-25 04:47:17.442 INFO: train_f_mae: 0.024333
train_f_rmse: 0.037578
2025-06-25 04:47:17.442 INFO: train_f_rmse: 0.037578
val_e/atom_mae: 0.004222
2025-06-25 04:47:17.444 INFO: val_e/atom_mae: 0.004222
val_e/atom_rmse: 0.005249
2025-06-25 04:47:17.445 INFO: val_e/atom_rmse: 0.005249
val_f_mae: 0.023621
2025-06-25 04:47:17.445 INFO: val_f_mae: 0.023621
val_f_rmse: 0.036368
2025-06-25 04:47:17.445 INFO: val_f_rmse: 0.036368
##### Step: 8 Learning rate: 0.01 #####
2025-06-25 04:48:02.439 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 1.5622, Val Loss: 1.2017
2025-06-25 04:48:02.439 INFO: Epoch 9, Train Loss: 1.5622, Val Loss: 1.2017
train_e/atom_mae: 0.006635
2025-06-25 04:48:02.440 INFO: train_e/atom_mae: 0.006635
train_e/atom_rmse: 0.008400
2025-06-25 04:48:02.440 INFO: train_e/atom_rmse: 0.008400
train_f_mae: 0.025187
2025-06-25 04:48:02.444 INFO: train_f_mae: 0.025187
train_f_rmse: 0.038430
2025-06-25 04:48:02.444 INFO: train_f_rmse: 0.038430
val_e/atom_mae: 0.005009
2025-06-25 04:48:02.447 INFO: val_e/atom_mae: 0.005009
val_e/atom_rmse: 0.005938
2025-06-25 04:48:02.447 INFO: val_e/atom_rmse: 0.005938
val_f_mae: 0.021772
2025-06-25 04:48:02.448 INFO: val_f_mae: 0.021772
val_f_rmse: 0.034045
2025-06-25 04:48:02.448 INFO: val_f_rmse: 0.034045
##### Step: 9 Learning rate: 0.01 #####
2025-06-25 04:48:47.540 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 1.3302, Val Loss: 1.0764
2025-06-25 04:48:47.540 INFO: Epoch 10, Train Loss: 1.3302, Val Loss: 1.0764
train_e/atom_mae: 0.007436
2025-06-25 04:48:47.541 INFO: train_e/atom_mae: 0.007436
train_e/atom_rmse: 0.009228
2025-06-25 04:48:47.541 INFO: train_e/atom_rmse: 0.009228
train_f_mae: 0.022579
2025-06-25 04:48:47.545 INFO: train_f_mae: 0.022579
train_f_rmse: 0.035031
2025-06-25 04:48:47.545 INFO: train_f_rmse: 0.035031
val_e/atom_mae: 0.004350
2025-06-25 04:48:47.548 INFO: val_e/atom_mae: 0.004350
val_e/atom_rmse: 0.005198
2025-06-25 04:48:47.548 INFO: val_e/atom_rmse: 0.005198
val_f_mae: 0.021392
2025-06-25 04:48:47.549 INFO: val_f_mae: 0.021392
val_f_rmse: 0.032307
2025-06-25 04:48:47.549 INFO: val_f_rmse: 0.032307
##### Step: 10 Learning rate: 0.01 #####
2025-06-25 04:49:32.560 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 1.3350, Val Loss: 1.5291
2025-06-25 04:49:32.560 INFO: Epoch 11, Train Loss: 1.3350, Val Loss: 1.5291
train_e/atom_mae: 0.005935
2025-06-25 04:49:32.561 INFO: train_e/atom_mae: 0.005935
train_e/atom_rmse: 0.007291
2025-06-25 04:49:32.561 INFO: train_e/atom_rmse: 0.007291
train_f_mae: 0.022724
2025-06-25 04:49:32.565 INFO: train_f_mae: 0.022724
train_f_rmse: 0.035646
2025-06-25 04:49:32.565 INFO: train_f_rmse: 0.035646
val_e/atom_mae: 0.011752
2025-06-25 04:49:32.568 INFO: val_e/atom_mae: 0.011752
val_e/atom_rmse: 0.012886
2025-06-25 04:49:32.568 INFO: val_e/atom_rmse: 0.012886
val_f_mae: 0.025504
2025-06-25 04:49:32.568 INFO: val_f_mae: 0.025504
val_f_rmse: 0.036444
2025-06-25 04:49:32.569 INFO: val_f_rmse: 0.036444
##### Step: 11 Learning rate: 0.01 #####
2025-06-25 04:50:17.720 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 1.1122, Val Loss: 1.2058
2025-06-25 04:50:17.721 INFO: Epoch 12, Train Loss: 1.1122, Val Loss: 1.2058
train_e/atom_mae: 0.006837
2025-06-25 04:50:17.721 INFO: train_e/atom_mae: 0.006837
train_e/atom_rmse: 0.008417
2025-06-25 04:50:17.722 INFO: train_e/atom_rmse: 0.008417
train_f_mae: 0.020941
2025-06-25 04:50:17.725 INFO: train_f_mae: 0.020941
train_f_rmse: 0.032039
2025-06-25 04:50:17.725 INFO: train_f_rmse: 0.032039
val_e/atom_mae: 0.004739
2025-06-25 04:50:17.728 INFO: val_e/atom_mae: 0.004739
val_e/atom_rmse: 0.005412
2025-06-25 04:50:17.729 INFO: val_e/atom_rmse: 0.005412
val_f_mae: 0.021833
2025-06-25 04:50:17.729 INFO: val_f_mae: 0.021833
val_f_rmse: 0.034210
2025-06-25 04:50:17.729 INFO: val_f_rmse: 0.034210
##### Step: 12 Learning rate: 0.01 #####
2025-06-25 04:51:02.940 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 1.2879, Val Loss: 1.3517
2025-06-25 04:51:02.940 INFO: Epoch 13, Train Loss: 1.2879, Val Loss: 1.3517
train_e/atom_mae: 0.006136
2025-06-25 04:51:02.941 INFO: train_e/atom_mae: 0.006136
train_e/atom_rmse: 0.007628
2025-06-25 04:51:02.941 INFO: train_e/atom_rmse: 0.007628
train_f_mae: 0.022739
2025-06-25 04:51:02.945 INFO: train_f_mae: 0.022739
train_f_rmse: 0.034893
2025-06-25 04:51:02.945 INFO: train_f_rmse: 0.034893
val_e/atom_mae: 0.005127
2025-06-25 04:51:02.948 INFO: val_e/atom_mae: 0.005127
val_e/atom_rmse: 0.006690
2025-06-25 04:51:02.948 INFO: val_e/atom_rmse: 0.006690
val_f_mae: 0.023184
2025-06-25 04:51:02.949 INFO: val_f_mae: 0.023184
val_f_rmse: 0.036021
2025-06-25 04:51:02.949 INFO: val_f_rmse: 0.036021
##### Step: 13 Learning rate: 0.01 #####
2025-06-25 04:51:48.077 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 1.1220, Val Loss: 0.9126
2025-06-25 04:51:48.078 INFO: Epoch 14, Train Loss: 1.1220, Val Loss: 0.9126
train_e/atom_mae: 0.005978
2025-06-25 04:51:48.079 INFO: train_e/atom_mae: 0.005978
train_e/atom_rmse: 0.007429
2025-06-25 04:51:48.079 INFO: train_e/atom_rmse: 0.007429
train_f_mae: 0.020660
2025-06-25 04:51:48.082 INFO: train_f_mae: 0.020660
train_f_rmse: 0.032485
2025-06-25 04:51:48.082 INFO: train_f_rmse: 0.032485
val_e/atom_mae: 0.006183
2025-06-25 04:51:48.085 INFO: val_e/atom_mae: 0.006183
val_e/atom_rmse: 0.007655
2025-06-25 04:51:48.086 INFO: val_e/atom_rmse: 0.007655
val_f_mae: 0.019465
2025-06-25 04:51:48.086 INFO: val_f_mae: 0.019465
val_f_rmse: 0.029012
2025-06-25 04:51:48.086 INFO: val_f_rmse: 0.029012
##### Step: 14 Learning rate: 0.01 #####
2025-06-25 04:52:33.097 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 1.0407, Val Loss: 0.9822
2025-06-25 04:52:33.097 INFO: Epoch 15, Train Loss: 1.0407, Val Loss: 0.9822
train_e/atom_mae: 0.006177
2025-06-25 04:52:33.098 INFO: train_e/atom_mae: 0.006177
train_e/atom_rmse: 0.007658
2025-06-25 04:52:33.098 INFO: train_e/atom_rmse: 0.007658
train_f_mae: 0.020117
2025-06-25 04:52:33.102 INFO: train_f_mae: 0.020117
train_f_rmse: 0.031141
2025-06-25 04:52:33.102 INFO: train_f_rmse: 0.031141
val_e/atom_mae: 0.007357
2025-06-25 04:52:33.104 INFO: val_e/atom_mae: 0.007357
val_e/atom_rmse: 0.009041
2025-06-25 04:52:33.105 INFO: val_e/atom_rmse: 0.009041
val_f_mae: 0.019858
2025-06-25 04:52:33.105 INFO: val_f_mae: 0.019858
val_f_rmse: 0.029720
2025-06-25 04:52:33.105 INFO: val_f_rmse: 0.029720
##### Step: 15 Learning rate: 0.01 #####
2025-06-25 04:53:18.300 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 1.0342, Val Loss: 1.1392
2025-06-25 04:53:18.300 INFO: Epoch 16, Train Loss: 1.0342, Val Loss: 1.1392
train_e/atom_mae: 0.006230
2025-06-25 04:53:18.301 INFO: train_e/atom_mae: 0.006230
train_e/atom_rmse: 0.007798
2025-06-25 04:53:18.301 INFO: train_e/atom_rmse: 0.007798
train_f_mae: 0.020182
2025-06-25 04:53:18.305 INFO: train_f_mae: 0.020182
train_f_rmse: 0.030993
2025-06-25 04:53:18.305 INFO: train_f_rmse: 0.030993
val_e/atom_mae: 0.014043
2025-06-25 04:53:18.308 INFO: val_e/atom_mae: 0.014043
val_e/atom_rmse: 0.014827
2025-06-25 04:53:18.308 INFO: val_e/atom_rmse: 0.014827
val_f_mae: 0.019797
2025-06-25 04:53:18.309 INFO: val_f_mae: 0.019797
val_f_rmse: 0.029549
2025-06-25 04:53:18.309 INFO: val_f_rmse: 0.029549
##### Step: 16 Learning rate: 0.01 #####
2025-06-25 04:54:03.336 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 1.0085, Val Loss: 0.9176
2025-06-25 04:54:03.336 INFO: Epoch 17, Train Loss: 1.0085, Val Loss: 0.9176
train_e/atom_mae: 0.006745
2025-06-25 04:54:03.337 INFO: train_e/atom_mae: 0.006745
train_e/atom_rmse: 0.008383
2025-06-25 04:54:03.337 INFO: train_e/atom_rmse: 0.008383
train_f_mae: 0.019607
2025-06-25 04:54:03.341 INFO: train_f_mae: 0.019607
train_f_rmse: 0.030388
2025-06-25 04:54:03.341 INFO: train_f_rmse: 0.030388
val_e/atom_mae: 0.006322
2025-06-25 04:54:03.344 INFO: val_e/atom_mae: 0.006322
val_e/atom_rmse: 0.007597
2025-06-25 04:54:03.344 INFO: val_e/atom_rmse: 0.007597
val_f_mae: 0.018624
2025-06-25 04:54:03.345 INFO: val_f_mae: 0.018624
val_f_rmse: 0.029117
2025-06-25 04:54:03.345 INFO: val_f_rmse: 0.029117
##### Step: 17 Learning rate: 0.01 #####
2025-06-25 04:54:48.481 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 0.9575, Val Loss: 1.1715
2025-06-25 04:54:48.482 INFO: Epoch 18, Train Loss: 0.9575, Val Loss: 1.1715
train_e/atom_mae: 0.006557
2025-06-25 04:54:48.483 INFO: train_e/atom_mae: 0.006557
train_e/atom_rmse: 0.008173
2025-06-25 04:54:48.483 INFO: train_e/atom_rmse: 0.008173
train_f_mae: 0.019434
2025-06-25 04:54:48.487 INFO: train_f_mae: 0.019434
train_f_rmse: 0.029609
2025-06-25 04:54:48.487 INFO: train_f_rmse: 0.029609
val_e/atom_mae: 0.009556
2025-06-25 04:54:48.489 INFO: val_e/atom_mae: 0.009556
val_e/atom_rmse: 0.010482
2025-06-25 04:54:48.490 INFO: val_e/atom_rmse: 0.010482
val_f_mae: 0.020724
2025-06-25 04:54:48.490 INFO: val_f_mae: 0.020724
val_f_rmse: 0.032226
2025-06-25 04:54:48.491 INFO: val_f_rmse: 0.032226
##### Step: 18 Learning rate: 0.01 #####
2025-06-25 04:55:33.495 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 0.9995, Val Loss: 1.3366
2025-06-25 04:55:33.495 INFO: Epoch 19, Train Loss: 0.9995, Val Loss: 1.3366
train_e/atom_mae: 0.006097
2025-06-25 04:55:33.496 INFO: train_e/atom_mae: 0.006097
train_e/atom_rmse: 0.007578
2025-06-25 04:55:33.496 INFO: train_e/atom_rmse: 0.007578
train_f_mae: 0.020237
2025-06-25 04:55:33.500 INFO: train_f_mae: 0.020237
train_f_rmse: 0.030496
2025-06-25 04:55:33.500 INFO: train_f_rmse: 0.030496
val_e/atom_mae: 0.005482
2025-06-25 04:55:33.503 INFO: val_e/atom_mae: 0.005482
val_e/atom_rmse: 0.007014
2025-06-25 04:55:33.503 INFO: val_e/atom_rmse: 0.007014
val_f_mae: 0.021953
2025-06-25 04:55:33.504 INFO: val_f_mae: 0.021953
val_f_rmse: 0.035736
2025-06-25 04:55:33.504 INFO: val_f_rmse: 0.035736
##### Step: 19 Learning rate: 0.01 #####
2025-06-25 04:56:18.655 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 0.9723, Val Loss: 0.6923
2025-06-25 04:56:18.655 INFO: Epoch 20, Train Loss: 0.9723, Val Loss: 0.6923
train_e/atom_mae: 0.005742
2025-06-25 04:56:18.656 INFO: train_e/atom_mae: 0.005742
train_e/atom_rmse: 0.007137
2025-06-25 04:56:18.656 INFO: train_e/atom_rmse: 0.007137
train_f_mae: 0.019830
2025-06-25 04:56:18.660 INFO: train_f_mae: 0.019830
train_f_rmse: 0.030177
2025-06-25 04:56:18.660 INFO: train_f_rmse: 0.030177
val_e/atom_mae: 0.005448
2025-06-25 04:56:18.663 INFO: val_e/atom_mae: 0.005448
val_e/atom_rmse: 0.006461
2025-06-25 04:56:18.663 INFO: val_e/atom_rmse: 0.006461
val_f_mae: 0.017307
2025-06-25 04:56:18.664 INFO: val_f_mae: 0.017307
val_f_rmse: 0.025334
2025-06-25 04:56:18.664 INFO: val_f_rmse: 0.025334
##### Step: 20 Learning rate: 0.005 #####
2025-06-25 04:57:03.629 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 0.6615, Val Loss: 0.5398
2025-06-25 04:57:03.630 INFO: Epoch 21, Train Loss: 0.6615, Val Loss: 0.5398
train_e/atom_mae: 0.005359
2025-06-25 04:57:03.631 INFO: train_e/atom_mae: 0.005359
train_e/atom_rmse: 0.006628
2025-06-25 04:57:03.631 INFO: train_e/atom_rmse: 0.006628
train_f_mae: 0.016035
2025-06-25 04:57:03.634 INFO: train_f_mae: 0.016035
train_f_rmse: 0.024664
2025-06-25 04:57:03.634 INFO: train_f_rmse: 0.024664
val_e/atom_mae: 0.004271
2025-06-25 04:57:03.637 INFO: val_e/atom_mae: 0.004271
val_e/atom_rmse: 0.004860
2025-06-25 04:57:03.638 INFO: val_e/atom_rmse: 0.004860
val_f_mae: 0.015032
2025-06-25 04:57:03.638 INFO: val_f_mae: 0.015032
val_f_rmse: 0.022610
2025-06-25 04:57:03.638 INFO: val_f_rmse: 0.022610
##### Step: 21 Learning rate: 0.005 #####
2025-06-25 04:57:48.748 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.6156, Val Loss: 0.5347
2025-06-25 04:57:48.748 INFO: Epoch 22, Train Loss: 0.6156, Val Loss: 0.5347
train_e/atom_mae: 0.005114
2025-06-25 04:57:48.749 INFO: train_e/atom_mae: 0.005114
train_e/atom_rmse: 0.006308
2025-06-25 04:57:48.749 INFO: train_e/atom_rmse: 0.006308
train_f_mae: 0.015954
2025-06-25 04:57:48.753 INFO: train_f_mae: 0.015954
train_f_rmse: 0.023821
2025-06-25 04:57:48.753 INFO: train_f_rmse: 0.023821
val_e/atom_mae: 0.004471
2025-06-25 04:57:48.756 INFO: val_e/atom_mae: 0.004471
val_e/atom_rmse: 0.004858
2025-06-25 04:57:48.756 INFO: val_e/atom_rmse: 0.004858
val_f_mae: 0.015145
2025-06-25 04:57:48.757 INFO: val_f_mae: 0.015145
val_f_rmse: 0.022497
2025-06-25 04:57:48.757 INFO: val_f_rmse: 0.022497
##### Step: 22 Learning rate: 0.005 #####
2025-06-25 04:58:33.786 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.6861, Val Loss: 0.6711
2025-06-25 04:58:33.786 INFO: Epoch 23, Train Loss: 0.6861, Val Loss: 0.6711
train_e/atom_mae: 0.005135
2025-06-25 04:58:33.787 INFO: train_e/atom_mae: 0.005135
train_e/atom_rmse: 0.006317
2025-06-25 04:58:33.787 INFO: train_e/atom_rmse: 0.006317
train_f_mae: 0.016810
2025-06-25 04:58:33.791 INFO: train_f_mae: 0.016810
train_f_rmse: 0.025255
2025-06-25 04:58:33.791 INFO: train_f_rmse: 0.025255
val_e/atom_mae: 0.004263
2025-06-25 04:58:33.794 INFO: val_e/atom_mae: 0.004263
val_e/atom_rmse: 0.005776
2025-06-25 04:58:33.794 INFO: val_e/atom_rmse: 0.005776
val_f_mae: 0.017734
2025-06-25 04:58:33.794 INFO: val_f_mae: 0.017734
val_f_rmse: 0.025115
2025-06-25 04:58:33.795 INFO: val_f_rmse: 0.025115
##### Step: 23 Learning rate: 0.005 #####
2025-06-25 04:59:18.892 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.5742, Val Loss: 0.6595
2025-06-25 04:59:18.893 INFO: Epoch 24, Train Loss: 0.5742, Val Loss: 0.6595
train_e/atom_mae: 0.004639
2025-06-25 04:59:18.894 INFO: train_e/atom_mae: 0.004639
train_e/atom_rmse: 0.005606
2025-06-25 04:59:18.894 INFO: train_e/atom_rmse: 0.005606
train_f_mae: 0.015563
2025-06-25 04:59:18.898 INFO: train_f_mae: 0.015563
train_f_rmse: 0.023155
2025-06-25 04:59:18.898 INFO: train_f_rmse: 0.023155
val_e/atom_mae: 0.004678
2025-06-25 04:59:18.900 INFO: val_e/atom_mae: 0.004678
val_e/atom_rmse: 0.005699
2025-06-25 04:59:18.901 INFO: val_e/atom_rmse: 0.005699
val_f_mae: 0.016484
2025-06-25 04:59:18.901 INFO: val_f_mae: 0.016484
val_f_rmse: 0.024904
2025-06-25 04:59:18.901 INFO: val_f_rmse: 0.024904
##### Step: 24 Learning rate: 0.005 #####
2025-06-25 05:00:04.022 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.5551, Val Loss: 0.6836
2025-06-25 05:00:04.022 INFO: Epoch 25, Train Loss: 0.5551, Val Loss: 0.6836
train_e/atom_mae: 0.004680
2025-06-25 05:00:04.023 INFO: train_e/atom_mae: 0.004680
train_e/atom_rmse: 0.005797
2025-06-25 05:00:04.023 INFO: train_e/atom_rmse: 0.005797
train_f_mae: 0.015188
2025-06-25 05:00:04.027 INFO: train_f_mae: 0.015188
train_f_rmse: 0.022681
2025-06-25 05:00:04.027 INFO: train_f_rmse: 0.022681
val_e/atom_mae: 0.010866
2025-06-25 05:00:04.030 INFO: val_e/atom_mae: 0.010866
val_e/atom_rmse: 0.011538
2025-06-25 05:00:04.030 INFO: val_e/atom_rmse: 0.011538
val_f_mae: 0.015778
2025-06-25 05:00:04.030 INFO: val_f_mae: 0.015778
val_f_rmse: 0.022859
2025-06-25 05:00:04.031 INFO: val_f_rmse: 0.022859
##### Step: 25 Learning rate: 0.005 #####
2025-06-25 05:00:49.116 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.5404, Val Loss: 0.5826
2025-06-25 05:00:49.116 INFO: Epoch 26, Train Loss: 0.5404, Val Loss: 0.5826
train_e/atom_mae: 0.004734
2025-06-25 05:00:49.117 INFO: train_e/atom_mae: 0.004734
train_e/atom_rmse: 0.005805
2025-06-25 05:00:49.117 INFO: train_e/atom_rmse: 0.005805
train_f_mae: 0.015246
2025-06-25 05:00:49.121 INFO: train_f_mae: 0.015246
train_f_rmse: 0.022352
2025-06-25 05:00:49.121 INFO: train_f_rmse: 0.022352
val_e/atom_mae: 0.009081
2025-06-25 05:00:49.124 INFO: val_e/atom_mae: 0.009081
val_e/atom_rmse: 0.010175
2025-06-25 05:00:49.124 INFO: val_e/atom_rmse: 0.010175
val_f_mae: 0.014910
2025-06-25 05:00:49.125 INFO: val_f_mae: 0.014910
val_f_rmse: 0.021384
2025-06-25 05:00:49.125 INFO: val_f_rmse: 0.021384
##### Step: 26 Learning rate: 0.005 #####
2025-06-25 05:01:34.235 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.5549, Val Loss: 0.5640
2025-06-25 05:01:34.235 INFO: Epoch 27, Train Loss: 0.5549, Val Loss: 0.5640
train_e/atom_mae: 0.005052
2025-06-25 05:01:34.236 INFO: train_e/atom_mae: 0.005052
train_e/atom_rmse: 0.006205
2025-06-25 05:01:34.236 INFO: train_e/atom_rmse: 0.006205
train_f_mae: 0.015150
2025-06-25 05:01:34.240 INFO: train_f_mae: 0.015150
train_f_rmse: 0.022545
2025-06-25 05:01:34.240 INFO: train_f_rmse: 0.022545
val_e/atom_mae: 0.003919
2025-06-25 05:01:34.243 INFO: val_e/atom_mae: 0.003919
val_e/atom_rmse: 0.005036
2025-06-25 05:01:34.243 INFO: val_e/atom_rmse: 0.005036
val_f_mae: 0.015675
2025-06-25 05:01:34.244 INFO: val_f_mae: 0.015675
val_f_rmse: 0.023094
2025-06-25 05:01:34.244 INFO: val_f_rmse: 0.023094
##### Step: 27 Learning rate: 0.005 #####
2025-06-25 05:02:19.311 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.5216, Val Loss: 0.5063
2025-06-25 05:02:19.311 INFO: Epoch 28, Train Loss: 0.5216, Val Loss: 0.5063
train_e/atom_mae: 0.004764
2025-06-25 05:02:19.312 INFO: train_e/atom_mae: 0.004764
train_e/atom_rmse: 0.005842
2025-06-25 05:02:19.312 INFO: train_e/atom_rmse: 0.005842
train_f_mae: 0.014936
2025-06-25 05:02:19.316 INFO: train_f_mae: 0.014936
train_f_rmse: 0.021916
2025-06-25 05:02:19.316 INFO: train_f_rmse: 0.021916
val_e/atom_mae: 0.005068
2025-06-25 05:02:19.319 INFO: val_e/atom_mae: 0.005068
val_e/atom_rmse: 0.006360
2025-06-25 05:02:19.319 INFO: val_e/atom_rmse: 0.006360
val_f_mae: 0.014421
2025-06-25 05:02:19.320 INFO: val_f_mae: 0.014421
val_f_rmse: 0.021385
2025-06-25 05:02:19.320 INFO: val_f_rmse: 0.021385
##### Step: 28 Learning rate: 0.005 #####
2025-06-25 05:03:04.476 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.5815, Val Loss: 0.5000
2025-06-25 05:03:04.477 INFO: Epoch 29, Train Loss: 0.5815, Val Loss: 0.5000
train_e/atom_mae: 0.004524
2025-06-25 05:03:04.478 INFO: train_e/atom_mae: 0.004524
train_e/atom_rmse: 0.005536
2025-06-25 05:03:04.478 INFO: train_e/atom_rmse: 0.005536
train_f_mae: 0.015471
2025-06-25 05:03:04.481 INFO: train_f_mae: 0.015471
train_f_rmse: 0.023334
2025-06-25 05:03:04.482 INFO: train_f_rmse: 0.023334
val_e/atom_mae: 0.004521
2025-06-25 05:03:04.484 INFO: val_e/atom_mae: 0.004521
val_e/atom_rmse: 0.005441
2025-06-25 05:03:04.485 INFO: val_e/atom_rmse: 0.005441
val_f_mae: 0.014597
2025-06-25 05:03:04.485 INFO: val_f_mae: 0.014597
val_f_rmse: 0.021545
2025-06-25 05:03:04.485 INFO: val_f_rmse: 0.021545
##### Step: 29 Learning rate: 0.005 #####
2025-06-25 05:03:49.535 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.5064, Val Loss: 0.4476
2025-06-25 05:03:49.536 INFO: Epoch 30, Train Loss: 0.5064, Val Loss: 0.4476
train_e/atom_mae: 0.004552
2025-06-25 05:03:49.537 INFO: train_e/atom_mae: 0.004552
train_e/atom_rmse: 0.005612
2025-06-25 05:03:49.537 INFO: train_e/atom_rmse: 0.005612
train_f_mae: 0.014746
2025-06-25 05:03:49.541 INFO: train_f_mae: 0.014746
train_f_rmse: 0.021640
2025-06-25 05:03:49.541 INFO: train_f_rmse: 0.021640
val_e/atom_mae: 0.003924
2025-06-25 05:03:49.543 INFO: val_e/atom_mae: 0.003924
val_e/atom_rmse: 0.005252
2025-06-25 05:03:49.544 INFO: val_e/atom_rmse: 0.005252
val_f_mae: 0.013845
2025-06-25 05:03:49.544 INFO: val_f_mae: 0.013845
val_f_rmse: 0.020353
2025-06-25 05:03:49.544 INFO: val_f_rmse: 0.020353
##### Step: 30 Learning rate: 0.005 #####
2025-06-25 05:04:34.750 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.5213, Val Loss: 0.4539
2025-06-25 05:04:34.750 INFO: Epoch 31, Train Loss: 0.5213, Val Loss: 0.4539
train_e/atom_mae: 0.004269
2025-06-25 05:04:34.751 INFO: train_e/atom_mae: 0.004269
train_e/atom_rmse: 0.005263
2025-06-25 05:04:34.751 INFO: train_e/atom_rmse: 0.005263
train_f_mae: 0.014849
2025-06-25 05:04:34.755 INFO: train_f_mae: 0.014849
train_f_rmse: 0.022085
2025-06-25 05:04:34.755 INFO: train_f_rmse: 0.022085
val_e/atom_mae: 0.003559
2025-06-25 05:04:34.758 INFO: val_e/atom_mae: 0.003559
val_e/atom_rmse: 0.004152
2025-06-25 05:04:34.758 INFO: val_e/atom_rmse: 0.004152
val_f_mae: 0.014084
2025-06-25 05:04:34.759 INFO: val_f_mae: 0.014084
val_f_rmse: 0.020808
2025-06-25 05:04:34.759 INFO: val_f_rmse: 0.020808
##### Step: 31 Learning rate: 0.005 #####
2025-06-25 05:05:19.799 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.5093, Val Loss: 0.4236
2025-06-25 05:05:19.799 INFO: Epoch 32, Train Loss: 0.5093, Val Loss: 0.4236
train_e/atom_mae: 0.004124
2025-06-25 05:05:19.800 INFO: train_e/atom_mae: 0.004124
train_e/atom_rmse: 0.004989
2025-06-25 05:05:19.800 INFO: train_e/atom_rmse: 0.004989
train_f_mae: 0.014804
2025-06-25 05:05:19.804 INFO: train_f_mae: 0.014804
train_f_rmse: 0.021891
2025-06-25 05:05:19.804 INFO: train_f_rmse: 0.021891
val_e/atom_mae: 0.003506
2025-06-25 05:05:19.807 INFO: val_e/atom_mae: 0.003506
val_e/atom_rmse: 0.004088
2025-06-25 05:05:19.807 INFO: val_e/atom_rmse: 0.004088
val_f_mae: 0.013784
2025-06-25 05:05:19.808 INFO: val_f_mae: 0.013784
val_f_rmse: 0.020086
2025-06-25 05:05:19.808 INFO: val_f_rmse: 0.020086
##### Step: 32 Learning rate: 0.005 #####
2025-06-25 05:06:05.010 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.4709, Val Loss: 0.5057
2025-06-25 05:06:05.010 INFO: Epoch 33, Train Loss: 0.4709, Val Loss: 0.5057
train_e/atom_mae: 0.004247
2025-06-25 05:06:05.011 INFO: train_e/atom_mae: 0.004247
train_e/atom_rmse: 0.005200
2025-06-25 05:06:05.011 INFO: train_e/atom_rmse: 0.005200
train_f_mae: 0.014331
2025-06-25 05:06:05.015 INFO: train_f_mae: 0.014331
train_f_rmse: 0.020933
2025-06-25 05:06:05.015 INFO: train_f_rmse: 0.020933
val_e/atom_mae: 0.003701
2025-06-25 05:06:05.018 INFO: val_e/atom_mae: 0.003701
val_e/atom_rmse: 0.004953
2025-06-25 05:06:05.018 INFO: val_e/atom_rmse: 0.004953
val_f_mae: 0.015067
2025-06-25 05:06:05.019 INFO: val_f_mae: 0.015067
val_f_rmse: 0.021817
2025-06-25 05:06:05.019 INFO: val_f_rmse: 0.021817
##### Step: 33 Learning rate: 0.005 #####
2025-06-25 05:06:50.074 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.5036, Val Loss: 0.4978
2025-06-25 05:06:50.075 INFO: Epoch 34, Train Loss: 0.5036, Val Loss: 0.4978
train_e/atom_mae: 0.004449
2025-06-25 05:06:50.076 INFO: train_e/atom_mae: 0.004449
train_e/atom_rmse: 0.005421
2025-06-25 05:06:50.076 INFO: train_e/atom_rmse: 0.005421
train_f_mae: 0.014631
2025-06-25 05:06:50.079 INFO: train_f_mae: 0.014631
train_f_rmse: 0.021634
2025-06-25 05:06:50.080 INFO: train_f_rmse: 0.021634
val_e/atom_mae: 0.004907
2025-06-25 05:06:50.083 INFO: val_e/atom_mae: 0.004907
val_e/atom_rmse: 0.005868
2025-06-25 05:06:50.083 INFO: val_e/atom_rmse: 0.005868
val_f_mae: 0.014742
2025-06-25 05:06:50.083 INFO: val_f_mae: 0.014742
val_f_rmse: 0.021357
2025-06-25 05:06:50.083 INFO: val_f_rmse: 0.021357
##### Step: 34 Learning rate: 0.005 #####
2025-06-25 05:07:35.256 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.4450, Val Loss: 0.4367
2025-06-25 05:07:35.256 INFO: Epoch 35, Train Loss: 0.4450, Val Loss: 0.4367
train_e/atom_mae: 0.003903
2025-06-25 05:07:35.257 INFO: train_e/atom_mae: 0.003903
train_e/atom_rmse: 0.004745
2025-06-25 05:07:35.257 INFO: train_e/atom_rmse: 0.004745
train_f_mae: 0.014021
2025-06-25 05:07:35.261 INFO: train_f_mae: 0.014021
train_f_rmse: 0.020440
2025-06-25 05:07:35.261 INFO: train_f_rmse: 0.020440
val_e/atom_mae: 0.003384
2025-06-25 05:07:35.264 INFO: val_e/atom_mae: 0.003384
val_e/atom_rmse: 0.003645
2025-06-25 05:07:35.264 INFO: val_e/atom_rmse: 0.003645
val_f_mae: 0.014049
2025-06-25 05:07:35.264 INFO: val_f_mae: 0.014049
val_f_rmse: 0.020509
2025-06-25 05:07:35.265 INFO: val_f_rmse: 0.020509
##### Step: 35 Learning rate: 0.005 #####
2025-06-25 05:08:20.270 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.5101, Val Loss: 0.4904
2025-06-25 05:08:20.271 INFO: Epoch 36, Train Loss: 0.5101, Val Loss: 0.4904
train_e/atom_mae: 0.004061
2025-06-25 05:08:20.272 INFO: train_e/atom_mae: 0.004061
train_e/atom_rmse: 0.005019
2025-06-25 05:08:20.272 INFO: train_e/atom_rmse: 0.005019
train_f_mae: 0.014680
2025-06-25 05:08:20.275 INFO: train_f_mae: 0.014680
train_f_rmse: 0.021899
2025-06-25 05:08:20.275 INFO: train_f_rmse: 0.021899
val_e/atom_mae: 0.004420
2025-06-25 05:08:20.278 INFO: val_e/atom_mae: 0.004420
val_e/atom_rmse: 0.005538
2025-06-25 05:08:20.279 INFO: val_e/atom_rmse: 0.005538
val_f_mae: 0.014867
2025-06-25 05:08:20.279 INFO: val_f_mae: 0.014867
val_f_rmse: 0.021291
2025-06-25 05:08:20.279 INFO: val_f_rmse: 0.021291
##### Step: 36 Learning rate: 0.005 #####
2025-06-25 05:09:05.462 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.4564, Val Loss: 0.5172
2025-06-25 05:09:05.462 INFO: Epoch 37, Train Loss: 0.4564, Val Loss: 0.5172
train_e/atom_mae: 0.003970
2025-06-25 05:09:05.463 INFO: train_e/atom_mae: 0.003970
train_e/atom_rmse: 0.004871
2025-06-25 05:09:05.464 INFO: train_e/atom_rmse: 0.004871
train_f_mae: 0.014161
2025-06-25 05:09:05.467 INFO: train_f_mae: 0.014161
train_f_rmse: 0.020680
2025-06-25 05:09:05.467 INFO: train_f_rmse: 0.020680
val_e/atom_mae: 0.003137
2025-06-25 05:09:05.470 INFO: val_e/atom_mae: 0.003137
val_e/atom_rmse: 0.003241
2025-06-25 05:09:05.471 INFO: val_e/atom_rmse: 0.003241
val_f_mae: 0.014868
2025-06-25 05:09:05.471 INFO: val_f_mae: 0.014868
val_f_rmse: 0.022462
2025-06-25 05:09:05.471 INFO: val_f_rmse: 0.022462
##### Step: 37 Learning rate: 0.005 #####
2025-06-25 05:09:50.514 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.4566, Val Loss: 0.4752
2025-06-25 05:09:50.514 INFO: Epoch 38, Train Loss: 0.4566, Val Loss: 0.4752
train_e/atom_mae: 0.003786
2025-06-25 05:09:50.515 INFO: train_e/atom_mae: 0.003786
train_e/atom_rmse: 0.004581
2025-06-25 05:09:50.515 INFO: train_e/atom_rmse: 0.004581
train_f_mae: 0.014072
2025-06-25 05:09:50.519 INFO: train_f_mae: 0.014072
train_f_rmse: 0.020766
2025-06-25 05:09:50.519 INFO: train_f_rmse: 0.020766
val_e/atom_mae: 0.003491
2025-06-25 05:09:50.522 INFO: val_e/atom_mae: 0.003491
val_e/atom_rmse: 0.003909
2025-06-25 05:09:50.522 INFO: val_e/atom_rmse: 0.003909
val_f_mae: 0.015081
2025-06-25 05:09:50.523 INFO: val_f_mae: 0.015081
val_f_rmse: 0.021371
2025-06-25 05:09:50.523 INFO: val_f_rmse: 0.021371
##### Step: 38 Learning rate: 0.005 #####
2025-06-25 05:10:35.626 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.4781, Val Loss: 0.4201
2025-06-25 05:10:35.626 INFO: Epoch 39, Train Loss: 0.4781, Val Loss: 0.4201
train_e/atom_mae: 0.004037
2025-06-25 05:10:35.627 INFO: train_e/atom_mae: 0.004037
train_e/atom_rmse: 0.005019
2025-06-25 05:10:35.627 INFO: train_e/atom_rmse: 0.005019
train_f_mae: 0.014324
2025-06-25 05:10:35.631 INFO: train_f_mae: 0.014324
train_f_rmse: 0.021156
2025-06-25 05:10:35.631 INFO: train_f_rmse: 0.021156
val_e/atom_mae: 0.003979
2025-06-25 05:10:35.634 INFO: val_e/atom_mae: 0.003979
val_e/atom_rmse: 0.004530
2025-06-25 05:10:35.634 INFO: val_e/atom_rmse: 0.004530
val_f_mae: 0.013568
2025-06-25 05:10:35.635 INFO: val_f_mae: 0.013568
val_f_rmse: 0.019882
2025-06-25 05:10:35.635 INFO: val_f_rmse: 0.019882
##### Step: 39 Learning rate: 0.005 #####
2025-06-25 05:11:20.602 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.5138, Val Loss: 0.3778
2025-06-25 05:11:20.603 INFO: Epoch 40, Train Loss: 0.5138, Val Loss: 0.3778
train_e/atom_mae: 0.003941
2025-06-25 05:11:20.604 INFO: train_e/atom_mae: 0.003941
train_e/atom_rmse: 0.004824
2025-06-25 05:11:20.604 INFO: train_e/atom_rmse: 0.004824
train_f_mae: 0.014832
2025-06-25 05:11:20.608 INFO: train_f_mae: 0.014832
train_f_rmse: 0.022038
2025-06-25 05:11:20.608 INFO: train_f_rmse: 0.022038
val_e/atom_mae: 0.003357
2025-06-25 05:11:20.610 INFO: val_e/atom_mae: 0.003357
val_e/atom_rmse: 0.003614
2025-06-25 05:11:20.611 INFO: val_e/atom_rmse: 0.003614
val_f_mae: 0.013176
2025-06-25 05:11:20.611 INFO: val_f_mae: 0.013176
val_f_rmse: 0.019026
2025-06-25 05:11:20.611 INFO: val_f_rmse: 0.019026
2025-06-25 05:11:20.623 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-25 05:12:05.698 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 0.3481, Val Loss: 0.3474
2025-06-25 05:12:05.698 INFO: Epoch 1, Train Loss: 0.3481, Val Loss: 0.3474
train_e/atom_mae: 0.003309
2025-06-25 05:12:05.699 INFO: train_e/atom_mae: 0.003309
train_e/atom_rmse: 0.003953
2025-06-25 05:12:05.700 INFO: train_e/atom_rmse: 0.003953
train_f_mae: 0.012279
2025-06-25 05:12:05.703 INFO: train_f_mae: 0.012279
train_f_rmse: 0.018144
2025-06-25 05:12:05.703 INFO: train_f_rmse: 0.018144
val_e/atom_mae: 0.003426
2025-06-25 05:12:05.706 INFO: val_e/atom_mae: 0.003426
val_e/atom_rmse: 0.004271
2025-06-25 05:12:05.707 INFO: val_e/atom_rmse: 0.004271
val_f_mae: 0.012521
2025-06-25 05:12:05.707 INFO: val_f_mae: 0.012521
val_f_rmse: 0.018036
2025-06-25 05:12:05.707 INFO: val_f_rmse: 0.018036
##### Step: 1 Learning rate: 0.004 #####
2025-06-25 05:12:50.752 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 0.3915, Val Loss: 0.3614
2025-06-25 05:12:50.753 INFO: Epoch 2, Train Loss: 0.3915, Val Loss: 0.3614
train_e/atom_mae: 0.003445
2025-06-25 05:12:50.754 INFO: train_e/atom_mae: 0.003445
train_e/atom_rmse: 0.004167
2025-06-25 05:12:50.754 INFO: train_e/atom_rmse: 0.004167
train_f_mae: 0.012980
2025-06-25 05:12:50.757 INFO: train_f_mae: 0.012980
train_f_rmse: 0.019248
2025-06-25 05:12:50.758 INFO: train_f_rmse: 0.019248
val_e/atom_mae: 0.002961
2025-06-25 05:12:50.760 INFO: val_e/atom_mae: 0.002961
val_e/atom_rmse: 0.002991
2025-06-25 05:12:50.761 INFO: val_e/atom_rmse: 0.002991
val_f_mae: 0.012869
2025-06-25 05:12:50.761 INFO: val_f_mae: 0.012869
val_f_rmse: 0.018723
2025-06-25 05:12:50.761 INFO: val_f_rmse: 0.018723
##### Step: 2 Learning rate: 0.006 #####
2025-06-25 05:13:35.903 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 0.4744, Val Loss: 0.3627
2025-06-25 05:13:35.903 INFO: Epoch 3, Train Loss: 0.4744, Val Loss: 0.3627
train_e/atom_mae: 0.004156
2025-06-25 05:13:35.904 INFO: train_e/atom_mae: 0.004156
train_e/atom_rmse: 0.005158
2025-06-25 05:13:35.904 INFO: train_e/atom_rmse: 0.005158
train_f_mae: 0.014274
2025-06-25 05:13:35.908 INFO: train_f_mae: 0.014274
train_f_rmse: 0.021028
2025-06-25 05:13:35.908 INFO: train_f_rmse: 0.021028
val_e/atom_mae: 0.004015
2025-06-25 05:13:35.911 INFO: val_e/atom_mae: 0.004015
val_e/atom_rmse: 0.004940
2025-06-25 05:13:35.911 INFO: val_e/atom_rmse: 0.004940
val_f_mae: 0.012629
2025-06-25 05:13:35.912 INFO: val_f_mae: 0.012629
val_f_rmse: 0.018252
2025-06-25 05:13:35.912 INFO: val_f_rmse: 0.018252
##### Step: 3 Learning rate: 0.008 #####
2025-06-25 05:14:20.770 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 0.5738, Val Loss: 0.6557
2025-06-25 05:14:20.770 INFO: Epoch 4, Train Loss: 0.5738, Val Loss: 0.6557
train_e/atom_mae: 0.004948
2025-06-25 05:14:20.771 INFO: train_e/atom_mae: 0.004948
train_e/atom_rmse: 0.006215
2025-06-25 05:14:20.771 INFO: train_e/atom_rmse: 0.006215
train_f_mae: 0.015274
2025-06-25 05:14:20.775 INFO: train_f_mae: 0.015274
train_f_rmse: 0.022958
2025-06-25 05:14:20.775 INFO: train_f_rmse: 0.022958
val_e/atom_mae: 0.007681
2025-06-25 05:14:20.778 INFO: val_e/atom_mae: 0.007681
val_e/atom_rmse: 0.008903
2025-06-25 05:14:20.778 INFO: val_e/atom_rmse: 0.008903
val_f_mae: 0.016143
2025-06-25 05:14:20.778 INFO: val_f_mae: 0.016143
val_f_rmse: 0.023661
2025-06-25 05:14:20.778 INFO: val_f_rmse: 0.023661
##### Step: 4 Learning rate: 0.01 #####
2025-06-25 05:15:05.581 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 0.6051, Val Loss: 0.5592
2025-06-25 05:15:05.582 INFO: Epoch 5, Train Loss: 0.6051, Val Loss: 0.5592
train_e/atom_mae: 0.004858
2025-06-25 05:15:05.583 INFO: train_e/atom_mae: 0.004858
train_e/atom_rmse: 0.006042
2025-06-25 05:15:05.583 INFO: train_e/atom_rmse: 0.006042
train_f_mae: 0.015752
2025-06-25 05:15:05.586 INFO: train_f_mae: 0.015752
train_f_rmse: 0.023684
2025-06-25 05:15:05.587 INFO: train_f_rmse: 0.023684
val_e/atom_mae: 0.004002
2025-06-25 05:15:05.589 INFO: val_e/atom_mae: 0.004002
val_e/atom_rmse: 0.004916
2025-06-25 05:15:05.590 INFO: val_e/atom_rmse: 0.004916
val_f_mae: 0.015376
2025-06-25 05:15:05.590 INFO: val_f_mae: 0.015376
val_f_rmse: 0.023020
2025-06-25 05:15:05.590 INFO: val_f_rmse: 0.023020
##### Step: 5 Learning rate: 0.01 #####
2025-06-25 05:15:50.507 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 0.6884, Val Loss: 1.1162
2025-06-25 05:15:50.507 INFO: Epoch 6, Train Loss: 0.6884, Val Loss: 1.1162
train_e/atom_mae: 0.004626
2025-06-25 05:15:50.508 INFO: train_e/atom_mae: 0.004626
train_e/atom_rmse: 0.005714
2025-06-25 05:15:50.508 INFO: train_e/atom_rmse: 0.005714
train_f_mae: 0.016972
2025-06-25 05:15:50.512 INFO: train_f_mae: 0.016972
train_f_rmse: 0.025474
2025-06-25 05:15:50.512 INFO: train_f_rmse: 0.025474
val_e/atom_mae: 0.005018
2025-06-25 05:15:50.515 INFO: val_e/atom_mae: 0.005018
val_e/atom_rmse: 0.006288
2025-06-25 05:15:50.515 INFO: val_e/atom_rmse: 0.006288
val_f_mae: 0.023011
2025-06-25 05:15:50.516 INFO: val_f_mae: 0.023011
val_f_rmse: 0.032685
2025-06-25 05:15:50.516 INFO: val_f_rmse: 0.032685
##### Step: 6 Learning rate: 0.01 #####
2025-06-25 05:16:35.315 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 0.7677, Val Loss: 0.7044
2025-06-25 05:16:35.316 INFO: Epoch 7, Train Loss: 0.7677, Val Loss: 0.7044
train_e/atom_mae: 0.005070
2025-06-25 05:16:35.317 INFO: train_e/atom_mae: 0.005070
train_e/atom_rmse: 0.006484
2025-06-25 05:16:35.317 INFO: train_e/atom_rmse: 0.006484
train_f_mae: 0.017487
2025-06-25 05:16:35.321 INFO: train_f_mae: 0.017487
train_f_rmse: 0.026773
2025-06-25 05:16:35.321 INFO: train_f_rmse: 0.026773
val_e/atom_mae: 0.004005
2025-06-25 05:16:35.323 INFO: val_e/atom_mae: 0.004005
val_e/atom_rmse: 0.004457
2025-06-25 05:16:35.324 INFO: val_e/atom_rmse: 0.004457
val_f_mae: 0.017972
2025-06-25 05:16:35.324 INFO: val_f_mae: 0.017972
val_f_rmse: 0.026083
2025-06-25 05:16:35.324 INFO: val_f_rmse: 0.026083
##### Step: 7 Learning rate: 0.01 #####
2025-06-25 05:17:20.331 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 0.7536, Val Loss: 0.9568
2025-06-25 05:17:20.332 INFO: Epoch 8, Train Loss: 0.7536, Val Loss: 0.9568
train_e/atom_mae: 0.005586
2025-06-25 05:17:20.332 INFO: train_e/atom_mae: 0.005586
train_e/atom_rmse: 0.006969
2025-06-25 05:17:20.333 INFO: train_e/atom_rmse: 0.006969
train_f_mae: 0.017146
2025-06-25 05:17:20.336 INFO: train_f_mae: 0.017146
train_f_rmse: 0.026360
2025-06-25 05:17:20.336 INFO: train_f_rmse: 0.026360
val_e/atom_mae: 0.004516
2025-06-25 05:17:20.339 INFO: val_e/atom_mae: 0.004516
val_e/atom_rmse: 0.005354
2025-06-25 05:17:20.339 INFO: val_e/atom_rmse: 0.005354
val_f_mae: 0.019844
2025-06-25 05:17:20.340 INFO: val_f_mae: 0.019844
val_f_rmse: 0.030366
2025-06-25 05:17:20.340 INFO: val_f_rmse: 0.030366
##### Step: 8 Learning rate: 0.01 #####
2025-06-25 05:18:05.140 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 0.8221, Val Loss: 0.6283
2025-06-25 05:18:05.141 INFO: Epoch 9, Train Loss: 0.8221, Val Loss: 0.6283
train_e/atom_mae: 0.005627
2025-06-25 05:18:05.141 INFO: train_e/atom_mae: 0.005627
train_e/atom_rmse: 0.006934
2025-06-25 05:18:05.142 INFO: train_e/atom_rmse: 0.006934
train_f_mae: 0.017921
2025-06-25 05:18:05.145 INFO: train_f_mae: 0.017921
train_f_rmse: 0.027639
2025-06-25 05:18:05.145 INFO: train_f_rmse: 0.027639
val_e/atom_mae: 0.004181
2025-06-25 05:18:05.148 INFO: val_e/atom_mae: 0.004181
val_e/atom_rmse: 0.005636
2025-06-25 05:18:05.148 INFO: val_e/atom_rmse: 0.005636
val_f_mae: 0.016793
2025-06-25 05:18:05.149 INFO: val_f_mae: 0.016793
val_f_rmse: 0.024286
2025-06-25 05:18:05.149 INFO: val_f_rmse: 0.024286
##### Step: 9 Learning rate: 0.01 #####
2025-06-25 05:18:50.122 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 0.6667, Val Loss: 0.5281
2025-06-25 05:18:50.122 INFO: Epoch 10, Train Loss: 0.6667, Val Loss: 0.5281
train_e/atom_mae: 0.004875
2025-06-25 05:18:50.123 INFO: train_e/atom_mae: 0.004875
train_e/atom_rmse: 0.005952
2025-06-25 05:18:50.123 INFO: train_e/atom_rmse: 0.005952
train_f_mae: 0.016436
2025-06-25 05:18:50.127 INFO: train_f_mae: 0.016436
train_f_rmse: 0.024977
2025-06-25 05:18:50.127 INFO: train_f_rmse: 0.024977
val_e/atom_mae: 0.003227
2025-06-25 05:18:50.130 INFO: val_e/atom_mae: 0.003227
val_e/atom_rmse: 0.003374
2025-06-25 05:18:50.130 INFO: val_e/atom_rmse: 0.003374
val_f_mae: 0.015831
2025-06-25 05:18:50.130 INFO: val_f_mae: 0.015831
val_f_rmse: 0.022679
2025-06-25 05:18:50.131 INFO: val_f_rmse: 0.022679
##### Step: 10 Learning rate: 0.01 #####
2025-06-25 05:19:35.189 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 0.6483, Val Loss: 1.0469
2025-06-25 05:19:35.189 INFO: Epoch 11, Train Loss: 0.6483, Val Loss: 1.0469
train_e/atom_mae: 0.005101
2025-06-25 05:19:35.190 INFO: train_e/atom_mae: 0.005101
train_e/atom_rmse: 0.006376
2025-06-25 05:19:35.191 INFO: train_e/atom_rmse: 0.006376
train_f_mae: 0.016185
2025-06-25 05:19:35.194 INFO: train_f_mae: 0.016185
train_f_rmse: 0.024477
2025-06-25 05:19:35.194 INFO: train_f_rmse: 0.024477
val_e/atom_mae: 0.013802
2025-06-25 05:19:35.197 INFO: val_e/atom_mae: 0.013802
val_e/atom_rmse: 0.014189
2025-06-25 05:19:35.198 INFO: val_e/atom_rmse: 0.014189
val_f_mae: 0.018934
2025-06-25 05:19:35.198 INFO: val_f_mae: 0.018934
val_f_rmse: 0.028343
2025-06-25 05:19:35.198 INFO: val_f_rmse: 0.028343
##### Step: 11 Learning rate: 0.01 #####
2025-06-25 05:20:20.336 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 0.7215, Val Loss: 1.1380
2025-06-25 05:20:20.336 INFO: Epoch 12, Train Loss: 0.7215, Val Loss: 1.1380
train_e/atom_mae: 0.005085
2025-06-25 05:20:20.337 INFO: train_e/atom_mae: 0.005085
train_e/atom_rmse: 0.006406
2025-06-25 05:20:20.337 INFO: train_e/atom_rmse: 0.006406
train_f_mae: 0.017137
2025-06-25 05:20:20.341 INFO: train_f_mae: 0.017137
train_f_rmse: 0.025920
2025-06-25 05:20:20.341 INFO: train_f_rmse: 0.025920
val_e/atom_mae: 0.003100
2025-06-25 05:20:20.344 INFO: val_e/atom_mae: 0.003100
val_e/atom_rmse: 0.003225
2025-06-25 05:20:20.344 INFO: val_e/atom_rmse: 0.003225
val_f_mae: 0.021224
2025-06-25 05:20:20.345 INFO: val_f_mae: 0.021224
val_f_rmse: 0.033547
2025-06-25 05:20:20.345 INFO: val_f_rmse: 0.033547
##### Step: 12 Learning rate: 0.01 #####
2025-06-25 05:21:05.357 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 0.5872, Val Loss: 0.5703
2025-06-25 05:21:05.357 INFO: Epoch 13, Train Loss: 0.5872, Val Loss: 0.5703
train_e/atom_mae: 0.005484
2025-06-25 05:21:05.358 INFO: train_e/atom_mae: 0.005484
train_e/atom_rmse: 0.006808
2025-06-25 05:21:05.359 INFO: train_e/atom_rmse: 0.006808
train_f_mae: 0.015295
2025-06-25 05:21:05.362 INFO: train_f_mae: 0.015295
train_f_rmse: 0.023046
2025-06-25 05:21:05.362 INFO: train_f_rmse: 0.023046
val_e/atom_mae: 0.003877
2025-06-25 05:21:05.365 INFO: val_e/atom_mae: 0.003877
val_e/atom_rmse: 0.004964
2025-06-25 05:21:05.366 INFO: val_e/atom_rmse: 0.004964
val_f_mae: 0.016409
2025-06-25 05:21:05.366 INFO: val_f_mae: 0.016409
val_f_rmse: 0.023248
2025-06-25 05:21:05.366 INFO: val_f_rmse: 0.023248
##### Step: 13 Learning rate: 0.01 #####
2025-06-25 05:21:50.566 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 0.6427, Val Loss: 0.6422
2025-06-25 05:21:50.566 INFO: Epoch 14, Train Loss: 0.6427, Val Loss: 0.6422
train_e/atom_mae: 0.005175
2025-06-25 05:21:50.567 INFO: train_e/atom_mae: 0.005175
train_e/atom_rmse: 0.006376
2025-06-25 05:21:50.567 INFO: train_e/atom_rmse: 0.006376
train_f_mae: 0.016216
2025-06-25 05:21:50.571 INFO: train_f_mae: 0.016216
train_f_rmse: 0.024362
2025-06-25 05:21:50.571 INFO: train_f_rmse: 0.024362
val_e/atom_mae: 0.005259
2025-06-25 05:21:50.574 INFO: val_e/atom_mae: 0.005259
val_e/atom_rmse: 0.006332
2025-06-25 05:21:50.574 INFO: val_e/atom_rmse: 0.006332
val_f_mae: 0.016473
2025-06-25 05:21:50.575 INFO: val_f_mae: 0.016473
val_f_rmse: 0.024365
2025-06-25 05:21:50.575 INFO: val_f_rmse: 0.024365
##### Step: 14 Learning rate: 0.01 #####
2025-06-25 05:22:35.638 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 0.6613, Val Loss: 0.5832
2025-06-25 05:22:35.639 INFO: Epoch 15, Train Loss: 0.6613, Val Loss: 0.5832
train_e/atom_mae: 0.004860
2025-06-25 05:22:35.640 INFO: train_e/atom_mae: 0.004860
train_e/atom_rmse: 0.006022
2025-06-25 05:22:35.640 INFO: train_e/atom_rmse: 0.006022
train_f_mae: 0.016095
2025-06-25 05:22:35.643 INFO: train_f_mae: 0.016095
train_f_rmse: 0.024847
2025-06-25 05:22:35.644 INFO: train_f_rmse: 0.024847
val_e/atom_mae: 0.007627
2025-06-25 05:22:35.646 INFO: val_e/atom_mae: 0.007627
val_e/atom_rmse: 0.008252
2025-06-25 05:22:35.647 INFO: val_e/atom_rmse: 0.008252
val_f_mae: 0.015575
2025-06-25 05:22:35.647 INFO: val_f_mae: 0.015575
val_f_rmse: 0.022379
2025-06-25 05:22:35.647 INFO: val_f_rmse: 0.022379
##### Step: 15 Learning rate: 0.01 #####
2025-06-25 05:23:20.778 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 0.6380, Val Loss: 0.5978
2025-06-25 05:23:20.778 INFO: Epoch 16, Train Loss: 0.6380, Val Loss: 0.5978
train_e/atom_mae: 0.004367
2025-06-25 05:23:20.779 INFO: train_e/atom_mae: 0.004367
train_e/atom_rmse: 0.005386
2025-06-25 05:23:20.780 INFO: train_e/atom_rmse: 0.005386
train_f_mae: 0.016375
2025-06-25 05:23:20.783 INFO: train_f_mae: 0.016375
train_f_rmse: 0.024555
2025-06-25 05:23:20.784 INFO: train_f_rmse: 0.024555
val_e/atom_mae: 0.004578
2025-06-25 05:23:20.786 INFO: val_e/atom_mae: 0.004578
val_e/atom_rmse: 0.005141
2025-06-25 05:23:20.787 INFO: val_e/atom_rmse: 0.005141
val_f_mae: 0.016640
2025-06-25 05:23:20.787 INFO: val_f_mae: 0.016640
val_f_rmse: 0.023787
2025-06-25 05:23:20.787 INFO: val_f_rmse: 0.023787
##### Step: 16 Learning rate: 0.01 #####
2025-06-25 05:24:05.817 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 0.6438, Val Loss: 0.7091
2025-06-25 05:24:05.817 INFO: Epoch 17, Train Loss: 0.6438, Val Loss: 0.7091
train_e/atom_mae: 0.004842
2025-06-25 05:24:05.818 INFO: train_e/atom_mae: 0.004842
train_e/atom_rmse: 0.005985
2025-06-25 05:24:05.818 INFO: train_e/atom_rmse: 0.005985
train_f_mae: 0.016394
2025-06-25 05:24:05.822 INFO: train_f_mae: 0.016394
train_f_rmse: 0.024504
2025-06-25 05:24:05.822 INFO: train_f_rmse: 0.024504
val_e/atom_mae: 0.003054
2025-06-25 05:24:05.825 INFO: val_e/atom_mae: 0.003054
val_e/atom_rmse: 0.003189
2025-06-25 05:24:05.825 INFO: val_e/atom_rmse: 0.003189
val_f_mae: 0.016164
2025-06-25 05:24:05.826 INFO: val_f_mae: 0.016164
val_f_rmse: 0.026396
2025-06-25 05:24:05.826 INFO: val_f_rmse: 0.026396
##### Step: 17 Learning rate: 0.01 #####
2025-06-25 05:24:50.992 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 0.6395, Val Loss: 0.5403
2025-06-25 05:24:50.992 INFO: Epoch 18, Train Loss: 0.6395, Val Loss: 0.5403
train_e/atom_mae: 0.004568
2025-06-25 05:24:50.993 INFO: train_e/atom_mae: 0.004568
train_e/atom_rmse: 0.005687
2025-06-25 05:24:50.993 INFO: train_e/atom_rmse: 0.005687
train_f_mae: 0.015675
2025-06-25 05:24:50.997 INFO: train_f_mae: 0.015675
train_f_rmse: 0.024503
2025-06-25 05:24:50.997 INFO: train_f_rmse: 0.024503
val_e/atom_mae: 0.003299
2025-06-25 05:24:51.000 INFO: val_e/atom_mae: 0.003299
val_e/atom_rmse: 0.004362
2025-06-25 05:24:51.000 INFO: val_e/atom_rmse: 0.004362
val_f_mae: 0.015662
2025-06-25 05:24:51.001 INFO: val_f_mae: 0.015662
val_f_rmse: 0.022745
2025-06-25 05:24:51.001 INFO: val_f_rmse: 0.022745
##### Step: 18 Learning rate: 0.01 #####
2025-06-25 05:25:36.022 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 0.8023, Val Loss: 0.8244
2025-06-25 05:25:36.022 INFO: Epoch 19, Train Loss: 0.8023, Val Loss: 0.8244
train_e/atom_mae: 0.005171
2025-06-25 05:25:36.023 INFO: train_e/atom_mae: 0.005171
train_e/atom_rmse: 0.006471
2025-06-25 05:25:36.023 INFO: train_e/atom_rmse: 0.006471
train_f_mae: 0.017950
2025-06-25 05:25:36.027 INFO: train_f_mae: 0.017950
train_f_rmse: 0.027416
2025-06-25 05:25:36.027 INFO: train_f_rmse: 0.027416
val_e/atom_mae: 0.004649
2025-06-25 05:25:36.030 INFO: val_e/atom_mae: 0.004649
val_e/atom_rmse: 0.005541
2025-06-25 05:25:36.030 INFO: val_e/atom_rmse: 0.005541
val_f_mae: 0.017854
2025-06-25 05:25:36.030 INFO: val_f_mae: 0.017854
val_f_rmse: 0.028057
2025-06-25 05:25:36.031 INFO: val_f_rmse: 0.028057
##### Step: 19 Learning rate: 0.01 #####
2025-06-25 05:26:21.212 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 0.6562, Val Loss: 0.5869
2025-06-25 05:26:21.212 INFO: Epoch 20, Train Loss: 0.6562, Val Loss: 0.5869
train_e/atom_mae: 0.005046
2025-06-25 05:26:21.213 INFO: train_e/atom_mae: 0.005046
train_e/atom_rmse: 0.006262
2025-06-25 05:26:21.213 INFO: train_e/atom_rmse: 0.006262
train_f_mae: 0.016426
2025-06-25 05:26:21.217 INFO: train_f_mae: 0.016426
train_f_rmse: 0.024674
2025-06-25 05:26:21.217 INFO: train_f_rmse: 0.024674
val_e/atom_mae: 0.004235
2025-06-25 05:26:21.220 INFO: val_e/atom_mae: 0.004235
val_e/atom_rmse: 0.005425
2025-06-25 05:26:21.220 INFO: val_e/atom_rmse: 0.005425
val_f_mae: 0.015706
2025-06-25 05:26:21.221 INFO: val_f_mae: 0.015706
val_f_rmse: 0.023479
2025-06-25 05:26:21.221 INFO: val_f_rmse: 0.023479
##### Step: 20 Learning rate: 0.005 #####
2025-06-25 05:27:06.334 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 0.4128, Val Loss: 0.4159
2025-06-25 05:27:06.334 INFO: Epoch 21, Train Loss: 0.4128, Val Loss: 0.4159
train_e/atom_mae: 0.003878
2025-06-25 05:27:06.335 INFO: train_e/atom_mae: 0.003878
train_e/atom_rmse: 0.004827
2025-06-25 05:27:06.335 INFO: train_e/atom_rmse: 0.004827
train_f_mae: 0.013409
2025-06-25 05:27:06.339 INFO: train_f_mae: 0.013409
train_f_rmse: 0.019611
2025-06-25 05:27:06.339 INFO: train_f_rmse: 0.019611
val_e/atom_mae: 0.002956
2025-06-25 05:27:06.342 INFO: val_e/atom_mae: 0.002956
val_e/atom_rmse: 0.003392
2025-06-25 05:27:06.342 INFO: val_e/atom_rmse: 0.003392
val_f_mae: 0.013945
2025-06-25 05:27:06.343 INFO: val_f_mae: 0.013945
val_f_rmse: 0.020050
2025-06-25 05:27:06.343 INFO: val_f_rmse: 0.020050
##### Step: 21 Learning rate: 0.005 #####
2025-06-25 05:27:51.491 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.3847, Val Loss: 0.3770
2025-06-25 05:27:51.492 INFO: Epoch 22, Train Loss: 0.3847, Val Loss: 0.3770
train_e/atom_mae: 0.003737
2025-06-25 05:27:51.493 INFO: train_e/atom_mae: 0.003737
train_e/atom_rmse: 0.004613
2025-06-25 05:27:51.493 INFO: train_e/atom_rmse: 0.004613
train_f_mae: 0.013011
2025-06-25 05:27:51.497 INFO: train_f_mae: 0.013011
train_f_rmse: 0.018945
2025-06-25 05:27:51.497 INFO: train_f_rmse: 0.018945
val_e/atom_mae: 0.002875
2025-06-25 05:27:51.500 INFO: val_e/atom_mae: 0.002875
val_e/atom_rmse: 0.003493
2025-06-25 05:27:51.500 INFO: val_e/atom_rmse: 0.003493
val_f_mae: 0.012909
2025-06-25 05:27:51.500 INFO: val_f_mae: 0.012909
val_f_rmse: 0.019033
2025-06-25 05:27:51.501 INFO: val_f_rmse: 0.019033
##### Step: 22 Learning rate: 0.005 #####
2025-06-25 05:28:36.579 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.4240, Val Loss: 0.3850
2025-06-25 05:28:36.580 INFO: Epoch 23, Train Loss: 0.4240, Val Loss: 0.3850
train_e/atom_mae: 0.003852
2025-06-25 05:28:36.580 INFO: train_e/atom_mae: 0.003852
train_e/atom_rmse: 0.004732
2025-06-25 05:28:36.581 INFO: train_e/atom_rmse: 0.004732
train_f_mae: 0.013614
2025-06-25 05:28:36.584 INFO: train_f_mae: 0.013614
train_f_rmse: 0.019923
2025-06-25 05:28:36.584 INFO: train_f_rmse: 0.019923
val_e/atom_mae: 0.002651
2025-06-25 05:28:36.587 INFO: val_e/atom_mae: 0.002651
val_e/atom_rmse: 0.003235
2025-06-25 05:28:36.588 INFO: val_e/atom_rmse: 0.003235
val_f_mae: 0.013282
2025-06-25 05:28:36.588 INFO: val_f_mae: 0.013282
val_f_rmse: 0.019297
2025-06-25 05:28:36.588 INFO: val_f_rmse: 0.019297
##### Step: 23 Learning rate: 0.005 #####
2025-06-25 05:29:21.756 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.3949, Val Loss: 0.3372
2025-06-25 05:29:21.756 INFO: Epoch 24, Train Loss: 0.3949, Val Loss: 0.3372
train_e/atom_mae: 0.003569
2025-06-25 05:29:21.757 INFO: train_e/atom_mae: 0.003569
train_e/atom_rmse: 0.004363
2025-06-25 05:29:21.757 INFO: train_e/atom_rmse: 0.004363
train_f_mae: 0.013044
2025-06-25 05:29:21.761 INFO: train_f_mae: 0.013044
train_f_rmse: 0.019283
2025-06-25 05:29:21.761 INFO: train_f_rmse: 0.019283
val_e/atom_mae: 0.002766
2025-06-25 05:29:21.764 INFO: val_e/atom_mae: 0.002766
val_e/atom_rmse: 0.003054
2025-06-25 05:29:21.764 INFO: val_e/atom_rmse: 0.003054
val_f_mae: 0.012344
2025-06-25 05:29:21.765 INFO: val_f_mae: 0.012344
val_f_rmse: 0.018052
2025-06-25 05:29:21.765 INFO: val_f_rmse: 0.018052
##### Step: 24 Learning rate: 0.005 #####
2025-06-25 05:30:06.899 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.3632, Val Loss: 0.3062
2025-06-25 05:30:06.900 INFO: Epoch 25, Train Loss: 0.3632, Val Loss: 0.3062
train_e/atom_mae: 0.003169
2025-06-25 05:30:06.901 INFO: train_e/atom_mae: 0.003169
train_e/atom_rmse: 0.003843
2025-06-25 05:30:06.901 INFO: train_e/atom_rmse: 0.003843
train_f_mae: 0.012814
2025-06-25 05:30:06.904 INFO: train_f_mae: 0.012814
train_f_rmse: 0.018582
2025-06-25 05:30:06.904 INFO: train_f_rmse: 0.018582
val_e/atom_mae: 0.002585
2025-06-25 05:30:06.907 INFO: val_e/atom_mae: 0.002585
val_e/atom_rmse: 0.003024
2025-06-25 05:30:06.908 INFO: val_e/atom_rmse: 0.003024
val_f_mae: 0.012194
2025-06-25 05:30:06.908 INFO: val_f_mae: 0.012194
val_f_rmse: 0.017178
2025-06-25 05:30:06.908 INFO: val_f_rmse: 0.017178
##### Step: 25 Learning rate: 0.005 #####
2025-06-25 05:30:52.032 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.3421, Val Loss: 0.3726
2025-06-25 05:30:52.032 INFO: Epoch 26, Train Loss: 0.3421, Val Loss: 0.3726
train_e/atom_mae: 0.003604
2025-06-25 05:30:52.033 INFO: train_e/atom_mae: 0.003604
train_e/atom_rmse: 0.004431
2025-06-25 05:30:52.033 INFO: train_e/atom_rmse: 0.004431
train_f_mae: 0.012249
2025-06-25 05:30:52.037 INFO: train_f_mae: 0.012249
train_f_rmse: 0.017841
2025-06-25 05:30:52.037 INFO: train_f_rmse: 0.017841
val_e/atom_mae: 0.007458
2025-06-25 05:30:52.040 INFO: val_e/atom_mae: 0.007458
val_e/atom_rmse: 0.007849
2025-06-25 05:30:52.040 INFO: val_e/atom_rmse: 0.007849
val_f_mae: 0.011872
2025-06-25 05:30:52.040 INFO: val_f_mae: 0.011872
val_f_rmse: 0.017264
2025-06-25 05:30:52.040 INFO: val_f_rmse: 0.017264
##### Step: 26 Learning rate: 0.005 #####
2025-06-25 05:31:37.168 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.3650, Val Loss: 0.3307
2025-06-25 05:31:37.169 INFO: Epoch 27, Train Loss: 0.3650, Val Loss: 0.3307
train_e/atom_mae: 0.003452
2025-06-25 05:31:37.169 INFO: train_e/atom_mae: 0.003452
train_e/atom_rmse: 0.004302
2025-06-25 05:31:37.170 INFO: train_e/atom_rmse: 0.004302
train_f_mae: 0.012686
2025-06-25 05:31:37.173 INFO: train_f_mae: 0.012686
train_f_rmse: 0.018509
2025-06-25 05:31:37.173 INFO: train_f_rmse: 0.018509
val_e/atom_mae: 0.002533
2025-06-25 05:31:37.176 INFO: val_e/atom_mae: 0.002533
val_e/atom_rmse: 0.002634
2025-06-25 05:31:37.177 INFO: val_e/atom_rmse: 0.002634
val_f_mae: 0.012600
2025-06-25 05:31:37.177 INFO: val_f_mae: 0.012600
val_f_rmse: 0.017952
2025-06-25 05:31:37.177 INFO: val_f_rmse: 0.017952
##### Step: 27 Learning rate: 0.005 #####
2025-06-25 05:32:22.224 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.3708, Val Loss: 0.3215
2025-06-25 05:32:22.224 INFO: Epoch 28, Train Loss: 0.3708, Val Loss: 0.3215
train_e/atom_mae: 0.003238
2025-06-25 05:32:22.225 INFO: train_e/atom_mae: 0.003238
train_e/atom_rmse: 0.004004
2025-06-25 05:32:22.225 INFO: train_e/atom_rmse: 0.004004
train_f_mae: 0.012807
2025-06-25 05:32:22.229 INFO: train_f_mae: 0.012807
train_f_rmse: 0.018745
2025-06-25 05:32:22.229 INFO: train_f_rmse: 0.018745
val_e/atom_mae: 0.003960
2025-06-25 05:32:22.231 INFO: val_e/atom_mae: 0.003960
val_e/atom_rmse: 0.004701
2025-06-25 05:32:22.232 INFO: val_e/atom_rmse: 0.004701
val_f_mae: 0.011982
2025-06-25 05:32:22.232 INFO: val_f_mae: 0.011982
val_f_rmse: 0.017168
2025-06-25 05:32:22.232 INFO: val_f_rmse: 0.017168
##### Step: 28 Learning rate: 0.005 #####
2025-06-25 05:33:07.374 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.3629, Val Loss: 0.3761
2025-06-25 05:33:07.374 INFO: Epoch 29, Train Loss: 0.3629, Val Loss: 0.3761
train_e/atom_mae: 0.003278
2025-06-25 05:33:07.375 INFO: train_e/atom_mae: 0.003278
train_e/atom_rmse: 0.004013
2025-06-25 05:33:07.375 INFO: train_e/atom_rmse: 0.004013
train_f_mae: 0.012729
2025-06-25 05:33:07.379 INFO: train_f_mae: 0.012729
train_f_rmse: 0.018532
2025-06-25 05:33:07.379 INFO: train_f_rmse: 0.018532
val_e/atom_mae: 0.005435
2025-06-25 05:33:07.382 INFO: val_e/atom_mae: 0.005435
val_e/atom_rmse: 0.006158
2025-06-25 05:33:07.383 INFO: val_e/atom_rmse: 0.006158
val_f_mae: 0.012918
2025-06-25 05:33:07.383 INFO: val_f_mae: 0.012918
val_f_rmse: 0.018173
2025-06-25 05:33:07.383 INFO: val_f_rmse: 0.018173
##### Step: 29 Learning rate: 0.005 #####
2025-06-25 05:33:52.392 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.3412, Val Loss: 0.3694
2025-06-25 05:33:52.393 INFO: Epoch 30, Train Loss: 0.3412, Val Loss: 0.3694
train_e/atom_mae: 0.003620
2025-06-25 05:33:52.394 INFO: train_e/atom_mae: 0.003620
train_e/atom_rmse: 0.004445
2025-06-25 05:33:52.394 INFO: train_e/atom_rmse: 0.004445
train_f_mae: 0.012230
2025-06-25 05:33:52.397 INFO: train_f_mae: 0.012230
train_f_rmse: 0.017812
2025-06-25 05:33:52.398 INFO: train_f_rmse: 0.017812
val_e/atom_mae: 0.002721
2025-06-25 05:33:52.400 INFO: val_e/atom_mae: 0.002721
val_e/atom_rmse: 0.003696
2025-06-25 05:33:52.401 INFO: val_e/atom_rmse: 0.003696
val_f_mae: 0.013082
2025-06-25 05:33:52.401 INFO: val_f_mae: 0.013082
val_f_rmse: 0.018786
2025-06-25 05:33:52.401 INFO: val_f_rmse: 0.018786
##### Step: 30 Learning rate: 0.005 #####
2025-06-25 05:34:37.542 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.3480, Val Loss: 0.5087
2025-06-25 05:34:37.543 INFO: Epoch 31, Train Loss: 0.3480, Val Loss: 0.5087
train_e/atom_mae: 0.003293
2025-06-25 05:34:37.543 INFO: train_e/atom_mae: 0.003293
train_e/atom_rmse: 0.004055
2025-06-25 05:34:37.544 INFO: train_e/atom_rmse: 0.004055
train_f_mae: 0.012307
2025-06-25 05:34:37.547 INFO: train_f_mae: 0.012307
train_f_rmse: 0.018113
2025-06-25 05:34:37.547 INFO: train_f_rmse: 0.018113
val_e/atom_mae: 0.004178
2025-06-25 05:34:37.550 INFO: val_e/atom_mae: 0.004178
val_e/atom_rmse: 0.004965
2025-06-25 05:34:37.550 INFO: val_e/atom_rmse: 0.004965
val_f_mae: 0.013790
2025-06-25 05:34:37.551 INFO: val_f_mae: 0.013790
val_f_rmse: 0.021883
2025-06-25 05:34:37.551 INFO: val_f_rmse: 0.021883
##### Step: 31 Learning rate: 0.005 #####
2025-06-25 05:35:22.455 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.3796, Val Loss: 0.5032
2025-06-25 05:35:22.455 INFO: Epoch 32, Train Loss: 0.3796, Val Loss: 0.5032
train_e/atom_mae: 0.003064
2025-06-25 05:35:22.456 INFO: train_e/atom_mae: 0.003064
train_e/atom_rmse: 0.003803
2025-06-25 05:35:22.457 INFO: train_e/atom_rmse: 0.003803
train_f_mae: 0.012734
2025-06-25 05:35:22.460 INFO: train_f_mae: 0.012734
train_f_rmse: 0.019029
2025-06-25 05:35:22.460 INFO: train_f_rmse: 0.019029
val_e/atom_mae: 0.002739
2025-06-25 05:35:22.463 INFO: val_e/atom_mae: 0.002739
val_e/atom_rmse: 0.003491
2025-06-25 05:35:22.463 INFO: val_e/atom_rmse: 0.003491
val_f_mae: 0.015252
2025-06-25 05:35:22.464 INFO: val_f_mae: 0.015252
val_f_rmse: 0.022101
2025-06-25 05:35:22.464 INFO: val_f_rmse: 0.022101
##### Step: 32 Learning rate: 0.005 #####
2025-06-25 05:36:07.505 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.3636, Val Loss: 0.3666
2025-06-25 05:36:07.505 INFO: Epoch 33, Train Loss: 0.3636, Val Loss: 0.3666
train_e/atom_mae: 0.003432
2025-06-25 05:36:07.506 INFO: train_e/atom_mae: 0.003432
train_e/atom_rmse: 0.004232
2025-06-25 05:36:07.506 INFO: train_e/atom_rmse: 0.004232
train_f_mae: 0.012531
2025-06-25 05:36:07.510 INFO: train_f_mae: 0.012531
train_f_rmse: 0.018492
2025-06-25 05:36:07.510 INFO: train_f_rmse: 0.018492
val_e/atom_mae: 0.002006
2025-06-25 05:36:07.513 INFO: val_e/atom_mae: 0.002006
val_e/atom_rmse: 0.002301
2025-06-25 05:36:07.513 INFO: val_e/atom_rmse: 0.002301
val_f_mae: 0.012965
2025-06-25 05:36:07.514 INFO: val_f_mae: 0.012965
val_f_rmse: 0.018979
2025-06-25 05:36:07.514 INFO: val_f_rmse: 0.018979
##### Step: 33 Learning rate: 0.005 #####
2025-06-25 05:36:52.462 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.3502, Val Loss: 0.3036
2025-06-25 05:36:52.462 INFO: Epoch 34, Train Loss: 0.3502, Val Loss: 0.3036
train_e/atom_mae: 0.003611
2025-06-25 05:36:52.463 INFO: train_e/atom_mae: 0.003611
train_e/atom_rmse: 0.004428
2025-06-25 05:36:52.463 INFO: train_e/atom_rmse: 0.004428
train_f_mae: 0.012416
2025-06-25 05:36:52.467 INFO: train_f_mae: 0.012416
train_f_rmse: 0.018068
2025-06-25 05:36:52.467 INFO: train_f_rmse: 0.018068
val_e/atom_mae: 0.002343
2025-06-25 05:36:52.470 INFO: val_e/atom_mae: 0.002343
val_e/atom_rmse: 0.002877
2025-06-25 05:36:52.470 INFO: val_e/atom_rmse: 0.002877
val_f_mae: 0.011842
2025-06-25 05:36:52.471 INFO: val_f_mae: 0.011842
val_f_rmse: 0.017133
2025-06-25 05:36:52.471 INFO: val_f_rmse: 0.017133
##### Step: 34 Learning rate: 0.005 #####
2025-06-25 05:37:37.628 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.3436, Val Loss: 0.3447
2025-06-25 05:37:37.628 INFO: Epoch 35, Train Loss: 0.3436, Val Loss: 0.3447
train_e/atom_mae: 0.002781
2025-06-25 05:37:37.629 INFO: train_e/atom_mae: 0.002781
train_e/atom_rmse: 0.003425
2025-06-25 05:37:37.629 INFO: train_e/atom_rmse: 0.003425
train_f_mae: 0.012338
2025-06-25 05:37:37.633 INFO: train_f_mae: 0.012338
train_f_rmse: 0.018149
2025-06-25 05:37:37.633 INFO: train_f_rmse: 0.018149
val_e/atom_mae: 0.002374
2025-06-25 05:37:37.636 INFO: val_e/atom_mae: 0.002374
val_e/atom_rmse: 0.002781
2025-06-25 05:37:37.636 INFO: val_e/atom_rmse: 0.002781
val_f_mae: 0.012423
2025-06-25 05:37:37.637 INFO: val_f_mae: 0.012423
val_f_rmse: 0.018311
2025-06-25 05:37:37.637 INFO: val_f_rmse: 0.018311
##### Step: 35 Learning rate: 0.005 #####
2025-06-25 05:38:22.647 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.3483, Val Loss: 0.3852
2025-06-25 05:38:22.647 INFO: Epoch 36, Train Loss: 0.3483, Val Loss: 0.3852
train_e/atom_mae: 0.002566
2025-06-25 05:38:22.648 INFO: train_e/atom_mae: 0.002566
train_e/atom_rmse: 0.003143
2025-06-25 05:38:22.648 INFO: train_e/atom_rmse: 0.003143
train_f_mae: 0.012318
2025-06-25 05:38:22.652 INFO: train_f_mae: 0.012318
train_f_rmse: 0.018340
2025-06-25 05:38:22.652 INFO: train_f_rmse: 0.018340
val_e/atom_mae: 0.002029
2025-06-25 05:38:22.655 INFO: val_e/atom_mae: 0.002029
val_e/atom_rmse: 0.002276
2025-06-25 05:38:22.655 INFO: val_e/atom_rmse: 0.002276
val_f_mae: 0.012757
2025-06-25 05:38:22.656 INFO: val_f_mae: 0.012757
val_f_rmse: 0.019466
2025-06-25 05:38:22.656 INFO: val_f_rmse: 0.019466
##### Step: 36 Learning rate: 0.005 #####
2025-06-25 05:39:07.807 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.3308, Val Loss: 0.3199
2025-06-25 05:39:07.807 INFO: Epoch 37, Train Loss: 0.3308, Val Loss: 0.3199
train_e/atom_mae: 0.002725
2025-06-25 05:39:07.808 INFO: train_e/atom_mae: 0.002725
train_e/atom_rmse: 0.003344
2025-06-25 05:39:07.809 INFO: train_e/atom_rmse: 0.003344
train_f_mae: 0.012246
2025-06-25 05:39:07.812 INFO: train_f_mae: 0.012246
train_f_rmse: 0.017811
2025-06-25 05:39:07.812 INFO: train_f_rmse: 0.017811
val_e/atom_mae: 0.002193
2025-06-25 05:39:07.815 INFO: val_e/atom_mae: 0.002193
val_e/atom_rmse: 0.002398
2025-06-25 05:39:07.816 INFO: val_e/atom_rmse: 0.002398
val_f_mae: 0.012313
2025-06-25 05:39:07.816 INFO: val_f_mae: 0.012313
val_f_rmse: 0.017690
2025-06-25 05:39:07.816 INFO: val_f_rmse: 0.017690
##### Step: 37 Learning rate: 0.005 #####
2025-06-25 05:39:52.839 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.3368, Val Loss: 0.2825
2025-06-25 05:39:52.839 INFO: Epoch 38, Train Loss: 0.3368, Val Loss: 0.2825
train_e/atom_mae: 0.003266
2025-06-25 05:39:52.840 INFO: train_e/atom_mae: 0.003266
train_e/atom_rmse: 0.004056
2025-06-25 05:39:52.840 INFO: train_e/atom_rmse: 0.004056
train_f_mae: 0.012176
2025-06-25 05:39:52.844 INFO: train_f_mae: 0.012176
train_f_rmse: 0.017801
2025-06-25 05:39:52.844 INFO: train_f_rmse: 0.017801
val_e/atom_mae: 0.001840
2025-06-25 05:39:52.847 INFO: val_e/atom_mae: 0.001840
val_e/atom_rmse: 0.001945
2025-06-25 05:39:52.847 INFO: val_e/atom_rmse: 0.001945
val_f_mae: 0.011648
2025-06-25 05:39:52.848 INFO: val_f_mae: 0.011648
val_f_rmse: 0.016670
2025-06-25 05:39:52.848 INFO: val_f_rmse: 0.016670
##### Step: 38 Learning rate: 0.005 #####
2025-06-25 05:40:38.024 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.3360, Val Loss: 0.3114
2025-06-25 05:40:38.024 INFO: Epoch 39, Train Loss: 0.3360, Val Loss: 0.3114
train_e/atom_mae: 0.002720
2025-06-25 05:40:38.025 INFO: train_e/atom_mae: 0.002720
train_e/atom_rmse: 0.003382
2025-06-25 05:40:38.025 INFO: train_e/atom_rmse: 0.003382
train_f_mae: 0.012124
2025-06-25 05:40:38.029 INFO: train_f_mae: 0.012124
train_f_rmse: 0.017948
2025-06-25 05:40:38.029 INFO: train_f_rmse: 0.017948
val_e/atom_mae: 0.002073
2025-06-25 05:40:38.032 INFO: val_e/atom_mae: 0.002073
val_e/atom_rmse: 0.002289
2025-06-25 05:40:38.032 INFO: val_e/atom_rmse: 0.002289
val_f_mae: 0.011775
2025-06-25 05:40:38.033 INFO: val_f_mae: 0.011775
val_f_rmse: 0.017465
2025-06-25 05:40:38.033 INFO: val_f_rmse: 0.017465
##### Step: 39 Learning rate: 0.005 #####
2025-06-25 05:41:23.074 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.3350, Val Loss: 0.4285
2025-06-25 05:41:23.074 INFO: Epoch 40, Train Loss: 0.3350, Val Loss: 0.4285
train_e/atom_mae: 0.002781
2025-06-25 05:41:23.075 INFO: train_e/atom_mae: 0.002781
train_e/atom_rmse: 0.003453
2025-06-25 05:41:23.075 INFO: train_e/atom_rmse: 0.003453
train_f_mae: 0.012263
2025-06-25 05:41:23.079 INFO: train_f_mae: 0.012263
train_f_rmse: 0.017903
2025-06-25 05:41:23.079 INFO: train_f_rmse: 0.017903
val_e/atom_mae: 0.008250
2025-06-25 05:41:23.082 INFO: val_e/atom_mae: 0.008250
val_e/atom_rmse: 0.008624
2025-06-25 05:41:23.082 INFO: val_e/atom_rmse: 0.008624
val_f_mae: 0.012685
2025-06-25 05:41:23.082 INFO: val_f_mae: 0.012685
val_f_rmse: 0.018397
2025-06-25 05:41:23.083 INFO: val_f_rmse: 0.018397
2025-06-25 05:41:23.090 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-25 05:42:08.196 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 0.2404, Val Loss: 0.2403
2025-06-25 05:42:08.197 INFO: Epoch 1, Train Loss: 0.2404, Val Loss: 0.2403
train_e/atom_mae: 0.002539
2025-06-25 05:42:08.198 INFO: train_e/atom_mae: 0.002539
train_e/atom_rmse: 0.003158
2025-06-25 05:42:08.198 INFO: train_e/atom_rmse: 0.003158
train_f_mae: 0.010549
2025-06-25 05:42:08.201 INFO: train_f_mae: 0.010549
train_f_rmse: 0.015110
2025-06-25 05:42:08.202 INFO: train_f_rmse: 0.015110
val_e/atom_mae: 0.003083
2025-06-25 05:42:08.204 INFO: val_e/atom_mae: 0.003083
val_e/atom_rmse: 0.003593
2025-06-25 05:42:08.205 INFO: val_e/atom_rmse: 0.003593
val_f_mae: 0.010469
2025-06-25 05:42:08.205 INFO: val_f_mae: 0.010469
val_f_rmse: 0.014990
2025-06-25 05:42:08.205 INFO: val_f_rmse: 0.014990
##### Step: 1 Learning rate: 0.004 #####
2025-06-25 05:42:53.263 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 0.2855, Val Loss: 0.2891
2025-06-25 05:42:53.264 INFO: Epoch 2, Train Loss: 0.2855, Val Loss: 0.2891
train_e/atom_mae: 0.002320
2025-06-25 05:42:53.265 INFO: train_e/atom_mae: 0.002320
train_e/atom_rmse: 0.002836
2025-06-25 05:42:53.265 INFO: train_e/atom_rmse: 0.002836
train_f_mae: 0.011509
2025-06-25 05:42:53.268 INFO: train_f_mae: 0.011509
train_f_rmse: 0.016606
2025-06-25 05:42:53.268 INFO: train_f_rmse: 0.016606
val_e/atom_mae: 0.002335
2025-06-25 05:42:53.271 INFO: val_e/atom_mae: 0.002335
val_e/atom_rmse: 0.002641
2025-06-25 05:42:53.272 INFO: val_e/atom_rmse: 0.002641
val_f_mae: 0.011819
2025-06-25 05:42:53.272 INFO: val_f_mae: 0.011819
val_f_rmse: 0.016753
2025-06-25 05:42:53.272 INFO: val_f_rmse: 0.016753
##### Step: 2 Learning rate: 0.006 #####
2025-06-25 05:43:38.400 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 0.3442, Val Loss: 0.4000
2025-06-25 05:43:38.400 INFO: Epoch 3, Train Loss: 0.3442, Val Loss: 0.4000
train_e/atom_mae: 0.002847
2025-06-25 05:43:38.401 INFO: train_e/atom_mae: 0.002847
train_e/atom_rmse: 0.003522
2025-06-25 05:43:38.401 INFO: train_e/atom_rmse: 0.003522
train_f_mae: 0.012327
2025-06-25 05:43:38.405 INFO: train_f_mae: 0.012327
train_f_rmse: 0.018142
2025-06-25 05:43:38.405 INFO: train_f_rmse: 0.018142
val_e/atom_mae: 0.008715
2025-06-25 05:43:38.408 INFO: val_e/atom_mae: 0.008715
val_e/atom_rmse: 0.009057
2025-06-25 05:43:38.408 INFO: val_e/atom_rmse: 0.009057
val_f_mae: 0.011675
2025-06-25 05:43:38.409 INFO: val_f_mae: 0.011675
val_f_rmse: 0.017341
2025-06-25 05:43:38.409 INFO: val_f_rmse: 0.017341
##### Step: 3 Learning rate: 0.008 #####
2025-06-25 05:44:23.669 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 0.4171, Val Loss: 0.3533
2025-06-25 05:44:23.669 INFO: Epoch 4, Train Loss: 0.4171, Val Loss: 0.3533
train_e/atom_mae: 0.003776
2025-06-25 05:44:23.670 INFO: train_e/atom_mae: 0.003776
train_e/atom_rmse: 0.004741
2025-06-25 05:44:23.670 INFO: train_e/atom_rmse: 0.004741
train_f_mae: 0.013233
2025-06-25 05:44:23.674 INFO: train_f_mae: 0.013233
train_f_rmse: 0.019746
2025-06-25 05:44:23.674 INFO: train_f_rmse: 0.019746
val_e/atom_mae: 0.003874
2025-06-25 05:44:23.677 INFO: val_e/atom_mae: 0.003874
val_e/atom_rmse: 0.004491
2025-06-25 05:44:23.677 INFO: val_e/atom_rmse: 0.004491
val_f_mae: 0.012509
2025-06-25 05:44:23.677 INFO: val_f_mae: 0.012509
val_f_rmse: 0.018134
2025-06-25 05:44:23.678 INFO: val_f_rmse: 0.018134
##### Step: 4 Learning rate: 0.01 #####
2025-06-25 05:45:08.733 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 0.5515, Val Loss: 0.4709
2025-06-25 05:45:08.734 INFO: Epoch 5, Train Loss: 0.5515, Val Loss: 0.4709
train_e/atom_mae: 0.003555
2025-06-25 05:45:08.735 INFO: train_e/atom_mae: 0.003555
train_e/atom_rmse: 0.004397
2025-06-25 05:45:08.735 INFO: train_e/atom_rmse: 0.004397
train_f_mae: 0.015192
2025-06-25 05:45:08.738 INFO: train_f_mae: 0.015192
train_f_rmse: 0.022980
2025-06-25 05:45:08.738 INFO: train_f_rmse: 0.022980
val_e/atom_mae: 0.002042
2025-06-25 05:45:08.741 INFO: val_e/atom_mae: 0.002042
val_e/atom_rmse: 0.002580
2025-06-25 05:45:08.742 INFO: val_e/atom_rmse: 0.002580
val_f_mae: 0.014777
2025-06-25 05:45:08.742 INFO: val_f_mae: 0.014777
val_f_rmse: 0.021513
2025-06-25 05:45:08.742 INFO: val_f_rmse: 0.021513
##### Step: 5 Learning rate: 0.01 #####
2025-06-25 05:45:53.869 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 0.4845, Val Loss: 0.4228
2025-06-25 05:45:53.869 INFO: Epoch 6, Train Loss: 0.4845, Val Loss: 0.4228
train_e/atom_mae: 0.004505
2025-06-25 05:45:53.870 INFO: train_e/atom_mae: 0.004505
train_e/atom_rmse: 0.005584
2025-06-25 05:45:53.870 INFO: train_e/atom_rmse: 0.005584
train_f_mae: 0.014134
2025-06-25 05:45:53.874 INFO: train_f_mae: 0.014134
train_f_rmse: 0.021138
2025-06-25 05:45:53.874 INFO: train_f_rmse: 0.021138
val_e/atom_mae: 0.006563
2025-06-25 05:45:53.876 INFO: val_e/atom_mae: 0.006563
val_e/atom_rmse: 0.007094
2025-06-25 05:45:53.877 INFO: val_e/atom_rmse: 0.007094
val_f_mae: 0.013122
2025-06-25 05:45:53.877 INFO: val_f_mae: 0.013122
val_f_rmse: 0.019024
2025-06-25 05:45:53.877 INFO: val_f_rmse: 0.019024
##### Step: 6 Learning rate: 0.01 #####
2025-06-25 05:46:38.908 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 0.4829, Val Loss: 0.4799
2025-06-25 05:46:38.909 INFO: Epoch 7, Train Loss: 0.4829, Val Loss: 0.4799
train_e/atom_mae: 0.003924
2025-06-25 05:46:38.910 INFO: train_e/atom_mae: 0.003924
train_e/atom_rmse: 0.004942
2025-06-25 05:46:38.910 INFO: train_e/atom_rmse: 0.004942
train_f_mae: 0.014103
2025-06-25 05:46:38.913 INFO: train_f_mae: 0.014103
train_f_rmse: 0.021292
2025-06-25 05:46:38.913 INFO: train_f_rmse: 0.021292
val_e/atom_mae: 0.003866
2025-06-25 05:46:38.916 INFO: val_e/atom_mae: 0.003866
val_e/atom_rmse: 0.004500
2025-06-25 05:46:38.917 INFO: val_e/atom_rmse: 0.004500
val_f_mae: 0.014328
2025-06-25 05:46:38.917 INFO: val_f_mae: 0.014328
val_f_rmse: 0.021340
2025-06-25 05:46:38.917 INFO: val_f_rmse: 0.021340
##### Step: 7 Learning rate: 0.01 #####
2025-06-25 05:47:24.064 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 0.5053, Val Loss: 0.4523
2025-06-25 05:47:24.064 INFO: Epoch 8, Train Loss: 0.5053, Val Loss: 0.4523
train_e/atom_mae: 0.003902
2025-06-25 05:47:24.065 INFO: train_e/atom_mae: 0.003902
train_e/atom_rmse: 0.004860
2025-06-25 05:47:24.065 INFO: train_e/atom_rmse: 0.004860
train_f_mae: 0.014467
2025-06-25 05:47:24.069 INFO: train_f_mae: 0.014467
train_f_rmse: 0.021834
2025-06-25 05:47:24.069 INFO: train_f_rmse: 0.021834
val_e/atom_mae: 0.003210
2025-06-25 05:47:24.071 INFO: val_e/atom_mae: 0.003210
val_e/atom_rmse: 0.003514
2025-06-25 05:47:24.072 INFO: val_e/atom_rmse: 0.003514
val_f_mae: 0.013868
2025-06-25 05:47:24.072 INFO: val_f_mae: 0.013868
val_f_rmse: 0.020913
2025-06-25 05:47:24.072 INFO: val_f_rmse: 0.020913
##### Step: 8 Learning rate: 0.01 #####
2025-06-25 05:48:09.016 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 0.5806, Val Loss: 0.4167
2025-06-25 05:48:09.016 INFO: Epoch 9, Train Loss: 0.5806, Val Loss: 0.4167
train_e/atom_mae: 0.003822
2025-06-25 05:48:09.017 INFO: train_e/atom_mae: 0.003822
train_e/atom_rmse: 0.004766
2025-06-25 05:48:09.017 INFO: train_e/atom_rmse: 0.004766
train_f_mae: 0.015063
2025-06-25 05:48:09.021 INFO: train_f_mae: 0.015063
train_f_rmse: 0.023519
2025-06-25 05:48:09.021 INFO: train_f_rmse: 0.023519
val_e/atom_mae: 0.002030
2025-06-25 05:48:09.024 INFO: val_e/atom_mae: 0.002030
val_e/atom_rmse: 0.002367
2025-06-25 05:48:09.024 INFO: val_e/atom_rmse: 0.002367
val_f_mae: 0.013498
2025-06-25 05:48:09.025 INFO: val_f_mae: 0.013498
val_f_rmse: 0.020247
2025-06-25 05:48:09.025 INFO: val_f_rmse: 0.020247
##### Step: 9 Learning rate: 0.01 #####
2025-06-25 05:48:54.132 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 0.5304, Val Loss: 0.4922
2025-06-25 05:48:54.132 INFO: Epoch 10, Train Loss: 0.5304, Val Loss: 0.4922
train_e/atom_mae: 0.004526
2025-06-25 05:48:54.133 INFO: train_e/atom_mae: 0.004526
train_e/atom_rmse: 0.005550
2025-06-25 05:48:54.133 INFO: train_e/atom_rmse: 0.005550
train_f_mae: 0.014486
2025-06-25 05:48:54.137 INFO: train_f_mae: 0.014486
train_f_rmse: 0.022206
2025-06-25 05:48:54.137 INFO: train_f_rmse: 0.022206
val_e/atom_mae: 0.002219
2025-06-25 05:48:54.140 INFO: val_e/atom_mae: 0.002219
val_e/atom_rmse: 0.002633
2025-06-25 05:48:54.140 INFO: val_e/atom_rmse: 0.002633
val_f_mae: 0.014536
2025-06-25 05:48:54.140 INFO: val_f_mae: 0.014536
val_f_rmse: 0.021995
2025-06-25 05:48:54.141 INFO: val_f_rmse: 0.021995
##### Step: 10 Learning rate: 0.01 #####
2025-06-25 05:49:39.168 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 0.5483, Val Loss: 0.6239
2025-06-25 05:49:39.169 INFO: Epoch 11, Train Loss: 0.5483, Val Loss: 0.6239
train_e/atom_mae: 0.003387
2025-06-25 05:49:39.169 INFO: train_e/atom_mae: 0.003387
train_e/atom_rmse: 0.004230
2025-06-25 05:49:39.170 INFO: train_e/atom_rmse: 0.004230
train_f_mae: 0.015236
2025-06-25 05:49:39.173 INFO: train_f_mae: 0.015236
train_f_rmse: 0.022949
2025-06-25 05:49:39.173 INFO: train_f_rmse: 0.022949
val_e/atom_mae: 0.002412
2025-06-25 05:49:39.176 INFO: val_e/atom_mae: 0.002412
val_e/atom_rmse: 0.002521
2025-06-25 05:49:39.177 INFO: val_e/atom_rmse: 0.002521
val_f_mae: 0.016825
2025-06-25 05:49:39.177 INFO: val_f_mae: 0.016825
val_f_rmse: 0.024823
2025-06-25 05:49:39.177 INFO: val_f_rmse: 0.024823
##### Step: 11 Learning rate: 0.01 #####
2025-06-25 05:50:24.325 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 0.5550, Val Loss: 0.4607
2025-06-25 05:50:24.325 INFO: Epoch 12, Train Loss: 0.5550, Val Loss: 0.4607
train_e/atom_mae: 0.003842
2025-06-25 05:50:24.326 INFO: train_e/atom_mae: 0.003842
train_e/atom_rmse: 0.004740
2025-06-25 05:50:24.326 INFO: train_e/atom_rmse: 0.004740
train_f_mae: 0.014891
2025-06-25 05:50:24.330 INFO: train_f_mae: 0.014891
train_f_rmse: 0.022975
2025-06-25 05:50:24.330 INFO: train_f_rmse: 0.022975
val_e/atom_mae: 0.002766
2025-06-25 05:50:24.333 INFO: val_e/atom_mae: 0.002766
val_e/atom_rmse: 0.003507
2025-06-25 05:50:24.333 INFO: val_e/atom_rmse: 0.003507
val_f_mae: 0.014188
2025-06-25 05:50:24.334 INFO: val_f_mae: 0.014188
val_f_rmse: 0.021115
2025-06-25 05:50:24.334 INFO: val_f_rmse: 0.021115
##### Step: 12 Learning rate: 0.01 #####
2025-06-25 05:51:09.369 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 0.5143, Val Loss: 0.3497
2025-06-25 05:51:09.369 INFO: Epoch 13, Train Loss: 0.5143, Val Loss: 0.3497
train_e/atom_mae: 0.003783
2025-06-25 05:51:09.370 INFO: train_e/atom_mae: 0.003783
train_e/atom_rmse: 0.004804
2025-06-25 05:51:09.370 INFO: train_e/atom_rmse: 0.004804
train_f_mae: 0.014408
2025-06-25 05:51:09.374 INFO: train_f_mae: 0.014408
train_f_rmse: 0.022053
2025-06-25 05:51:09.374 INFO: train_f_rmse: 0.022053
val_e/atom_mae: 0.002454
2025-06-25 05:51:09.377 INFO: val_e/atom_mae: 0.002454
val_e/atom_rmse: 0.003174
2025-06-25 05:51:09.377 INFO: val_e/atom_rmse: 0.003174
val_f_mae: 0.012505
2025-06-25 05:51:09.378 INFO: val_f_mae: 0.012505
val_f_rmse: 0.018371
2025-06-25 05:51:09.378 INFO: val_f_rmse: 0.018371
##### Step: 13 Learning rate: 0.01 #####
2025-06-25 05:51:54.536 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 0.4951, Val Loss: 0.5062
2025-06-25 05:51:54.536 INFO: Epoch 14, Train Loss: 0.4951, Val Loss: 0.5062
train_e/atom_mae: 0.003400
2025-06-25 05:51:54.537 INFO: train_e/atom_mae: 0.003400
train_e/atom_rmse: 0.004274
2025-06-25 05:51:54.537 INFO: train_e/atom_rmse: 0.004274
train_f_mae: 0.014346
2025-06-25 05:51:54.541 INFO: train_f_mae: 0.014346
train_f_rmse: 0.021747
2025-06-25 05:51:54.541 INFO: train_f_rmse: 0.021747
val_e/atom_mae: 0.002451
2025-06-25 05:51:54.544 INFO: val_e/atom_mae: 0.002451
val_e/atom_rmse: 0.003117
2025-06-25 05:51:54.544 INFO: val_e/atom_rmse: 0.003117
val_f_mae: 0.014819
2025-06-25 05:51:54.545 INFO: val_f_mae: 0.014819
val_f_rmse: 0.022236
2025-06-25 05:51:54.545 INFO: val_f_rmse: 0.022236
##### Step: 14 Learning rate: 0.01 #####
2025-06-25 05:52:39.588 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 0.5598, Val Loss: 0.6260
2025-06-25 05:52:39.589 INFO: Epoch 15, Train Loss: 0.5598, Val Loss: 0.6260
train_e/atom_mae: 0.003526
2025-06-25 05:52:39.590 INFO: train_e/atom_mae: 0.003526
train_e/atom_rmse: 0.004373
2025-06-25 05:52:39.590 INFO: train_e/atom_rmse: 0.004373
train_f_mae: 0.015401
2025-06-25 05:52:39.593 INFO: train_f_mae: 0.015401
train_f_rmse: 0.023166
2025-06-25 05:52:39.594 INFO: train_f_rmse: 0.023166
val_e/atom_mae: 0.005068
2025-06-25 05:52:39.596 INFO: val_e/atom_mae: 0.005068
val_e/atom_rmse: 0.005972
2025-06-25 05:52:39.597 INFO: val_e/atom_rmse: 0.005972
val_f_mae: 0.016166
2025-06-25 05:52:39.597 INFO: val_f_mae: 0.016166
val_f_rmse: 0.024143
2025-06-25 05:52:39.597 INFO: val_f_rmse: 0.024143
##### Step: 15 Learning rate: 0.01 #####
2025-06-25 05:53:24.836 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 0.5482, Val Loss: 0.3795
2025-06-25 05:53:24.837 INFO: Epoch 16, Train Loss: 0.5482, Val Loss: 0.3795
train_e/atom_mae: 0.004788
2025-06-25 05:53:24.838 INFO: train_e/atom_mae: 0.004788
train_e/atom_rmse: 0.005900
2025-06-25 05:53:24.838 INFO: train_e/atom_rmse: 0.005900
train_f_mae: 0.014892
2025-06-25 05:53:24.841 INFO: train_f_mae: 0.014892
train_f_rmse: 0.022496
2025-06-25 05:53:24.842 INFO: train_f_rmse: 0.022496
val_e/atom_mae: 0.002498
2025-06-25 05:53:24.844 INFO: val_e/atom_mae: 0.002498
val_e/atom_rmse: 0.002959
2025-06-25 05:53:24.845 INFO: val_e/atom_rmse: 0.002959
val_f_mae: 0.012463
2025-06-25 05:53:24.845 INFO: val_f_mae: 0.012463
val_f_rmse: 0.019206
2025-06-25 05:53:24.845 INFO: val_f_rmse: 0.019206
##### Step: 16 Learning rate: 0.01 #####
2025-06-25 05:54:09.889 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 0.4570, Val Loss: 0.4963
2025-06-25 05:54:09.889 INFO: Epoch 17, Train Loss: 0.4570, Val Loss: 0.4963
train_e/atom_mae: 0.004103
2025-06-25 05:54:09.890 INFO: train_e/atom_mae: 0.004103
train_e/atom_rmse: 0.005031
2025-06-25 05:54:09.890 INFO: train_e/atom_rmse: 0.005031
train_f_mae: 0.013759
2025-06-25 05:54:09.894 INFO: train_f_mae: 0.013759
train_f_rmse: 0.020649
2025-06-25 05:54:09.894 INFO: train_f_rmse: 0.020649
val_e/atom_mae: 0.004183
2025-06-25 05:54:09.897 INFO: val_e/atom_mae: 0.004183
val_e/atom_rmse: 0.004643
2025-06-25 05:54:09.897 INFO: val_e/atom_rmse: 0.004643
val_f_mae: 0.014673
2025-06-25 05:54:09.897 INFO: val_f_mae: 0.014673
val_f_rmse: 0.021684
2025-06-25 05:54:09.898 INFO: val_f_rmse: 0.021684
##### Step: 17 Learning rate: 0.01 #####
2025-06-25 05:54:55.097 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 0.5548, Val Loss: 0.4800
2025-06-25 05:54:55.097 INFO: Epoch 18, Train Loss: 0.5548, Val Loss: 0.4800
train_e/atom_mae: 0.003861
2025-06-25 05:54:55.098 INFO: train_e/atom_mae: 0.003861
train_e/atom_rmse: 0.004816
2025-06-25 05:54:55.098 INFO: train_e/atom_rmse: 0.004816
train_f_mae: 0.015282
2025-06-25 05:54:55.102 INFO: train_f_mae: 0.015282
train_f_rmse: 0.022952
2025-06-25 05:54:55.102 INFO: train_f_rmse: 0.022952
val_e/atom_mae: 0.005910
2025-06-25 05:54:55.105 INFO: val_e/atom_mae: 0.005910
val_e/atom_rmse: 0.006671
2025-06-25 05:54:55.105 INFO: val_e/atom_rmse: 0.006671
val_f_mae: 0.013467
2025-06-25 05:54:55.106 INFO: val_f_mae: 0.013467
val_f_rmse: 0.020644
2025-06-25 05:54:55.106 INFO: val_f_rmse: 0.020644
##### Step: 18 Learning rate: 0.01 #####
2025-06-25 05:55:40.157 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 0.4686, Val Loss: 0.4397
2025-06-25 05:55:40.158 INFO: Epoch 19, Train Loss: 0.4686, Val Loss: 0.4397
train_e/atom_mae: 0.003448
2025-06-25 05:55:40.159 INFO: train_e/atom_mae: 0.003448
train_e/atom_rmse: 0.004310
2025-06-25 05:55:40.159 INFO: train_e/atom_rmse: 0.004310
train_f_mae: 0.014086
2025-06-25 05:55:40.162 INFO: train_f_mae: 0.014086
train_f_rmse: 0.021122
2025-06-25 05:55:40.163 INFO: train_f_rmse: 0.021122
val_e/atom_mae: 0.002202
2025-06-25 05:55:40.165 INFO: val_e/atom_mae: 0.002202
val_e/atom_rmse: 0.002443
2025-06-25 05:55:40.166 INFO: val_e/atom_rmse: 0.002443
val_f_mae: 0.014620
2025-06-25 05:55:40.166 INFO: val_f_mae: 0.014620
val_f_rmse: 0.020797
2025-06-25 05:55:40.166 INFO: val_f_rmse: 0.020797
##### Step: 19 Learning rate: 0.01 #####
2025-06-25 05:56:25.336 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 0.5606, Val Loss: 0.5151
2025-06-25 05:56:25.336 INFO: Epoch 20, Train Loss: 0.5606, Val Loss: 0.5151
train_e/atom_mae: 0.003815
2025-06-25 05:56:25.337 INFO: train_e/atom_mae: 0.003815
train_e/atom_rmse: 0.004821
2025-06-25 05:56:25.337 INFO: train_e/atom_rmse: 0.004821
train_f_mae: 0.015315
2025-06-25 05:56:25.341 INFO: train_f_mae: 0.015315
train_f_rmse: 0.023076
2025-06-25 05:56:25.341 INFO: train_f_rmse: 0.023076
val_e/atom_mae: 0.002666
2025-06-25 05:56:25.344 INFO: val_e/atom_mae: 0.002666
val_e/atom_rmse: 0.002842
2025-06-25 05:56:25.344 INFO: val_e/atom_rmse: 0.002842
val_f_mae: 0.015630
2025-06-25 05:56:25.345 INFO: val_f_mae: 0.015630
val_f_rmse: 0.022479
2025-06-25 05:56:25.345 INFO: val_f_rmse: 0.022479
##### Step: 20 Learning rate: 0.005 #####
2025-06-25 05:57:10.470 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 0.3534, Val Loss: 0.2837
2025-06-25 05:57:10.470 INFO: Epoch 21, Train Loss: 0.3534, Val Loss: 0.2837
train_e/atom_mae: 0.003011
2025-06-25 05:57:10.471 INFO: train_e/atom_mae: 0.003011
train_e/atom_rmse: 0.003854
2025-06-25 05:57:10.471 INFO: train_e/atom_rmse: 0.003854
train_f_mae: 0.012479
2025-06-25 05:57:10.475 INFO: train_f_mae: 0.012479
train_f_rmse: 0.018314
2025-06-25 05:57:10.475 INFO: train_f_rmse: 0.018314
val_e/atom_mae: 0.001699
2025-06-25 05:57:10.478 INFO: val_e/atom_mae: 0.001699
val_e/atom_rmse: 0.001754
2025-06-25 05:57:10.478 INFO: val_e/atom_rmse: 0.001754
val_f_mae: 0.011695
2025-06-25 05:57:10.478 INFO: val_f_mae: 0.011695
val_f_rmse: 0.016732
2025-06-25 05:57:10.479 INFO: val_f_rmse: 0.016732
##### Step: 21 Learning rate: 0.005 #####
2025-06-25 05:57:55.644 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.3012, Val Loss: 0.2959
2025-06-25 05:57:55.644 INFO: Epoch 22, Train Loss: 0.3012, Val Loss: 0.2959
train_e/atom_mae: 0.002265
2025-06-25 05:57:55.645 INFO: train_e/atom_mae: 0.002265
train_e/atom_rmse: 0.002808
2025-06-25 05:57:55.645 INFO: train_e/atom_rmse: 0.002808
train_f_mae: 0.011684
2025-06-25 05:57:55.649 INFO: train_f_mae: 0.011684
train_f_rmse: 0.017079
2025-06-25 05:57:55.649 INFO: train_f_rmse: 0.017079
val_e/atom_mae: 0.002416
2025-06-25 05:57:55.652 INFO: val_e/atom_mae: 0.002416
val_e/atom_rmse: 0.003002
2025-06-25 05:57:55.652 INFO: val_e/atom_rmse: 0.003002
val_f_mae: 0.011640
2025-06-25 05:57:55.653 INFO: val_f_mae: 0.011640
val_f_rmse: 0.016882
2025-06-25 05:57:55.653 INFO: val_f_rmse: 0.016882
##### Step: 22 Learning rate: 0.005 #####
2025-06-25 05:58:40.779 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.2908, Val Loss: 0.3112
2025-06-25 05:58:40.779 INFO: Epoch 23, Train Loss: 0.2908, Val Loss: 0.3112
train_e/atom_mae: 0.002338
2025-06-25 05:58:40.780 INFO: train_e/atom_mae: 0.002338
train_e/atom_rmse: 0.002913
2025-06-25 05:58:40.780 INFO: train_e/atom_rmse: 0.002913
train_f_mae: 0.011503
2025-06-25 05:58:40.784 INFO: train_f_mae: 0.011503
train_f_rmse: 0.016750
2025-06-25 05:58:40.784 INFO: train_f_rmse: 0.016750
val_e/atom_mae: 0.001630
2025-06-25 05:58:40.787 INFO: val_e/atom_mae: 0.001630
val_e/atom_rmse: 0.001745
2025-06-25 05:58:40.787 INFO: val_e/atom_rmse: 0.001745
val_f_mae: 0.012341
2025-06-25 05:58:40.787 INFO: val_f_mae: 0.012341
val_f_rmse: 0.017537
2025-06-25 05:58:40.788 INFO: val_f_rmse: 0.017537
##### Step: 23 Learning rate: 0.005 #####
2025-06-25 05:59:25.901 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.2892, Val Loss: 0.2995
2025-06-25 05:59:25.902 INFO: Epoch 24, Train Loss: 0.2892, Val Loss: 0.2995
train_e/atom_mae: 0.002596
2025-06-25 05:59:25.903 INFO: train_e/atom_mae: 0.002596
train_e/atom_rmse: 0.003251
2025-06-25 05:59:25.903 INFO: train_e/atom_rmse: 0.003251
train_f_mae: 0.011536
2025-06-25 05:59:25.906 INFO: train_f_mae: 0.011536
train_f_rmse: 0.016627
2025-06-25 05:59:25.907 INFO: train_f_rmse: 0.016627
val_e/atom_mae: 0.006107
2025-06-25 05:59:25.909 INFO: val_e/atom_mae: 0.006107
val_e/atom_rmse: 0.006316
2025-06-25 05:59:25.910 INFO: val_e/atom_rmse: 0.006316
val_f_mae: 0.011171
2025-06-25 05:59:25.910 INFO: val_f_mae: 0.011171
val_f_rmse: 0.015850
2025-06-25 05:59:25.910 INFO: val_f_rmse: 0.015850
##### Step: 24 Learning rate: 0.005 #####
2025-06-25 06:00:11.006 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.2933, Val Loss: 0.2502
2025-06-25 06:00:11.007 INFO: Epoch 25, Train Loss: 0.2933, Val Loss: 0.2502
train_e/atom_mae: 0.002943
2025-06-25 06:00:11.007 INFO: train_e/atom_mae: 0.002943
train_e/atom_rmse: 0.003592
2025-06-25 06:00:11.008 INFO: train_e/atom_rmse: 0.003592
train_f_mae: 0.011335
2025-06-25 06:00:11.011 INFO: train_f_mae: 0.011335
train_f_rmse: 0.016663
2025-06-25 06:00:11.012 INFO: train_f_rmse: 0.016663
val_e/atom_mae: 0.003140
2025-06-25 06:00:11.014 INFO: val_e/atom_mae: 0.003140
val_e/atom_rmse: 0.003581
2025-06-25 06:00:11.015 INFO: val_e/atom_rmse: 0.003581
val_f_mae: 0.010803
2025-06-25 06:00:11.015 INFO: val_f_mae: 0.010803
val_f_rmse: 0.015320
2025-06-25 06:00:11.015 INFO: val_f_rmse: 0.015320
##### Step: 25 Learning rate: 0.005 #####
2025-06-25 06:00:56.059 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.2934, Val Loss: 0.2563
2025-06-25 06:00:56.059 INFO: Epoch 26, Train Loss: 0.2934, Val Loss: 0.2563
train_e/atom_mae: 0.002501
2025-06-25 06:00:56.060 INFO: train_e/atom_mae: 0.002501
train_e/atom_rmse: 0.003098
2025-06-25 06:00:56.060 INFO: train_e/atom_rmse: 0.003098
train_f_mae: 0.011501
2025-06-25 06:00:56.064 INFO: train_f_mae: 0.011501
train_f_rmse: 0.016786
2025-06-25 06:00:56.064 INFO: train_f_rmse: 0.016786
val_e/atom_mae: 0.002480
2025-06-25 06:00:56.067 INFO: val_e/atom_mae: 0.002480
val_e/atom_rmse: 0.002911
2025-06-25 06:00:56.067 INFO: val_e/atom_rmse: 0.002911
val_f_mae: 0.011166
2025-06-25 06:00:56.068 INFO: val_f_mae: 0.011166
val_f_rmse: 0.015685
2025-06-25 06:00:56.068 INFO: val_f_rmse: 0.015685
##### Step: 26 Learning rate: 0.005 #####
2025-06-25 06:01:41.237 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.3076, Val Loss: 0.2603
2025-06-25 06:01:41.237 INFO: Epoch 27, Train Loss: 0.3076, Val Loss: 0.2603
train_e/atom_mae: 0.002802
2025-06-25 06:01:41.238 INFO: train_e/atom_mae: 0.002802
train_e/atom_rmse: 0.003456
2025-06-25 06:01:41.238 INFO: train_e/atom_rmse: 0.003456
train_f_mae: 0.011745
2025-06-25 06:01:41.242 INFO: train_f_mae: 0.011745
train_f_rmse: 0.017121
2025-06-25 06:01:41.242 INFO: train_f_rmse: 0.017121
val_e/atom_mae: 0.001748
2025-06-25 06:01:41.245 INFO: val_e/atom_mae: 0.001748
val_e/atom_rmse: 0.001890
2025-06-25 06:01:41.245 INFO: val_e/atom_rmse: 0.001890
val_f_mae: 0.011040
2025-06-25 06:01:41.246 INFO: val_f_mae: 0.011040
val_f_rmse: 0.015999
2025-06-25 06:01:41.246 INFO: val_f_rmse: 0.015999
##### Step: 27 Learning rate: 0.005 #####
2025-06-25 06:02:26.284 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.2702, Val Loss: 0.2714
2025-06-25 06:02:26.284 INFO: Epoch 28, Train Loss: 0.2702, Val Loss: 0.2714
train_e/atom_mae: 0.002417
2025-06-25 06:02:26.285 INFO: train_e/atom_mae: 0.002417
train_e/atom_rmse: 0.003106
2025-06-25 06:02:26.285 INFO: train_e/atom_rmse: 0.003106
train_f_mae: 0.011033
2025-06-25 06:02:26.289 INFO: train_f_mae: 0.011033
train_f_rmse: 0.016078
2025-06-25 06:02:26.289 INFO: train_f_rmse: 0.016078
val_e/atom_mae: 0.002323
2025-06-25 06:02:26.292 INFO: val_e/atom_mae: 0.002323
val_e/atom_rmse: 0.002566
2025-06-25 06:02:26.292 INFO: val_e/atom_rmse: 0.002566
val_f_mae: 0.011447
2025-06-25 06:02:26.293 INFO: val_f_mae: 0.011447
val_f_rmse: 0.016230
2025-06-25 06:02:26.293 INFO: val_f_rmse: 0.016230
##### Step: 28 Learning rate: 0.005 #####
2025-06-25 06:03:11.490 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.2845, Val Loss: 0.2442
2025-06-25 06:03:11.490 INFO: Epoch 29, Train Loss: 0.2845, Val Loss: 0.2442
train_e/atom_mae: 0.002498
2025-06-25 06:03:11.491 INFO: train_e/atom_mae: 0.002498
train_e/atom_rmse: 0.003168
2025-06-25 06:03:11.491 INFO: train_e/atom_rmse: 0.003168
train_f_mae: 0.011329
2025-06-25 06:03:11.495 INFO: train_f_mae: 0.011329
train_f_rmse: 0.016504
2025-06-25 06:03:11.495 INFO: train_f_rmse: 0.016504
val_e/atom_mae: 0.001456
2025-06-25 06:03:11.498 INFO: val_e/atom_mae: 0.001456
val_e/atom_rmse: 0.001952
2025-06-25 06:03:11.498 INFO: val_e/atom_rmse: 0.001952
val_f_mae: 0.010834
2025-06-25 06:03:11.499 INFO: val_f_mae: 0.010834
val_f_rmse: 0.015479
2025-06-25 06:03:11.499 INFO: val_f_rmse: 0.015479
##### Step: 29 Learning rate: 0.005 #####
2025-06-25 06:03:56.490 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.2734, Val Loss: 0.3238
2025-06-25 06:03:56.490 INFO: Epoch 30, Train Loss: 0.2734, Val Loss: 0.3238
train_e/atom_mae: 0.002033
2025-06-25 06:03:56.491 INFO: train_e/atom_mae: 0.002033
train_e/atom_rmse: 0.002569
2025-06-25 06:03:56.491 INFO: train_e/atom_rmse: 0.002569
train_f_mae: 0.011171
2025-06-25 06:03:56.495 INFO: train_f_mae: 0.011171
train_f_rmse: 0.016292
2025-06-25 06:03:56.495 INFO: train_f_rmse: 0.016292
val_e/atom_mae: 0.001548
2025-06-25 06:03:56.498 INFO: val_e/atom_mae: 0.001548
val_e/atom_rmse: 0.002009
2025-06-25 06:03:56.498 INFO: val_e/atom_rmse: 0.002009
val_f_mae: 0.012029
2025-06-25 06:03:56.499 INFO: val_f_mae: 0.012029
val_f_rmse: 0.017857
2025-06-25 06:03:56.499 INFO: val_f_rmse: 0.017857
##### Step: 30 Learning rate: 0.005 #####
2025-06-25 06:04:41.601 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.2591, Val Loss: 0.2547
2025-06-25 06:04:41.602 INFO: Epoch 31, Train Loss: 0.2591, Val Loss: 0.2547
train_e/atom_mae: 0.002423
2025-06-25 06:04:41.602 INFO: train_e/atom_mae: 0.002423
train_e/atom_rmse: 0.002934
2025-06-25 06:04:41.603 INFO: train_e/atom_rmse: 0.002934
train_f_mae: 0.010932
2025-06-25 06:04:41.606 INFO: train_f_mae: 0.010932
train_f_rmse: 0.015771
2025-06-25 06:04:41.606 INFO: train_f_rmse: 0.015771
val_e/atom_mae: 0.002201
2025-06-25 06:04:41.609 INFO: val_e/atom_mae: 0.002201
val_e/atom_rmse: 0.002594
2025-06-25 06:04:41.610 INFO: val_e/atom_rmse: 0.002594
val_f_mae: 0.010883
2025-06-25 06:04:41.610 INFO: val_f_mae: 0.010883
val_f_rmse: 0.015701
2025-06-25 06:04:41.610 INFO: val_f_rmse: 0.015701
##### Step: 31 Learning rate: 0.005 #####
2025-06-25 06:05:26.641 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.2737, Val Loss: 0.2860
2025-06-25 06:05:26.641 INFO: Epoch 32, Train Loss: 0.2737, Val Loss: 0.2860
train_e/atom_mae: 0.002060
2025-06-25 06:05:26.642 INFO: train_e/atom_mae: 0.002060
train_e/atom_rmse: 0.002609
2025-06-25 06:05:26.643 INFO: train_e/atom_rmse: 0.002609
train_f_mae: 0.011243
2025-06-25 06:05:26.646 INFO: train_f_mae: 0.011243
train_f_rmse: 0.016292
2025-06-25 06:05:26.646 INFO: train_f_rmse: 0.016292
val_e/atom_mae: 0.001323
2025-06-25 06:05:26.649 INFO: val_e/atom_mae: 0.001323
val_e/atom_rmse: 0.001679
2025-06-25 06:05:26.649 INFO: val_e/atom_rmse: 0.001679
val_f_mae: 0.011323
2025-06-25 06:05:26.650 INFO: val_f_mae: 0.011323
val_f_rmse: 0.016811
2025-06-25 06:05:26.650 INFO: val_f_rmse: 0.016811
##### Step: 32 Learning rate: 0.005 #####
2025-06-25 06:06:11.817 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.2736, Val Loss: 0.3309
2025-06-25 06:06:11.817 INFO: Epoch 33, Train Loss: 0.2736, Val Loss: 0.3309
train_e/atom_mae: 0.002215
2025-06-25 06:06:11.818 INFO: train_e/atom_mae: 0.002215
train_e/atom_rmse: 0.002789
2025-06-25 06:06:11.818 INFO: train_e/atom_rmse: 0.002789
train_f_mae: 0.011119
2025-06-25 06:06:11.822 INFO: train_f_mae: 0.011119
train_f_rmse: 0.016255
2025-06-25 06:06:11.822 INFO: train_f_rmse: 0.016255
val_e/atom_mae: 0.001174
2025-06-25 06:06:11.825 INFO: val_e/atom_mae: 0.001174
val_e/atom_rmse: 0.001343
2025-06-25 06:06:11.825 INFO: val_e/atom_rmse: 0.001343
val_f_mae: 0.011589
2025-06-25 06:06:11.826 INFO: val_f_mae: 0.011589
val_f_rmse: 0.018132
2025-06-25 06:06:11.826 INFO: val_f_rmse: 0.018132
##### Step: 33 Learning rate: 0.005 #####
2025-06-25 06:06:56.804 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.2888, Val Loss: 0.3174
2025-06-25 06:06:56.804 INFO: Epoch 34, Train Loss: 0.2888, Val Loss: 0.3174
train_e/atom_mae: 0.002053
2025-06-25 06:06:56.805 INFO: train_e/atom_mae: 0.002053
train_e/atom_rmse: 0.002613
2025-06-25 06:06:56.805 INFO: train_e/atom_rmse: 0.002613
train_f_mae: 0.011280
2025-06-25 06:06:56.809 INFO: train_f_mae: 0.011280
train_f_rmse: 0.016750
2025-06-25 06:06:56.809 INFO: train_f_rmse: 0.016750
val_e/atom_mae: 0.002670
2025-06-25 06:06:56.812 INFO: val_e/atom_mae: 0.002670
val_e/atom_rmse: 0.003089
2025-06-25 06:06:56.812 INFO: val_e/atom_rmse: 0.003089
val_f_mae: 0.011991
2025-06-25 06:06:56.813 INFO: val_f_mae: 0.011991
val_f_rmse: 0.017490
2025-06-25 06:06:56.813 INFO: val_f_rmse: 0.017490
##### Step: 34 Learning rate: 0.005 #####
2025-06-25 06:07:42.011 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.2726, Val Loss: 0.2498
2025-06-25 06:07:42.011 INFO: Epoch 35, Train Loss: 0.2726, Val Loss: 0.2498
train_e/atom_mae: 0.002497
2025-06-25 06:07:42.012 INFO: train_e/atom_mae: 0.002497
train_e/atom_rmse: 0.003076
2025-06-25 06:07:42.013 INFO: train_e/atom_rmse: 0.003076
train_f_mae: 0.010919
2025-06-25 06:07:42.016 INFO: train_f_mae: 0.010919
train_f_rmse: 0.016159
2025-06-25 06:07:42.016 INFO: train_f_rmse: 0.016159
val_e/atom_mae: 0.001731
2025-06-25 06:07:42.019 INFO: val_e/atom_mae: 0.001731
val_e/atom_rmse: 0.002093
2025-06-25 06:07:42.020 INFO: val_e/atom_rmse: 0.002093
val_f_mae: 0.010919
2025-06-25 06:07:42.020 INFO: val_f_mae: 0.010919
val_f_rmse: 0.015636
2025-06-25 06:07:42.020 INFO: val_f_rmse: 0.015636
##### Step: 35 Learning rate: 0.005 #####
2025-06-25 06:08:27.044 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.2855, Val Loss: 0.3057
2025-06-25 06:08:27.045 INFO: Epoch 36, Train Loss: 0.2855, Val Loss: 0.3057
train_e/atom_mae: 0.002108
2025-06-25 06:08:27.046 INFO: train_e/atom_mae: 0.002108
train_e/atom_rmse: 0.002617
2025-06-25 06:08:27.046 INFO: train_e/atom_rmse: 0.002617
train_f_mae: 0.011387
2025-06-25 06:08:27.049 INFO: train_f_mae: 0.011387
train_f_rmse: 0.016650
2025-06-25 06:08:27.049 INFO: train_f_rmse: 0.016650
val_e/atom_mae: 0.001765
2025-06-25 06:08:27.052 INFO: val_e/atom_mae: 0.001765
val_e/atom_rmse: 0.002151
2025-06-25 06:08:27.053 INFO: val_e/atom_rmse: 0.002151
val_f_mae: 0.011907
2025-06-25 06:08:27.053 INFO: val_f_mae: 0.011907
val_f_rmse: 0.017324
2025-06-25 06:08:27.053 INFO: val_f_rmse: 0.017324
##### Step: 36 Learning rate: 0.005 #####
2025-06-25 06:09:12.242 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.2806, Val Loss: 0.2593
2025-06-25 06:09:12.243 INFO: Epoch 37, Train Loss: 0.2806, Val Loss: 0.2593
train_e/atom_mae: 0.002008
2025-06-25 06:09:12.244 INFO: train_e/atom_mae: 0.002008
train_e/atom_rmse: 0.002472
2025-06-25 06:09:12.244 INFO: train_e/atom_rmse: 0.002472
train_f_mae: 0.011318
2025-06-25 06:09:12.247 INFO: train_f_mae: 0.011318
train_f_rmse: 0.016530
2025-06-25 06:09:12.247 INFO: train_f_rmse: 0.016530
val_e/atom_mae: 0.001774
2025-06-25 06:09:12.250 INFO: val_e/atom_mae: 0.001774
val_e/atom_rmse: 0.001878
2025-06-25 06:09:12.251 INFO: val_e/atom_rmse: 0.001878
val_f_mae: 0.011281
2025-06-25 06:09:12.251 INFO: val_f_mae: 0.011281
val_f_rmse: 0.015971
2025-06-25 06:09:12.251 INFO: val_f_rmse: 0.015971
##### Step: 37 Learning rate: 0.005 #####
2025-06-25 06:09:57.310 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.2789, Val Loss: 0.2560
2025-06-25 06:09:57.310 INFO: Epoch 38, Train Loss: 0.2789, Val Loss: 0.2560
train_e/atom_mae: 0.001968
2025-06-25 06:09:57.311 INFO: train_e/atom_mae: 0.001968
train_e/atom_rmse: 0.002513
2025-06-25 06:09:57.311 INFO: train_e/atom_rmse: 0.002513
train_f_mae: 0.011265
2025-06-25 06:09:57.315 INFO: train_f_mae: 0.011265
train_f_rmse: 0.016470
2025-06-25 06:09:57.315 INFO: train_f_rmse: 0.016470
val_e/atom_mae: 0.002185
2025-06-25 06:09:57.318 INFO: val_e/atom_mae: 0.002185
val_e/atom_rmse: 0.002394
2025-06-25 06:09:57.318 INFO: val_e/atom_rmse: 0.002394
val_f_mae: 0.010995
2025-06-25 06:09:57.318 INFO: val_f_mae: 0.010995
val_f_rmse: 0.015780
2025-06-25 06:09:57.319 INFO: val_f_rmse: 0.015780
##### Step: 38 Learning rate: 0.005 #####
2025-06-25 06:10:42.457 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.2708, Val Loss: 0.2683
2025-06-25 06:10:42.457 INFO: Epoch 39, Train Loss: 0.2708, Val Loss: 0.2683
train_e/atom_mae: 0.001904
2025-06-25 06:10:42.458 INFO: train_e/atom_mae: 0.001904
train_e/atom_rmse: 0.002345
2025-06-25 06:10:42.458 INFO: train_e/atom_rmse: 0.002345
train_f_mae: 0.011234
2025-06-25 06:10:42.462 INFO: train_f_mae: 0.011234
train_f_rmse: 0.016252
2025-06-25 06:10:42.462 INFO: train_f_rmse: 0.016252
val_e/atom_mae: 0.001706
2025-06-25 06:10:42.465 INFO: val_e/atom_mae: 0.001706
val_e/atom_rmse: 0.002128
2025-06-25 06:10:42.465 INFO: val_e/atom_rmse: 0.002128
val_f_mae: 0.011494
2025-06-25 06:10:42.465 INFO: val_f_mae: 0.011494
val_f_rmse: 0.016212
2025-06-25 06:10:42.465 INFO: val_f_rmse: 0.016212
##### Step: 39 Learning rate: 0.005 #####
2025-06-25 06:11:27.519 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.2554, Val Loss: 0.2698
2025-06-25 06:11:27.519 INFO: Epoch 40, Train Loss: 0.2554, Val Loss: 0.2698
train_e/atom_mae: 0.002647
2025-06-25 06:11:27.520 INFO: train_e/atom_mae: 0.002647
train_e/atom_rmse: 0.003242
2025-06-25 06:11:27.520 INFO: train_e/atom_rmse: 0.003242
train_f_mae: 0.010777
2025-06-25 06:11:27.524 INFO: train_f_mae: 0.010777
train_f_rmse: 0.015578
2025-06-25 06:11:27.524 INFO: train_f_rmse: 0.015578
val_e/atom_mae: 0.001048
2025-06-25 06:11:27.526 INFO: val_e/atom_mae: 0.001048
val_e/atom_rmse: 0.001478
2025-06-25 06:11:27.527 INFO: val_e/atom_rmse: 0.001478
val_f_mae: 0.011424
2025-06-25 06:11:27.527 INFO: val_f_mae: 0.011424
val_f_rmse: 0.016344
2025-06-25 06:11:27.527 INFO: val_f_rmse: 0.016344
2025-06-25 06:11:27.534 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-25 06:12:12.647 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 0.1984, Val Loss: 0.1871
2025-06-25 06:12:12.647 INFO: Epoch 1, Train Loss: 0.1984, Val Loss: 0.1871
train_e/atom_mae: 0.001446
2025-06-25 06:12:12.648 INFO: train_e/atom_mae: 0.001446
train_e/atom_rmse: 0.001820
2025-06-25 06:12:12.648 INFO: train_e/atom_rmse: 0.001820
train_f_mae: 0.009754
2025-06-25 06:12:12.652 INFO: train_f_mae: 0.009754
train_f_rmse: 0.013944
2025-06-25 06:12:12.652 INFO: train_f_rmse: 0.013944
val_e/atom_mae: 0.001067
2025-06-25 06:12:12.655 INFO: val_e/atom_mae: 0.001067
val_e/atom_rmse: 0.001381
2025-06-25 06:12:12.655 INFO: val_e/atom_rmse: 0.001381
val_f_mae: 0.009732
2025-06-25 06:12:12.656 INFO: val_f_mae: 0.009732
val_f_rmse: 0.013595
2025-06-25 06:12:12.656 INFO: val_f_rmse: 0.013595
##### Step: 1 Learning rate: 0.004 #####
2025-06-25 06:12:57.800 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 0.2419, Val Loss: 0.2661
2025-06-25 06:12:57.800 INFO: Epoch 2, Train Loss: 0.2419, Val Loss: 0.2661
train_e/atom_mae: 0.001849
2025-06-25 06:12:57.801 INFO: train_e/atom_mae: 0.001849
train_e/atom_rmse: 0.002332
2025-06-25 06:12:57.801 INFO: train_e/atom_rmse: 0.002332
train_f_mae: 0.010584
2025-06-25 06:12:57.804 INFO: train_f_mae: 0.010584
train_f_rmse: 0.015339
2025-06-25 06:12:57.805 INFO: train_f_rmse: 0.015339
val_e/atom_mae: 0.002626
2025-06-25 06:12:57.807 INFO: val_e/atom_mae: 0.002626
val_e/atom_rmse: 0.002725
2025-06-25 06:12:57.808 INFO: val_e/atom_rmse: 0.002725
val_f_mae: 0.011099
2025-06-25 06:12:57.808 INFO: val_f_mae: 0.011099
val_f_rmse: 0.016035
2025-06-25 06:12:57.808 INFO: val_f_rmse: 0.016035
##### Step: 2 Learning rate: 0.006 #####
2025-06-25 06:13:42.940 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 0.2943, Val Loss: 0.3128
2025-06-25 06:13:42.940 INFO: Epoch 3, Train Loss: 0.2943, Val Loss: 0.3128
train_e/atom_mae: 0.002703
2025-06-25 06:13:42.941 INFO: train_e/atom_mae: 0.002703
train_e/atom_rmse: 0.003292
2025-06-25 06:13:42.941 INFO: train_e/atom_rmse: 0.003292
train_f_mae: 0.011322
2025-06-25 06:13:42.945 INFO: train_f_mae: 0.011322
train_f_rmse: 0.016768
2025-06-25 06:13:42.945 INFO: train_f_rmse: 0.016768
val_e/atom_mae: 0.001927
2025-06-25 06:13:42.948 INFO: val_e/atom_mae: 0.001927
val_e/atom_rmse: 0.002223
2025-06-25 06:13:42.948 INFO: val_e/atom_rmse: 0.002223
val_f_mae: 0.012216
2025-06-25 06:13:42.949 INFO: val_f_mae: 0.012216
val_f_rmse: 0.017517
2025-06-25 06:13:42.949 INFO: val_f_rmse: 0.017517
##### Step: 3 Learning rate: 0.008 #####
2025-06-25 06:14:28.106 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 0.3428, Val Loss: 0.5295
2025-06-25 06:14:28.107 INFO: Epoch 4, Train Loss: 0.3428, Val Loss: 0.5295
train_e/atom_mae: 0.002763
2025-06-25 06:14:28.108 INFO: train_e/atom_mae: 0.002763
train_e/atom_rmse: 0.003391
2025-06-25 06:14:28.108 INFO: train_e/atom_rmse: 0.003391
train_f_mae: 0.012058
2025-06-25 06:14:28.111 INFO: train_f_mae: 0.012058
train_f_rmse: 0.018136
2025-06-25 06:14:28.112 INFO: train_f_rmse: 0.018136
val_e/atom_mae: 0.003302
2025-06-25 06:14:28.114 INFO: val_e/atom_mae: 0.003302
val_e/atom_rmse: 0.003573
2025-06-25 06:14:28.115 INFO: val_e/atom_rmse: 0.003573
val_f_mae: 0.014457
2025-06-25 06:14:28.115 INFO: val_f_mae: 0.014457
val_f_rmse: 0.022673
2025-06-25 06:14:28.115 INFO: val_f_rmse: 0.022673
##### Step: 4 Learning rate: 0.01 #####
2025-06-25 06:15:13.220 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 0.4652, Val Loss: 0.4759
2025-06-25 06:15:13.221 INFO: Epoch 5, Train Loss: 0.4652, Val Loss: 0.4759
train_e/atom_mae: 0.003767
2025-06-25 06:15:13.221 INFO: train_e/atom_mae: 0.003767
train_e/atom_rmse: 0.004793
2025-06-25 06:15:13.222 INFO: train_e/atom_rmse: 0.004793
train_f_mae: 0.013695
2025-06-25 06:15:13.225 INFO: train_f_mae: 0.013695
train_f_rmse: 0.020914
2025-06-25 06:15:13.225 INFO: train_f_rmse: 0.020914
val_e/atom_mae: 0.003619
2025-06-25 06:15:13.228 INFO: val_e/atom_mae: 0.003619
val_e/atom_rmse: 0.004575
2025-06-25 06:15:13.229 INFO: val_e/atom_rmse: 0.004575
val_f_mae: 0.013796
2025-06-25 06:15:13.229 INFO: val_f_mae: 0.013796
val_f_rmse: 0.021226
2025-06-25 06:15:13.229 INFO: val_f_rmse: 0.021226
##### Step: 5 Learning rate: 0.01 #####
2025-06-25 06:15:58.393 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 0.4482, Val Loss: 0.3438
2025-06-25 06:15:58.393 INFO: Epoch 6, Train Loss: 0.4482, Val Loss: 0.3438
train_e/atom_mae: 0.003453
2025-06-25 06:15:58.394 INFO: train_e/atom_mae: 0.003453
train_e/atom_rmse: 0.004586
2025-06-25 06:15:58.394 INFO: train_e/atom_rmse: 0.004586
train_f_mae: 0.013804
2025-06-25 06:15:58.398 INFO: train_f_mae: 0.013804
train_f_rmse: 0.020560
2025-06-25 06:15:58.398 INFO: train_f_rmse: 0.020560
val_e/atom_mae: 0.002186
2025-06-25 06:15:58.400 INFO: val_e/atom_mae: 0.002186
val_e/atom_rmse: 0.002491
2025-06-25 06:15:58.401 INFO: val_e/atom_rmse: 0.002491
val_f_mae: 0.012229
2025-06-25 06:15:58.401 INFO: val_f_mae: 0.012229
val_f_rmse: 0.018338
2025-06-25 06:15:58.402 INFO: val_f_rmse: 0.018338
##### Step: 6 Learning rate: 0.01 #####
2025-06-25 06:16:43.463 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 0.4529, Val Loss: 0.4171
2025-06-25 06:16:43.463 INFO: Epoch 7, Train Loss: 0.4529, Val Loss: 0.4171
train_e/atom_mae: 0.003285
2025-06-25 06:16:43.464 INFO: train_e/atom_mae: 0.003285
train_e/atom_rmse: 0.004223
2025-06-25 06:16:43.464 INFO: train_e/atom_rmse: 0.004223
train_f_mae: 0.013812
2025-06-25 06:16:43.468 INFO: train_f_mae: 0.013812
train_f_rmse: 0.020769
2025-06-25 06:16:43.468 INFO: train_f_rmse: 0.020769
val_e/atom_mae: 0.001877
2025-06-25 06:16:43.471 INFO: val_e/atom_mae: 0.001877
val_e/atom_rmse: 0.002308
2025-06-25 06:16:43.471 INFO: val_e/atom_rmse: 0.002308
val_f_mae: 0.013492
2025-06-25 06:16:43.472 INFO: val_f_mae: 0.013492
val_f_rmse: 0.020266
2025-06-25 06:16:43.472 INFO: val_f_rmse: 0.020266
##### Step: 7 Learning rate: 0.01 #####
2025-06-25 06:17:28.645 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 0.4727, Val Loss: 0.4551
2025-06-25 06:17:28.645 INFO: Epoch 8, Train Loss: 0.4727, Val Loss: 0.4551
train_e/atom_mae: 0.002422
2025-06-25 06:17:28.646 INFO: train_e/atom_mae: 0.002422
train_e/atom_rmse: 0.003030
2025-06-25 06:17:28.646 INFO: train_e/atom_rmse: 0.003030
train_f_mae: 0.014154
2025-06-25 06:17:28.650 INFO: train_f_mae: 0.014154
train_f_rmse: 0.021485
2025-06-25 06:17:28.650 INFO: train_f_rmse: 0.021485
val_e/atom_mae: 0.002546
2025-06-25 06:17:28.653 INFO: val_e/atom_mae: 0.002546
val_e/atom_rmse: 0.002999
2025-06-25 06:17:28.653 INFO: val_e/atom_rmse: 0.002999
val_f_mae: 0.013920
2025-06-25 06:17:28.653 INFO: val_f_mae: 0.013920
val_f_rmse: 0.021077
2025-06-25 06:17:28.654 INFO: val_f_rmse: 0.021077
##### Step: 8 Learning rate: 0.01 #####
2025-06-25 06:18:13.709 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 0.4389, Val Loss: 0.3477
2025-06-25 06:18:13.709 INFO: Epoch 9, Train Loss: 0.4389, Val Loss: 0.3477
train_e/atom_mae: 0.004251
2025-06-25 06:18:13.710 INFO: train_e/atom_mae: 0.004251
train_e/atom_rmse: 0.005343
2025-06-25 06:18:13.710 INFO: train_e/atom_rmse: 0.005343
train_f_mae: 0.013342
2025-06-25 06:18:13.714 INFO: train_f_mae: 0.013342
train_f_rmse: 0.020110
2025-06-25 06:18:13.714 INFO: train_f_rmse: 0.020110
val_e/atom_mae: 0.002039
2025-06-25 06:18:13.717 INFO: val_e/atom_mae: 0.002039
val_e/atom_rmse: 0.002317
2025-06-25 06:18:13.717 INFO: val_e/atom_rmse: 0.002317
val_f_mae: 0.012612
2025-06-25 06:18:13.717 INFO: val_f_mae: 0.012612
val_f_rmse: 0.018471
2025-06-25 06:18:13.718 INFO: val_f_rmse: 0.018471
##### Step: 9 Learning rate: 0.01 #####
2025-06-25 06:18:58.942 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 0.4721, Val Loss: 0.4789
2025-06-25 06:18:58.942 INFO: Epoch 10, Train Loss: 0.4721, Val Loss: 0.4789
train_e/atom_mae: 0.003115
2025-06-25 06:18:58.943 INFO: train_e/atom_mae: 0.003115
train_e/atom_rmse: 0.003946
2025-06-25 06:18:58.943 INFO: train_e/atom_rmse: 0.003946
train_f_mae: 0.014094
2025-06-25 06:18:58.947 INFO: train_f_mae: 0.014094
train_f_rmse: 0.021289
2025-06-25 06:18:58.947 INFO: train_f_rmse: 0.021289
val_e/atom_mae: 0.002225
2025-06-25 06:18:58.950 INFO: val_e/atom_mae: 0.002225
val_e/atom_rmse: 0.002567
2025-06-25 06:18:58.950 INFO: val_e/atom_rmse: 0.002567
val_f_mae: 0.013717
2025-06-25 06:18:58.951 INFO: val_f_mae: 0.013717
val_f_rmse: 0.021701
2025-06-25 06:18:58.951 INFO: val_f_rmse: 0.021701
##### Step: 10 Learning rate: 0.01 #####
2025-06-25 06:19:44.024 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 0.4596, Val Loss: 0.4459
2025-06-25 06:19:44.024 INFO: Epoch 11, Train Loss: 0.4596, Val Loss: 0.4459
train_e/atom_mae: 0.002885
2025-06-25 06:19:44.025 INFO: train_e/atom_mae: 0.002885
train_e/atom_rmse: 0.003603
2025-06-25 06:19:44.025 INFO: train_e/atom_rmse: 0.003603
train_f_mae: 0.013857
2025-06-25 06:19:44.029 INFO: train_f_mae: 0.013857
train_f_rmse: 0.021068
2025-06-25 06:19:44.029 INFO: train_f_rmse: 0.021068
val_e/atom_mae: 0.004500
2025-06-25 06:19:44.032 INFO: val_e/atom_mae: 0.004500
val_e/atom_rmse: 0.004716
2025-06-25 06:19:44.032 INFO: val_e/atom_rmse: 0.004716
val_f_mae: 0.013541
2025-06-25 06:19:44.033 INFO: val_f_mae: 0.013541
val_f_rmse: 0.020468
2025-06-25 06:19:44.033 INFO: val_f_rmse: 0.020468
##### Step: 11 Learning rate: 0.01 #####
2025-06-25 06:20:29.252 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 0.4982, Val Loss: 0.5727
2025-06-25 06:20:29.252 INFO: Epoch 12, Train Loss: 0.4982, Val Loss: 0.5727
train_e/atom_mae: 0.002907
2025-06-25 06:20:29.253 INFO: train_e/atom_mae: 0.002907
train_e/atom_rmse: 0.003726
2025-06-25 06:20:29.253 INFO: train_e/atom_rmse: 0.003726
train_f_mae: 0.014240
2025-06-25 06:20:29.257 INFO: train_f_mae: 0.014240
train_f_rmse: 0.021942
2025-06-25 06:20:29.257 INFO: train_f_rmse: 0.021942
val_e/atom_mae: 0.004040
2025-06-25 06:20:29.260 INFO: val_e/atom_mae: 0.004040
val_e/atom_rmse: 0.004757
2025-06-25 06:20:29.260 INFO: val_e/atom_rmse: 0.004757
val_f_mae: 0.015376
2025-06-25 06:20:29.260 INFO: val_f_mae: 0.015376
val_f_rmse: 0.023352
2025-06-25 06:20:29.261 INFO: val_f_rmse: 0.023352
##### Step: 12 Learning rate: 0.01 #####
2025-06-25 06:21:14.323 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 0.4957, Val Loss: 0.4151
2025-06-25 06:21:14.324 INFO: Epoch 13, Train Loss: 0.4957, Val Loss: 0.4151
train_e/atom_mae: 0.003256
2025-06-25 06:21:14.325 INFO: train_e/atom_mae: 0.003256
train_e/atom_rmse: 0.004046
2025-06-25 06:21:14.325 INFO: train_e/atom_rmse: 0.004046
train_f_mae: 0.014205
2025-06-25 06:21:14.328 INFO: train_f_mae: 0.014205
train_f_rmse: 0.021814
2025-06-25 06:21:14.329 INFO: train_f_rmse: 0.021814
val_e/atom_mae: 0.001528
2025-06-25 06:21:14.331 INFO: val_e/atom_mae: 0.001528
val_e/atom_rmse: 0.002013
2025-06-25 06:21:14.332 INFO: val_e/atom_rmse: 0.002013
val_f_mae: 0.013398
2025-06-25 06:21:14.332 INFO: val_f_mae: 0.013398
val_f_rmse: 0.020254
2025-06-25 06:21:14.332 INFO: val_f_rmse: 0.020254
##### Step: 13 Learning rate: 0.01 #####
2025-06-25 06:21:59.544 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 0.4691, Val Loss: 0.4587
2025-06-25 06:21:59.545 INFO: Epoch 14, Train Loss: 0.4691, Val Loss: 0.4587
train_e/atom_mae: 0.003164
2025-06-25 06:21:59.545 INFO: train_e/atom_mae: 0.003164
train_e/atom_rmse: 0.003903
2025-06-25 06:21:59.546 INFO: train_e/atom_rmse: 0.003903
train_f_mae: 0.014055
2025-06-25 06:21:59.549 INFO: train_f_mae: 0.014055
train_f_rmse: 0.021229
2025-06-25 06:21:59.549 INFO: train_f_rmse: 0.021229
val_e/atom_mae: 0.002606
2025-06-25 06:21:59.552 INFO: val_e/atom_mae: 0.002606
val_e/atom_rmse: 0.003091
2025-06-25 06:21:59.553 INFO: val_e/atom_rmse: 0.003091
val_f_mae: 0.014132
2025-06-25 06:21:59.553 INFO: val_f_mae: 0.014132
val_f_rmse: 0.021145
2025-06-25 06:21:59.553 INFO: val_f_rmse: 0.021145
##### Step: 14 Learning rate: 0.01 #####
2025-06-25 06:22:44.617 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 0.4893, Val Loss: 0.5899
2025-06-25 06:22:44.618 INFO: Epoch 15, Train Loss: 0.4893, Val Loss: 0.5899
train_e/atom_mae: 0.003576
2025-06-25 06:22:44.619 INFO: train_e/atom_mae: 0.003576
train_e/atom_rmse: 0.004547
2025-06-25 06:22:44.619 INFO: train_e/atom_rmse: 0.004547
train_f_mae: 0.013885
2025-06-25 06:22:44.622 INFO: train_f_mae: 0.013885
train_f_rmse: 0.021547
2025-06-25 06:22:44.622 INFO: train_f_rmse: 0.021547
val_e/atom_mae: 0.003198
2025-06-25 06:22:44.625 INFO: val_e/atom_mae: 0.003198
val_e/atom_rmse: 0.003501
2025-06-25 06:22:44.626 INFO: val_e/atom_rmse: 0.003501
val_f_mae: 0.016934
2025-06-25 06:22:44.626 INFO: val_f_mae: 0.016934
val_f_rmse: 0.023981
2025-06-25 06:22:44.626 INFO: val_f_rmse: 0.023981
##### Step: 15 Learning rate: 0.01 #####
2025-06-25 06:23:29.808 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 0.5178, Val Loss: 0.3845
2025-06-25 06:23:29.808 INFO: Epoch 16, Train Loss: 0.5178, Val Loss: 0.3845
train_e/atom_mae: 0.003887
2025-06-25 06:23:29.809 INFO: train_e/atom_mae: 0.003887
train_e/atom_rmse: 0.004836
2025-06-25 06:23:29.809 INFO: train_e/atom_rmse: 0.004836
train_f_mae: 0.014635
2025-06-25 06:23:29.813 INFO: train_f_mae: 0.014635
train_f_rmse: 0.022124
2025-06-25 06:23:29.813 INFO: train_f_rmse: 0.022124
val_e/atom_mae: 0.002813
2025-06-25 06:23:29.816 INFO: val_e/atom_mae: 0.002813
val_e/atom_rmse: 0.003069
2025-06-25 06:23:29.816 INFO: val_e/atom_rmse: 0.003069
val_f_mae: 0.013270
2025-06-25 06:23:29.817 INFO: val_f_mae: 0.013270
val_f_rmse: 0.019315
2025-06-25 06:23:29.817 INFO: val_f_rmse: 0.019315
##### Step: 16 Learning rate: 0.01 #####
2025-06-25 06:24:14.833 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 0.4691, Val Loss: 0.6461
2025-06-25 06:24:14.833 INFO: Epoch 17, Train Loss: 0.4691, Val Loss: 0.6461
train_e/atom_mae: 0.003592
2025-06-25 06:24:14.834 INFO: train_e/atom_mae: 0.003592
train_e/atom_rmse: 0.004595
2025-06-25 06:24:14.834 INFO: train_e/atom_rmse: 0.004595
train_f_mae: 0.014000
2025-06-25 06:24:14.838 INFO: train_f_mae: 0.014000
train_f_rmse: 0.021060
2025-06-25 06:24:14.838 INFO: train_f_rmse: 0.021060
val_e/atom_mae: 0.001636
2025-06-25 06:24:14.841 INFO: val_e/atom_mae: 0.001636
val_e/atom_rmse: 0.001899
2025-06-25 06:24:14.841 INFO: val_e/atom_rmse: 0.001899
val_f_mae: 0.016313
2025-06-25 06:24:14.842 INFO: val_f_mae: 0.016313
val_f_rmse: 0.025333
2025-06-25 06:24:14.842 INFO: val_f_rmse: 0.025333
##### Step: 17 Learning rate: 0.01 #####
2025-06-25 06:24:59.959 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 0.4722, Val Loss: 0.5302
2025-06-25 06:24:59.960 INFO: Epoch 18, Train Loss: 0.4722, Val Loss: 0.5302
train_e/atom_mae: 0.003689
2025-06-25 06:24:59.960 INFO: train_e/atom_mae: 0.003689
train_e/atom_rmse: 0.004505
2025-06-25 06:24:59.961 INFO: train_e/atom_rmse: 0.004505
train_f_mae: 0.014039
2025-06-25 06:24:59.964 INFO: train_f_mae: 0.014039
train_f_rmse: 0.021158
2025-06-25 06:24:59.964 INFO: train_f_rmse: 0.021158
val_e/atom_mae: 0.008489
2025-06-25 06:24:59.967 INFO: val_e/atom_mae: 0.008489
val_e/atom_rmse: 0.008657
2025-06-25 06:24:59.968 INFO: val_e/atom_rmse: 0.008657
val_f_mae: 0.014129
2025-06-25 06:24:59.968 INFO: val_f_mae: 0.014129
val_f_rmse: 0.020964
2025-06-25 06:24:59.968 INFO: val_f_rmse: 0.020964
##### Step: 18 Learning rate: 0.01 #####
2025-06-25 06:25:45.039 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 0.4015, Val Loss: 0.3733
2025-06-25 06:25:45.040 INFO: Epoch 19, Train Loss: 0.4015, Val Loss: 0.3733
train_e/atom_mae: 0.003261
2025-06-25 06:25:45.040 INFO: train_e/atom_mae: 0.003261
train_e/atom_rmse: 0.003981
2025-06-25 06:25:45.041 INFO: train_e/atom_rmse: 0.003981
train_f_mae: 0.013116
2025-06-25 06:25:45.044 INFO: train_f_mae: 0.013116
train_f_rmse: 0.019554
2025-06-25 06:25:45.044 INFO: train_f_rmse: 0.019554
val_e/atom_mae: 0.005446
2025-06-25 06:25:45.047 INFO: val_e/atom_mae: 0.005446
val_e/atom_rmse: 0.005601
2025-06-25 06:25:45.048 INFO: val_e/atom_rmse: 0.005601
val_f_mae: 0.012557
2025-06-25 06:25:45.048 INFO: val_f_mae: 0.012557
val_f_rmse: 0.018312
2025-06-25 06:25:45.048 INFO: val_f_rmse: 0.018312
##### Step: 19 Learning rate: 0.01 #####
2025-06-25 06:26:30.195 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 0.4746, Val Loss: 0.6795
2025-06-25 06:26:30.196 INFO: Epoch 20, Train Loss: 0.4746, Val Loss: 0.6795
train_e/atom_mae: 0.004012
2025-06-25 06:26:30.197 INFO: train_e/atom_mae: 0.004012
train_e/atom_rmse: 0.005213
2025-06-25 06:26:30.197 INFO: train_e/atom_rmse: 0.005213
train_f_mae: 0.013961
2025-06-25 06:26:30.201 INFO: train_f_mae: 0.013961
train_f_rmse: 0.021018
2025-06-25 06:26:30.201 INFO: train_f_rmse: 0.021018
val_e/atom_mae: 0.008208
2025-06-25 06:26:30.204 INFO: val_e/atom_mae: 0.008208
val_e/atom_rmse: 0.008449
2025-06-25 06:26:30.204 INFO: val_e/atom_rmse: 0.008449
val_f_mae: 0.016145
2025-06-25 06:26:30.204 INFO: val_f_mae: 0.016145
val_f_rmse: 0.024355
2025-06-25 06:26:30.204 INFO: val_f_rmse: 0.024355
##### Step: 20 Learning rate: 0.005 #####
2025-06-25 06:27:15.303 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 0.3100, Val Loss: 0.2744
2025-06-25 06:27:15.303 INFO: Epoch 21, Train Loss: 0.3100, Val Loss: 0.2744
train_e/atom_mae: 0.003367
2025-06-25 06:27:15.304 INFO: train_e/atom_mae: 0.003367
train_e/atom_rmse: 0.004115
2025-06-25 06:27:15.304 INFO: train_e/atom_rmse: 0.004115
train_f_mae: 0.011411
2025-06-25 06:27:15.308 INFO: train_f_mae: 0.011411
train_f_rmse: 0.017015
2025-06-25 06:27:15.308 INFO: train_f_rmse: 0.017015
val_e/atom_mae: 0.002701
2025-06-25 06:27:15.311 INFO: val_e/atom_mae: 0.002701
val_e/atom_rmse: 0.002828
2025-06-25 06:27:15.311 INFO: val_e/atom_rmse: 0.002828
val_f_mae: 0.011313
2025-06-25 06:27:15.312 INFO: val_f_mae: 0.011313
val_f_rmse: 0.016270
2025-06-25 06:27:15.312 INFO: val_f_rmse: 0.016270
##### Step: 21 Learning rate: 0.005 #####
2025-06-25 06:28:00.431 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.2563, Val Loss: 0.2341
2025-06-25 06:28:00.431 INFO: Epoch 22, Train Loss: 0.2563, Val Loss: 0.2341
train_e/atom_mae: 0.001778
2025-06-25 06:28:00.432 INFO: train_e/atom_mae: 0.001778
train_e/atom_rmse: 0.002206
2025-06-25 06:28:00.432 INFO: train_e/atom_rmse: 0.002206
train_f_mae: 0.010854
2025-06-25 06:28:00.436 INFO: train_f_mae: 0.010854
train_f_rmse: 0.015825
2025-06-25 06:28:00.436 INFO: train_f_rmse: 0.015825
val_e/atom_mae: 0.001163
2025-06-25 06:28:00.439 INFO: val_e/atom_mae: 0.001163
val_e/atom_rmse: 0.001429
2025-06-25 06:28:00.439 INFO: val_e/atom_rmse: 0.001429
val_f_mae: 0.010522
2025-06-25 06:28:00.440 INFO: val_f_mae: 0.010522
val_f_rmse: 0.015221
2025-06-25 06:28:00.440 INFO: val_f_rmse: 0.015221
##### Step: 22 Learning rate: 0.005 #####
2025-06-25 06:28:45.586 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.2678, Val Loss: 0.2492
2025-06-25 06:28:45.586 INFO: Epoch 23, Train Loss: 0.2678, Val Loss: 0.2492
train_e/atom_mae: 0.001574
2025-06-25 06:28:45.587 INFO: train_e/atom_mae: 0.001574
train_e/atom_rmse: 0.001963
2025-06-25 06:28:45.587 INFO: train_e/atom_rmse: 0.001963
train_f_mae: 0.010994
2025-06-25 06:28:45.591 INFO: train_f_mae: 0.010994
train_f_rmse: 0.016222
2025-06-25 06:28:45.591 INFO: train_f_rmse: 0.016222
val_e/atom_mae: 0.002321
2025-06-25 06:28:45.594 INFO: val_e/atom_mae: 0.002321
val_e/atom_rmse: 0.002716
2025-06-25 06:28:45.594 INFO: val_e/atom_rmse: 0.002716
val_f_mae: 0.010485
2025-06-25 06:28:45.595 INFO: val_f_mae: 0.010485
val_f_rmse: 0.015499
2025-06-25 06:28:45.595 INFO: val_f_rmse: 0.015499
##### Step: 23 Learning rate: 0.005 #####
2025-06-25 06:29:30.699 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.2570, Val Loss: 0.2919
2025-06-25 06:29:30.699 INFO: Epoch 24, Train Loss: 0.2570, Val Loss: 0.2919
train_e/atom_mae: 0.002374
2025-06-25 06:29:30.700 INFO: train_e/atom_mae: 0.002374
train_e/atom_rmse: 0.003012
2025-06-25 06:29:30.700 INFO: train_e/atom_rmse: 0.003012
train_f_mae: 0.010750
2025-06-25 06:29:30.704 INFO: train_f_mae: 0.010750
train_f_rmse: 0.015686
2025-06-25 06:29:30.704 INFO: train_f_rmse: 0.015686
val_e/atom_mae: 0.004133
2025-06-25 06:29:30.707 INFO: val_e/atom_mae: 0.004133
val_e/atom_rmse: 0.004297
2025-06-25 06:29:30.707 INFO: val_e/atom_rmse: 0.004297
val_f_mae: 0.011119
2025-06-25 06:29:30.708 INFO: val_f_mae: 0.011119
val_f_rmse: 0.016419
2025-06-25 06:29:30.708 INFO: val_f_rmse: 0.016419
##### Step: 24 Learning rate: 0.005 #####
2025-06-25 06:30:15.860 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.2549, Val Loss: 0.2694
2025-06-25 06:30:15.861 INFO: Epoch 25, Train Loss: 0.2549, Val Loss: 0.2694
train_e/atom_mae: 0.001560
2025-06-25 06:30:15.862 INFO: train_e/atom_mae: 0.001560
train_e/atom_rmse: 0.001986
2025-06-25 06:30:15.862 INFO: train_e/atom_rmse: 0.001986
train_f_mae: 0.010721
2025-06-25 06:30:15.865 INFO: train_f_mae: 0.010721
train_f_rmse: 0.015817
2025-06-25 06:30:15.865 INFO: train_f_rmse: 0.015817
val_e/atom_mae: 0.003205
2025-06-25 06:30:15.868 INFO: val_e/atom_mae: 0.003205
val_e/atom_rmse: 0.003273
2025-06-25 06:30:15.869 INFO: val_e/atom_rmse: 0.003273
val_f_mae: 0.010918
2025-06-25 06:30:15.869 INFO: val_f_mae: 0.010918
val_f_rmse: 0.016014
2025-06-25 06:30:15.869 INFO: val_f_rmse: 0.016014
##### Step: 25 Learning rate: 0.005 #####
2025-06-25 06:31:00.896 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.2511, Val Loss: 0.2568
2025-06-25 06:31:00.897 INFO: Epoch 26, Train Loss: 0.2511, Val Loss: 0.2568
train_e/atom_mae: 0.001642
2025-06-25 06:31:00.898 INFO: train_e/atom_mae: 0.001642
train_e/atom_rmse: 0.002058
2025-06-25 06:31:00.898 INFO: train_e/atom_rmse: 0.002058
train_f_mae: 0.010814
2025-06-25 06:31:00.901 INFO: train_f_mae: 0.010814
train_f_rmse: 0.015685
2025-06-25 06:31:00.902 INFO: train_f_rmse: 0.015685
val_e/atom_mae: 0.003100
2025-06-25 06:31:00.904 INFO: val_e/atom_mae: 0.003100
val_e/atom_rmse: 0.003138
2025-06-25 06:31:00.905 INFO: val_e/atom_rmse: 0.003138
val_f_mae: 0.011154
2025-06-25 06:31:00.905 INFO: val_f_mae: 0.011154
val_f_rmse: 0.015649
2025-06-25 06:31:00.905 INFO: val_f_rmse: 0.015649
##### Step: 26 Learning rate: 0.005 #####
2025-06-25 06:31:46.071 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.2364, Val Loss: 0.2337
2025-06-25 06:31:46.071 INFO: Epoch 27, Train Loss: 0.2364, Val Loss: 0.2337
train_e/atom_mae: 0.001906
2025-06-25 06:31:46.072 INFO: train_e/atom_mae: 0.001906
train_e/atom_rmse: 0.002401
2025-06-25 06:31:46.072 INFO: train_e/atom_rmse: 0.002401
train_f_mae: 0.010434
2025-06-25 06:31:46.076 INFO: train_f_mae: 0.010434
train_f_rmse: 0.015148
2025-06-25 06:31:46.076 INFO: train_f_rmse: 0.015148
val_e/atom_mae: 0.001758
2025-06-25 06:31:46.079 INFO: val_e/atom_mae: 0.001758
val_e/atom_rmse: 0.002233
2025-06-25 06:31:46.079 INFO: val_e/atom_rmse: 0.002233
val_f_mae: 0.010521
2025-06-25 06:31:46.080 INFO: val_f_mae: 0.010521
val_f_rmse: 0.015088
2025-06-25 06:31:46.080 INFO: val_f_rmse: 0.015088
##### Step: 27 Learning rate: 0.005 #####
2025-06-25 06:32:31.111 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.2398, Val Loss: 0.2246
2025-06-25 06:32:31.112 INFO: Epoch 28, Train Loss: 0.2398, Val Loss: 0.2246
train_e/atom_mae: 0.002580
2025-06-25 06:32:31.113 INFO: train_e/atom_mae: 0.002580
train_e/atom_rmse: 0.003105
2025-06-25 06:32:31.113 INFO: train_e/atom_rmse: 0.003105
train_f_mae: 0.010444
2025-06-25 06:32:31.116 INFO: train_f_mae: 0.010444
train_f_rmse: 0.015104
2025-06-25 06:32:31.116 INFO: train_f_rmse: 0.015104
val_e/atom_mae: 0.001547
2025-06-25 06:32:31.119 INFO: val_e/atom_mae: 0.001547
val_e/atom_rmse: 0.001944
2025-06-25 06:32:31.120 INFO: val_e/atom_rmse: 0.001944
val_f_mae: 0.010481
2025-06-25 06:32:31.120 INFO: val_f_mae: 0.010481
val_f_rmse: 0.014833
2025-06-25 06:32:31.120 INFO: val_f_rmse: 0.014833
##### Step: 28 Learning rate: 0.005 #####
2025-06-25 06:33:16.316 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.2446, Val Loss: 0.2062
2025-06-25 06:33:16.316 INFO: Epoch 29, Train Loss: 0.2446, Val Loss: 0.2062
train_e/atom_mae: 0.002293
2025-06-25 06:33:16.317 INFO: train_e/atom_mae: 0.002293
train_e/atom_rmse: 0.002865
2025-06-25 06:33:16.317 INFO: train_e/atom_rmse: 0.002865
train_f_mae: 0.010453
2025-06-25 06:33:16.321 INFO: train_f_mae: 0.010453
train_f_rmse: 0.015318
2025-06-25 06:33:16.321 INFO: train_f_rmse: 0.015318
val_e/atom_mae: 0.001172
2025-06-25 06:33:16.324 INFO: val_e/atom_mae: 0.001172
val_e/atom_rmse: 0.001441
2025-06-25 06:33:16.324 INFO: val_e/atom_rmse: 0.001441
val_f_mae: 0.010118
2025-06-25 06:33:16.324 INFO: val_f_mae: 0.010118
val_f_rmse: 0.014274
2025-06-25 06:33:16.324 INFO: val_f_rmse: 0.014274
##### Step: 29 Learning rate: 0.005 #####
2025-06-25 06:34:01.349 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.2348, Val Loss: 0.2548
2025-06-25 06:34:01.349 INFO: Epoch 30, Train Loss: 0.2348, Val Loss: 0.2548
train_e/atom_mae: 0.001906
2025-06-25 06:34:01.350 INFO: train_e/atom_mae: 0.001906
train_e/atom_rmse: 0.002348
2025-06-25 06:34:01.350 INFO: train_e/atom_rmse: 0.002348
train_f_mae: 0.010510
2025-06-25 06:34:01.354 INFO: train_f_mae: 0.010510
train_f_rmse: 0.015105
2025-06-25 06:34:01.354 INFO: train_f_rmse: 0.015105
val_e/atom_mae: 0.001484
2025-06-25 06:34:01.356 INFO: val_e/atom_mae: 0.001484
val_e/atom_rmse: 0.001563
2025-06-25 06:34:01.357 INFO: val_e/atom_rmse: 0.001563
val_f_mae: 0.011248
2025-06-25 06:34:01.357 INFO: val_f_mae: 0.011248
val_f_rmse: 0.015871
2025-06-25 06:34:01.357 INFO: val_f_rmse: 0.015871
##### Step: 30 Learning rate: 0.005 #####
2025-06-25 06:34:46.587 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.2476, Val Loss: 0.2052
2025-06-25 06:34:46.587 INFO: Epoch 31, Train Loss: 0.2476, Val Loss: 0.2052
train_e/atom_mae: 0.002029
2025-06-25 06:34:46.588 INFO: train_e/atom_mae: 0.002029
train_e/atom_rmse: 0.002578
2025-06-25 06:34:46.588 INFO: train_e/atom_rmse: 0.002578
train_f_mae: 0.010611
2025-06-25 06:34:46.592 INFO: train_f_mae: 0.010611
train_f_rmse: 0.015478
2025-06-25 06:34:46.592 INFO: train_f_rmse: 0.015478
val_e/atom_mae: 0.002105
2025-06-25 06:34:46.595 INFO: val_e/atom_mae: 0.002105
val_e/atom_rmse: 0.002319
2025-06-25 06:34:46.595 INFO: val_e/atom_rmse: 0.002319
val_f_mae: 0.009935
2025-06-25 06:34:46.596 INFO: val_f_mae: 0.009935
val_f_rmse: 0.014097
2025-06-25 06:34:46.596 INFO: val_f_rmse: 0.014097
##### Step: 31 Learning rate: 0.005 #####
2025-06-25 06:35:31.638 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.2328, Val Loss: 0.2650
2025-06-25 06:35:31.638 INFO: Epoch 32, Train Loss: 0.2328, Val Loss: 0.2650
train_e/atom_mae: 0.002097
2025-06-25 06:35:31.639 INFO: train_e/atom_mae: 0.002097
train_e/atom_rmse: 0.002551
2025-06-25 06:35:31.639 INFO: train_e/atom_rmse: 0.002551
train_f_mae: 0.010373
2025-06-25 06:35:31.643 INFO: train_f_mae: 0.010373
train_f_rmse: 0.014996
2025-06-25 06:35:31.643 INFO: train_f_rmse: 0.014996
val_e/atom_mae: 0.001302
2025-06-25 06:35:31.646 INFO: val_e/atom_mae: 0.001302
val_e/atom_rmse: 0.001561
2025-06-25 06:35:31.646 INFO: val_e/atom_rmse: 0.001561
val_f_mae: 0.011075
2025-06-25 06:35:31.647 INFO: val_f_mae: 0.011075
val_f_rmse: 0.016188
2025-06-25 06:35:31.647 INFO: val_f_rmse: 0.016188
##### Step: 32 Learning rate: 0.005 #####
2025-06-25 06:36:16.861 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.2705, Val Loss: 0.2145
2025-06-25 06:36:16.861 INFO: Epoch 33, Train Loss: 0.2705, Val Loss: 0.2145
train_e/atom_mae: 0.002398
2025-06-25 06:36:16.862 INFO: train_e/atom_mae: 0.002398
train_e/atom_rmse: 0.003067
2025-06-25 06:36:16.862 INFO: train_e/atom_rmse: 0.003067
train_f_mae: 0.011006
2025-06-25 06:36:16.866 INFO: train_f_mae: 0.011006
train_f_rmse: 0.016097
2025-06-25 06:36:16.866 INFO: train_f_rmse: 0.016097
val_e/atom_mae: 0.000566
2025-06-25 06:36:16.869 INFO: val_e/atom_mae: 0.000566
val_e/atom_rmse: 0.000666
2025-06-25 06:36:16.869 INFO: val_e/atom_rmse: 0.000666
val_f_mae: 0.010298
2025-06-25 06:36:16.870 INFO: val_f_mae: 0.010298
val_f_rmse: 0.014628
2025-06-25 06:36:16.870 INFO: val_f_rmse: 0.014628
##### Step: 33 Learning rate: 0.005 #####
2025-06-25 06:37:01.921 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.2311, Val Loss: 0.2104
2025-06-25 06:37:01.921 INFO: Epoch 34, Train Loss: 0.2311, Val Loss: 0.2104
train_e/atom_mae: 0.001452
2025-06-25 06:37:01.922 INFO: train_e/atom_mae: 0.001452
train_e/atom_rmse: 0.001875
2025-06-25 06:37:01.922 INFO: train_e/atom_rmse: 0.001875
train_f_mae: 0.010440
2025-06-25 06:37:01.926 INFO: train_f_mae: 0.010440
train_f_rmse: 0.015062
2025-06-25 06:37:01.926 INFO: train_f_rmse: 0.015062
val_e/atom_mae: 0.001525
2025-06-25 06:37:01.929 INFO: val_e/atom_mae: 0.001525
val_e/atom_rmse: 0.001611
2025-06-25 06:37:01.929 INFO: val_e/atom_rmse: 0.001611
val_f_mae: 0.009912
2025-06-25 06:37:01.930 INFO: val_f_mae: 0.009912
val_f_rmse: 0.014395
2025-06-25 06:37:01.930 INFO: val_f_rmse: 0.014395
##### Step: 34 Learning rate: 0.005 #####
2025-06-25 06:37:47.097 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.2375, Val Loss: 0.2426
2025-06-25 06:37:47.097 INFO: Epoch 35, Train Loss: 0.2375, Val Loss: 0.2426
train_e/atom_mae: 0.001895
2025-06-25 06:37:47.098 INFO: train_e/atom_mae: 0.001895
train_e/atom_rmse: 0.002362
2025-06-25 06:37:47.098 INFO: train_e/atom_rmse: 0.002362
train_f_mae: 0.010456
2025-06-25 06:37:47.102 INFO: train_f_mae: 0.010456
train_f_rmse: 0.015192
2025-06-25 06:37:47.102 INFO: train_f_rmse: 0.015192
val_e/atom_mae: 0.000884
2025-06-25 06:37:47.105 INFO: val_e/atom_mae: 0.000884
val_e/atom_rmse: 0.001005
2025-06-25 06:37:47.105 INFO: val_e/atom_rmse: 0.001005
val_f_mae: 0.010570
2025-06-25 06:37:47.106 INFO: val_f_mae: 0.010570
val_f_rmse: 0.015537
2025-06-25 06:37:47.106 INFO: val_f_rmse: 0.015537
##### Step: 35 Learning rate: 0.005 #####
2025-06-25 06:38:32.166 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.2418, Val Loss: 0.2660
2025-06-25 06:38:32.166 INFO: Epoch 36, Train Loss: 0.2418, Val Loss: 0.2660
train_e/atom_mae: 0.001977
2025-06-25 06:38:32.167 INFO: train_e/atom_mae: 0.001977
train_e/atom_rmse: 0.002447
2025-06-25 06:38:32.167 INFO: train_e/atom_rmse: 0.002447
train_f_mae: 0.010501
2025-06-25 06:38:32.171 INFO: train_f_mae: 0.010501
train_f_rmse: 0.015315
2025-06-25 06:38:32.171 INFO: train_f_rmse: 0.015315
val_e/atom_mae: 0.004320
2025-06-25 06:38:32.174 INFO: val_e/atom_mae: 0.004320
val_e/atom_rmse: 0.004340
2025-06-25 06:38:32.174 INFO: val_e/atom_rmse: 0.004340
val_f_mae: 0.010862
2025-06-25 06:38:32.174 INFO: val_f_mae: 0.010862
val_f_rmse: 0.015596
2025-06-25 06:38:32.175 INFO: val_f_rmse: 0.015596
##### Step: 36 Learning rate: 0.005 #####
2025-06-25 06:39:17.327 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.2625, Val Loss: 0.2296
2025-06-25 06:39:17.327 INFO: Epoch 37, Train Loss: 0.2625, Val Loss: 0.2296
train_e/atom_mae: 0.001982
2025-06-25 06:39:17.328 INFO: train_e/atom_mae: 0.001982
train_e/atom_rmse: 0.002529
2025-06-25 06:39:17.328 INFO: train_e/atom_rmse: 0.002529
train_f_mae: 0.010828
2025-06-25 06:39:17.332 INFO: train_f_mae: 0.010828
train_f_rmse: 0.015960
2025-06-25 06:39:17.332 INFO: train_f_rmse: 0.015960
val_e/atom_mae: 0.001187
2025-06-25 06:39:17.335 INFO: val_e/atom_mae: 0.001187
val_e/atom_rmse: 0.001242
2025-06-25 06:39:17.335 INFO: val_e/atom_rmse: 0.001242
val_f_mae: 0.010511
2025-06-25 06:39:17.335 INFO: val_f_mae: 0.010511
val_f_rmse: 0.015091
2025-06-25 06:39:17.336 INFO: val_f_rmse: 0.015091
##### Step: 37 Learning rate: 0.005 #####
2025-06-25 06:40:02.405 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.2399, Val Loss: 0.1986
2025-06-25 06:40:02.405 INFO: Epoch 38, Train Loss: 0.2399, Val Loss: 0.1986
train_e/atom_mae: 0.001569
2025-06-25 06:40:02.406 INFO: train_e/atom_mae: 0.001569
train_e/atom_rmse: 0.001970
2025-06-25 06:40:02.406 INFO: train_e/atom_rmse: 0.001970
train_f_mae: 0.010568
2025-06-25 06:40:02.410 INFO: train_f_mae: 0.010568
train_f_rmse: 0.015335
2025-06-25 06:40:02.410 INFO: train_f_rmse: 0.015335
val_e/atom_mae: 0.001298
2025-06-25 06:40:02.413 INFO: val_e/atom_mae: 0.001298
val_e/atom_rmse: 0.001446
2025-06-25 06:40:02.413 INFO: val_e/atom_rmse: 0.001446
val_f_mae: 0.009862
2025-06-25 06:40:02.413 INFO: val_f_mae: 0.009862
val_f_rmse: 0.014002
2025-06-25 06:40:02.414 INFO: val_f_rmse: 0.014002
##### Step: 38 Learning rate: 0.005 #####
2025-06-25 06:40:47.759 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.2323, Val Loss: 0.2708
2025-06-25 06:40:47.759 INFO: Epoch 39, Train Loss: 0.2323, Val Loss: 0.2708
train_e/atom_mae: 0.001927
2025-06-25 06:40:47.760 INFO: train_e/atom_mae: 0.001927
train_e/atom_rmse: 0.002405
2025-06-25 06:40:47.760 INFO: train_e/atom_rmse: 0.002405
train_f_mae: 0.010299
2025-06-25 06:40:47.764 INFO: train_f_mae: 0.010299
train_f_rmse: 0.015009
2025-06-25 06:40:47.764 INFO: train_f_rmse: 0.015009
val_e/atom_mae: 0.000638
2025-06-25 06:40:47.767 INFO: val_e/atom_mae: 0.000638
val_e/atom_rmse: 0.000762
2025-06-25 06:40:47.767 INFO: val_e/atom_rmse: 0.000762
val_f_mae: 0.010900
2025-06-25 06:40:47.768 INFO: val_f_mae: 0.010900
val_f_rmse: 0.016435
2025-06-25 06:40:47.768 INFO: val_f_rmse: 0.016435
##### Step: 39 Learning rate: 0.005 #####
2025-06-25 06:41:32.874 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.2531, Val Loss: 0.2172
2025-06-25 06:41:32.875 INFO: Epoch 40, Train Loss: 0.2531, Val Loss: 0.2172
train_e/atom_mae: 0.002230
2025-06-25 06:41:32.876 INFO: train_e/atom_mae: 0.002230
train_e/atom_rmse: 0.002867
2025-06-25 06:41:32.876 INFO: train_e/atom_rmse: 0.002867
train_f_mae: 0.010643
2025-06-25 06:41:32.879 INFO: train_f_mae: 0.010643
train_f_rmse: 0.015593
2025-06-25 06:41:32.880 INFO: train_f_rmse: 0.015593
val_e/atom_mae: 0.000382
2025-06-25 06:41:32.882 INFO: val_e/atom_mae: 0.000382
val_e/atom_rmse: 0.000491
2025-06-25 06:41:32.883 INFO: val_e/atom_rmse: 0.000491
val_f_mae: 0.010403
2025-06-25 06:41:32.883 INFO: val_f_mae: 0.010403
val_f_rmse: 0.014726
2025-06-25 06:41:32.883 INFO: val_f_rmse: 0.014726
2025-06-25 06:41:32.896 INFO: Second train loop:
2025-06-25 06:41:32.896 INFO: training
##### Step: 40 Learning rate: 0.0025 #####
2025-06-25 06:42:17.985 INFO: ##### Step: 40 Learning rate: 0.0025 #####
Epoch 1, Train Loss: 0.2039, Val Loss: 0.1486
2025-06-25 06:42:17.985 INFO: Epoch 1, Train Loss: 0.2039, Val Loss: 0.1486
train_e/atom_mae: 0.001226
2025-06-25 06:42:17.986 INFO: train_e/atom_mae: 0.001226
train_e/atom_rmse: 0.001630
2025-06-25 06:42:17.986 INFO: train_e/atom_rmse: 0.001630
train_f_mae: 0.009189
2025-06-25 06:42:17.990 INFO: train_f_mae: 0.009189
train_f_rmse: 0.013106
2025-06-25 06:42:17.990 INFO: train_f_rmse: 0.013106
val_e/atom_mae: 0.000375
2025-06-25 06:42:17.993 INFO: val_e/atom_mae: 0.000375
val_e/atom_rmse: 0.000404
2025-06-25 06:42:17.993 INFO: val_e/atom_rmse: 0.000404
val_f_mae: 0.008659
2025-06-25 06:42:17.994 INFO: val_f_mae: 0.008659
val_f_rmse: 0.012109
2025-06-25 06:42:17.994 INFO: val_f_rmse: 0.012109
##### Step: 41 Learning rate: 0.0025 #####
2025-06-25 06:43:03.128 INFO: ##### Step: 41 Learning rate: 0.0025 #####
Epoch 2, Train Loss: 0.1848, Val Loss: 0.1806
2025-06-25 06:43:03.128 INFO: Epoch 2, Train Loss: 0.1848, Val Loss: 0.1806
train_e/atom_mae: 0.000830
2025-06-25 06:43:03.129 INFO: train_e/atom_mae: 0.000830
train_e/atom_rmse: 0.001119
2025-06-25 06:43:03.129 INFO: train_e/atom_rmse: 0.001119
train_f_mae: 0.009119
2025-06-25 06:43:03.133 INFO: train_f_mae: 0.009119
train_f_rmse: 0.013026
2025-06-25 06:43:03.133 INFO: train_f_rmse: 0.013026
val_e/atom_mae: 0.000393
2025-06-25 06:43:03.136 INFO: val_e/atom_mae: 0.000393
val_e/atom_rmse: 0.000461
2025-06-25 06:43:03.136 INFO: val_e/atom_rmse: 0.000461
val_f_mae: 0.009456
2025-06-25 06:43:03.136 INFO: val_f_mae: 0.009456
val_f_rmse: 0.013342
2025-06-25 06:43:03.137 INFO: val_f_rmse: 0.013342
##### Step: 42 Learning rate: 0.0025 #####
2025-06-25 06:43:48.205 INFO: ##### Step: 42 Learning rate: 0.0025 #####
Epoch 3, Train Loss: 0.2055, Val Loss: 0.1774
2025-06-25 06:43:48.205 INFO: Epoch 3, Train Loss: 0.2055, Val Loss: 0.1774
train_e/atom_mae: 0.001299
2025-06-25 06:43:48.206 INFO: train_e/atom_mae: 0.001299
train_e/atom_rmse: 0.001644
2025-06-25 06:43:48.207 INFO: train_e/atom_rmse: 0.001644
train_f_mae: 0.009211
2025-06-25 06:43:48.210 INFO: train_f_mae: 0.009211
train_f_rmse: 0.013145
2025-06-25 06:43:48.210 INFO: train_f_rmse: 0.013145
val_e/atom_mae: 0.001168
2025-06-25 06:43:48.213 INFO: val_e/atom_mae: 0.001168
val_e/atom_rmse: 0.001202
2025-06-25 06:43:48.213 INFO: val_e/atom_rmse: 0.001202
val_f_mae: 0.008925
2025-06-25 06:43:48.214 INFO: val_f_mae: 0.008925
val_f_rmse: 0.012644
2025-06-25 06:43:48.214 INFO: val_f_rmse: 0.012644
##### Step: 43 Learning rate: 0.0025 #####
2025-06-25 06:44:33.315 INFO: ##### Step: 43 Learning rate: 0.0025 #####
Epoch 4, Train Loss: 0.1914, Val Loss: 0.2335
2025-06-25 06:44:33.315 INFO: Epoch 4, Train Loss: 0.1914, Val Loss: 0.2335
train_e/atom_mae: 0.001013
2025-06-25 06:44:33.316 INFO: train_e/atom_mae: 0.001013
train_e/atom_rmse: 0.001267
2025-06-25 06:44:33.316 INFO: train_e/atom_rmse: 0.001267
train_f_mae: 0.009209
2025-06-25 06:44:33.320 INFO: train_f_mae: 0.009209
train_f_rmse: 0.013116
2025-06-25 06:44:33.320 INFO: train_f_rmse: 0.013116
val_e/atom_mae: 0.002030
2025-06-25 06:44:33.323 INFO: val_e/atom_mae: 0.002030
val_e/atom_rmse: 0.002071
2025-06-25 06:44:33.323 INFO: val_e/atom_rmse: 0.002071
val_f_mae: 0.009519
2025-06-25 06:44:33.324 INFO: val_f_mae: 0.009519
val_f_rmse: 0.013476
2025-06-25 06:44:33.324 INFO: val_f_rmse: 0.013476
##### Step: 44 Learning rate: 0.0025 #####
2025-06-25 06:45:18.293 INFO: ##### Step: 44 Learning rate: 0.0025 #####
Epoch 5, Train Loss: 0.1856, Val Loss: 0.1874
2025-06-25 06:45:18.293 INFO: Epoch 5, Train Loss: 0.1856, Val Loss: 0.1874
train_e/atom_mae: 0.001114
2025-06-25 06:45:18.294 INFO: train_e/atom_mae: 0.001114
train_e/atom_rmse: 0.001385
2025-06-25 06:45:18.294 INFO: train_e/atom_rmse: 0.001385
train_f_mae: 0.009009
2025-06-25 06:45:18.298 INFO: train_f_mae: 0.009009
train_f_rmse: 0.012744
2025-06-25 06:45:18.298 INFO: train_f_rmse: 0.012744
val_e/atom_mae: 0.001127
2025-06-25 06:45:18.301 INFO: val_e/atom_mae: 0.001127
val_e/atom_rmse: 0.001262
2025-06-25 06:45:18.301 INFO: val_e/atom_rmse: 0.001262
val_f_mae: 0.009308
2025-06-25 06:45:18.302 INFO: val_f_mae: 0.009308
val_f_rmse: 0.012968
2025-06-25 06:45:18.302 INFO: val_f_rmse: 0.012968
##### Step: 45 Learning rate: 0.0025 #####
2025-06-25 06:46:03.480 INFO: ##### Step: 45 Learning rate: 0.0025 #####
Epoch 6, Train Loss: 0.2015, Val Loss: 0.1970
2025-06-25 06:46:03.481 INFO: Epoch 6, Train Loss: 0.2015, Val Loss: 0.1970
train_e/atom_mae: 0.001041
2025-06-25 06:46:03.482 INFO: train_e/atom_mae: 0.001041
train_e/atom_rmse: 0.001310
2025-06-25 06:46:03.482 INFO: train_e/atom_rmse: 0.001310
train_f_mae: 0.009298
2025-06-25 06:46:03.485 INFO: train_f_mae: 0.009298
train_f_rmse: 0.013443
2025-06-25 06:46:03.485 INFO: train_f_rmse: 0.013443
val_e/atom_mae: 0.001371
2025-06-25 06:46:03.488 INFO: val_e/atom_mae: 0.001371
val_e/atom_rmse: 0.001401
2025-06-25 06:46:03.489 INFO: val_e/atom_rmse: 0.001401
val_f_mae: 0.009390
2025-06-25 06:46:03.489 INFO: val_f_mae: 0.009390
val_f_rmse: 0.013162
2025-06-25 06:46:03.489 INFO: val_f_rmse: 0.013162
##### Step: 46 Learning rate: 0.0025 #####
2025-06-25 06:46:48.489 INFO: ##### Step: 46 Learning rate: 0.0025 #####
Epoch 7, Train Loss: 0.1824, Val Loss: 0.1903
2025-06-25 06:46:48.490 INFO: Epoch 7, Train Loss: 0.1824, Val Loss: 0.1903
train_e/atom_mae: 0.000874
2025-06-25 06:46:48.490 INFO: train_e/atom_mae: 0.000874
train_e/atom_rmse: 0.001150
2025-06-25 06:46:48.491 INFO: train_e/atom_rmse: 0.001150
train_f_mae: 0.009072
2025-06-25 06:46:48.494 INFO: train_f_mae: 0.009072
train_f_rmse: 0.012898
2025-06-25 06:46:48.494 INFO: train_f_rmse: 0.012898
val_e/atom_mae: 0.000766
2025-06-25 06:46:48.497 INFO: val_e/atom_mae: 0.000766
val_e/atom_rmse: 0.000857
2025-06-25 06:46:48.497 INFO: val_e/atom_rmse: 0.000857
val_f_mae: 0.009344
2025-06-25 06:46:48.498 INFO: val_f_mae: 0.009344
val_f_rmse: 0.013471
2025-06-25 06:46:48.498 INFO: val_f_rmse: 0.013471
##### Step: 47 Learning rate: 0.0025 #####
2025-06-25 06:47:33.563 INFO: ##### Step: 47 Learning rate: 0.0025 #####
Epoch 8, Train Loss: 0.1942, Val Loss: 0.2793
2025-06-25 06:47:33.563 INFO: Epoch 8, Train Loss: 0.1942, Val Loss: 0.2793
train_e/atom_mae: 0.001155
2025-06-25 06:47:33.564 INFO: train_e/atom_mae: 0.001155
train_e/atom_rmse: 0.001457
2025-06-25 06:47:33.564 INFO: train_e/atom_rmse: 0.001457
train_f_mae: 0.009077
2025-06-25 06:47:33.568 INFO: train_f_mae: 0.009077
train_f_rmse: 0.012980
2025-06-25 06:47:33.568 INFO: train_f_rmse: 0.012980
val_e/atom_mae: 0.002532
2025-06-25 06:47:33.571 INFO: val_e/atom_mae: 0.002532
val_e/atom_rmse: 0.002545
2025-06-25 06:47:33.572 INFO: val_e/atom_rmse: 0.002545
val_f_mae: 0.009482
2025-06-25 06:47:33.572 INFO: val_f_mae: 0.009482
val_f_rmse: 0.014174
2025-06-25 06:47:33.572 INFO: val_f_rmse: 0.014174
##### Step: 48 Learning rate: 0.0025 #####
2025-06-25 06:48:18.553 INFO: ##### Step: 48 Learning rate: 0.0025 #####
Epoch 9, Train Loss: 0.1986, Val Loss: 0.1785
2025-06-25 06:48:18.554 INFO: Epoch 9, Train Loss: 0.1986, Val Loss: 0.1785
train_e/atom_mae: 0.001183
2025-06-25 06:48:18.555 INFO: train_e/atom_mae: 0.001183
train_e/atom_rmse: 0.001549
2025-06-25 06:48:18.555 INFO: train_e/atom_rmse: 0.001549
train_f_mae: 0.009052
2025-06-25 06:48:18.559 INFO: train_f_mae: 0.009052
train_f_rmse: 0.013022
2025-06-25 06:48:18.559 INFO: train_f_rmse: 0.013022
val_e/atom_mae: 0.000501
2025-06-25 06:48:18.561 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000618
2025-06-25 06:48:18.562 INFO: val_e/atom_rmse: 0.000618
val_f_mae: 0.009431
2025-06-25 06:48:18.562 INFO: val_f_mae: 0.009431
val_f_rmse: 0.013184
2025-06-25 06:48:18.562 INFO: val_f_rmse: 0.013184
##### Step: 49 Learning rate: 0.0025 #####
2025-06-25 06:49:03.688 INFO: ##### Step: 49 Learning rate: 0.0025 #####
Epoch 10, Train Loss: 0.1968, Val Loss: 0.2515
2025-06-25 06:49:03.688 INFO: Epoch 10, Train Loss: 0.1968, Val Loss: 0.2515
train_e/atom_mae: 0.000997
2025-06-25 06:49:03.689 INFO: train_e/atom_mae: 0.000997
train_e/atom_rmse: 0.001316
2025-06-25 06:49:03.689 INFO: train_e/atom_rmse: 0.001316
train_f_mae: 0.009234
2025-06-25 06:49:03.693 INFO: train_f_mae: 0.009234
train_f_rmse: 0.013260
2025-06-25 06:49:03.693 INFO: train_f_rmse: 0.013260
val_e/atom_mae: 0.001840
2025-06-25 06:49:03.696 INFO: val_e/atom_mae: 0.001840
val_e/atom_rmse: 0.001904
2025-06-25 06:49:03.696 INFO: val_e/atom_rmse: 0.001904
val_f_mae: 0.010078
2025-06-25 06:49:03.697 INFO: val_f_mae: 0.010078
val_f_rmse: 0.014409
2025-06-25 06:49:03.697 INFO: val_f_rmse: 0.014409
##### Step: 50 Learning rate: 0.0025 #####
2025-06-25 06:49:48.703 INFO: ##### Step: 50 Learning rate: 0.0025 #####
Epoch 11, Train Loss: 0.2044, Val Loss: 0.1736
2025-06-25 06:49:48.704 INFO: Epoch 11, Train Loss: 0.2044, Val Loss: 0.1736
train_e/atom_mae: 0.001113
2025-06-25 06:49:48.705 INFO: train_e/atom_mae: 0.001113
train_e/atom_rmse: 0.001462
2025-06-25 06:49:48.705 INFO: train_e/atom_rmse: 0.001462
train_f_mae: 0.009242
2025-06-25 06:49:48.709 INFO: train_f_mae: 0.009242
train_f_rmse: 0.013361
2025-06-25 06:49:48.709 INFO: train_f_rmse: 0.013361
val_e/atom_mae: 0.000338
2025-06-25 06:49:48.711 INFO: val_e/atom_mae: 0.000338
val_e/atom_rmse: 0.000376
2025-06-25 06:49:48.712 INFO: val_e/atom_rmse: 0.000376
val_f_mae: 0.009307
2025-06-25 06:49:48.712 INFO: val_f_mae: 0.009307
val_f_rmse: 0.013112
2025-06-25 06:49:48.712 INFO: val_f_rmse: 0.013112
##### Step: 51 Learning rate: 0.0025 #####
2025-06-25 06:50:33.882 INFO: ##### Step: 51 Learning rate: 0.0025 #####
Epoch 12, Train Loss: 0.1869, Val Loss: 0.1819
2025-06-25 06:50:33.883 INFO: Epoch 12, Train Loss: 0.1869, Val Loss: 0.1819
train_e/atom_mae: 0.000903
2025-06-25 06:50:33.884 INFO: train_e/atom_mae: 0.000903
train_e/atom_rmse: 0.001151
2025-06-25 06:50:33.884 INFO: train_e/atom_rmse: 0.001151
train_f_mae: 0.009145
2025-06-25 06:50:33.887 INFO: train_f_mae: 0.009145
train_f_rmse: 0.013071
2025-06-25 06:50:33.888 INFO: train_f_rmse: 0.013071
val_e/atom_mae: 0.001391
2025-06-25 06:50:33.890 INFO: val_e/atom_mae: 0.001391
val_e/atom_rmse: 0.001433
2025-06-25 06:50:33.891 INFO: val_e/atom_rmse: 0.001433
val_f_mae: 0.008889
2025-06-25 06:50:33.891 INFO: val_f_mae: 0.008889
val_f_rmse: 0.012533
2025-06-25 06:50:33.891 INFO: val_f_rmse: 0.012533
##### Step: 52 Learning rate: 0.0025 #####
2025-06-25 06:51:18.912 INFO: ##### Step: 52 Learning rate: 0.0025 #####
Epoch 13, Train Loss: 0.1891, Val Loss: 0.1547
2025-06-25 06:51:18.912 INFO: Epoch 13, Train Loss: 0.1891, Val Loss: 0.1547
train_e/atom_mae: 0.001079
2025-06-25 06:51:18.913 INFO: train_e/atom_mae: 0.001079
train_e/atom_rmse: 0.001400
2025-06-25 06:51:18.914 INFO: train_e/atom_rmse: 0.001400
train_f_mae: 0.009003
2025-06-25 06:51:18.917 INFO: train_f_mae: 0.009003
train_f_rmse: 0.012859
2025-06-25 06:51:18.917 INFO: train_f_rmse: 0.012859
val_e/atom_mae: 0.000274
2025-06-25 06:51:18.920 INFO: val_e/atom_mae: 0.000274
val_e/atom_rmse: 0.000327
2025-06-25 06:51:18.920 INFO: val_e/atom_rmse: 0.000327
val_f_mae: 0.008838
2025-06-25 06:51:18.921 INFO: val_f_mae: 0.008838
val_f_rmse: 0.012384
2025-06-25 06:51:18.921 INFO: val_f_rmse: 0.012384
##### Step: 53 Learning rate: 0.0025 #####
2025-06-25 06:52:04.095 INFO: ##### Step: 53 Learning rate: 0.0025 #####
Epoch 14, Train Loss: 0.2023, Val Loss: 0.1687
2025-06-25 06:52:04.095 INFO: Epoch 14, Train Loss: 0.2023, Val Loss: 0.1687
train_e/atom_mae: 0.001177
2025-06-25 06:52:04.096 INFO: train_e/atom_mae: 0.001177
train_e/atom_rmse: 0.001446
2025-06-25 06:52:04.096 INFO: train_e/atom_rmse: 0.001446
train_f_mae: 0.009250
2025-06-25 06:52:04.100 INFO: train_f_mae: 0.009250
train_f_rmse: 0.013305
2025-06-25 06:52:04.100 INFO: train_f_rmse: 0.013305
val_e/atom_mae: 0.001092
2025-06-25 06:52:04.103 INFO: val_e/atom_mae: 0.001092
val_e/atom_rmse: 0.001128
2025-06-25 06:52:04.103 INFO: val_e/atom_rmse: 0.001128
val_f_mae: 0.008888
2025-06-25 06:52:04.103 INFO: val_f_mae: 0.008888
val_f_rmse: 0.012380
2025-06-25 06:52:04.104 INFO: val_f_rmse: 0.012380
##### Step: 54 Learning rate: 0.0025 #####
2025-06-25 06:52:49.149 INFO: ##### Step: 54 Learning rate: 0.0025 #####
Epoch 15, Train Loss: 0.1942, Val Loss: 0.1698
2025-06-25 06:52:49.149 INFO: Epoch 15, Train Loss: 0.1942, Val Loss: 0.1698
train_e/atom_mae: 0.000911
2025-06-25 06:52:49.150 INFO: train_e/atom_mae: 0.000911
train_e/atom_rmse: 0.001151
2025-06-25 06:52:49.150 INFO: train_e/atom_rmse: 0.001151
train_f_mae: 0.009295
2025-06-25 06:52:49.154 INFO: train_f_mae: 0.009295
train_f_rmse: 0.013347
2025-06-25 06:52:49.154 INFO: train_f_rmse: 0.013347
val_e/atom_mae: 0.000509
2025-06-25 06:52:49.157 INFO: val_e/atom_mae: 0.000509
val_e/atom_rmse: 0.000544
2025-06-25 06:52:49.157 INFO: val_e/atom_rmse: 0.000544
val_f_mae: 0.009080
2025-06-25 06:52:49.157 INFO: val_f_mae: 0.009080
val_f_rmse: 0.012892
2025-06-25 06:52:49.158 INFO: val_f_rmse: 0.012892
##### Step: 55 Learning rate: 0.0025 #####
2025-06-25 06:53:34.303 INFO: ##### Step: 55 Learning rate: 0.0025 #####
Epoch 16, Train Loss: 0.2135, Val Loss: 0.2352
2025-06-25 06:53:34.304 INFO: Epoch 16, Train Loss: 0.2135, Val Loss: 0.2352
train_e/atom_mae: 0.001266
2025-06-25 06:53:34.305 INFO: train_e/atom_mae: 0.001266
train_e/atom_rmse: 0.001694
2025-06-25 06:53:34.305 INFO: train_e/atom_rmse: 0.001694
train_f_mae: 0.009271
2025-06-25 06:53:34.308 INFO: train_f_mae: 0.009271
train_f_rmse: 0.013370
2025-06-25 06:53:34.309 INFO: train_f_rmse: 0.013370
val_e/atom_mae: 0.000719
2025-06-25 06:53:34.311 INFO: val_e/atom_mae: 0.000719
val_e/atom_rmse: 0.000833
2025-06-25 06:53:34.312 INFO: val_e/atom_rmse: 0.000833
val_f_mae: 0.010072
2025-06-25 06:53:34.312 INFO: val_f_mae: 0.010072
val_f_rmse: 0.015061
2025-06-25 06:53:34.312 INFO: val_f_rmse: 0.015061
##### Step: 56 Learning rate: 0.0025 #####
2025-06-25 06:54:19.343 INFO: ##### Step: 56 Learning rate: 0.0025 #####
Epoch 17, Train Loss: 0.1815, Val Loss: 0.2034
2025-06-25 06:54:19.343 INFO: Epoch 17, Train Loss: 0.1815, Val Loss: 0.2034
train_e/atom_mae: 0.000874
2025-06-25 06:54:19.344 INFO: train_e/atom_mae: 0.000874
train_e/atom_rmse: 0.001116
2025-06-25 06:54:19.344 INFO: train_e/atom_rmse: 0.001116
train_f_mae: 0.009009
2025-06-25 06:54:19.348 INFO: train_f_mae: 0.009009
train_f_rmse: 0.012901
2025-06-25 06:54:19.348 INFO: train_f_rmse: 0.012901
val_e/atom_mae: 0.001022
2025-06-25 06:54:19.350 INFO: val_e/atom_mae: 0.001022
val_e/atom_rmse: 0.001060
2025-06-25 06:54:19.351 INFO: val_e/atom_rmse: 0.001060
val_f_mae: 0.009603
2025-06-25 06:54:19.351 INFO: val_f_mae: 0.009603
val_f_rmse: 0.013778
2025-06-25 06:54:19.351 INFO: val_f_rmse: 0.013778
##### Step: 57 Learning rate: 0.0025 #####
2025-06-25 06:55:04.452 INFO: ##### Step: 57 Learning rate: 0.0025 #####
Epoch 18, Train Loss: 0.1902, Val Loss: 0.1925
2025-06-25 06:55:04.452 INFO: Epoch 18, Train Loss: 0.1902, Val Loss: 0.1925
train_e/atom_mae: 0.001067
2025-06-25 06:55:04.453 INFO: train_e/atom_mae: 0.001067
train_e/atom_rmse: 0.001347
2025-06-25 06:55:04.453 INFO: train_e/atom_rmse: 0.001347
train_f_mae: 0.009073
2025-06-25 06:55:04.457 INFO: train_f_mae: 0.009073
train_f_rmse: 0.012970
2025-06-25 06:55:04.457 INFO: train_f_rmse: 0.012970
val_e/atom_mae: 0.001105
2025-06-25 06:55:04.460 INFO: val_e/atom_mae: 0.001105
val_e/atom_rmse: 0.001248
2025-06-25 06:55:04.460 INFO: val_e/atom_rmse: 0.001248
val_f_mae: 0.009200
2025-06-25 06:55:04.461 INFO: val_f_mae: 0.009200
val_f_rmse: 0.013179
2025-06-25 06:55:04.461 INFO: val_f_rmse: 0.013179
##### Step: 58 Learning rate: 0.0025 #####
2025-06-25 06:55:49.458 INFO: ##### Step: 58 Learning rate: 0.0025 #####
Epoch 19, Train Loss: 0.1795, Val Loss: 0.1545
2025-06-25 06:55:49.458 INFO: Epoch 19, Train Loss: 0.1795, Val Loss: 0.1545
train_e/atom_mae: 0.000799
2025-06-25 06:55:49.459 INFO: train_e/atom_mae: 0.000799
train_e/atom_rmse: 0.001031
2025-06-25 06:55:49.459 INFO: train_e/atom_rmse: 0.001031
train_f_mae: 0.009032
2025-06-25 06:55:49.463 INFO: train_f_mae: 0.009032
train_f_rmse: 0.012907
2025-06-25 06:55:49.463 INFO: train_f_rmse: 0.012907
val_e/atom_mae: 0.000730
2025-06-25 06:55:49.465 INFO: val_e/atom_mae: 0.000730
val_e/atom_rmse: 0.000772
2025-06-25 06:55:49.466 INFO: val_e/atom_rmse: 0.000772
val_f_mae: 0.008578
2025-06-25 06:55:49.466 INFO: val_f_mae: 0.008578
val_f_rmse: 0.012137
2025-06-25 06:55:49.466 INFO: val_f_rmse: 0.012137
##### Step: 59 Learning rate: 0.0025 #####
2025-06-25 06:56:34.527 INFO: ##### Step: 59 Learning rate: 0.0025 #####
Epoch 20, Train Loss: 0.1656, Val Loss: 0.1910
2025-06-25 06:56:34.528 INFO: Epoch 20, Train Loss: 0.1656, Val Loss: 0.1910
train_e/atom_mae: 0.000669
2025-06-25 06:56:34.528 INFO: train_e/atom_mae: 0.000669
train_e/atom_rmse: 0.000899
2025-06-25 06:56:34.529 INFO: train_e/atom_rmse: 0.000899
train_f_mae: 0.008811
2025-06-25 06:56:34.532 INFO: train_f_mae: 0.008811
train_f_rmse: 0.012484
2025-06-25 06:56:34.532 INFO: train_f_rmse: 0.012484
val_e/atom_mae: 0.000664
2025-06-25 06:56:34.535 INFO: val_e/atom_mae: 0.000664
val_e/atom_rmse: 0.000747
2025-06-25 06:56:34.536 INFO: val_e/atom_rmse: 0.000747
val_f_mae: 0.009386
2025-06-25 06:56:34.536 INFO: val_f_mae: 0.009386
val_f_rmse: 0.013574
2025-06-25 06:56:34.536 INFO: val_f_rmse: 0.013574
##### Step: 60 Learning rate: 0.00125 #####
2025-06-25 06:57:19.646 INFO: ##### Step: 60 Learning rate: 0.00125 #####
Epoch 21, Train Loss: 0.1451, Val Loss: 0.1377
2025-06-25 06:57:19.646 INFO: Epoch 21, Train Loss: 0.1451, Val Loss: 0.1377
train_e/atom_mae: 0.000556
2025-06-25 06:57:19.647 INFO: train_e/atom_mae: 0.000556
train_e/atom_rmse: 0.000690
2025-06-25 06:57:19.647 INFO: train_e/atom_rmse: 0.000690
train_f_mae: 0.008412
2025-06-25 06:57:19.651 INFO: train_f_mae: 0.008412
train_f_rmse: 0.011802
2025-06-25 06:57:19.651 INFO: train_f_rmse: 0.011802
val_e/atom_mae: 0.000756
2025-06-25 06:57:19.653 INFO: val_e/atom_mae: 0.000756
val_e/atom_rmse: 0.000764
2025-06-25 06:57:19.654 INFO: val_e/atom_rmse: 0.000764
val_f_mae: 0.008187
2025-06-25 06:57:19.654 INFO: val_f_mae: 0.008187
val_f_rmse: 0.011430
2025-06-25 06:57:19.654 INFO: val_f_rmse: 0.011430
##### Step: 61 Learning rate: 0.00125 #####
2025-06-25 06:58:04.675 INFO: ##### Step: 61 Learning rate: 0.00125 #####
Epoch 22, Train Loss: 0.1486, Val Loss: 0.1385
2025-06-25 06:58:04.675 INFO: Epoch 22, Train Loss: 0.1486, Val Loss: 0.1385
train_e/atom_mae: 0.000688
2025-06-25 06:58:04.676 INFO: train_e/atom_mae: 0.000688
train_e/atom_rmse: 0.000895
2025-06-25 06:58:04.676 INFO: train_e/atom_rmse: 0.000895
train_f_mae: 0.008389
2025-06-25 06:58:04.680 INFO: train_f_mae: 0.008389
train_f_rmse: 0.011788
2025-06-25 06:58:04.680 INFO: train_f_rmse: 0.011788
val_e/atom_mae: 0.000501
2025-06-25 06:58:04.683 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000573
2025-06-25 06:58:04.683 INFO: val_e/atom_rmse: 0.000573
val_f_mae: 0.008352
2025-06-25 06:58:04.684 INFO: val_f_mae: 0.008352
val_f_rmse: 0.011599
2025-06-25 06:58:04.684 INFO: val_f_rmse: 0.011599
##### Step: 62 Learning rate: 0.00125 #####
2025-06-25 06:58:49.763 INFO: ##### Step: 62 Learning rate: 0.00125 #####
Epoch 23, Train Loss: 0.1451, Val Loss: 0.1465
2025-06-25 06:58:49.763 INFO: Epoch 23, Train Loss: 0.1451, Val Loss: 0.1465
train_e/atom_mae: 0.000602
2025-06-25 06:58:49.764 INFO: train_e/atom_mae: 0.000602
train_e/atom_rmse: 0.000765
2025-06-25 06:58:49.764 INFO: train_e/atom_rmse: 0.000765
train_f_mae: 0.008342
2025-06-25 06:58:49.768 INFO: train_f_mae: 0.008342
train_f_rmse: 0.011746
2025-06-25 06:58:49.768 INFO: train_f_rmse: 0.011746
val_e/atom_mae: 0.000546
2025-06-25 06:58:49.771 INFO: val_e/atom_mae: 0.000546
val_e/atom_rmse: 0.000683
2025-06-25 06:58:49.771 INFO: val_e/atom_rmse: 0.000683
val_f_mae: 0.008575
2025-06-25 06:58:49.771 INFO: val_f_mae: 0.008575
val_f_rmse: 0.011868
2025-06-25 06:58:49.772 INFO: val_f_rmse: 0.011868
##### Step: 63 Learning rate: 0.00125 #####
2025-06-25 06:59:34.750 INFO: ##### Step: 63 Learning rate: 0.00125 #####
Epoch 24, Train Loss: 0.1401, Val Loss: 0.1503
2025-06-25 06:59:34.751 INFO: Epoch 24, Train Loss: 0.1401, Val Loss: 0.1503
train_e/atom_mae: 0.000581
2025-06-25 06:59:34.752 INFO: train_e/atom_mae: 0.000581
train_e/atom_rmse: 0.000717
2025-06-25 06:59:34.752 INFO: train_e/atom_rmse: 0.000717
train_f_mae: 0.008290
2025-06-25 06:59:34.755 INFO: train_f_mae: 0.008290
train_f_rmse: 0.011570
2025-06-25 06:59:34.755 INFO: train_f_rmse: 0.011570
val_e/atom_mae: 0.000468
2025-06-25 06:59:34.758 INFO: val_e/atom_mae: 0.000468
val_e/atom_rmse: 0.000606
2025-06-25 06:59:34.759 INFO: val_e/atom_rmse: 0.000606
val_f_mae: 0.008705
2025-06-25 06:59:34.759 INFO: val_f_mae: 0.008705
val_f_rmse: 0.012079
2025-06-25 06:59:34.759 INFO: val_f_rmse: 0.012079
##### Step: 64 Learning rate: 0.00125 #####
2025-06-25 07:00:19.905 INFO: ##### Step: 64 Learning rate: 0.00125 #####
Epoch 25, Train Loss: 0.1450, Val Loss: 0.1481
2025-06-25 07:00:19.906 INFO: Epoch 25, Train Loss: 0.1450, Val Loss: 0.1481
train_e/atom_mae: 0.000624
2025-06-25 07:00:19.907 INFO: train_e/atom_mae: 0.000624
train_e/atom_rmse: 0.000780
2025-06-25 07:00:19.907 INFO: train_e/atom_rmse: 0.000780
train_f_mae: 0.008363
2025-06-25 07:00:19.910 INFO: train_f_mae: 0.008363
train_f_rmse: 0.011732
2025-06-25 07:00:19.911 INFO: train_f_rmse: 0.011732
val_e/atom_mae: 0.000730
2025-06-25 07:00:19.913 INFO: val_e/atom_mae: 0.000730
val_e/atom_rmse: 0.000802
2025-06-25 07:00:19.914 INFO: val_e/atom_rmse: 0.000802
val_f_mae: 0.008390
2025-06-25 07:00:19.914 INFO: val_f_mae: 0.008390
val_f_rmse: 0.011846
2025-06-25 07:00:19.914 INFO: val_f_rmse: 0.011846
##### Step: 65 Learning rate: 0.00125 #####
2025-06-25 07:01:04.874 INFO: ##### Step: 65 Learning rate: 0.00125 #####
Epoch 26, Train Loss: 0.1460, Val Loss: 0.1467
2025-06-25 07:01:04.875 INFO: Epoch 26, Train Loss: 0.1460, Val Loss: 0.1467
train_e/atom_mae: 0.000760
2025-06-25 07:01:04.875 INFO: train_e/atom_mae: 0.000760
train_e/atom_rmse: 0.000971
2025-06-25 07:01:04.876 INFO: train_e/atom_rmse: 0.000971
train_f_mae: 0.008286
2025-06-25 07:01:04.879 INFO: train_f_mae: 0.008286
train_f_rmse: 0.011599
2025-06-25 07:01:04.879 INFO: train_f_rmse: 0.011599
val_e/atom_mae: 0.000861
2025-06-25 07:01:04.882 INFO: val_e/atom_mae: 0.000861
val_e/atom_rmse: 0.000934
2025-06-25 07:01:04.883 INFO: val_e/atom_rmse: 0.000934
val_f_mae: 0.008404
2025-06-25 07:01:04.883 INFO: val_f_mae: 0.008404
val_f_rmse: 0.011669
2025-06-25 07:01:04.883 INFO: val_f_rmse: 0.011669
##### Step: 66 Learning rate: 0.00125 #####
2025-06-25 07:01:49.856 INFO: ##### Step: 66 Learning rate: 0.00125 #####
Epoch 27, Train Loss: 0.1501, Val Loss: 0.1402
2025-06-25 07:01:49.856 INFO: Epoch 27, Train Loss: 0.1501, Val Loss: 0.1402
train_e/atom_mae: 0.000758
2025-06-25 07:01:49.857 INFO: train_e/atom_mae: 0.000758
train_e/atom_rmse: 0.000937
2025-06-25 07:01:49.857 INFO: train_e/atom_rmse: 0.000937
train_f_mae: 0.008388
2025-06-25 07:01:49.861 INFO: train_f_mae: 0.008388
train_f_rmse: 0.011809
2025-06-25 07:01:49.861 INFO: train_f_rmse: 0.011809
val_e/atom_mae: 0.000308
2025-06-25 07:01:49.864 INFO: val_e/atom_mae: 0.000308
val_e/atom_rmse: 0.000373
2025-06-25 07:01:49.864 INFO: val_e/atom_rmse: 0.000373
val_f_mae: 0.008473
2025-06-25 07:01:49.865 INFO: val_f_mae: 0.008473
val_f_rmse: 0.011771
2025-06-25 07:01:49.865 INFO: val_f_rmse: 0.011771
##### Step: 67 Learning rate: 0.00125 #####
2025-06-25 07:02:34.630 INFO: ##### Step: 67 Learning rate: 0.00125 #####
Epoch 28, Train Loss: 0.1496, Val Loss: 0.1574
2025-06-25 07:02:34.630 INFO: Epoch 28, Train Loss: 0.1496, Val Loss: 0.1574
train_e/atom_mae: 0.000828
2025-06-25 07:02:34.631 INFO: train_e/atom_mae: 0.000828
train_e/atom_rmse: 0.001013
2025-06-25 07:02:34.631 INFO: train_e/atom_rmse: 0.001013
train_f_mae: 0.008336
2025-06-25 07:02:34.635 INFO: train_f_mae: 0.008336
train_f_rmse: 0.011710
2025-06-25 07:02:34.635 INFO: train_f_rmse: 0.011710
val_e/atom_mae: 0.000904
2025-06-25 07:02:34.638 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001059
2025-06-25 07:02:34.638 INFO: val_e/atom_rmse: 0.001059
val_f_mae: 0.008538
2025-06-25 07:02:34.638 INFO: val_f_mae: 0.008538
val_f_rmse: 0.011994
2025-06-25 07:02:34.639 INFO: val_f_rmse: 0.011994
##### Step: 68 Learning rate: 0.00125 #####
2025-06-25 07:03:19.542 INFO: ##### Step: 68 Learning rate: 0.00125 #####
Epoch 29, Train Loss: 0.1402, Val Loss: 0.1363
2025-06-25 07:03:19.542 INFO: Epoch 29, Train Loss: 0.1402, Val Loss: 0.1363
train_e/atom_mae: 0.000617
2025-06-25 07:03:19.543 INFO: train_e/atom_mae: 0.000617
train_e/atom_rmse: 0.000784
2025-06-25 07:03:19.543 INFO: train_e/atom_rmse: 0.000784
train_f_mae: 0.008225
2025-06-25 07:03:19.547 INFO: train_f_mae: 0.008225
train_f_rmse: 0.011521
2025-06-25 07:03:19.547 INFO: train_f_rmse: 0.011521
val_e/atom_mae: 0.000264
2025-06-25 07:03:19.550 INFO: val_e/atom_mae: 0.000264
val_e/atom_rmse: 0.000341
2025-06-25 07:03:19.550 INFO: val_e/atom_rmse: 0.000341
val_f_mae: 0.008373
2025-06-25 07:03:19.551 INFO: val_f_mae: 0.008373
val_f_rmse: 0.011615
2025-06-25 07:03:19.551 INFO: val_f_rmse: 0.011615
##### Step: 69 Learning rate: 0.00125 #####
2025-06-25 07:04:04.320 INFO: ##### Step: 69 Learning rate: 0.00125 #####
Epoch 30, Train Loss: 0.1486, Val Loss: 0.1842
2025-06-25 07:04:04.320 INFO: Epoch 30, Train Loss: 0.1486, Val Loss: 0.1842
train_e/atom_mae: 0.000747
2025-06-25 07:04:04.321 INFO: train_e/atom_mae: 0.000747
train_e/atom_rmse: 0.000916
2025-06-25 07:04:04.321 INFO: train_e/atom_rmse: 0.000916
train_f_mae: 0.008385
2025-06-25 07:04:04.325 INFO: train_f_mae: 0.008385
train_f_rmse: 0.011768
2025-06-25 07:04:04.325 INFO: train_f_rmse: 0.011768
val_e/atom_mae: 0.001018
2025-06-25 07:04:04.328 INFO: val_e/atom_mae: 0.001018
val_e/atom_rmse: 0.001177
2025-06-25 07:04:04.328 INFO: val_e/atom_rmse: 0.001177
val_f_mae: 0.008885
2025-06-25 07:04:04.329 INFO: val_f_mae: 0.008885
val_f_rmse: 0.012941
2025-06-25 07:04:04.329 INFO: val_f_rmse: 0.012941
##### Step: 70 Learning rate: 0.00125 #####
2025-06-25 07:04:49.247 INFO: ##### Step: 70 Learning rate: 0.00125 #####
Epoch 31, Train Loss: 0.1405, Val Loss: 0.1412
2025-06-25 07:04:49.247 INFO: Epoch 31, Train Loss: 0.1405, Val Loss: 0.1412
train_e/atom_mae: 0.000605
2025-06-25 07:04:49.249 INFO: train_e/atom_mae: 0.000605
train_e/atom_rmse: 0.000741
2025-06-25 07:04:49.249 INFO: train_e/atom_rmse: 0.000741
train_f_mae: 0.008224
2025-06-25 07:04:49.252 INFO: train_f_mae: 0.008224
train_f_rmse: 0.011570
2025-06-25 07:04:49.252 INFO: train_f_rmse: 0.011570
val_e/atom_mae: 0.000978
2025-06-25 07:04:49.255 INFO: val_e/atom_mae: 0.000978
val_e/atom_rmse: 0.001006
2025-06-25 07:04:49.255 INFO: val_e/atom_rmse: 0.001006
val_f_mae: 0.008177
2025-06-25 07:04:49.256 INFO: val_f_mae: 0.008177
val_f_rmse: 0.011356
2025-06-25 07:04:49.256 INFO: val_f_rmse: 0.011356
##### Step: 71 Learning rate: 0.00125 #####
2025-06-25 07:05:34.047 INFO: ##### Step: 71 Learning rate: 0.00125 #####
Epoch 32, Train Loss: 0.1407, Val Loss: 0.1365
2025-06-25 07:05:34.047 INFO: Epoch 32, Train Loss: 0.1407, Val Loss: 0.1365
train_e/atom_mae: 0.000770
2025-06-25 07:05:34.048 INFO: train_e/atom_mae: 0.000770
train_e/atom_rmse: 0.000963
2025-06-25 07:05:34.048 INFO: train_e/atom_rmse: 0.000963
train_f_mae: 0.008153
2025-06-25 07:05:34.052 INFO: train_f_mae: 0.008153
train_f_rmse: 0.011377
2025-06-25 07:05:34.052 INFO: train_f_rmse: 0.011377
val_e/atom_mae: 0.000432
2025-06-25 07:05:34.054 INFO: val_e/atom_mae: 0.000432
val_e/atom_rmse: 0.000499
2025-06-25 07:05:34.055 INFO: val_e/atom_rmse: 0.000499
val_f_mae: 0.008255
2025-06-25 07:05:34.055 INFO: val_f_mae: 0.008255
val_f_rmse: 0.011553
2025-06-25 07:05:34.055 INFO: val_f_rmse: 0.011553
##### Step: 72 Learning rate: 0.00125 #####
2025-06-25 07:06:19.193 INFO: ##### Step: 72 Learning rate: 0.00125 #####
Epoch 33, Train Loss: 0.1379, Val Loss: 0.1532
2025-06-25 07:06:19.194 INFO: Epoch 33, Train Loss: 0.1379, Val Loss: 0.1532
train_e/atom_mae: 0.000564
2025-06-25 07:06:19.195 INFO: train_e/atom_mae: 0.000564
train_e/atom_rmse: 0.000691
2025-06-25 07:06:19.195 INFO: train_e/atom_rmse: 0.000691
train_f_mae: 0.008204
2025-06-25 07:06:19.199 INFO: train_f_mae: 0.008204
train_f_rmse: 0.011496
2025-06-25 07:06:19.199 INFO: train_f_rmse: 0.011496
val_e/atom_mae: 0.000659
2025-06-25 07:06:19.201 INFO: val_e/atom_mae: 0.000659
val_e/atom_rmse: 0.000825
2025-06-25 07:06:19.202 INFO: val_e/atom_rmse: 0.000825
val_f_mae: 0.008595
2025-06-25 07:06:19.202 INFO: val_f_mae: 0.008595
val_f_rmse: 0.012040
2025-06-25 07:06:19.202 INFO: val_f_rmse: 0.012040
##### Step: 73 Learning rate: 0.00125 #####
2025-06-25 07:07:04.244 INFO: ##### Step: 73 Learning rate: 0.00125 #####
Epoch 34, Train Loss: 0.1407, Val Loss: 0.1335
2025-06-25 07:07:04.245 INFO: Epoch 34, Train Loss: 0.1407, Val Loss: 0.1335
train_e/atom_mae: 0.000652
2025-06-25 07:07:04.245 INFO: train_e/atom_mae: 0.000652
train_e/atom_rmse: 0.000800
2025-06-25 07:07:04.246 INFO: train_e/atom_rmse: 0.000800
train_f_mae: 0.008236
2025-06-25 07:07:04.249 INFO: train_f_mae: 0.008236
train_f_rmse: 0.011530
2025-06-25 07:07:04.249 INFO: train_f_rmse: 0.011530
val_e/atom_mae: 0.000322
2025-06-25 07:07:04.252 INFO: val_e/atom_mae: 0.000322
val_e/atom_rmse: 0.000359
2025-06-25 07:07:04.253 INFO: val_e/atom_rmse: 0.000359
val_f_mae: 0.008234
2025-06-25 07:07:04.253 INFO: val_f_mae: 0.008234
val_f_rmse: 0.011485
2025-06-25 07:07:04.253 INFO: val_f_rmse: 0.011485
##### Step: 74 Learning rate: 0.00125 #####
2025-06-25 07:07:49.450 INFO: ##### Step: 74 Learning rate: 0.00125 #####
Epoch 35, Train Loss: 0.1437, Val Loss: 0.1417
2025-06-25 07:07:49.451 INFO: Epoch 35, Train Loss: 0.1437, Val Loss: 0.1417
train_e/atom_mae: 0.000547
2025-06-25 07:07:49.452 INFO: train_e/atom_mae: 0.000547
train_e/atom_rmse: 0.000694
2025-06-25 07:07:49.452 INFO: train_e/atom_rmse: 0.000694
train_f_mae: 0.008330
2025-06-25 07:07:49.455 INFO: train_f_mae: 0.008330
train_f_rmse: 0.011741
2025-06-25 07:07:49.455 INFO: train_f_rmse: 0.011741
val_e/atom_mae: 0.000540
2025-06-25 07:07:49.458 INFO: val_e/atom_mae: 0.000540
val_e/atom_rmse: 0.000574
2025-06-25 07:07:49.459 INFO: val_e/atom_rmse: 0.000574
val_f_mae: 0.008316
2025-06-25 07:07:49.459 INFO: val_f_mae: 0.008316
val_f_rmse: 0.011735
2025-06-25 07:07:49.459 INFO: val_f_rmse: 0.011735
##### Step: 75 Learning rate: 0.00125 #####
2025-06-25 07:08:34.513 INFO: ##### Step: 75 Learning rate: 0.00125 #####
Epoch 36, Train Loss: 0.1416, Val Loss: 0.1446
2025-06-25 07:08:34.513 INFO: Epoch 36, Train Loss: 0.1416, Val Loss: 0.1446
train_e/atom_mae: 0.000619
2025-06-25 07:08:34.514 INFO: train_e/atom_mae: 0.000619
train_e/atom_rmse: 0.000764
2025-06-25 07:08:34.514 INFO: train_e/atom_rmse: 0.000764
train_f_mae: 0.008227
2025-06-25 07:08:34.518 INFO: train_f_mae: 0.008227
train_f_rmse: 0.011599
2025-06-25 07:08:34.518 INFO: train_f_rmse: 0.011599
val_e/atom_mae: 0.000482
2025-06-25 07:08:34.521 INFO: val_e/atom_mae: 0.000482
val_e/atom_rmse: 0.000541
2025-06-25 07:08:34.521 INFO: val_e/atom_rmse: 0.000541
val_f_mae: 0.008427
2025-06-25 07:08:34.522 INFO: val_f_mae: 0.008427
val_f_rmse: 0.011879
2025-06-25 07:08:34.522 INFO: val_f_rmse: 0.011879
##### Step: 76 Learning rate: 0.00125 #####
2025-06-25 07:09:19.635 INFO: ##### Step: 76 Learning rate: 0.00125 #####
Epoch 37, Train Loss: 0.1438, Val Loss: 0.1556
2025-06-25 07:09:19.635 INFO: Epoch 37, Train Loss: 0.1438, Val Loss: 0.1556
train_e/atom_mae: 0.000650
2025-06-25 07:09:19.636 INFO: train_e/atom_mae: 0.000650
train_e/atom_rmse: 0.000802
2025-06-25 07:09:19.636 INFO: train_e/atom_rmse: 0.000802
train_f_mae: 0.008292
2025-06-25 07:09:19.640 INFO: train_f_mae: 0.008292
train_f_rmse: 0.011664
2025-06-25 07:09:19.640 INFO: train_f_rmse: 0.011664
val_e/atom_mae: 0.001243
2025-06-25 07:09:19.643 INFO: val_e/atom_mae: 0.001243
val_e/atom_rmse: 0.001291
2025-06-25 07:09:19.643 INFO: val_e/atom_rmse: 0.001291
val_f_mae: 0.008312
2025-06-25 07:09:19.644 INFO: val_f_mae: 0.008312
val_f_rmse: 0.011637
2025-06-25 07:09:19.644 INFO: val_f_rmse: 0.011637
##### Step: 77 Learning rate: 0.00125 #####
2025-06-25 07:10:04.699 INFO: ##### Step: 77 Learning rate: 0.00125 #####
Epoch 38, Train Loss: 0.1447, Val Loss: 0.1373
2025-06-25 07:10:04.700 INFO: Epoch 38, Train Loss: 0.1447, Val Loss: 0.1373
train_e/atom_mae: 0.000711
2025-06-25 07:10:04.701 INFO: train_e/atom_mae: 0.000711
train_e/atom_rmse: 0.000936
2025-06-25 07:10:04.701 INFO: train_e/atom_rmse: 0.000936
train_f_mae: 0.008253
2025-06-25 07:10:04.704 INFO: train_f_mae: 0.008253
train_f_rmse: 0.011581
2025-06-25 07:10:04.704 INFO: train_f_rmse: 0.011581
val_e/atom_mae: 0.000784
2025-06-25 07:10:04.707 INFO: val_e/atom_mae: 0.000784
val_e/atom_rmse: 0.000803
2025-06-25 07:10:04.708 INFO: val_e/atom_rmse: 0.000803
val_f_mae: 0.008124
2025-06-25 07:10:04.708 INFO: val_f_mae: 0.008124
val_f_rmse: 0.011381
2025-06-25 07:10:04.708 INFO: val_f_rmse: 0.011381
##### Step: 78 Learning rate: 0.00125 #####
2025-06-25 07:10:49.563 INFO: ##### Step: 78 Learning rate: 0.00125 #####
Epoch 39, Train Loss: 0.1344, Val Loss: 0.1343
2025-06-25 07:10:49.563 INFO: Epoch 39, Train Loss: 0.1344, Val Loss: 0.1343
train_e/atom_mae: 0.000523
2025-06-25 07:10:49.564 INFO: train_e/atom_mae: 0.000523
train_e/atom_rmse: 0.000652
2025-06-25 07:10:49.564 INFO: train_e/atom_rmse: 0.000652
train_f_mae: 0.008132
2025-06-25 07:10:49.568 INFO: train_f_mae: 0.008132
train_f_rmse: 0.011369
2025-06-25 07:10:49.568 INFO: train_f_rmse: 0.011369
val_e/atom_mae: 0.000277
2025-06-25 07:10:49.571 INFO: val_e/atom_mae: 0.000277
val_e/atom_rmse: 0.000338
2025-06-25 07:10:49.571 INFO: val_e/atom_rmse: 0.000338
val_f_mae: 0.008242
2025-06-25 07:10:49.571 INFO: val_f_mae: 0.008242
val_f_rmse: 0.011531
2025-06-25 07:10:49.572 INFO: val_f_rmse: 0.011531
##### Step: 79 Learning rate: 0.00125 #####
2025-06-25 07:11:34.217 INFO: ##### Step: 79 Learning rate: 0.00125 #####
Epoch 40, Train Loss: 0.1369, Val Loss: 0.1307
2025-06-25 07:11:34.218 INFO: Epoch 40, Train Loss: 0.1369, Val Loss: 0.1307
train_e/atom_mae: 0.000512
2025-06-25 07:11:34.219 INFO: train_e/atom_mae: 0.000512
train_e/atom_rmse: 0.000640
2025-06-25 07:11:34.219 INFO: train_e/atom_rmse: 0.000640
train_f_mae: 0.008164
2025-06-25 07:11:34.222 INFO: train_f_mae: 0.008164
train_f_rmse: 0.011488
2025-06-25 07:11:34.223 INFO: train_f_rmse: 0.011488
val_e/atom_mae: 0.000255
2025-06-25 07:11:34.225 INFO: val_e/atom_mae: 0.000255
val_e/atom_rmse: 0.000331
2025-06-25 07:11:34.226 INFO: val_e/atom_rmse: 0.000331
val_f_mae: 0.008165
2025-06-25 07:11:34.226 INFO: val_f_mae: 0.008165
val_f_rmse: 0.011375
2025-06-25 07:11:34.226 INFO: val_f_rmse: 0.011375
##### Step: 80 Learning rate: 0.000625 #####
2025-06-25 07:12:18.772 INFO: ##### Step: 80 Learning rate: 0.000625 #####
Epoch 41, Train Loss: 0.1241, Val Loss: 0.1212
2025-06-25 07:12:18.773 INFO: Epoch 41, Train Loss: 0.1241, Val Loss: 0.1212
train_e/atom_mae: 0.000359
2025-06-25 07:12:18.774 INFO: train_e/atom_mae: 0.000359
train_e/atom_rmse: 0.000458
2025-06-25 07:12:18.774 INFO: train_e/atom_rmse: 0.000458
train_f_mae: 0.007918
2025-06-25 07:12:18.777 INFO: train_f_mae: 0.007918
train_f_rmse: 0.011024
2025-06-25 07:12:18.777 INFO: train_f_rmse: 0.011024
val_e/atom_mae: 0.000220
2025-06-25 07:12:18.780 INFO: val_e/atom_mae: 0.000220
val_e/atom_rmse: 0.000255
2025-06-25 07:12:18.781 INFO: val_e/atom_rmse: 0.000255
val_f_mae: 0.007919
2025-06-25 07:12:18.781 INFO: val_f_mae: 0.007919
val_f_rmse: 0.010972
2025-06-25 07:12:18.781 INFO: val_f_rmse: 0.010972
##### Step: 81 Learning rate: 0.000625 #####
2025-06-25 07:13:03.394 INFO: ##### Step: 81 Learning rate: 0.000625 #####
Epoch 42, Train Loss: 0.1233, Val Loss: 0.1186
2025-06-25 07:13:03.394 INFO: Epoch 42, Train Loss: 0.1233, Val Loss: 0.1186
train_e/atom_mae: 0.000372
2025-06-25 07:13:03.395 INFO: train_e/atom_mae: 0.000372
train_e/atom_rmse: 0.000472
2025-06-25 07:13:03.395 INFO: train_e/atom_rmse: 0.000472
train_f_mae: 0.007889
2025-06-25 07:13:03.399 INFO: train_f_mae: 0.007889
train_f_rmse: 0.010983
2025-06-25 07:13:03.399 INFO: train_f_rmse: 0.010983
val_e/atom_mae: 0.000255
2025-06-25 07:13:03.401 INFO: val_e/atom_mae: 0.000255
val_e/atom_rmse: 0.000300
2025-06-25 07:13:03.402 INFO: val_e/atom_rmse: 0.000300
val_f_mae: 0.007824
2025-06-25 07:13:03.402 INFO: val_f_mae: 0.007824
val_f_rmse: 0.010842
2025-06-25 07:13:03.402 INFO: val_f_rmse: 0.010842
##### Step: 82 Learning rate: 0.000625 #####
2025-06-25 07:13:47.989 INFO: ##### Step: 82 Learning rate: 0.000625 #####
Epoch 43, Train Loss: 0.1249, Val Loss: 0.1301
2025-06-25 07:13:47.989 INFO: Epoch 43, Train Loss: 0.1249, Val Loss: 0.1301
train_e/atom_mae: 0.000419
2025-06-25 07:13:47.990 INFO: train_e/atom_mae: 0.000419
train_e/atom_rmse: 0.000523
2025-06-25 07:13:47.990 INFO: train_e/atom_rmse: 0.000523
train_f_mae: 0.007923
2025-06-25 07:13:47.994 INFO: train_f_mae: 0.007923
train_f_rmse: 0.011026
2025-06-25 07:13:47.994 INFO: train_f_rmse: 0.011026
val_e/atom_mae: 0.000740
2025-06-25 07:13:47.997 INFO: val_e/atom_mae: 0.000740
val_e/atom_rmse: 0.000830
2025-06-25 07:13:47.997 INFO: val_e/atom_rmse: 0.000830
val_f_mae: 0.007963
2025-06-25 07:13:47.998 INFO: val_f_mae: 0.007963
val_f_rmse: 0.011033
2025-06-25 07:13:47.998 INFO: val_f_rmse: 0.011033
##### Step: 83 Learning rate: 0.000625 #####
2025-06-25 07:14:32.627 INFO: ##### Step: 83 Learning rate: 0.000625 #####
Epoch 44, Train Loss: 0.1222, Val Loss: 0.1358
2025-06-25 07:14:32.627 INFO: Epoch 44, Train Loss: 0.1222, Val Loss: 0.1358
train_e/atom_mae: 0.000344
2025-06-25 07:14:32.628 INFO: train_e/atom_mae: 0.000344
train_e/atom_rmse: 0.000437
2025-06-25 07:14:32.628 INFO: train_e/atom_rmse: 0.000437
train_f_mae: 0.007873
2025-06-25 07:14:32.632 INFO: train_f_mae: 0.007873
train_f_rmse: 0.010951
2025-06-25 07:14:32.632 INFO: train_f_rmse: 0.010951
val_e/atom_mae: 0.000528
2025-06-25 07:14:32.635 INFO: val_e/atom_mae: 0.000528
val_e/atom_rmse: 0.000571
2025-06-25 07:14:32.635 INFO: val_e/atom_rmse: 0.000571
val_f_mae: 0.008198
2025-06-25 07:14:32.636 INFO: val_f_mae: 0.008198
val_f_rmse: 0.011482
2025-06-25 07:14:32.636 INFO: val_f_rmse: 0.011482
##### Step: 84 Learning rate: 0.000625 #####
2025-06-25 07:15:17.143 INFO: ##### Step: 84 Learning rate: 0.000625 #####
Epoch 45, Train Loss: 0.1209, Val Loss: 0.1195
2025-06-25 07:15:17.144 INFO: Epoch 45, Train Loss: 0.1209, Val Loss: 0.1195
train_e/atom_mae: 0.000388
2025-06-25 07:15:17.144 INFO: train_e/atom_mae: 0.000388
train_e/atom_rmse: 0.000499
2025-06-25 07:15:17.145 INFO: train_e/atom_rmse: 0.000499
train_f_mae: 0.007817
2025-06-25 07:15:17.148 INFO: train_f_mae: 0.007817
train_f_rmse: 0.010855
2025-06-25 07:15:17.148 INFO: train_f_rmse: 0.010855
val_e/atom_mae: 0.000119
2025-06-25 07:15:17.151 INFO: val_e/atom_mae: 0.000119
val_e/atom_rmse: 0.000170
2025-06-25 07:15:17.151 INFO: val_e/atom_rmse: 0.000170
val_f_mae: 0.007821
2025-06-25 07:15:17.152 INFO: val_f_mae: 0.007821
val_f_rmse: 0.010914
2025-06-25 07:15:17.152 INFO: val_f_rmse: 0.010914
##### Step: 85 Learning rate: 0.000625 #####
2025-06-25 07:16:01.800 INFO: ##### Step: 85 Learning rate: 0.000625 #####
Epoch 46, Train Loss: 0.1192, Val Loss: 0.1241
2025-06-25 07:16:01.800 INFO: Epoch 46, Train Loss: 0.1192, Val Loss: 0.1241
train_e/atom_mae: 0.000350
2025-06-25 07:16:01.801 INFO: train_e/atom_mae: 0.000350
train_e/atom_rmse: 0.000434
2025-06-25 07:16:01.801 INFO: train_e/atom_rmse: 0.000434
train_f_mae: 0.007785
2025-06-25 07:16:01.805 INFO: train_f_mae: 0.007785
train_f_rmse: 0.010813
2025-06-25 07:16:01.805 INFO: train_f_rmse: 0.010813
val_e/atom_mae: 0.000418
2025-06-25 07:16:01.808 INFO: val_e/atom_mae: 0.000418
val_e/atom_rmse: 0.000443
2025-06-25 07:16:01.808 INFO: val_e/atom_rmse: 0.000443
val_f_mae: 0.007927
2025-06-25 07:16:01.809 INFO: val_f_mae: 0.007927
val_f_rmse: 0.011034
2025-06-25 07:16:01.809 INFO: val_f_rmse: 0.011034
##### Step: 86 Learning rate: 0.000625 #####
2025-06-25 07:16:46.292 INFO: ##### Step: 86 Learning rate: 0.000625 #####
Epoch 47, Train Loss: 0.1212, Val Loss: 0.1176
2025-06-25 07:16:46.292 INFO: Epoch 47, Train Loss: 0.1212, Val Loss: 0.1176
train_e/atom_mae: 0.000376
2025-06-25 07:16:46.293 INFO: train_e/atom_mae: 0.000376
train_e/atom_rmse: 0.000478
2025-06-25 07:16:46.293 INFO: train_e/atom_rmse: 0.000478
train_f_mae: 0.007843
2025-06-25 07:16:46.297 INFO: train_f_mae: 0.007843
train_f_rmse: 0.010884
2025-06-25 07:16:46.297 INFO: train_f_rmse: 0.010884
val_e/atom_mae: 0.000233
2025-06-25 07:16:46.300 INFO: val_e/atom_mae: 0.000233
val_e/atom_rmse: 0.000264
2025-06-25 07:16:46.300 INFO: val_e/atom_rmse: 0.000264
val_f_mae: 0.007777
2025-06-25 07:16:46.301 INFO: val_f_mae: 0.007777
val_f_rmse: 0.010807
2025-06-25 07:16:46.301 INFO: val_f_rmse: 0.010807
##### Step: 87 Learning rate: 0.000625 #####
2025-06-25 07:17:30.948 INFO: ##### Step: 87 Learning rate: 0.000625 #####
Epoch 48, Train Loss: 0.1200, Val Loss: 0.1237
2025-06-25 07:17:30.948 INFO: Epoch 48, Train Loss: 0.1200, Val Loss: 0.1237
train_e/atom_mae: 0.000354
2025-06-25 07:17:30.949 INFO: train_e/atom_mae: 0.000354
train_e/atom_rmse: 0.000447
2025-06-25 07:17:30.949 INFO: train_e/atom_rmse: 0.000447
train_f_mae: 0.007818
2025-06-25 07:17:30.953 INFO: train_f_mae: 0.007818
train_f_rmse: 0.010842
2025-06-25 07:17:30.953 INFO: train_f_rmse: 0.010842
val_e/atom_mae: 0.000136
2025-06-25 07:17:30.956 INFO: val_e/atom_mae: 0.000136
val_e/atom_rmse: 0.000185
2025-06-25 07:17:30.956 INFO: val_e/atom_rmse: 0.000185
val_f_mae: 0.008011
2025-06-25 07:17:30.957 INFO: val_f_mae: 0.008011
val_f_rmse: 0.011105
2025-06-25 07:17:30.957 INFO: val_f_rmse: 0.011105
##### Step: 88 Learning rate: 0.000625 #####
2025-06-25 07:18:15.470 INFO: ##### Step: 88 Learning rate: 0.000625 #####
Epoch 49, Train Loss: 0.1240, Val Loss: 0.1277
2025-06-25 07:18:15.470 INFO: Epoch 49, Train Loss: 0.1240, Val Loss: 0.1277
train_e/atom_mae: 0.000430
2025-06-25 07:18:15.471 INFO: train_e/atom_mae: 0.000430
train_e/atom_rmse: 0.000541
2025-06-25 07:18:15.471 INFO: train_e/atom_rmse: 0.000541
train_f_mae: 0.007877
2025-06-25 07:18:15.475 INFO: train_f_mae: 0.007877
train_f_rmse: 0.010977
2025-06-25 07:18:15.475 INFO: train_f_rmse: 0.010977
val_e/atom_mae: 0.000457
2025-06-25 07:18:15.478 INFO: val_e/atom_mae: 0.000457
val_e/atom_rmse: 0.000523
2025-06-25 07:18:15.478 INFO: val_e/atom_rmse: 0.000523
val_f_mae: 0.007946
2025-06-25 07:18:15.478 INFO: val_f_mae: 0.007946
val_f_rmse: 0.011154
2025-06-25 07:18:15.479 INFO: val_f_rmse: 0.011154
##### Step: 89 Learning rate: 0.000625 #####
2025-06-25 07:19:00.060 INFO: ##### Step: 89 Learning rate: 0.000625 #####
Epoch 50, Train Loss: 0.1217, Val Loss: 0.1271
2025-06-25 07:19:00.061 INFO: Epoch 50, Train Loss: 0.1217, Val Loss: 0.1271
train_e/atom_mae: 0.000335
2025-06-25 07:19:00.062 INFO: train_e/atom_mae: 0.000335
train_e/atom_rmse: 0.000432
2025-06-25 07:19:00.062 INFO: train_e/atom_rmse: 0.000432
train_f_mae: 0.007858
2025-06-25 07:19:00.065 INFO: train_f_mae: 0.007858
train_f_rmse: 0.010931
2025-06-25 07:19:00.065 INFO: train_f_rmse: 0.010931
val_e/atom_mae: 0.000331
2025-06-25 07:19:00.068 INFO: val_e/atom_mae: 0.000331
val_e/atom_rmse: 0.000401
2025-06-25 07:19:00.068 INFO: val_e/atom_rmse: 0.000401
val_f_mae: 0.008066
2025-06-25 07:19:00.069 INFO: val_f_mae: 0.008066
val_f_rmse: 0.011188
2025-06-25 07:19:00.069 INFO: val_f_rmse: 0.011188
##### Step: 90 Learning rate: 0.000625 #####
2025-06-25 07:19:44.637 INFO: ##### Step: 90 Learning rate: 0.000625 #####
Epoch 51, Train Loss: 0.1191, Val Loss: 0.1229
2025-06-25 07:19:44.637 INFO: Epoch 51, Train Loss: 0.1191, Val Loss: 0.1229
train_e/atom_mae: 0.000363
2025-06-25 07:19:44.638 INFO: train_e/atom_mae: 0.000363
train_e/atom_rmse: 0.000465
2025-06-25 07:19:44.638 INFO: train_e/atom_rmse: 0.000465
train_f_mae: 0.007790
2025-06-25 07:19:44.642 INFO: train_f_mae: 0.007790
train_f_rmse: 0.010792
2025-06-25 07:19:44.642 INFO: train_f_rmse: 0.010792
val_e/atom_mae: 0.000585
2025-06-25 07:19:44.645 INFO: val_e/atom_mae: 0.000585
val_e/atom_rmse: 0.000630
2025-06-25 07:19:44.645 INFO: val_e/atom_rmse: 0.000630
val_f_mae: 0.007808
2025-06-25 07:19:44.646 INFO: val_f_mae: 0.007808
val_f_rmse: 0.010867
2025-06-25 07:19:44.646 INFO: val_f_rmse: 0.010867
##### Step: 91 Learning rate: 0.000625 #####
2025-06-25 07:20:28.985 INFO: ##### Step: 91 Learning rate: 0.000625 #####
Epoch 52, Train Loss: 0.1173, Val Loss: 0.1264
2025-06-25 07:20:28.986 INFO: Epoch 52, Train Loss: 0.1173, Val Loss: 0.1264
train_e/atom_mae: 0.000311
2025-06-25 07:20:28.988 INFO: train_e/atom_mae: 0.000311
train_e/atom_rmse: 0.000408
2025-06-25 07:20:28.988 INFO: train_e/atom_rmse: 0.000408
train_f_mae: 0.007741
2025-06-25 07:20:28.992 INFO: train_f_mae: 0.007741
train_f_rmse: 0.010739
2025-06-25 07:20:28.992 INFO: train_f_rmse: 0.010739
val_e/atom_mae: 0.000754
2025-06-25 07:20:28.995 INFO: val_e/atom_mae: 0.000754
val_e/atom_rmse: 0.000780
2025-06-25 07:20:28.995 INFO: val_e/atom_rmse: 0.000780
val_f_mae: 0.007907
2025-06-25 07:20:28.995 INFO: val_f_mae: 0.007907
val_f_rmse: 0.010910
2025-06-25 07:20:28.995 INFO: val_f_rmse: 0.010910
##### Step: 92 Learning rate: 0.000625 #####
2025-06-25 07:21:13.416 INFO: ##### Step: 92 Learning rate: 0.000625 #####
Epoch 53, Train Loss: 0.1227, Val Loss: 0.1237
2025-06-25 07:21:13.416 INFO: Epoch 53, Train Loss: 0.1227, Val Loss: 0.1237
train_e/atom_mae: 0.000442
2025-06-25 07:21:13.417 INFO: train_e/atom_mae: 0.000442
train_e/atom_rmse: 0.000579
2025-06-25 07:21:13.417 INFO: train_e/atom_rmse: 0.000579
train_f_mae: 0.007833
2025-06-25 07:21:13.421 INFO: train_f_mae: 0.007833
train_f_rmse: 0.010892
2025-06-25 07:21:13.421 INFO: train_f_rmse: 0.010892
val_e/atom_mae: 0.000290
2025-06-25 07:21:13.423 INFO: val_e/atom_mae: 0.000290
val_e/atom_rmse: 0.000305
2025-06-25 07:21:13.424 INFO: val_e/atom_rmse: 0.000305
val_f_mae: 0.007943
2025-06-25 07:21:13.424 INFO: val_f_mae: 0.007943
val_f_rmse: 0.011073
2025-06-25 07:21:13.424 INFO: val_f_rmse: 0.011073
##### Step: 93 Learning rate: 0.000625 #####
2025-06-25 07:21:57.715 INFO: ##### Step: 93 Learning rate: 0.000625 #####
Epoch 54, Train Loss: 0.1201, Val Loss: 0.1188
2025-06-25 07:21:57.715 INFO: Epoch 54, Train Loss: 0.1201, Val Loss: 0.1188
train_e/atom_mae: 0.000423
2025-06-25 07:21:57.716 INFO: train_e/atom_mae: 0.000423
train_e/atom_rmse: 0.000534
2025-06-25 07:21:57.716 INFO: train_e/atom_rmse: 0.000534
train_f_mae: 0.007784
2025-06-25 07:21:57.720 INFO: train_f_mae: 0.007784
train_f_rmse: 0.010801
2025-06-25 07:21:57.720 INFO: train_f_rmse: 0.010801
val_e/atom_mae: 0.000274
2025-06-25 07:21:57.723 INFO: val_e/atom_mae: 0.000274
val_e/atom_rmse: 0.000305
2025-06-25 07:21:57.723 INFO: val_e/atom_rmse: 0.000305
val_f_mae: 0.007836
2025-06-25 07:21:57.724 INFO: val_f_mae: 0.007836
val_f_rmse: 0.010847
2025-06-25 07:21:57.724 INFO: val_f_rmse: 0.010847
##### Step: 94 Learning rate: 0.000625 #####
2025-06-25 07:22:42.162 INFO: ##### Step: 94 Learning rate: 0.000625 #####
Epoch 55, Train Loss: 0.1185, Val Loss: 0.1194
2025-06-25 07:22:42.163 INFO: Epoch 55, Train Loss: 0.1185, Val Loss: 0.1194
train_e/atom_mae: 0.000352
2025-06-25 07:22:42.164 INFO: train_e/atom_mae: 0.000352
train_e/atom_rmse: 0.000431
2025-06-25 07:22:42.164 INFO: train_e/atom_rmse: 0.000431
train_f_mae: 0.007771
2025-06-25 07:22:42.167 INFO: train_f_mae: 0.007771
train_f_rmse: 0.010781
2025-06-25 07:22:42.167 INFO: train_f_rmse: 0.010781
val_e/atom_mae: 0.000417
2025-06-25 07:22:42.170 INFO: val_e/atom_mae: 0.000417
val_e/atom_rmse: 0.000432
2025-06-25 07:22:42.170 INFO: val_e/atom_rmse: 0.000432
val_f_mae: 0.007827
2025-06-25 07:22:42.171 INFO: val_f_mae: 0.007827
val_f_rmse: 0.010822
2025-06-25 07:22:42.171 INFO: val_f_rmse: 0.010822
##### Step: 95 Learning rate: 0.000625 #####
2025-06-25 07:23:26.429 INFO: ##### Step: 95 Learning rate: 0.000625 #####
Epoch 56, Train Loss: 0.1187, Val Loss: 0.1172
2025-06-25 07:23:26.430 INFO: Epoch 56, Train Loss: 0.1187, Val Loss: 0.1172
train_e/atom_mae: 0.000321
2025-06-25 07:23:26.430 INFO: train_e/atom_mae: 0.000321
train_e/atom_rmse: 0.000407
2025-06-25 07:23:26.431 INFO: train_e/atom_rmse: 0.000407
train_f_mae: 0.007787
2025-06-25 07:23:26.434 INFO: train_f_mae: 0.007787
train_f_rmse: 0.010803
2025-06-25 07:23:26.434 INFO: train_f_rmse: 0.010803
val_e/atom_mae: 0.000350
2025-06-25 07:23:26.437 INFO: val_e/atom_mae: 0.000350
val_e/atom_rmse: 0.000375
2025-06-25 07:23:26.437 INFO: val_e/atom_rmse: 0.000375
val_f_mae: 0.007736
2025-06-25 07:23:26.438 INFO: val_f_mae: 0.007736
val_f_rmse: 0.010747
2025-06-25 07:23:26.438 INFO: val_f_rmse: 0.010747
##### Step: 96 Learning rate: 0.000625 #####
2025-06-25 07:24:10.767 INFO: ##### Step: 96 Learning rate: 0.000625 #####
Epoch 57, Train Loss: 0.1190, Val Loss: 0.1167
2025-06-25 07:24:10.767 INFO: Epoch 57, Train Loss: 0.1190, Val Loss: 0.1167
train_e/atom_mae: 0.000436
2025-06-25 07:24:10.768 INFO: train_e/atom_mae: 0.000436
train_e/atom_rmse: 0.000550
2025-06-25 07:24:10.768 INFO: train_e/atom_rmse: 0.000550
train_f_mae: 0.007751
2025-06-25 07:24:10.772 INFO: train_f_mae: 0.007751
train_f_rmse: 0.010741
2025-06-25 07:24:10.772 INFO: train_f_rmse: 0.010741
val_e/atom_mae: 0.000161
2025-06-25 07:24:10.774 INFO: val_e/atom_mae: 0.000161
val_e/atom_rmse: 0.000193
2025-06-25 07:24:10.775 INFO: val_e/atom_rmse: 0.000193
val_f_mae: 0.007766
2025-06-25 07:24:10.775 INFO: val_f_mae: 0.007766
val_f_rmse: 0.010780
2025-06-25 07:24:10.775 INFO: val_f_rmse: 0.010780
##### Step: 97 Learning rate: 0.000625 #####
2025-06-25 07:24:55.014 INFO: ##### Step: 97 Learning rate: 0.000625 #####
Epoch 58, Train Loss: 0.1194, Val Loss: 0.1217
2025-06-25 07:24:55.014 INFO: Epoch 58, Train Loss: 0.1194, Val Loss: 0.1217
train_e/atom_mae: 0.000352
2025-06-25 07:24:55.015 INFO: train_e/atom_mae: 0.000352
train_e/atom_rmse: 0.000434
2025-06-25 07:24:55.015 INFO: train_e/atom_rmse: 0.000434
train_f_mae: 0.007778
2025-06-25 07:24:55.019 INFO: train_f_mae: 0.007778
train_f_rmse: 0.010822
2025-06-25 07:24:55.019 INFO: train_f_rmse: 0.010822
val_e/atom_mae: 0.000301
2025-06-25 07:24:55.021 INFO: val_e/atom_mae: 0.000301
val_e/atom_rmse: 0.000342
2025-06-25 07:24:55.022 INFO: val_e/atom_rmse: 0.000342
val_f_mae: 0.007879
2025-06-25 07:24:55.022 INFO: val_f_mae: 0.007879
val_f_rmse: 0.010966
2025-06-25 07:24:55.022 INFO: val_f_rmse: 0.010966
##### Step: 98 Learning rate: 0.000625 #####
2025-06-25 07:25:39.354 INFO: ##### Step: 98 Learning rate: 0.000625 #####
Epoch 59, Train Loss: 0.1219, Val Loss: 0.1270
2025-06-25 07:25:39.354 INFO: Epoch 59, Train Loss: 0.1219, Val Loss: 0.1270
train_e/atom_mae: 0.000370
2025-06-25 07:25:39.355 INFO: train_e/atom_mae: 0.000370
train_e/atom_rmse: 0.000481
2025-06-25 07:25:39.355 INFO: train_e/atom_rmse: 0.000481
train_f_mae: 0.007830
2025-06-25 07:25:39.358 INFO: train_f_mae: 0.007830
train_f_rmse: 0.010916
2025-06-25 07:25:39.359 INFO: train_f_rmse: 0.010916
val_e/atom_mae: 0.000233
2025-06-25 07:25:39.361 INFO: val_e/atom_mae: 0.000233
val_e/atom_rmse: 0.000287
2025-06-25 07:25:39.362 INFO: val_e/atom_rmse: 0.000287
val_f_mae: 0.008014
2025-06-25 07:25:39.362 INFO: val_f_mae: 0.008014
val_f_rmse: 0.011224
2025-06-25 07:25:39.362 INFO: val_f_rmse: 0.011224
##### Step: 99 Learning rate: 0.000625 #####
2025-06-25 07:26:23.648 INFO: ##### Step: 99 Learning rate: 0.000625 #####
Epoch 60, Train Loss: 0.1214, Val Loss: 0.1172
2025-06-25 07:26:23.648 INFO: Epoch 60, Train Loss: 0.1214, Val Loss: 0.1172
train_e/atom_mae: 0.000423
2025-06-25 07:26:23.649 INFO: train_e/atom_mae: 0.000423
train_e/atom_rmse: 0.000560
2025-06-25 07:26:23.649 INFO: train_e/atom_rmse: 0.000560
train_f_mae: 0.007803
2025-06-25 07:26:23.653 INFO: train_f_mae: 0.007803
train_f_rmse: 0.010843
2025-06-25 07:26:23.653 INFO: train_f_rmse: 0.010843
val_e/atom_mae: 0.000224
2025-06-25 07:26:23.655 INFO: val_e/atom_mae: 0.000224
val_e/atom_rmse: 0.000271
2025-06-25 07:26:23.656 INFO: val_e/atom_rmse: 0.000271
val_f_mae: 0.007769
2025-06-25 07:26:23.656 INFO: val_f_mae: 0.007769
val_f_rmse: 0.010787
2025-06-25 07:26:23.656 INFO: val_f_rmse: 0.010787
##### Step: 100 Learning rate: 0.0003125 #####
2025-06-25 07:27:07.917 INFO: ##### Step: 100 Learning rate: 0.0003125 #####
Epoch 61, Train Loss: 0.1117, Val Loss: 0.1213
2025-06-25 07:27:07.917 INFO: Epoch 61, Train Loss: 0.1117, Val Loss: 0.1213
train_e/atom_mae: 0.000223
2025-06-25 07:27:07.918 INFO: train_e/atom_mae: 0.000223
train_e/atom_rmse: 0.000288
2025-06-25 07:27:07.918 INFO: train_e/atom_rmse: 0.000288
train_f_mae: 0.007595
2025-06-25 07:27:07.922 INFO: train_f_mae: 0.007595
train_f_rmse: 0.010519
2025-06-25 07:27:07.922 INFO: train_f_rmse: 0.010519
val_e/atom_mae: 0.000451
2025-06-25 07:27:07.925 INFO: val_e/atom_mae: 0.000451
val_e/atom_rmse: 0.000532
2025-06-25 07:27:07.925 INFO: val_e/atom_rmse: 0.000532
val_f_mae: 0.007816
2025-06-25 07:27:07.926 INFO: val_f_mae: 0.007816
val_f_rmse: 0.010858
2025-06-25 07:27:07.926 INFO: val_f_rmse: 0.010858
##### Step: 101 Learning rate: 0.0003125 #####
2025-06-25 07:27:52.204 INFO: ##### Step: 101 Learning rate: 0.0003125 #####
Epoch 62, Train Loss: 0.1120, Val Loss: 0.1161
2025-06-25 07:27:52.205 INFO: Epoch 62, Train Loss: 0.1120, Val Loss: 0.1161
train_e/atom_mae: 0.000230
2025-06-25 07:27:52.205 INFO: train_e/atom_mae: 0.000230
train_e/atom_rmse: 0.000292
2025-06-25 07:27:52.206 INFO: train_e/atom_rmse: 0.000292
train_f_mae: 0.007602
2025-06-25 07:27:52.209 INFO: train_f_mae: 0.007602
train_f_rmse: 0.010533
2025-06-25 07:27:52.209 INFO: train_f_rmse: 0.010533
val_e/atom_mae: 0.000463
2025-06-25 07:27:52.212 INFO: val_e/atom_mae: 0.000463
val_e/atom_rmse: 0.000484
2025-06-25 07:27:52.212 INFO: val_e/atom_rmse: 0.000484
val_f_mae: 0.007691
2025-06-25 07:27:52.213 INFO: val_f_mae: 0.007691
val_f_rmse: 0.010642
2025-06-25 07:27:52.213 INFO: val_f_rmse: 0.010642
##### Step: 102 Learning rate: 0.0003125 #####
2025-06-25 07:28:36.648 INFO: ##### Step: 102 Learning rate: 0.0003125 #####
Epoch 63, Train Loss: 0.1117, Val Loss: 0.1139
2025-06-25 07:28:36.648 INFO: Epoch 63, Train Loss: 0.1117, Val Loss: 0.1139
train_e/atom_mae: 0.000277
2025-06-25 07:28:36.649 INFO: train_e/atom_mae: 0.000277
train_e/atom_rmse: 0.000340
2025-06-25 07:28:36.649 INFO: train_e/atom_rmse: 0.000340
train_f_mae: 0.007587
2025-06-25 07:28:36.653 INFO: train_f_mae: 0.007587
train_f_rmse: 0.010503
2025-06-25 07:28:36.653 INFO: train_f_rmse: 0.010503
val_e/atom_mae: 0.000083
2025-06-25 07:28:36.656 INFO: val_e/atom_mae: 0.000083
val_e/atom_rmse: 0.000114
2025-06-25 07:28:36.656 INFO: val_e/atom_rmse: 0.000114
val_f_mae: 0.007710
2025-06-25 07:28:36.656 INFO: val_f_mae: 0.007710
val_f_rmse: 0.010663
2025-06-25 07:28:36.657 INFO: val_f_rmse: 0.010663
##### Step: 103 Learning rate: 0.0003125 #####
2025-06-25 07:29:21.381 INFO: ##### Step: 103 Learning rate: 0.0003125 #####
Epoch 64, Train Loss: 0.1123, Val Loss: 0.1210
2025-06-25 07:29:21.381 INFO: Epoch 64, Train Loss: 0.1123, Val Loss: 0.1210
train_e/atom_mae: 0.000240
2025-06-25 07:29:21.382 INFO: train_e/atom_mae: 0.000240
train_e/atom_rmse: 0.000300
2025-06-25 07:29:21.382 INFO: train_e/atom_rmse: 0.000300
train_f_mae: 0.007624
2025-06-25 07:29:21.386 INFO: train_f_mae: 0.007624
train_f_rmse: 0.010546
2025-06-25 07:29:21.386 INFO: train_f_rmse: 0.010546
val_e/atom_mae: 0.000443
2025-06-25 07:29:21.389 INFO: val_e/atom_mae: 0.000443
val_e/atom_rmse: 0.000465
2025-06-25 07:29:21.389 INFO: val_e/atom_rmse: 0.000465
val_f_mae: 0.007804
2025-06-25 07:29:21.390 INFO: val_f_mae: 0.007804
val_f_rmse: 0.010882
2025-06-25 07:29:21.390 INFO: val_f_rmse: 0.010882
##### Step: 104 Learning rate: 0.0003125 #####
2025-06-25 07:30:05.965 INFO: ##### Step: 104 Learning rate: 0.0003125 #####
Epoch 65, Train Loss: 0.1121, Val Loss: 0.1145
2025-06-25 07:30:05.965 INFO: Epoch 65, Train Loss: 0.1121, Val Loss: 0.1145
train_e/atom_mae: 0.000247
2025-06-25 07:30:05.966 INFO: train_e/atom_mae: 0.000247
train_e/atom_rmse: 0.000307
2025-06-25 07:30:05.966 INFO: train_e/atom_rmse: 0.000307
train_f_mae: 0.007595
2025-06-25 07:30:05.970 INFO: train_f_mae: 0.007595
train_f_rmse: 0.010533
2025-06-25 07:30:05.970 INFO: train_f_rmse: 0.010533
val_e/atom_mae: 0.000152
2025-06-25 07:30:05.972 INFO: val_e/atom_mae: 0.000152
val_e/atom_rmse: 0.000179
2025-06-25 07:30:05.973 INFO: val_e/atom_rmse: 0.000179
val_f_mae: 0.007703
2025-06-25 07:30:05.973 INFO: val_f_mae: 0.007703
val_f_rmse: 0.010683
2025-06-25 07:30:05.973 INFO: val_f_rmse: 0.010683
##### Step: 105 Learning rate: 0.0003125 #####
2025-06-25 07:30:50.645 INFO: ##### Step: 105 Learning rate: 0.0003125 #####
Epoch 66, Train Loss: 0.1110, Val Loss: 0.1118
2025-06-25 07:30:50.646 INFO: Epoch 66, Train Loss: 0.1110, Val Loss: 0.1118
train_e/atom_mae: 0.000244
2025-06-25 07:30:50.646 INFO: train_e/atom_mae: 0.000244
train_e/atom_rmse: 0.000322
2025-06-25 07:30:50.647 INFO: train_e/atom_rmse: 0.000322
train_f_mae: 0.007579
2025-06-25 07:30:50.650 INFO: train_f_mae: 0.007579
train_f_rmse: 0.010474
2025-06-25 07:30:50.650 INFO: train_f_rmse: 0.010474
val_e/atom_mae: 0.000136
2025-06-25 07:30:50.653 INFO: val_e/atom_mae: 0.000136
val_e/atom_rmse: 0.000169
2025-06-25 07:30:50.653 INFO: val_e/atom_rmse: 0.000169
val_f_mae: 0.007633
2025-06-25 07:30:50.654 INFO: val_f_mae: 0.007633
val_f_rmse: 0.010558
2025-06-25 07:30:50.654 INFO: val_f_rmse: 0.010558
##### Step: 106 Learning rate: 0.0003125 #####
2025-06-25 07:31:35.247 INFO: ##### Step: 106 Learning rate: 0.0003125 #####
Epoch 67, Train Loss: 0.1125, Val Loss: 0.1111
2025-06-25 07:31:35.247 INFO: Epoch 67, Train Loss: 0.1125, Val Loss: 0.1111
train_e/atom_mae: 0.000267
2025-06-25 07:31:35.248 INFO: train_e/atom_mae: 0.000267
train_e/atom_rmse: 0.000335
2025-06-25 07:31:35.248 INFO: train_e/atom_rmse: 0.000335
train_f_mae: 0.007613
2025-06-25 07:31:35.251 INFO: train_f_mae: 0.007613
train_f_rmse: 0.010542
2025-06-25 07:31:35.252 INFO: train_f_rmse: 0.010542
val_e/atom_mae: 0.000123
2025-06-25 07:31:35.254 INFO: val_e/atom_mae: 0.000123
val_e/atom_rmse: 0.000154
2025-06-25 07:31:35.255 INFO: val_e/atom_rmse: 0.000154
val_f_mae: 0.007615
2025-06-25 07:31:35.255 INFO: val_f_mae: 0.007615
val_f_rmse: 0.010529
2025-06-25 07:31:35.255 INFO: val_f_rmse: 0.010529
##### Step: 107 Learning rate: 0.0003125 #####
2025-06-25 07:32:19.871 INFO: ##### Step: 107 Learning rate: 0.0003125 #####
Epoch 68, Train Loss: 0.1115, Val Loss: 0.1192
2025-06-25 07:32:19.872 INFO: Epoch 68, Train Loss: 0.1115, Val Loss: 0.1192
train_e/atom_mae: 0.000250
2025-06-25 07:32:19.873 INFO: train_e/atom_mae: 0.000250
train_e/atom_rmse: 0.000313
2025-06-25 07:32:19.873 INFO: train_e/atom_rmse: 0.000313
train_f_mae: 0.007589
2025-06-25 07:32:19.876 INFO: train_f_mae: 0.007589
train_f_rmse: 0.010504
2025-06-25 07:32:19.876 INFO: train_f_rmse: 0.010504
val_e/atom_mae: 0.000471
2025-06-25 07:32:19.879 INFO: val_e/atom_mae: 0.000471
val_e/atom_rmse: 0.000578
2025-06-25 07:32:19.879 INFO: val_e/atom_rmse: 0.000578
val_f_mae: 0.007707
2025-06-25 07:32:19.880 INFO: val_f_mae: 0.007707
val_f_rmse: 0.010733
2025-06-25 07:32:19.880 INFO: val_f_rmse: 0.010733
##### Step: 108 Learning rate: 0.0003125 #####
2025-06-25 07:33:04.431 INFO: ##### Step: 108 Learning rate: 0.0003125 #####
Epoch 69, Train Loss: 0.1121, Val Loss: 0.1166
2025-06-25 07:33:04.432 INFO: Epoch 69, Train Loss: 0.1121, Val Loss: 0.1166
train_e/atom_mae: 0.000250
2025-06-25 07:33:04.433 INFO: train_e/atom_mae: 0.000250
train_e/atom_rmse: 0.000320
2025-06-25 07:33:04.433 INFO: train_e/atom_rmse: 0.000320
train_f_mae: 0.007601
2025-06-25 07:33:04.436 INFO: train_f_mae: 0.007601
train_f_rmse: 0.010528
2025-06-25 07:33:04.436 INFO: train_f_rmse: 0.010528
val_e/atom_mae: 0.000125
2025-06-25 07:33:04.439 INFO: val_e/atom_mae: 0.000125
val_e/atom_rmse: 0.000155
2025-06-25 07:33:04.439 INFO: val_e/atom_rmse: 0.000155
val_f_mae: 0.007763
2025-06-25 07:33:04.440 INFO: val_f_mae: 0.007763
val_f_rmse: 0.010783
2025-06-25 07:33:04.440 INFO: val_f_rmse: 0.010783
##### Step: 109 Learning rate: 0.0003125 #####
2025-06-25 07:33:48.973 INFO: ##### Step: 109 Learning rate: 0.0003125 #####
Epoch 70, Train Loss: 0.1109, Val Loss: 0.1119
2025-06-25 07:33:48.973 INFO: Epoch 70, Train Loss: 0.1109, Val Loss: 0.1119
train_e/atom_mae: 0.000240
2025-06-25 07:33:48.974 INFO: train_e/atom_mae: 0.000240
train_e/atom_rmse: 0.000298
2025-06-25 07:33:48.974 INFO: train_e/atom_rmse: 0.000298
train_f_mae: 0.007570
2025-06-25 07:33:48.978 INFO: train_f_mae: 0.007570
train_f_rmse: 0.010478
2025-06-25 07:33:48.978 INFO: train_f_rmse: 0.010478
val_e/atom_mae: 0.000160
2025-06-25 07:33:48.981 INFO: val_e/atom_mae: 0.000160
val_e/atom_rmse: 0.000199
2025-06-25 07:33:48.981 INFO: val_e/atom_rmse: 0.000199
val_f_mae: 0.007611
2025-06-25 07:33:48.981 INFO: val_f_mae: 0.007611
val_f_rmse: 0.010557
2025-06-25 07:33:48.981 INFO: val_f_rmse: 0.010557
##### Step: 110 Learning rate: 0.0003125 #####
2025-06-25 07:34:33.546 INFO: ##### Step: 110 Learning rate: 0.0003125 #####
Epoch 71, Train Loss: 0.1113, Val Loss: 0.1159
2025-06-25 07:34:33.546 INFO: Epoch 71, Train Loss: 0.1113, Val Loss: 0.1159
train_e/atom_mae: 0.000283
2025-06-25 07:34:33.547 INFO: train_e/atom_mae: 0.000283
train_e/atom_rmse: 0.000348
2025-06-25 07:34:33.547 INFO: train_e/atom_rmse: 0.000348
train_f_mae: 0.007573
2025-06-25 07:34:33.551 INFO: train_f_mae: 0.007573
train_f_rmse: 0.010482
2025-06-25 07:34:33.551 INFO: train_f_rmse: 0.010482
val_e/atom_mae: 0.000328
2025-06-25 07:34:33.554 INFO: val_e/atom_mae: 0.000328
val_e/atom_rmse: 0.000389
2025-06-25 07:34:33.554 INFO: val_e/atom_rmse: 0.000389
val_f_mae: 0.007696
2025-06-25 07:34:33.555 INFO: val_f_mae: 0.007696
val_f_rmse: 0.010679
2025-06-25 07:34:33.555 INFO: val_f_rmse: 0.010679
##### Step: 111 Learning rate: 0.0003125 #####
2025-06-25 07:35:18.086 INFO: ##### Step: 111 Learning rate: 0.0003125 #####
Epoch 72, Train Loss: 0.1106, Val Loss: 0.1121
2025-06-25 07:35:18.087 INFO: Epoch 72, Train Loss: 0.1106, Val Loss: 0.1121
train_e/atom_mae: 0.000257
2025-06-25 07:35:18.087 INFO: train_e/atom_mae: 0.000257
train_e/atom_rmse: 0.000321
2025-06-25 07:35:18.088 INFO: train_e/atom_rmse: 0.000321
train_f_mae: 0.007557
2025-06-25 07:35:18.091 INFO: train_f_mae: 0.007557
train_f_rmse: 0.010455
2025-06-25 07:35:18.091 INFO: train_f_rmse: 0.010455
val_e/atom_mae: 0.000133
2025-06-25 07:35:18.094 INFO: val_e/atom_mae: 0.000133
val_e/atom_rmse: 0.000163
2025-06-25 07:35:18.094 INFO: val_e/atom_rmse: 0.000163
val_f_mae: 0.007649
2025-06-25 07:35:18.095 INFO: val_f_mae: 0.007649
val_f_rmse: 0.010571
2025-06-25 07:35:18.095 INFO: val_f_rmse: 0.010571
##### Step: 112 Learning rate: 0.0003125 #####
2025-06-25 07:36:02.628 INFO: ##### Step: 112 Learning rate: 0.0003125 #####
Epoch 73, Train Loss: 0.1104, Val Loss: 0.1138
2025-06-25 07:36:02.628 INFO: Epoch 73, Train Loss: 0.1104, Val Loss: 0.1138
train_e/atom_mae: 0.000219
2025-06-25 07:36:02.629 INFO: train_e/atom_mae: 0.000219
train_e/atom_rmse: 0.000280
2025-06-25 07:36:02.629 INFO: train_e/atom_rmse: 0.000280
train_f_mae: 0.007561
2025-06-25 07:36:02.633 INFO: train_f_mae: 0.007561
train_f_rmse: 0.010460
2025-06-25 07:36:02.633 INFO: train_f_rmse: 0.010460
val_e/atom_mae: 0.000127
2025-06-25 07:36:02.636 INFO: val_e/atom_mae: 0.000127
val_e/atom_rmse: 0.000158
2025-06-25 07:36:02.636 INFO: val_e/atom_rmse: 0.000158
val_f_mae: 0.007684
2025-06-25 07:36:02.636 INFO: val_f_mae: 0.007684
val_f_rmse: 0.010654
2025-06-25 07:36:02.637 INFO: val_f_rmse: 0.010654
##### Step: 113 Learning rate: 0.0003125 #####
2025-06-25 07:36:47.071 INFO: ##### Step: 113 Learning rate: 0.0003125 #####
Epoch 74, Train Loss: 0.1106, Val Loss: 0.1120
2025-06-25 07:36:47.071 INFO: Epoch 74, Train Loss: 0.1106, Val Loss: 0.1120
train_e/atom_mae: 0.000257
2025-06-25 07:36:47.072 INFO: train_e/atom_mae: 0.000257
train_e/atom_rmse: 0.000322
2025-06-25 07:36:47.072 INFO: train_e/atom_rmse: 0.000322
train_f_mae: 0.007551
2025-06-25 07:36:47.076 INFO: train_f_mae: 0.007551
train_f_rmse: 0.010455
2025-06-25 07:36:47.076 INFO: train_f_rmse: 0.010455
val_e/atom_mae: 0.000247
2025-06-25 07:36:47.078 INFO: val_e/atom_mae: 0.000247
val_e/atom_rmse: 0.000289
2025-06-25 07:36:47.079 INFO: val_e/atom_rmse: 0.000289
val_f_mae: 0.007619
2025-06-25 07:36:47.079 INFO: val_f_mae: 0.007619
val_f_rmse: 0.010534
2025-06-25 07:36:47.079 INFO: val_f_rmse: 0.010534
##### Step: 114 Learning rate: 0.0003125 #####
2025-06-25 07:37:31.558 INFO: ##### Step: 114 Learning rate: 0.0003125 #####
Epoch 75, Train Loss: 0.1106, Val Loss: 0.1110
2025-06-25 07:37:31.558 INFO: Epoch 75, Train Loss: 0.1106, Val Loss: 0.1110
train_e/atom_mae: 0.000249
2025-06-25 07:37:31.559 INFO: train_e/atom_mae: 0.000249
train_e/atom_rmse: 0.000323
2025-06-25 07:37:31.559 INFO: train_e/atom_rmse: 0.000323
train_f_mae: 0.007556
2025-06-25 07:37:31.563 INFO: train_f_mae: 0.007556
train_f_rmse: 0.010458
2025-06-25 07:37:31.563 INFO: train_f_rmse: 0.010458
val_e/atom_mae: 0.000187
2025-06-25 07:37:31.565 INFO: val_e/atom_mae: 0.000187
val_e/atom_rmse: 0.000230
2025-06-25 07:37:31.566 INFO: val_e/atom_rmse: 0.000230
val_f_mae: 0.007590
2025-06-25 07:37:31.566 INFO: val_f_mae: 0.007590
val_f_rmse: 0.010504
2025-06-25 07:37:31.566 INFO: val_f_rmse: 0.010504
##### Step: 115 Learning rate: 0.0003125 #####
2025-06-25 07:38:16.032 INFO: ##### Step: 115 Learning rate: 0.0003125 #####
Epoch 76, Train Loss: 0.1117, Val Loss: 0.1262
2025-06-25 07:38:16.032 INFO: Epoch 76, Train Loss: 0.1117, Val Loss: 0.1262
train_e/atom_mae: 0.000273
2025-06-25 07:38:16.033 INFO: train_e/atom_mae: 0.000273
train_e/atom_rmse: 0.000339
2025-06-25 07:38:16.033 INFO: train_e/atom_rmse: 0.000339
train_f_mae: 0.007591
2025-06-25 07:38:16.036 INFO: train_f_mae: 0.007591
train_f_rmse: 0.010504
2025-06-25 07:38:16.037 INFO: train_f_rmse: 0.010504
val_e/atom_mae: 0.000431
2025-06-25 07:38:16.039 INFO: val_e/atom_mae: 0.000431
val_e/atom_rmse: 0.000497
2025-06-25 07:38:16.040 INFO: val_e/atom_rmse: 0.000497
val_f_mae: 0.007951
2025-06-25 07:38:16.040 INFO: val_f_mae: 0.007951
val_f_rmse: 0.011098
2025-06-25 07:38:16.040 INFO: val_f_rmse: 0.011098
##### Step: 116 Learning rate: 0.0003125 #####
2025-06-25 07:39:00.523 INFO: ##### Step: 116 Learning rate: 0.0003125 #####
Epoch 77, Train Loss: 0.1099, Val Loss: 0.1118
2025-06-25 07:39:00.523 INFO: Epoch 77, Train Loss: 0.1099, Val Loss: 0.1118
train_e/atom_mae: 0.000207
2025-06-25 07:39:00.524 INFO: train_e/atom_mae: 0.000207
train_e/atom_rmse: 0.000261
2025-06-25 07:39:00.524 INFO: train_e/atom_rmse: 0.000261
train_f_mae: 0.007551
2025-06-25 07:39:00.528 INFO: train_f_mae: 0.007551
train_f_rmse: 0.010443
2025-06-25 07:39:00.528 INFO: train_f_rmse: 0.010443
val_e/atom_mae: 0.000148
2025-06-25 07:39:00.531 INFO: val_e/atom_mae: 0.000148
val_e/atom_rmse: 0.000183
2025-06-25 07:39:00.531 INFO: val_e/atom_rmse: 0.000183
val_f_mae: 0.007631
2025-06-25 07:39:00.532 INFO: val_f_mae: 0.007631
val_f_rmse: 0.010555
2025-06-25 07:39:00.532 INFO: val_f_rmse: 0.010555
##### Step: 117 Learning rate: 0.0003125 #####
2025-06-25 07:39:44.989 INFO: ##### Step: 117 Learning rate: 0.0003125 #####
Epoch 78, Train Loss: 0.1101, Val Loss: 0.1109
2025-06-25 07:39:44.989 INFO: Epoch 78, Train Loss: 0.1101, Val Loss: 0.1109
train_e/atom_mae: 0.000229
2025-06-25 07:39:44.990 INFO: train_e/atom_mae: 0.000229
train_e/atom_rmse: 0.000286
2025-06-25 07:39:44.990 INFO: train_e/atom_rmse: 0.000286
train_f_mae: 0.007549
2025-06-25 07:39:44.994 INFO: train_f_mae: 0.007549
train_f_rmse: 0.010444
2025-06-25 07:39:44.994 INFO: train_f_rmse: 0.010444
val_e/atom_mae: 0.000164
2025-06-25 07:39:44.996 INFO: val_e/atom_mae: 0.000164
val_e/atom_rmse: 0.000205
2025-06-25 07:39:44.997 INFO: val_e/atom_rmse: 0.000205
val_f_mae: 0.007602
2025-06-25 07:39:44.997 INFO: val_f_mae: 0.007602
val_f_rmse: 0.010505
2025-06-25 07:39:44.997 INFO: val_f_rmse: 0.010505
##### Step: 118 Learning rate: 0.0003125 #####
2025-06-25 07:40:29.458 INFO: ##### Step: 118 Learning rate: 0.0003125 #####
Epoch 79, Train Loss: 0.1098, Val Loss: 0.1144
2025-06-25 07:40:29.458 INFO: Epoch 79, Train Loss: 0.1098, Val Loss: 0.1144
train_e/atom_mae: 0.000255
2025-06-25 07:40:29.459 INFO: train_e/atom_mae: 0.000255
train_e/atom_rmse: 0.000320
2025-06-25 07:40:29.459 INFO: train_e/atom_rmse: 0.000320
train_f_mae: 0.007534
2025-06-25 07:40:29.463 INFO: train_f_mae: 0.007534
train_f_rmse: 0.010420
2025-06-25 07:40:29.463 INFO: train_f_rmse: 0.010420
val_e/atom_mae: 0.000514
2025-06-25 07:40:29.466 INFO: val_e/atom_mae: 0.000514
val_e/atom_rmse: 0.000580
2025-06-25 07:40:29.466 INFO: val_e/atom_rmse: 0.000580
val_f_mae: 0.007591
2025-06-25 07:40:29.467 INFO: val_f_mae: 0.007591
val_f_rmse: 0.010506
2025-06-25 07:40:29.467 INFO: val_f_rmse: 0.010506
##### Step: 119 Learning rate: 0.0003125 #####
2025-06-25 07:41:13.916 INFO: ##### Step: 119 Learning rate: 0.0003125 #####
Epoch 80, Train Loss: 0.1085, Val Loss: 0.1108
2025-06-25 07:41:13.916 INFO: Epoch 80, Train Loss: 0.1085, Val Loss: 0.1108
train_e/atom_mae: 0.000237
2025-06-25 07:41:13.917 INFO: train_e/atom_mae: 0.000237
train_e/atom_rmse: 0.000297
2025-06-25 07:41:13.917 INFO: train_e/atom_rmse: 0.000297
train_f_mae: 0.007501
2025-06-25 07:41:13.921 INFO: train_f_mae: 0.007501
train_f_rmse: 0.010365
2025-06-25 07:41:13.921 INFO: train_f_rmse: 0.010365
val_e/atom_mae: 0.000118
2025-06-25 07:41:13.924 INFO: val_e/atom_mae: 0.000118
val_e/atom_rmse: 0.000150
2025-06-25 07:41:13.924 INFO: val_e/atom_rmse: 0.000150
val_f_mae: 0.007589
2025-06-25 07:41:13.925 INFO: val_f_mae: 0.007589
val_f_rmse: 0.010513
2025-06-25 07:41:13.925 INFO: val_f_rmse: 0.010513
##### Step: 120 Learning rate: 0.00015625 #####
2025-06-25 07:41:58.132 INFO: ##### Step: 120 Learning rate: 0.00015625 #####
Epoch 81, Train Loss: 0.1067, Val Loss: 0.1101
2025-06-25 07:41:58.132 INFO: Epoch 81, Train Loss: 0.1067, Val Loss: 0.1101
train_e/atom_mae: 0.000183
2025-06-25 07:41:58.133 INFO: train_e/atom_mae: 0.000183
train_e/atom_rmse: 0.000231
2025-06-25 07:41:58.133 INFO: train_e/atom_rmse: 0.000231
train_f_mae: 0.007457
2025-06-25 07:41:58.137 INFO: train_f_mae: 0.007457
train_f_rmse: 0.010300
2025-06-25 07:41:58.137 INFO: train_f_rmse: 0.010300
val_e/atom_mae: 0.000362
2025-06-25 07:41:58.139 INFO: val_e/atom_mae: 0.000362
val_e/atom_rmse: 0.000386
2025-06-25 07:41:58.140 INFO: val_e/atom_rmse: 0.000386
val_f_mae: 0.007536
2025-06-25 07:41:58.140 INFO: val_f_mae: 0.007536
val_f_rmse: 0.010405
2025-06-25 07:41:58.140 INFO: val_f_rmse: 0.010405
##### Step: 121 Learning rate: 0.00015625 #####
2025-06-25 07:42:42.222 INFO: ##### Step: 121 Learning rate: 0.00015625 #####
Epoch 82, Train Loss: 0.1065, Val Loss: 0.1104
2025-06-25 07:42:42.222 INFO: Epoch 82, Train Loss: 0.1065, Val Loss: 0.1104
train_e/atom_mae: 0.000187
2025-06-25 07:42:42.223 INFO: train_e/atom_mae: 0.000187
train_e/atom_rmse: 0.000234
2025-06-25 07:42:42.223 INFO: train_e/atom_rmse: 0.000234
train_f_mae: 0.007453
2025-06-25 07:42:42.227 INFO: train_f_mae: 0.007453
train_f_rmse: 0.010289
2025-06-25 07:42:42.227 INFO: train_f_rmse: 0.010289
val_e/atom_mae: 0.000099
2025-06-25 07:42:42.230 INFO: val_e/atom_mae: 0.000099
val_e/atom_rmse: 0.000135
2025-06-25 07:42:42.230 INFO: val_e/atom_rmse: 0.000135
val_f_mae: 0.007592
2025-06-25 07:42:42.230 INFO: val_f_mae: 0.007592
val_f_rmse: 0.010499
2025-06-25 07:42:42.231 INFO: val_f_rmse: 0.010499
##### Step: 122 Learning rate: 0.00015625 #####
2025-06-25 07:43:26.329 INFO: ##### Step: 122 Learning rate: 0.00015625 #####
Epoch 83, Train Loss: 0.1065, Val Loss: 0.1089
2025-06-25 07:43:26.329 INFO: Epoch 83, Train Loss: 0.1065, Val Loss: 0.1089
train_e/atom_mae: 0.000169
2025-06-25 07:43:26.330 INFO: train_e/atom_mae: 0.000169
train_e/atom_rmse: 0.000212
2025-06-25 07:43:26.330 INFO: train_e/atom_rmse: 0.000212
train_f_mae: 0.007453
2025-06-25 07:43:26.333 INFO: train_f_mae: 0.007453
train_f_rmse: 0.010293
2025-06-25 07:43:26.333 INFO: train_f_rmse: 0.010293
val_e/atom_mae: 0.000168
2025-06-25 07:43:26.336 INFO: val_e/atom_mae: 0.000168
val_e/atom_rmse: 0.000194
2025-06-25 07:43:26.336 INFO: val_e/atom_rmse: 0.000194
val_f_mae: 0.007542
2025-06-25 07:43:26.337 INFO: val_f_mae: 0.007542
val_f_rmse: 0.010414
2025-06-25 07:43:26.337 INFO: val_f_rmse: 0.010414
##### Step: 123 Learning rate: 0.00015625 #####
2025-06-25 07:44:10.438 INFO: ##### Step: 123 Learning rate: 0.00015625 #####
Epoch 84, Train Loss: 0.1063, Val Loss: 0.1103
2025-06-25 07:44:10.439 INFO: Epoch 84, Train Loss: 0.1063, Val Loss: 0.1103
train_e/atom_mae: 0.000179
2025-06-25 07:44:10.440 INFO: train_e/atom_mae: 0.000179
train_e/atom_rmse: 0.000221
2025-06-25 07:44:10.440 INFO: train_e/atom_rmse: 0.000221
train_f_mae: 0.007446
2025-06-25 07:44:10.443 INFO: train_f_mae: 0.007446
train_f_rmse: 0.010280
2025-06-25 07:44:10.443 INFO: train_f_rmse: 0.010280
val_e/atom_mae: 0.000272
2025-06-25 07:44:10.446 INFO: val_e/atom_mae: 0.000272
val_e/atom_rmse: 0.000311
2025-06-25 07:44:10.446 INFO: val_e/atom_rmse: 0.000311
val_f_mae: 0.007570
2025-06-25 07:44:10.447 INFO: val_f_mae: 0.007570
val_f_rmse: 0.010448
2025-06-25 07:44:10.447 INFO: val_f_rmse: 0.010448
##### Step: 124 Learning rate: 0.00015625 #####
2025-06-25 07:44:54.547 INFO: ##### Step: 124 Learning rate: 0.00015625 #####
Epoch 85, Train Loss: 0.1063, Val Loss: 0.1085
2025-06-25 07:44:54.547 INFO: Epoch 85, Train Loss: 0.1063, Val Loss: 0.1085
train_e/atom_mae: 0.000182
2025-06-25 07:44:54.548 INFO: train_e/atom_mae: 0.000182
train_e/atom_rmse: 0.000232
2025-06-25 07:44:54.548 INFO: train_e/atom_rmse: 0.000232
train_f_mae: 0.007450
2025-06-25 07:44:54.551 INFO: train_f_mae: 0.007450
train_f_rmse: 0.010280
2025-06-25 07:44:54.552 INFO: train_f_rmse: 0.010280
val_e/atom_mae: 0.000152
2025-06-25 07:44:54.554 INFO: val_e/atom_mae: 0.000152
val_e/atom_rmse: 0.000182
2025-06-25 07:44:54.554 INFO: val_e/atom_rmse: 0.000182
val_f_mae: 0.007534
2025-06-25 07:44:54.555 INFO: val_f_mae: 0.007534
val_f_rmse: 0.010395
2025-06-25 07:44:54.555 INFO: val_f_rmse: 0.010395
##### Step: 125 Learning rate: 0.00015625 #####
2025-06-25 07:45:38.658 INFO: ##### Step: 125 Learning rate: 0.00015625 #####
Epoch 86, Train Loss: 0.1062, Val Loss: 0.1086
2025-06-25 07:45:38.658 INFO: Epoch 86, Train Loss: 0.1062, Val Loss: 0.1086
train_e/atom_mae: 0.000169
2025-06-25 07:45:38.659 INFO: train_e/atom_mae: 0.000169
train_e/atom_rmse: 0.000223
2025-06-25 07:45:38.659 INFO: train_e/atom_rmse: 0.000223
train_f_mae: 0.007442
2025-06-25 07:45:38.662 INFO: train_f_mae: 0.007442
train_f_rmse: 0.010278
2025-06-25 07:45:38.662 INFO: train_f_rmse: 0.010278
val_e/atom_mae: 0.000098
2025-06-25 07:45:38.665 INFO: val_e/atom_mae: 0.000098
val_e/atom_rmse: 0.000127
2025-06-25 07:45:38.665 INFO: val_e/atom_rmse: 0.000127
val_f_mae: 0.007553
2025-06-25 07:45:38.666 INFO: val_f_mae: 0.007553
val_f_rmse: 0.010414
2025-06-25 07:45:38.666 INFO: val_f_rmse: 0.010414
##### Step: 126 Learning rate: 0.00015625 #####
2025-06-25 07:46:22.769 INFO: ##### Step: 126 Learning rate: 0.00015625 #####
Epoch 87, Train Loss: 0.1065, Val Loss: 0.1092
2025-06-25 07:46:22.769 INFO: Epoch 87, Train Loss: 0.1065, Val Loss: 0.1092
train_e/atom_mae: 0.000200
2025-06-25 07:46:22.770 INFO: train_e/atom_mae: 0.000200
train_e/atom_rmse: 0.000251
2025-06-25 07:46:22.770 INFO: train_e/atom_rmse: 0.000251
train_f_mae: 0.007443
2025-06-25 07:46:22.773 INFO: train_f_mae: 0.007443
train_f_rmse: 0.010284
2025-06-25 07:46:22.774 INFO: train_f_rmse: 0.010284
val_e/atom_mae: 0.000115
2025-06-25 07:46:22.776 INFO: val_e/atom_mae: 0.000115
val_e/atom_rmse: 0.000137
2025-06-25 07:46:22.776 INFO: val_e/atom_rmse: 0.000137
val_f_mae: 0.007537
2025-06-25 07:46:22.777 INFO: val_f_mae: 0.007537
val_f_rmse: 0.010439
2025-06-25 07:46:22.777 INFO: val_f_rmse: 0.010439
##### Step: 127 Learning rate: 0.00015625 #####
2025-06-25 07:47:06.901 INFO: ##### Step: 127 Learning rate: 0.00015625 #####
Epoch 88, Train Loss: 0.1062, Val Loss: 0.1103
2025-06-25 07:47:06.902 INFO: Epoch 88, Train Loss: 0.1062, Val Loss: 0.1103
train_e/atom_mae: 0.000176
2025-06-25 07:47:06.903 INFO: train_e/atom_mae: 0.000176
train_e/atom_rmse: 0.000230
2025-06-25 07:47:06.903 INFO: train_e/atom_rmse: 0.000230
train_f_mae: 0.007441
2025-06-25 07:47:06.906 INFO: train_f_mae: 0.007441
train_f_rmse: 0.010273
2025-06-25 07:47:06.906 INFO: train_f_rmse: 0.010273
val_e/atom_mae: 0.000406
2025-06-25 07:47:06.909 INFO: val_e/atom_mae: 0.000406
val_e/atom_rmse: 0.000435
2025-06-25 07:47:06.909 INFO: val_e/atom_rmse: 0.000435
val_f_mae: 0.007525
2025-06-25 07:47:06.910 INFO: val_f_mae: 0.007525
val_f_rmse: 0.010394
2025-06-25 07:47:06.910 INFO: val_f_rmse: 0.010394
##### Step: 128 Learning rate: 0.00015625 #####
2025-06-25 07:47:51.027 INFO: ##### Step: 128 Learning rate: 0.00015625 #####
Epoch 89, Train Loss: 0.1061, Val Loss: 0.1094
2025-06-25 07:47:51.027 INFO: Epoch 89, Train Loss: 0.1061, Val Loss: 0.1094
train_e/atom_mae: 0.000192
2025-06-25 07:47:51.028 INFO: train_e/atom_mae: 0.000192
train_e/atom_rmse: 0.000236
2025-06-25 07:47:51.028 INFO: train_e/atom_rmse: 0.000236
train_f_mae: 0.007435
2025-06-25 07:47:51.031 INFO: train_f_mae: 0.007435
train_f_rmse: 0.010265
2025-06-25 07:47:51.031 INFO: train_f_rmse: 0.010265
val_e/atom_mae: 0.000171
2025-06-25 07:47:51.034 INFO: val_e/atom_mae: 0.000171
val_e/atom_rmse: 0.000192
2025-06-25 07:47:51.034 INFO: val_e/atom_rmse: 0.000192
val_f_mae: 0.007545
2025-06-25 07:47:51.035 INFO: val_f_mae: 0.007545
val_f_rmse: 0.010439
2025-06-25 07:47:51.035 INFO: val_f_rmse: 0.010439
##### Step: 129 Learning rate: 0.00015625 #####
2025-06-25 07:48:35.150 INFO: ##### Step: 129 Learning rate: 0.00015625 #####
Epoch 90, Train Loss: 0.1066, Val Loss: 0.1082
2025-06-25 07:48:35.150 INFO: Epoch 90, Train Loss: 0.1066, Val Loss: 0.1082
train_e/atom_mae: 0.000191
2025-06-25 07:48:35.151 INFO: train_e/atom_mae: 0.000191
train_e/atom_rmse: 0.000238
2025-06-25 07:48:35.151 INFO: train_e/atom_rmse: 0.000238
train_f_mae: 0.007444
2025-06-25 07:48:35.154 INFO: train_f_mae: 0.007444
train_f_rmse: 0.010292
2025-06-25 07:48:35.155 INFO: train_f_rmse: 0.010292
val_e/atom_mae: 0.000145
2025-06-25 07:48:35.157 INFO: val_e/atom_mae: 0.000145
val_e/atom_rmse: 0.000175
2025-06-25 07:48:35.157 INFO: val_e/atom_rmse: 0.000175
val_f_mae: 0.007517
2025-06-25 07:48:35.158 INFO: val_f_mae: 0.007517
val_f_rmse: 0.010386
2025-06-25 07:48:35.158 INFO: val_f_rmse: 0.010386
##### Step: 130 Learning rate: 0.00015625 #####
2025-06-25 07:49:19.282 INFO: ##### Step: 130 Learning rate: 0.00015625 #####
Epoch 91, Train Loss: 0.1061, Val Loss: 0.1088
2025-06-25 07:49:19.282 INFO: Epoch 91, Train Loss: 0.1061, Val Loss: 0.1088
train_e/atom_mae: 0.000183
2025-06-25 07:49:19.283 INFO: train_e/atom_mae: 0.000183
train_e/atom_rmse: 0.000229
2025-06-25 07:49:19.283 INFO: train_e/atom_rmse: 0.000229
train_f_mae: 0.007433
2025-06-25 07:49:19.287 INFO: train_f_mae: 0.007433
train_f_rmse: 0.010268
2025-06-25 07:49:19.287 INFO: train_f_rmse: 0.010268
val_e/atom_mae: 0.000195
2025-06-25 07:49:19.289 INFO: val_e/atom_mae: 0.000195
val_e/atom_rmse: 0.000239
2025-06-25 07:49:19.290 INFO: val_e/atom_rmse: 0.000239
val_f_mae: 0.007525
2025-06-25 07:49:19.290 INFO: val_f_mae: 0.007525
val_f_rmse: 0.010396
2025-06-25 07:49:19.290 INFO: val_f_rmse: 0.010396
##### Step: 131 Learning rate: 0.00015625 #####
2025-06-25 07:50:03.412 INFO: ##### Step: 131 Learning rate: 0.00015625 #####
Epoch 92, Train Loss: 0.1057, Val Loss: 0.1083
2025-06-25 07:50:03.413 INFO: Epoch 92, Train Loss: 0.1057, Val Loss: 0.1083
train_e/atom_mae: 0.000164
2025-06-25 07:50:03.414 INFO: train_e/atom_mae: 0.000164
train_e/atom_rmse: 0.000210
2025-06-25 07:50:03.414 INFO: train_e/atom_rmse: 0.000210
train_f_mae: 0.007429
2025-06-25 07:50:03.417 INFO: train_f_mae: 0.007429
train_f_rmse: 0.010253
2025-06-25 07:50:03.417 INFO: train_f_rmse: 0.010253
val_e/atom_mae: 0.000116
2025-06-25 07:50:03.420 INFO: val_e/atom_mae: 0.000116
val_e/atom_rmse: 0.000149
2025-06-25 07:50:03.420 INFO: val_e/atom_rmse: 0.000149
val_f_mae: 0.007517
2025-06-25 07:50:03.421 INFO: val_f_mae: 0.007517
val_f_rmse: 0.010395
2025-06-25 07:50:03.421 INFO: val_f_rmse: 0.010395
##### Step: 132 Learning rate: 0.00015625 #####
2025-06-25 07:50:47.541 INFO: ##### Step: 132 Learning rate: 0.00015625 #####
Epoch 93, Train Loss: 0.1059, Val Loss: 0.1100
2025-06-25 07:50:47.542 INFO: Epoch 93, Train Loss: 0.1059, Val Loss: 0.1100
train_e/atom_mae: 0.000191
2025-06-25 07:50:47.543 INFO: train_e/atom_mae: 0.000191
train_e/atom_rmse: 0.000238
2025-06-25 07:50:47.543 INFO: train_e/atom_rmse: 0.000238
train_f_mae: 0.007432
2025-06-25 07:50:47.546 INFO: train_f_mae: 0.007432
train_f_rmse: 0.010259
2025-06-25 07:50:47.546 INFO: train_f_rmse: 0.010259
val_e/atom_mae: 0.000162
2025-06-25 07:50:47.549 INFO: val_e/atom_mae: 0.000162
val_e/atom_rmse: 0.000195
2025-06-25 07:50:47.549 INFO: val_e/atom_rmse: 0.000195
val_f_mae: 0.007578
2025-06-25 07:50:47.550 INFO: val_f_mae: 0.007578
val_f_rmse: 0.010464
2025-06-25 07:50:47.550 INFO: val_f_rmse: 0.010464
##### Step: 133 Learning rate: 0.00015625 #####
2025-06-25 07:51:31.680 INFO: ##### Step: 133 Learning rate: 0.00015625 #####
Epoch 94, Train Loss: 0.1061, Val Loss: 0.1082
2025-06-25 07:51:31.681 INFO: Epoch 94, Train Loss: 0.1061, Val Loss: 0.1082
train_e/atom_mae: 0.000184
2025-06-25 07:51:31.681 INFO: train_e/atom_mae: 0.000184
train_e/atom_rmse: 0.000234
2025-06-25 07:51:31.682 INFO: train_e/atom_rmse: 0.000234
train_f_mae: 0.007433
2025-06-25 07:51:31.685 INFO: train_f_mae: 0.007433
train_f_rmse: 0.010266
2025-06-25 07:51:31.685 INFO: train_f_rmse: 0.010266
val_e/atom_mae: 0.000116
2025-06-25 07:51:31.688 INFO: val_e/atom_mae: 0.000116
val_e/atom_rmse: 0.000153
2025-06-25 07:51:31.688 INFO: val_e/atom_rmse: 0.000153
val_f_mae: 0.007524
2025-06-25 07:51:31.688 INFO: val_f_mae: 0.007524
val_f_rmse: 0.010386
2025-06-25 07:51:31.689 INFO: val_f_rmse: 0.010386
##### Step: 134 Learning rate: 0.00015625 #####
2025-06-25 07:52:15.811 INFO: ##### Step: 134 Learning rate: 0.00015625 #####
Epoch 95, Train Loss: 0.1062, Val Loss: 0.1075
2025-06-25 07:52:15.811 INFO: Epoch 95, Train Loss: 0.1062, Val Loss: 0.1075
train_e/atom_mae: 0.000191
2025-06-25 07:52:15.812 INFO: train_e/atom_mae: 0.000191
train_e/atom_rmse: 0.000236
2025-06-25 07:52:15.812 INFO: train_e/atom_rmse: 0.000236
train_f_mae: 0.007439
2025-06-25 07:52:15.815 INFO: train_f_mae: 0.007439
train_f_rmse: 0.010272
2025-06-25 07:52:15.816 INFO: train_f_rmse: 0.010272
val_e/atom_mae: 0.000084
2025-06-25 07:52:15.818 INFO: val_e/atom_mae: 0.000084
val_e/atom_rmse: 0.000116
2025-06-25 07:52:15.818 INFO: val_e/atom_rmse: 0.000116
val_f_mae: 0.007507
2025-06-25 07:52:15.819 INFO: val_f_mae: 0.007507
val_f_rmse: 0.010361
2025-06-25 07:52:15.819 INFO: val_f_rmse: 0.010361
##### Step: 135 Learning rate: 0.00015625 #####
2025-06-25 07:52:59.909 INFO: ##### Step: 135 Learning rate: 0.00015625 #####
Epoch 96, Train Loss: 0.1064, Val Loss: 0.1089
2025-06-25 07:52:59.909 INFO: Epoch 96, Train Loss: 0.1064, Val Loss: 0.1089
train_e/atom_mae: 0.000201
2025-06-25 07:52:59.910 INFO: train_e/atom_mae: 0.000201
train_e/atom_rmse: 0.000253
2025-06-25 07:52:59.910 INFO: train_e/atom_rmse: 0.000253
train_f_mae: 0.007441
2025-06-25 07:52:59.914 INFO: train_f_mae: 0.007441
train_f_rmse: 0.010278
2025-06-25 07:52:59.914 INFO: train_f_rmse: 0.010278
val_e/atom_mae: 0.000258
2025-06-25 07:52:59.916 INFO: val_e/atom_mae: 0.000258
val_e/atom_rmse: 0.000308
2025-06-25 07:52:59.917 INFO: val_e/atom_rmse: 0.000308
val_f_mae: 0.007521
2025-06-25 07:52:59.917 INFO: val_f_mae: 0.007521
val_f_rmse: 0.010382
2025-06-25 07:52:59.917 INFO: val_f_rmse: 0.010382
##### Step: 136 Learning rate: 0.00015625 #####
2025-06-25 07:53:44.042 INFO: ##### Step: 136 Learning rate: 0.00015625 #####
Epoch 97, Train Loss: 0.1060, Val Loss: 0.1088
2025-06-25 07:53:44.042 INFO: Epoch 97, Train Loss: 0.1060, Val Loss: 0.1088
train_e/atom_mae: 0.000193
2025-06-25 07:53:44.043 INFO: train_e/atom_mae: 0.000193
train_e/atom_rmse: 0.000242
2025-06-25 07:53:44.043 INFO: train_e/atom_rmse: 0.000242
train_f_mae: 0.007431
2025-06-25 07:53:44.047 INFO: train_f_mae: 0.007431
train_f_rmse: 0.010262
2025-06-25 07:53:44.047 INFO: train_f_rmse: 0.010262
val_e/atom_mae: 0.000169
2025-06-25 07:53:44.050 INFO: val_e/atom_mae: 0.000169
val_e/atom_rmse: 0.000196
2025-06-25 07:53:44.050 INFO: val_e/atom_rmse: 0.000196
val_f_mae: 0.007530
2025-06-25 07:53:44.050 INFO: val_f_mae: 0.007530
val_f_rmse: 0.010408
2025-06-25 07:53:44.051 INFO: val_f_rmse: 0.010408
##### Step: 137 Learning rate: 0.00015625 #####
2025-06-25 07:54:28.159 INFO: ##### Step: 137 Learning rate: 0.00015625 #####
Epoch 98, Train Loss: 0.1063, Val Loss: 0.1277
2025-06-25 07:54:28.160 INFO: Epoch 98, Train Loss: 0.1063, Val Loss: 0.1277
train_e/atom_mae: 0.000221
2025-06-25 07:54:28.160 INFO: train_e/atom_mae: 0.000221
train_e/atom_rmse: 0.000275
2025-06-25 07:54:28.161 INFO: train_e/atom_rmse: 0.000275
train_f_mae: 0.007430
2025-06-25 07:54:28.164 INFO: train_f_mae: 0.007430
train_f_rmse: 0.010266
2025-06-25 07:54:28.164 INFO: train_f_rmse: 0.010266
val_e/atom_mae: 0.000462
2025-06-25 07:54:28.167 INFO: val_e/atom_mae: 0.000462
val_e/atom_rmse: 0.000500
2025-06-25 07:54:28.167 INFO: val_e/atom_rmse: 0.000500
val_f_mae: 0.007978
2025-06-25 07:54:28.167 INFO: val_f_mae: 0.007978
val_f_rmse: 0.011167
2025-06-25 07:54:28.168 INFO: val_f_rmse: 0.011167
##### Step: 138 Learning rate: 0.00015625 #####
2025-06-25 07:55:12.256 INFO: ##### Step: 138 Learning rate: 0.00015625 #####
Epoch 99, Train Loss: 0.1068, Val Loss: 0.1082
2025-06-25 07:55:12.256 INFO: Epoch 99, Train Loss: 0.1068, Val Loss: 0.1082
train_e/atom_mae: 0.000200
2025-06-25 07:55:12.257 INFO: train_e/atom_mae: 0.000200
train_e/atom_rmse: 0.000254
2025-06-25 07:55:12.257 INFO: train_e/atom_rmse: 0.000254
train_f_mae: 0.007452
2025-06-25 07:55:12.260 INFO: train_f_mae: 0.007452
train_f_rmse: 0.010297
2025-06-25 07:55:12.260 INFO: train_f_rmse: 0.010297
val_e/atom_mae: 0.000306
2025-06-25 07:55:12.263 INFO: val_e/atom_mae: 0.000306
val_e/atom_rmse: 0.000326
2025-06-25 07:55:12.263 INFO: val_e/atom_rmse: 0.000326
val_f_mae: 0.007492
2025-06-25 07:55:12.264 INFO: val_f_mae: 0.007492
val_f_rmse: 0.010339
2025-06-25 07:55:12.264 INFO: val_f_rmse: 0.010339
##### Step: 139 Learning rate: 0.00015625 #####
2025-06-25 07:55:56.483 INFO: ##### Step: 139 Learning rate: 0.00015625 #####
Epoch 100, Train Loss: 0.1051, Val Loss: 0.1096
2025-06-25 07:55:56.483 INFO: Epoch 100, Train Loss: 0.1051, Val Loss: 0.1096
train_e/atom_mae: 0.000173
2025-06-25 07:55:56.484 INFO: train_e/atom_mae: 0.000173
train_e/atom_rmse: 0.000219
2025-06-25 07:55:56.484 INFO: train_e/atom_rmse: 0.000219
train_f_mae: 0.007409
2025-06-25 07:55:56.488 INFO: train_f_mae: 0.007409
train_f_rmse: 0.010225
2025-06-25 07:55:56.488 INFO: train_f_rmse: 0.010225
val_e/atom_mae: 0.000365
2025-06-25 07:55:56.490 INFO: val_e/atom_mae: 0.000365
val_e/atom_rmse: 0.000401
2025-06-25 07:55:56.491 INFO: val_e/atom_rmse: 0.000401
val_f_mae: 0.007515
2025-06-25 07:55:56.491 INFO: val_f_mae: 0.007515
val_f_rmse: 0.010376
2025-06-25 07:55:56.491 INFO: val_f_rmse: 0.010376
2025-06-25 07:55:56.503 INFO: Third train loop:
##### Step: 140 Learning rate: 7.8125e-05 #####
2025-06-25 07:56:40.582 INFO: ##### Step: 140 Learning rate: 7.8125e-05 #####
Epoch 1, Train Loss: 0.1073, Val Loss: 0.1094
2025-06-25 07:56:40.582 INFO: Epoch 1, Train Loss: 0.1073, Val Loss: 0.1094
train_e/atom_mae: 0.000138
2025-06-25 07:56:40.583 INFO: train_e/atom_mae: 0.000138
train_e/atom_rmse: 0.000179
2025-06-25 07:56:40.583 INFO: train_e/atom_rmse: 0.000179
train_f_mae: 0.007373
2025-06-25 07:56:40.586 INFO: train_f_mae: 0.007373
train_f_rmse: 0.010170
2025-06-25 07:56:40.586 INFO: train_f_rmse: 0.010170
val_e/atom_mae: 0.000127
2025-06-25 07:56:40.589 INFO: val_e/atom_mae: 0.000127
val_e/atom_rmse: 0.000147
2025-06-25 07:56:40.589 INFO: val_e/atom_rmse: 0.000147
val_f_mae: 0.007483
2025-06-25 07:56:40.590 INFO: val_f_mae: 0.007483
val_f_rmse: 0.010334
2025-06-25 07:56:40.590 INFO: val_f_rmse: 0.010334
##### Step: 141 Learning rate: 7.8125e-05 #####
2025-06-25 07:57:24.656 INFO: ##### Step: 141 Learning rate: 7.8125e-05 #####
Epoch 2, Train Loss: 0.1063, Val Loss: 0.1075
2025-06-25 07:57:24.656 INFO: Epoch 2, Train Loss: 0.1063, Val Loss: 0.1075
train_e/atom_mae: 0.000115
2025-06-25 07:57:24.657 INFO: train_e/atom_mae: 0.000115
train_e/atom_rmse: 0.000150
2025-06-25 07:57:24.657 INFO: train_e/atom_rmse: 0.000150
train_f_mae: 0.007377
2025-06-25 07:57:24.661 INFO: train_f_mae: 0.007377
train_f_rmse: 0.010180
2025-06-25 07:57:24.661 INFO: train_f_rmse: 0.010180
val_e/atom_mae: 0.000072
2025-06-25 07:57:24.663 INFO: val_e/atom_mae: 0.000072
val_e/atom_rmse: 0.000095
2025-06-25 07:57:24.664 INFO: val_e/atom_rmse: 0.000095
val_f_mae: 0.007466
2025-06-25 07:57:24.664 INFO: val_f_mae: 0.007466
val_f_rmse: 0.010314
2025-06-25 07:57:24.664 INFO: val_f_rmse: 0.010314
##### Step: 142 Learning rate: 7.8125e-05 #####
2025-06-25 07:58:08.737 INFO: ##### Step: 142 Learning rate: 7.8125e-05 #####
Epoch 3, Train Loss: 0.1053, Val Loss: 0.1070
2025-06-25 07:58:08.737 INFO: Epoch 3, Train Loss: 0.1053, Val Loss: 0.1070
train_e/atom_mae: 0.000098
2025-06-25 07:58:08.738 INFO: train_e/atom_mae: 0.000098
train_e/atom_rmse: 0.000127
2025-06-25 07:58:08.738 INFO: train_e/atom_rmse: 0.000127
train_f_mae: 0.007369
2025-06-25 07:58:08.741 INFO: train_f_mae: 0.007369
train_f_rmse: 0.010166
2025-06-25 07:58:08.742 INFO: train_f_rmse: 0.010166
val_e/atom_mae: 0.000061
2025-06-25 07:58:08.744 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000085
2025-06-25 07:58:08.744 INFO: val_e/atom_rmse: 0.000085
val_f_mae: 0.007456
2025-06-25 07:58:08.745 INFO: val_f_mae: 0.007456
val_f_rmse: 0.010303
2025-06-25 07:58:08.745 INFO: val_f_rmse: 0.010303
##### Step: 143 Learning rate: 7.8125e-05 #####
2025-06-25 07:58:52.821 INFO: ##### Step: 143 Learning rate: 7.8125e-05 #####
Epoch 4, Train Loss: 0.1071, Val Loss: 0.1092
2025-06-25 07:58:52.821 INFO: Epoch 4, Train Loss: 0.1071, Val Loss: 0.1092
train_e/atom_mae: 0.000130
2025-06-25 07:58:52.822 INFO: train_e/atom_mae: 0.000130
train_e/atom_rmse: 0.000168
2025-06-25 07:58:52.822 INFO: train_e/atom_rmse: 0.000168
train_f_mae: 0.007382
2025-06-25 07:58:52.826 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010185
2025-06-25 07:58:52.826 INFO: train_f_rmse: 0.010185
val_e/atom_mae: 0.000121
2025-06-25 07:58:52.828 INFO: val_e/atom_mae: 0.000121
val_e/atom_rmse: 0.000144
2025-06-25 07:58:52.829 INFO: val_e/atom_rmse: 0.000144
val_f_mae: 0.007485
2025-06-25 07:58:52.829 INFO: val_f_mae: 0.007485
val_f_rmse: 0.010328
2025-06-25 07:58:52.829 INFO: val_f_rmse: 0.010328
##### Step: 144 Learning rate: 7.8125e-05 #####
2025-06-25 07:59:36.930 INFO: ##### Step: 144 Learning rate: 7.8125e-05 #####
Epoch 5, Train Loss: 0.1059, Val Loss: 0.1085
2025-06-25 07:59:36.930 INFO: Epoch 5, Train Loss: 0.1059, Val Loss: 0.1085
train_e/atom_mae: 0.000107
2025-06-25 07:59:36.931 INFO: train_e/atom_mae: 0.000107
train_e/atom_rmse: 0.000137
2025-06-25 07:59:36.931 INFO: train_e/atom_rmse: 0.000137
train_f_mae: 0.007378
2025-06-25 07:59:36.935 INFO: train_f_mae: 0.007378
train_f_rmse: 0.010180
2025-06-25 07:59:36.935 INFO: train_f_rmse: 0.010180
val_e/atom_mae: 0.000115
2025-06-25 07:59:36.937 INFO: val_e/atom_mae: 0.000115
val_e/atom_rmse: 0.000138
2025-06-25 07:59:36.938 INFO: val_e/atom_rmse: 0.000138
val_f_mae: 0.007473
2025-06-25 07:59:36.938 INFO: val_f_mae: 0.007473
val_f_rmse: 0.010304
2025-06-25 07:59:36.938 INFO: val_f_rmse: 0.010304
##### Step: 145 Learning rate: 7.8125e-05 #####
2025-06-25 08:00:20.987 INFO: ##### Step: 145 Learning rate: 7.8125e-05 #####
Epoch 6, Train Loss: 0.1059, Val Loss: 0.1103
2025-06-25 08:00:20.987 INFO: Epoch 6, Train Loss: 0.1059, Val Loss: 0.1103
train_e/atom_mae: 0.000109
2025-06-25 08:00:20.988 INFO: train_e/atom_mae: 0.000109
train_e/atom_rmse: 0.000142
2025-06-25 08:00:20.988 INFO: train_e/atom_rmse: 0.000142
train_f_mae: 0.007376
2025-06-25 08:00:20.992 INFO: train_f_mae: 0.007376
train_f_rmse: 0.010173
2025-06-25 08:00:20.992 INFO: train_f_rmse: 0.010173
val_e/atom_mae: 0.000145
2025-06-25 08:00:20.994 INFO: val_e/atom_mae: 0.000145
val_e/atom_rmse: 0.000176
2025-06-25 08:00:20.995 INFO: val_e/atom_rmse: 0.000176
val_f_mae: 0.007473
2025-06-25 08:00:20.995 INFO: val_f_mae: 0.007473
val_f_rmse: 0.010322
2025-06-25 08:00:20.995 INFO: val_f_rmse: 0.010322
##### Step: 146 Learning rate: 7.8125e-05 #####
2025-06-25 08:01:05.078 INFO: ##### Step: 146 Learning rate: 7.8125e-05 #####
Epoch 7, Train Loss: 0.1095, Val Loss: 0.1070
2025-06-25 08:01:05.078 INFO: Epoch 7, Train Loss: 0.1095, Val Loss: 0.1070
train_e/atom_mae: 0.000183
2025-06-25 08:01:05.079 INFO: train_e/atom_mae: 0.000183
train_e/atom_rmse: 0.000224
2025-06-25 08:01:05.079 INFO: train_e/atom_rmse: 0.000224
train_f_mae: 0.007370
2025-06-25 08:01:05.083 INFO: train_f_mae: 0.007370
train_f_rmse: 0.010168
2025-06-25 08:01:05.083 INFO: train_f_rmse: 0.010168
val_e/atom_mae: 0.000072
2025-06-25 08:01:05.085 INFO: val_e/atom_mae: 0.000072
val_e/atom_rmse: 0.000096
2025-06-25 08:01:05.086 INFO: val_e/atom_rmse: 0.000096
val_f_mae: 0.007463
2025-06-25 08:01:05.086 INFO: val_f_mae: 0.007463
val_f_rmse: 0.010292
2025-06-25 08:01:05.086 INFO: val_f_rmse: 0.010292
##### Step: 147 Learning rate: 7.8125e-05 #####
2025-06-25 08:01:49.172 INFO: ##### Step: 147 Learning rate: 7.8125e-05 #####
Epoch 8, Train Loss: 0.1063, Val Loss: 0.1091
2025-06-25 08:01:49.173 INFO: Epoch 8, Train Loss: 0.1063, Val Loss: 0.1091
train_e/atom_mae: 0.000123
2025-06-25 08:01:49.174 INFO: train_e/atom_mae: 0.000123
train_e/atom_rmse: 0.000156
2025-06-25 08:01:49.174 INFO: train_e/atom_rmse: 0.000156
train_f_mae: 0.007370
2025-06-25 08:01:49.177 INFO: train_f_mae: 0.007370
train_f_rmse: 0.010166
2025-06-25 08:01:49.177 INFO: train_f_rmse: 0.010166
val_e/atom_mae: 0.000063
2025-06-25 08:01:49.180 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000089
2025-06-25 08:01:49.180 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.007522
2025-06-25 08:01:49.181 INFO: val_f_mae: 0.007522
val_f_rmse: 0.010399
2025-06-25 08:01:49.181 INFO: val_f_rmse: 0.010399
##### Step: 148 Learning rate: 7.8125e-05 #####
2025-06-25 08:02:33.273 INFO: ##### Step: 148 Learning rate: 7.8125e-05 #####
Epoch 9, Train Loss: 0.1065, Val Loss: 0.1078
2025-06-25 08:02:33.273 INFO: Epoch 9, Train Loss: 0.1065, Val Loss: 0.1078
train_e/atom_mae: 0.000123
2025-06-25 08:02:33.274 INFO: train_e/atom_mae: 0.000123
train_e/atom_rmse: 0.000159
2025-06-25 08:02:33.274 INFO: train_e/atom_rmse: 0.000159
train_f_mae: 0.007373
2025-06-25 08:02:33.278 INFO: train_f_mae: 0.007373
train_f_rmse: 0.010170
2025-06-25 08:02:33.278 INFO: train_f_rmse: 0.010170
val_e/atom_mae: 0.000080
2025-06-25 08:02:33.280 INFO: val_e/atom_mae: 0.000080
val_e/atom_rmse: 0.000106
2025-06-25 08:02:33.281 INFO: val_e/atom_rmse: 0.000106
val_f_mae: 0.007481
2025-06-25 08:02:33.281 INFO: val_f_mae: 0.007481
val_f_rmse: 0.010317
2025-06-25 08:02:33.281 INFO: val_f_rmse: 0.010317
##### Step: 149 Learning rate: 7.8125e-05 #####
2025-06-25 08:03:17.375 INFO: ##### Step: 149 Learning rate: 7.8125e-05 #####
Epoch 10, Train Loss: 0.1059, Val Loss: 0.1081
2025-06-25 08:03:17.375 INFO: Epoch 10, Train Loss: 0.1059, Val Loss: 0.1081
train_e/atom_mae: 0.000109
2025-06-25 08:03:17.376 INFO: train_e/atom_mae: 0.000109
train_e/atom_rmse: 0.000140
2025-06-25 08:03:17.376 INFO: train_e/atom_rmse: 0.000140
train_f_mae: 0.007374
2025-06-25 08:03:17.379 INFO: train_f_mae: 0.007374
train_f_rmse: 0.010174
2025-06-25 08:03:17.379 INFO: train_f_rmse: 0.010174
val_e/atom_mae: 0.000110
2025-06-25 08:03:17.382 INFO: val_e/atom_mae: 0.000110
val_e/atom_rmse: 0.000134
2025-06-25 08:03:17.382 INFO: val_e/atom_rmse: 0.000134
val_f_mae: 0.007453
2025-06-25 08:03:17.383 INFO: val_f_mae: 0.007453
val_f_rmse: 0.010292
2025-06-25 08:03:17.383 INFO: val_f_rmse: 0.010292
##### Step: 150 Learning rate: 7.8125e-05 #####
2025-06-25 08:04:01.463 INFO: ##### Step: 150 Learning rate: 7.8125e-05 #####
Epoch 11, Train Loss: 0.1067, Val Loss: 0.1128
2025-06-25 08:04:01.463 INFO: Epoch 11, Train Loss: 0.1067, Val Loss: 0.1128
train_e/atom_mae: 0.000123
2025-06-25 08:04:01.464 INFO: train_e/atom_mae: 0.000123
train_e/atom_rmse: 0.000162
2025-06-25 08:04:01.464 INFO: train_e/atom_rmse: 0.000162
train_f_mae: 0.007371
2025-06-25 08:04:01.467 INFO: train_f_mae: 0.007371
train_f_rmse: 0.010177
2025-06-25 08:04:01.467 INFO: train_f_rmse: 0.010177
val_e/atom_mae: 0.000188
2025-06-25 08:04:01.470 INFO: val_e/atom_mae: 0.000188
val_e/atom_rmse: 0.000211
2025-06-25 08:04:01.470 INFO: val_e/atom_rmse: 0.000211
val_f_mae: 0.007496
2025-06-25 08:04:01.471 INFO: val_f_mae: 0.007496
val_f_rmse: 0.010365
2025-06-25 08:04:01.471 INFO: val_f_rmse: 0.010365
##### Step: 151 Learning rate: 7.8125e-05 #####
2025-06-25 08:04:45.573 INFO: ##### Step: 151 Learning rate: 7.8125e-05 #####
Epoch 12, Train Loss: 0.1058, Val Loss: 0.1089
2025-06-25 08:04:45.573 INFO: Epoch 12, Train Loss: 0.1058, Val Loss: 0.1089
train_e/atom_mae: 0.000113
2025-06-25 08:04:45.574 INFO: train_e/atom_mae: 0.000113
train_e/atom_rmse: 0.000144
2025-06-25 08:04:45.574 INFO: train_e/atom_rmse: 0.000144
train_f_mae: 0.007371
2025-06-25 08:04:45.578 INFO: train_f_mae: 0.007371
train_f_rmse: 0.010166
2025-06-25 08:04:45.578 INFO: train_f_rmse: 0.010166
val_e/atom_mae: 0.000122
2025-06-25 08:04:45.580 INFO: val_e/atom_mae: 0.000122
val_e/atom_rmse: 0.000145
2025-06-25 08:04:45.581 INFO: val_e/atom_rmse: 0.000145
val_f_mae: 0.007472
2025-06-25 08:04:45.581 INFO: val_f_mae: 0.007472
val_f_rmse: 0.010311
2025-06-25 08:04:45.581 INFO: val_f_rmse: 0.010311
##### Step: 152 Learning rate: 7.8125e-05 #####
2025-06-25 08:05:29.645 INFO: ##### Step: 152 Learning rate: 7.8125e-05 #####
Epoch 13, Train Loss: 0.1099, Val Loss: 0.1097
2025-06-25 08:05:29.646 INFO: Epoch 13, Train Loss: 0.1099, Val Loss: 0.1097
train_e/atom_mae: 0.000181
2025-06-25 08:05:29.647 INFO: train_e/atom_mae: 0.000181
train_e/atom_rmse: 0.000226
2025-06-25 08:05:29.647 INFO: train_e/atom_rmse: 0.000226
train_f_mae: 0.007380
2025-06-25 08:05:29.650 INFO: train_f_mae: 0.007380
train_f_rmse: 0.010181
2025-06-25 08:05:29.650 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000143
2025-06-25 08:05:29.653 INFO: val_e/atom_mae: 0.000143
val_e/atom_rmse: 0.000166
2025-06-25 08:05:29.653 INFO: val_e/atom_rmse: 0.000166
val_f_mae: 0.007467
2025-06-25 08:05:29.654 INFO: val_f_mae: 0.007467
val_f_rmse: 0.010315
2025-06-25 08:05:29.654 INFO: val_f_rmse: 0.010315
##### Step: 153 Learning rate: 7.8125e-05 #####
2025-06-25 08:06:13.754 INFO: ##### Step: 153 Learning rate: 7.8125e-05 #####
Epoch 14, Train Loss: 0.1061, Val Loss: 0.1074
2025-06-25 08:06:13.754 INFO: Epoch 14, Train Loss: 0.1061, Val Loss: 0.1074
train_e/atom_mae: 0.000123
2025-06-25 08:06:13.755 INFO: train_e/atom_mae: 0.000123
train_e/atom_rmse: 0.000157
2025-06-25 08:06:13.755 INFO: train_e/atom_rmse: 0.000157
train_f_mae: 0.007362
2025-06-25 08:06:13.759 INFO: train_f_mae: 0.007362
train_f_rmse: 0.010153
2025-06-25 08:06:13.759 INFO: train_f_rmse: 0.010153
val_e/atom_mae: 0.000076
2025-06-25 08:06:13.761 INFO: val_e/atom_mae: 0.000076
val_e/atom_rmse: 0.000099
2025-06-25 08:06:13.762 INFO: val_e/atom_rmse: 0.000099
val_f_mae: 0.007469
2025-06-25 08:06:13.762 INFO: val_f_mae: 0.007469
val_f_rmse: 0.010303
2025-06-25 08:06:13.762 INFO: val_f_rmse: 0.010303
##### Step: 154 Learning rate: 7.8125e-05 #####
2025-06-25 08:06:57.837 INFO: ##### Step: 154 Learning rate: 7.8125e-05 #####
Epoch 15, Train Loss: 0.1058, Val Loss: 0.1083
2025-06-25 08:06:57.837 INFO: Epoch 15, Train Loss: 0.1058, Val Loss: 0.1083
train_e/atom_mae: 0.000112
2025-06-25 08:06:57.838 INFO: train_e/atom_mae: 0.000112
train_e/atom_rmse: 0.000144
2025-06-25 08:06:57.838 INFO: train_e/atom_rmse: 0.000144
train_f_mae: 0.007370
2025-06-25 08:06:57.842 INFO: train_f_mae: 0.007370
train_f_rmse: 0.010163
2025-06-25 08:06:57.842 INFO: train_f_rmse: 0.010163
val_e/atom_mae: 0.000114
2025-06-25 08:06:57.845 INFO: val_e/atom_mae: 0.000114
val_e/atom_rmse: 0.000137
2025-06-25 08:06:57.845 INFO: val_e/atom_rmse: 0.000137
val_f_mae: 0.007468
2025-06-25 08:06:57.845 INFO: val_f_mae: 0.007468
val_f_rmse: 0.010298
2025-06-25 08:06:57.845 INFO: val_f_rmse: 0.010298
##### Step: 155 Learning rate: 7.8125e-05 #####
2025-06-25 08:07:41.895 INFO: ##### Step: 155 Learning rate: 7.8125e-05 #####
Epoch 16, Train Loss: 0.1065, Val Loss: 0.1133
2025-06-25 08:07:41.895 INFO: Epoch 16, Train Loss: 0.1065, Val Loss: 0.1133
train_e/atom_mae: 0.000120
2025-06-25 08:07:41.896 INFO: train_e/atom_mae: 0.000120
train_e/atom_rmse: 0.000154
2025-06-25 08:07:41.896 INFO: train_e/atom_rmse: 0.000154
train_f_mae: 0.007378
2025-06-25 08:07:41.900 INFO: train_f_mae: 0.007378
train_f_rmse: 0.010182
2025-06-25 08:07:41.900 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000185
2025-06-25 08:07:41.902 INFO: val_e/atom_mae: 0.000185
val_e/atom_rmse: 0.000205
2025-06-25 08:07:41.903 INFO: val_e/atom_rmse: 0.000205
val_f_mae: 0.007520
2025-06-25 08:07:41.903 INFO: val_f_mae: 0.007520
val_f_rmse: 0.010402
2025-06-25 08:07:41.903 INFO: val_f_rmse: 0.010402
##### Step: 156 Learning rate: 7.8125e-05 #####
2025-06-25 08:08:25.979 INFO: ##### Step: 156 Learning rate: 7.8125e-05 #####
Epoch 17, Train Loss: 0.1071, Val Loss: 0.1084
2025-06-25 08:08:25.979 INFO: Epoch 17, Train Loss: 0.1071, Val Loss: 0.1084
train_e/atom_mae: 0.000139
2025-06-25 08:08:25.980 INFO: train_e/atom_mae: 0.000139
train_e/atom_rmse: 0.000174
2025-06-25 08:08:25.980 INFO: train_e/atom_rmse: 0.000174
train_f_mae: 0.007372
2025-06-25 08:08:25.984 INFO: train_f_mae: 0.007372
train_f_rmse: 0.010172
2025-06-25 08:08:25.984 INFO: train_f_rmse: 0.010172
val_e/atom_mae: 0.000113
2025-06-25 08:08:25.986 INFO: val_e/atom_mae: 0.000113
val_e/atom_rmse: 0.000137
2025-06-25 08:08:25.987 INFO: val_e/atom_rmse: 0.000137
val_f_mae: 0.007455
2025-06-25 08:08:25.987 INFO: val_f_mae: 0.007455
val_f_rmse: 0.010302
2025-06-25 08:08:25.987 INFO: val_f_rmse: 0.010302
##### Step: 157 Learning rate: 7.8125e-05 #####
2025-06-25 08:09:10.059 INFO: ##### Step: 157 Learning rate: 7.8125e-05 #####
Epoch 18, Train Loss: 0.1061, Val Loss: 0.1083
2025-06-25 08:09:10.059 INFO: Epoch 18, Train Loss: 0.1061, Val Loss: 0.1083
train_e/atom_mae: 0.000126
2025-06-25 08:09:10.060 INFO: train_e/atom_mae: 0.000126
train_e/atom_rmse: 0.000157
2025-06-25 08:09:10.060 INFO: train_e/atom_rmse: 0.000157
train_f_mae: 0.007362
2025-06-25 08:09:10.064 INFO: train_f_mae: 0.007362
train_f_rmse: 0.010153
2025-06-25 08:09:10.064 INFO: train_f_rmse: 0.010153
val_e/atom_mae: 0.000099
2025-06-25 08:09:10.066 INFO: val_e/atom_mae: 0.000099
val_e/atom_rmse: 0.000131
2025-06-25 08:09:10.067 INFO: val_e/atom_rmse: 0.000131
val_f_mae: 0.007457
2025-06-25 08:09:10.067 INFO: val_f_mae: 0.007457
val_f_rmse: 0.010305
2025-06-25 08:09:10.067 INFO: val_f_rmse: 0.010305
##### Step: 158 Learning rate: 7.8125e-05 #####
2025-06-25 08:09:54.153 INFO: ##### Step: 158 Learning rate: 7.8125e-05 #####
Epoch 19, Train Loss: 0.1065, Val Loss: 0.1143
2025-06-25 08:09:54.153 INFO: Epoch 19, Train Loss: 0.1065, Val Loss: 0.1143
train_e/atom_mae: 0.000128
2025-06-25 08:09:54.154 INFO: train_e/atom_mae: 0.000128
train_e/atom_rmse: 0.000161
2025-06-25 08:09:54.154 INFO: train_e/atom_rmse: 0.000161
train_f_mae: 0.007370
2025-06-25 08:09:54.157 INFO: train_f_mae: 0.007370
train_f_rmse: 0.010164
2025-06-25 08:09:54.158 INFO: train_f_rmse: 0.010164
val_e/atom_mae: 0.000245
2025-06-25 08:09:54.160 INFO: val_e/atom_mae: 0.000245
val_e/atom_rmse: 0.000265
2025-06-25 08:09:54.160 INFO: val_e/atom_rmse: 0.000265
val_f_mae: 0.007454
2025-06-25 08:09:54.161 INFO: val_f_mae: 0.007454
val_f_rmse: 0.010288
2025-06-25 08:09:54.161 INFO: val_f_rmse: 0.010288
##### Step: 159 Learning rate: 7.8125e-05 #####
2025-06-25 08:10:38.251 INFO: ##### Step: 159 Learning rate: 7.8125e-05 #####
Epoch 20, Train Loss: 0.1056, Val Loss: 0.1082
2025-06-25 08:10:38.251 INFO: Epoch 20, Train Loss: 0.1056, Val Loss: 0.1082
train_e/atom_mae: 0.000110
2025-06-25 08:10:38.252 INFO: train_e/atom_mae: 0.000110
train_e/atom_rmse: 0.000142
2025-06-25 08:10:38.252 INFO: train_e/atom_rmse: 0.000142
train_f_mae: 0.007368
2025-06-25 08:10:38.255 INFO: train_f_mae: 0.007368
train_f_rmse: 0.010158
2025-06-25 08:10:38.255 INFO: train_f_rmse: 0.010158
val_e/atom_mae: 0.000121
2025-06-25 08:10:38.258 INFO: val_e/atom_mae: 0.000121
val_e/atom_rmse: 0.000145
2025-06-25 08:10:38.258 INFO: val_e/atom_rmse: 0.000145
val_f_mae: 0.007452
2025-06-25 08:10:38.259 INFO: val_f_mae: 0.007452
val_f_rmse: 0.010280
2025-06-25 08:10:38.259 INFO: val_f_rmse: 0.010280
##### Step: 160 Learning rate: 3.90625e-05 #####
2025-06-25 08:11:22.350 INFO: ##### Step: 160 Learning rate: 3.90625e-05 #####
Epoch 21, Train Loss: 0.1046, Val Loss: 0.1066
2025-06-25 08:11:22.350 INFO: Epoch 21, Train Loss: 0.1046, Val Loss: 0.1066
train_e/atom_mae: 0.000098
2025-06-25 08:11:22.351 INFO: train_e/atom_mae: 0.000098
train_e/atom_rmse: 0.000126
2025-06-25 08:11:22.351 INFO: train_e/atom_rmse: 0.000126
train_f_mae: 0.007350
2025-06-25 08:11:22.355 INFO: train_f_mae: 0.007350
train_f_rmse: 0.010131
2025-06-25 08:11:22.355 INFO: train_f_rmse: 0.010131
val_e/atom_mae: 0.000063
2025-06-25 08:11:22.357 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000090
2025-06-25 08:11:22.358 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.007444
2025-06-25 08:11:22.358 INFO: val_f_mae: 0.007444
val_f_rmse: 0.010276
2025-06-25 08:11:22.358 INFO: val_f_rmse: 0.010276
##### Step: 161 Learning rate: 3.90625e-05 #####
2025-06-25 08:12:06.441 INFO: ##### Step: 161 Learning rate: 3.90625e-05 #####
Epoch 22, Train Loss: 0.1042, Val Loss: 0.1072
2025-06-25 08:12:06.441 INFO: Epoch 22, Train Loss: 0.1042, Val Loss: 0.1072
train_e/atom_mae: 0.000086
2025-06-25 08:12:06.442 INFO: train_e/atom_mae: 0.000086
train_e/atom_rmse: 0.000115
2025-06-25 08:12:06.442 INFO: train_e/atom_rmse: 0.000115
train_f_mae: 0.007349
2025-06-25 08:12:06.445 INFO: train_f_mae: 0.007349
train_f_rmse: 0.010132
2025-06-25 08:12:06.445 INFO: train_f_rmse: 0.010132
val_e/atom_mae: 0.000069
2025-06-25 08:12:06.448 INFO: val_e/atom_mae: 0.000069
val_e/atom_rmse: 0.000092
2025-06-25 08:12:06.448 INFO: val_e/atom_rmse: 0.000092
val_f_mae: 0.007462
2025-06-25 08:12:06.449 INFO: val_f_mae: 0.007462
val_f_rmse: 0.010305
2025-06-25 08:12:06.449 INFO: val_f_rmse: 0.010305
##### Step: 162 Learning rate: 3.90625e-05 #####
2025-06-25 08:12:50.529 INFO: ##### Step: 162 Learning rate: 3.90625e-05 #####
Epoch 23, Train Loss: 0.1041, Val Loss: 0.1079
2025-06-25 08:12:50.529 INFO: Epoch 23, Train Loss: 0.1041, Val Loss: 0.1079
train_e/atom_mae: 0.000087
2025-06-25 08:12:50.530 INFO: train_e/atom_mae: 0.000087
train_e/atom_rmse: 0.000114
2025-06-25 08:12:50.531 INFO: train_e/atom_rmse: 0.000114
train_f_mae: 0.007347
2025-06-25 08:12:50.534 INFO: train_f_mae: 0.007347
train_f_rmse: 0.010125
2025-06-25 08:12:50.534 INFO: train_f_rmse: 0.010125
val_e/atom_mae: 0.000122
2025-06-25 08:12:50.537 INFO: val_e/atom_mae: 0.000122
val_e/atom_rmse: 0.000145
2025-06-25 08:12:50.537 INFO: val_e/atom_rmse: 0.000145
val_f_mae: 0.007439
2025-06-25 08:12:50.537 INFO: val_f_mae: 0.007439
val_f_rmse: 0.010262
2025-06-25 08:12:50.537 INFO: val_f_rmse: 0.010262
##### Step: 163 Learning rate: 3.90625e-05 #####
2025-06-25 08:13:34.621 INFO: ##### Step: 163 Learning rate: 3.90625e-05 #####
Epoch 24, Train Loss: 0.1042, Val Loss: 0.1070
2025-06-25 08:13:34.622 INFO: Epoch 24, Train Loss: 0.1042, Val Loss: 0.1070
train_e/atom_mae: 0.000084
2025-06-25 08:13:34.622 INFO: train_e/atom_mae: 0.000084
train_e/atom_rmse: 0.000110
2025-06-25 08:13:34.623 INFO: train_e/atom_rmse: 0.000110
train_f_mae: 0.007351
2025-06-25 08:13:34.626 INFO: train_f_mae: 0.007351
train_f_rmse: 0.010135
2025-06-25 08:13:34.626 INFO: train_f_rmse: 0.010135
val_e/atom_mae: 0.000073
2025-06-25 08:13:34.629 INFO: val_e/atom_mae: 0.000073
val_e/atom_rmse: 0.000099
2025-06-25 08:13:34.629 INFO: val_e/atom_rmse: 0.000099
val_f_mae: 0.007446
2025-06-25 08:13:34.629 INFO: val_f_mae: 0.007446
val_f_rmse: 0.010287
2025-06-25 08:13:34.630 INFO: val_f_rmse: 0.010287
##### Step: 164 Learning rate: 3.90625e-05 #####
2025-06-25 08:14:18.705 INFO: ##### Step: 164 Learning rate: 3.90625e-05 #####
Epoch 25, Train Loss: 0.1042, Val Loss: 0.1079
2025-06-25 08:14:18.705 INFO: Epoch 25, Train Loss: 0.1042, Val Loss: 0.1079
train_e/atom_mae: 0.000091
2025-06-25 08:14:18.706 INFO: train_e/atom_mae: 0.000091
train_e/atom_rmse: 0.000119
2025-06-25 08:14:18.706 INFO: train_e/atom_rmse: 0.000119
train_f_mae: 0.007343
2025-06-25 08:14:18.709 INFO: train_f_mae: 0.007343
train_f_rmse: 0.010124
2025-06-25 08:14:18.709 INFO: train_f_rmse: 0.010124
val_e/atom_mae: 0.000119
2025-06-25 08:14:18.712 INFO: val_e/atom_mae: 0.000119
val_e/atom_rmse: 0.000138
2025-06-25 08:14:18.712 INFO: val_e/atom_rmse: 0.000138
val_f_mae: 0.007444
2025-06-25 08:14:18.713 INFO: val_f_mae: 0.007444
val_f_rmse: 0.010274
2025-06-25 08:14:18.713 INFO: val_f_rmse: 0.010274
##### Step: 165 Learning rate: 3.90625e-05 #####
2025-06-25 08:15:02.805 INFO: ##### Step: 165 Learning rate: 3.90625e-05 #####
Epoch 26, Train Loss: 0.1042, Val Loss: 0.1074
2025-06-25 08:15:02.805 INFO: Epoch 26, Train Loss: 0.1042, Val Loss: 0.1074
train_e/atom_mae: 0.000097
2025-06-25 08:15:02.806 INFO: train_e/atom_mae: 0.000097
train_e/atom_rmse: 0.000125
2025-06-25 08:15:02.806 INFO: train_e/atom_rmse: 0.000125
train_f_mae: 0.007341
2025-06-25 08:15:02.810 INFO: train_f_mae: 0.007341
train_f_rmse: 0.010115
2025-06-25 08:15:02.810 INFO: train_f_rmse: 0.010115
val_e/atom_mae: 0.000096
2025-06-25 08:15:02.812 INFO: val_e/atom_mae: 0.000096
val_e/atom_rmse: 0.000117
2025-06-25 08:15:02.813 INFO: val_e/atom_rmse: 0.000117
val_f_mae: 0.007450
2025-06-25 08:15:02.813 INFO: val_f_mae: 0.007450
val_f_rmse: 0.010283
2025-06-25 08:15:02.813 INFO: val_f_rmse: 0.010283
##### Step: 166 Learning rate: 3.90625e-05 #####
2025-06-25 08:15:46.907 INFO: ##### Step: 166 Learning rate: 3.90625e-05 #####
Epoch 27, Train Loss: 0.1040, Val Loss: 0.1068
2025-06-25 08:15:46.907 INFO: Epoch 27, Train Loss: 0.1040, Val Loss: 0.1068
train_e/atom_mae: 0.000091
2025-06-25 08:15:46.908 INFO: train_e/atom_mae: 0.000091
train_e/atom_rmse: 0.000117
2025-06-25 08:15:46.908 INFO: train_e/atom_rmse: 0.000117
train_f_mae: 0.007340
2025-06-25 08:15:46.912 INFO: train_f_mae: 0.007340
train_f_rmse: 0.010117
2025-06-25 08:15:46.912 INFO: train_f_rmse: 0.010117
val_e/atom_mae: 0.000061
2025-06-25 08:15:46.915 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000088
2025-06-25 08:15:46.915 INFO: val_e/atom_rmse: 0.000088
val_f_mae: 0.007457
2025-06-25 08:15:46.915 INFO: val_f_mae: 0.007457
val_f_rmse: 0.010290
2025-06-25 08:15:46.915 INFO: val_f_rmse: 0.010290
##### Step: 167 Learning rate: 3.90625e-05 #####
2025-06-25 08:16:31.049 INFO: ##### Step: 167 Learning rate: 3.90625e-05 #####
Epoch 28, Train Loss: 0.1042, Val Loss: 0.1071
2025-06-25 08:16:31.050 INFO: Epoch 28, Train Loss: 0.1042, Val Loss: 0.1071
train_e/atom_mae: 0.000088
2025-06-25 08:16:31.050 INFO: train_e/atom_mae: 0.000088
train_e/atom_rmse: 0.000115
2025-06-25 08:16:31.051 INFO: train_e/atom_rmse: 0.000115
train_f_mae: 0.007347
2025-06-25 08:16:31.054 INFO: train_f_mae: 0.007347
train_f_rmse: 0.010129
2025-06-25 08:16:31.054 INFO: train_f_rmse: 0.010129
val_e/atom_mae: 0.000067
2025-06-25 08:16:31.057 INFO: val_e/atom_mae: 0.000067
val_e/atom_rmse: 0.000093
2025-06-25 08:16:31.057 INFO: val_e/atom_rmse: 0.000093
val_f_mae: 0.007458
2025-06-25 08:16:31.057 INFO: val_f_mae: 0.007458
val_f_rmse: 0.010296
2025-06-25 08:16:31.058 INFO: val_f_rmse: 0.010296
##### Step: 168 Learning rate: 3.90625e-05 #####
2025-06-25 08:17:15.209 INFO: ##### Step: 168 Learning rate: 3.90625e-05 #####
Epoch 29, Train Loss: 0.1041, Val Loss: 0.1069
2025-06-25 08:17:15.209 INFO: Epoch 29, Train Loss: 0.1041, Val Loss: 0.1069
train_e/atom_mae: 0.000087
2025-06-25 08:17:15.210 INFO: train_e/atom_mae: 0.000087
train_e/atom_rmse: 0.000114
2025-06-25 08:17:15.210 INFO: train_e/atom_rmse: 0.000114
train_f_mae: 0.007343
2025-06-25 08:17:15.214 INFO: train_f_mae: 0.007343
train_f_rmse: 0.010125
2025-06-25 08:17:15.214 INFO: train_f_rmse: 0.010125
val_e/atom_mae: 0.000094
2025-06-25 08:17:15.217 INFO: val_e/atom_mae: 0.000094
val_e/atom_rmse: 0.000121
2025-06-25 08:17:15.217 INFO: val_e/atom_rmse: 0.000121
val_f_mae: 0.007433
2025-06-25 08:17:15.217 INFO: val_f_mae: 0.007433
val_f_rmse: 0.010254
2025-06-25 08:17:15.217 INFO: val_f_rmse: 0.010254
##### Step: 169 Learning rate: 3.90625e-05 #####
2025-06-25 08:17:59.387 INFO: ##### Step: 169 Learning rate: 3.90625e-05 #####
Epoch 30, Train Loss: 0.1044, Val Loss: 0.1068
2025-06-25 08:17:59.387 INFO: Epoch 30, Train Loss: 0.1044, Val Loss: 0.1068
train_e/atom_mae: 0.000101
2025-06-25 08:17:59.388 INFO: train_e/atom_mae: 0.000101
train_e/atom_rmse: 0.000131
2025-06-25 08:17:59.388 INFO: train_e/atom_rmse: 0.000131
train_f_mae: 0.007337
2025-06-25 08:17:59.392 INFO: train_f_mae: 0.007337
train_f_rmse: 0.010117
2025-06-25 08:17:59.392 INFO: train_f_rmse: 0.010117
val_e/atom_mae: 0.000084
2025-06-25 08:17:59.394 INFO: val_e/atom_mae: 0.000084
val_e/atom_rmse: 0.000113
2025-06-25 08:17:59.395 INFO: val_e/atom_rmse: 0.000113
val_f_mae: 0.007440
2025-06-25 08:17:59.395 INFO: val_f_mae: 0.007440
val_f_rmse: 0.010261
2025-06-25 08:17:59.395 INFO: val_f_rmse: 0.010261
##### Step: 170 Learning rate: 3.90625e-05 #####
2025-06-25 08:18:43.564 INFO: ##### Step: 170 Learning rate: 3.90625e-05 #####
Epoch 31, Train Loss: 0.1045, Val Loss: 0.1072
2025-06-25 08:18:43.565 INFO: Epoch 31, Train Loss: 0.1045, Val Loss: 0.1072
train_e/atom_mae: 0.000105
2025-06-25 08:18:43.565 INFO: train_e/atom_mae: 0.000105
train_e/atom_rmse: 0.000135
2025-06-25 08:18:43.566 INFO: train_e/atom_rmse: 0.000135
train_f_mae: 0.007337
2025-06-25 08:18:43.569 INFO: train_f_mae: 0.007337
train_f_rmse: 0.010116
2025-06-25 08:18:43.569 INFO: train_f_rmse: 0.010116
val_e/atom_mae: 0.000103
2025-06-25 08:18:43.572 INFO: val_e/atom_mae: 0.000103
val_e/atom_rmse: 0.000123
2025-06-25 08:18:43.572 INFO: val_e/atom_rmse: 0.000123
val_f_mae: 0.007438
2025-06-25 08:18:43.572 INFO: val_f_mae: 0.007438
val_f_rmse: 0.010267
2025-06-25 08:18:43.573 INFO: val_f_rmse: 0.010267
##### Step: 171 Learning rate: 3.90625e-05 #####
2025-06-25 08:19:27.711 INFO: ##### Step: 171 Learning rate: 3.90625e-05 #####
Epoch 32, Train Loss: 0.1048, Val Loss: 0.1070
2025-06-25 08:19:27.711 INFO: Epoch 32, Train Loss: 0.1048, Val Loss: 0.1070
train_e/atom_mae: 0.000110
2025-06-25 08:19:27.712 INFO: train_e/atom_mae: 0.000110
train_e/atom_rmse: 0.000142
2025-06-25 08:19:27.712 INFO: train_e/atom_rmse: 0.000142
train_f_mae: 0.007338
2025-06-25 08:19:27.715 INFO: train_f_mae: 0.007338
train_f_rmse: 0.010115
2025-06-25 08:19:27.716 INFO: train_f_rmse: 0.010115
val_e/atom_mae: 0.000084
2025-06-25 08:19:27.718 INFO: val_e/atom_mae: 0.000084
val_e/atom_rmse: 0.000112
2025-06-25 08:19:27.718 INFO: val_e/atom_rmse: 0.000112
val_f_mae: 0.007444
2025-06-25 08:19:27.719 INFO: val_f_mae: 0.007444
val_f_rmse: 0.010272
2025-06-25 08:19:27.719 INFO: val_f_rmse: 0.010272
##### Step: 172 Learning rate: 3.90625e-05 #####
2025-06-25 08:20:11.898 INFO: ##### Step: 172 Learning rate: 3.90625e-05 #####
Epoch 33, Train Loss: 0.1039, Val Loss: 0.1069
2025-06-25 08:20:11.898 INFO: Epoch 33, Train Loss: 0.1039, Val Loss: 0.1069
train_e/atom_mae: 0.000085
2025-06-25 08:20:11.899 INFO: train_e/atom_mae: 0.000085
train_e/atom_rmse: 0.000111
2025-06-25 08:20:11.899 INFO: train_e/atom_rmse: 0.000111
train_f_mae: 0.007342
2025-06-25 08:20:11.903 INFO: train_f_mae: 0.007342
train_f_rmse: 0.010122
2025-06-25 08:20:11.903 INFO: train_f_rmse: 0.010122
val_e/atom_mae: 0.000094
2025-06-25 08:20:11.905 INFO: val_e/atom_mae: 0.000094
val_e/atom_rmse: 0.000120
2025-06-25 08:20:11.906 INFO: val_e/atom_rmse: 0.000120
val_f_mae: 0.007437
2025-06-25 08:20:11.906 INFO: val_f_mae: 0.007437
val_f_rmse: 0.010256
2025-06-25 08:20:11.906 INFO: val_f_rmse: 0.010256
##### Step: 173 Learning rate: 3.90625e-05 #####
2025-06-25 08:20:56.051 INFO: ##### Step: 173 Learning rate: 3.90625e-05 #####
Epoch 34, Train Loss: 0.1040, Val Loss: 0.1064
2025-06-25 08:20:56.051 INFO: Epoch 34, Train Loss: 0.1040, Val Loss: 0.1064
train_e/atom_mae: 0.000086
2025-06-25 08:20:56.052 INFO: train_e/atom_mae: 0.000086
train_e/atom_rmse: 0.000114
2025-06-25 08:20:56.052 INFO: train_e/atom_rmse: 0.000114
train_f_mae: 0.007343
2025-06-25 08:20:56.056 INFO: train_f_mae: 0.007343
train_f_rmse: 0.010123
2025-06-25 08:20:56.056 INFO: train_f_rmse: 0.010123
val_e/atom_mae: 0.000077
2025-06-25 08:20:56.058 INFO: val_e/atom_mae: 0.000077
val_e/atom_rmse: 0.000102
2025-06-25 08:20:56.059 INFO: val_e/atom_rmse: 0.000102
val_f_mae: 0.007429
2025-06-25 08:20:56.059 INFO: val_f_mae: 0.007429
val_f_rmse: 0.010251
2025-06-25 08:20:56.059 INFO: val_f_rmse: 0.010251
##### Step: 174 Learning rate: 3.90625e-05 #####
2025-06-25 08:21:40.210 INFO: ##### Step: 174 Learning rate: 3.90625e-05 #####
Epoch 35, Train Loss: 0.1038, Val Loss: 0.1063
2025-06-25 08:21:40.211 INFO: Epoch 35, Train Loss: 0.1038, Val Loss: 0.1063
train_e/atom_mae: 0.000081
2025-06-25 08:21:40.212 INFO: train_e/atom_mae: 0.000081
train_e/atom_rmse: 0.000107
2025-06-25 08:21:40.212 INFO: train_e/atom_rmse: 0.000107
train_f_mae: 0.007341
2025-06-25 08:21:40.215 INFO: train_f_mae: 0.007341
train_f_rmse: 0.010121
2025-06-25 08:21:40.215 INFO: train_f_rmse: 0.010121
val_e/atom_mae: 0.000076
2025-06-25 08:21:40.218 INFO: val_e/atom_mae: 0.000076
val_e/atom_rmse: 0.000102
2025-06-25 08:21:40.218 INFO: val_e/atom_rmse: 0.000102
val_f_mae: 0.007432
2025-06-25 08:21:40.219 INFO: val_f_mae: 0.007432
val_f_rmse: 0.010249
2025-06-25 08:21:40.219 INFO: val_f_rmse: 0.010249
##### Step: 175 Learning rate: 3.90625e-05 #####
2025-06-25 08:22:24.331 INFO: ##### Step: 175 Learning rate: 3.90625e-05 #####
Epoch 36, Train Loss: 0.1035, Val Loss: 0.1074
2025-06-25 08:22:24.331 INFO: Epoch 36, Train Loss: 0.1035, Val Loss: 0.1074
train_e/atom_mae: 0.000077
2025-06-25 08:22:24.332 INFO: train_e/atom_mae: 0.000077
train_e/atom_rmse: 0.000103
2025-06-25 08:22:24.332 INFO: train_e/atom_rmse: 0.000103
train_f_mae: 0.007337
2025-06-25 08:22:24.336 INFO: train_f_mae: 0.007337
train_f_rmse: 0.010111
2025-06-25 08:22:24.336 INFO: train_f_rmse: 0.010111
val_e/atom_mae: 0.000088
2025-06-25 08:22:24.338 INFO: val_e/atom_mae: 0.000088
val_e/atom_rmse: 0.000117
2025-06-25 08:22:24.339 INFO: val_e/atom_rmse: 0.000117
val_f_mae: 0.007446
2025-06-25 08:22:24.339 INFO: val_f_mae: 0.007446
val_f_rmse: 0.010281
2025-06-25 08:22:24.339 INFO: val_f_rmse: 0.010281
##### Step: 176 Learning rate: 3.90625e-05 #####
2025-06-25 08:23:08.488 INFO: ##### Step: 176 Learning rate: 3.90625e-05 #####
Epoch 37, Train Loss: 0.1042, Val Loss: 0.1064
2025-06-25 08:23:08.489 INFO: Epoch 37, Train Loss: 0.1042, Val Loss: 0.1064
train_e/atom_mae: 0.000100
2025-06-25 08:23:08.489 INFO: train_e/atom_mae: 0.000100
train_e/atom_rmse: 0.000127
2025-06-25 08:23:08.490 INFO: train_e/atom_rmse: 0.000127
train_f_mae: 0.007336
2025-06-25 08:23:08.493 INFO: train_f_mae: 0.007336
train_f_rmse: 0.010109
2025-06-25 08:23:08.493 INFO: train_f_rmse: 0.010109
val_e/atom_mae: 0.000077
2025-06-25 08:23:08.496 INFO: val_e/atom_mae: 0.000077
val_e/atom_rmse: 0.000101
2025-06-25 08:23:08.496 INFO: val_e/atom_rmse: 0.000101
val_f_mae: 0.007430
2025-06-25 08:23:08.496 INFO: val_f_mae: 0.007430
val_f_rmse: 0.010254
2025-06-25 08:23:08.497 INFO: val_f_rmse: 0.010254
##### Step: 177 Learning rate: 3.90625e-05 #####
2025-06-25 08:23:52.635 INFO: ##### Step: 177 Learning rate: 3.90625e-05 #####
Epoch 38, Train Loss: 0.1040, Val Loss: 0.1066
2025-06-25 08:23:52.635 INFO: Epoch 38, Train Loss: 0.1040, Val Loss: 0.1066
train_e/atom_mae: 0.000095
2025-06-25 08:23:52.636 INFO: train_e/atom_mae: 0.000095
train_e/atom_rmse: 0.000122
2025-06-25 08:23:52.636 INFO: train_e/atom_rmse: 0.000122
train_f_mae: 0.007334
2025-06-25 08:23:52.640 INFO: train_f_mae: 0.007334
train_f_rmse: 0.010108
2025-06-25 08:23:52.640 INFO: train_f_rmse: 0.010108
val_e/atom_mae: 0.000077
2025-06-25 08:23:52.642 INFO: val_e/atom_mae: 0.000077
val_e/atom_rmse: 0.000107
2025-06-25 08:23:52.643 INFO: val_e/atom_rmse: 0.000107
val_f_mae: 0.007436
2025-06-25 08:23:52.643 INFO: val_f_mae: 0.007436
val_f_rmse: 0.010257
2025-06-25 08:23:52.643 INFO: val_f_rmse: 0.010257
##### Step: 178 Learning rate: 3.90625e-05 #####
2025-06-25 08:24:36.798 INFO: ##### Step: 178 Learning rate: 3.90625e-05 #####
Epoch 39, Train Loss: 0.1038, Val Loss: 0.1077
2025-06-25 08:24:36.798 INFO: Epoch 39, Train Loss: 0.1038, Val Loss: 0.1077
train_e/atom_mae: 0.000089
2025-06-25 08:24:36.799 INFO: train_e/atom_mae: 0.000089
train_e/atom_rmse: 0.000116
2025-06-25 08:24:36.799 INFO: train_e/atom_rmse: 0.000116
train_f_mae: 0.007333
2025-06-25 08:24:36.802 INFO: train_f_mae: 0.007333
train_f_rmse: 0.010110
2025-06-25 08:24:36.802 INFO: train_f_rmse: 0.010110
val_e/atom_mae: 0.000113
2025-06-25 08:24:36.805 INFO: val_e/atom_mae: 0.000113
val_e/atom_rmse: 0.000138
2025-06-25 08:24:36.805 INFO: val_e/atom_rmse: 0.000138
val_f_mae: 0.007445
2025-06-25 08:24:36.806 INFO: val_f_mae: 0.007445
val_f_rmse: 0.010266
2025-06-25 08:24:36.806 INFO: val_f_rmse: 0.010266
##### Step: 179 Learning rate: 3.90625e-05 #####
2025-06-25 08:25:20.937 INFO: ##### Step: 179 Learning rate: 3.90625e-05 #####
Epoch 40, Train Loss: 0.1043, Val Loss: 0.1068
2025-06-25 08:25:20.937 INFO: Epoch 40, Train Loss: 0.1043, Val Loss: 0.1068
train_e/atom_mae: 0.000099
2025-06-25 08:25:20.938 INFO: train_e/atom_mae: 0.000099
train_e/atom_rmse: 0.000128
2025-06-25 08:25:20.939 INFO: train_e/atom_rmse: 0.000128
train_f_mae: 0.007339
2025-06-25 08:25:20.942 INFO: train_f_mae: 0.007339
train_f_rmse: 0.010116
2025-06-25 08:25:20.942 INFO: train_f_rmse: 0.010116
val_e/atom_mae: 0.000100
2025-06-25 08:25:20.945 INFO: val_e/atom_mae: 0.000100
val_e/atom_rmse: 0.000121
2025-06-25 08:25:20.945 INFO: val_e/atom_rmse: 0.000121
val_f_mae: 0.007425
2025-06-25 08:25:20.945 INFO: val_f_mae: 0.007425
val_f_rmse: 0.010248
2025-06-25 08:25:20.946 INFO: val_f_rmse: 0.010248
##### Step: 180 Learning rate: 1.953125e-05 #####
2025-06-25 08:26:05.128 INFO: ##### Step: 180 Learning rate: 1.953125e-05 #####
Epoch 41, Train Loss: 0.1032, Val Loss: 0.1067
2025-06-25 08:26:05.128 INFO: Epoch 41, Train Loss: 0.1032, Val Loss: 0.1067
train_e/atom_mae: 0.000073
2025-06-25 08:26:05.129 INFO: train_e/atom_mae: 0.000073
train_e/atom_rmse: 0.000097
2025-06-25 08:26:05.129 INFO: train_e/atom_rmse: 0.000097
train_f_mae: 0.007331
2025-06-25 08:26:05.132 INFO: train_f_mae: 0.007331
train_f_rmse: 0.010103
2025-06-25 08:26:05.133 INFO: train_f_rmse: 0.010103
val_e/atom_mae: 0.000071
2025-06-25 08:26:05.135 INFO: val_e/atom_mae: 0.000071
val_e/atom_rmse: 0.000093
2025-06-25 08:26:05.135 INFO: val_e/atom_rmse: 0.000093
val_f_mae: 0.007446
2025-06-25 08:26:05.136 INFO: val_f_mae: 0.007446
val_f_rmse: 0.010280
2025-06-25 08:26:05.136 INFO: val_f_rmse: 0.010280
##### Step: 181 Learning rate: 1.953125e-05 #####
2025-06-25 08:26:49.218 INFO: ##### Step: 181 Learning rate: 1.953125e-05 #####
Epoch 42, Train Loss: 0.1033, Val Loss: 0.1060
2025-06-25 08:26:49.219 INFO: Epoch 42, Train Loss: 0.1033, Val Loss: 0.1060
train_e/atom_mae: 0.000077
2025-06-25 08:26:49.219 INFO: train_e/atom_mae: 0.000077
train_e/atom_rmse: 0.000103
2025-06-25 08:26:49.220 INFO: train_e/atom_rmse: 0.000103
train_f_mae: 0.007330
2025-06-25 08:26:49.223 INFO: train_f_mae: 0.007330
train_f_rmse: 0.010101
2025-06-25 08:26:49.223 INFO: train_f_rmse: 0.010101
val_e/atom_mae: 0.000065
2025-06-25 08:26:49.226 INFO: val_e/atom_mae: 0.000065
val_e/atom_rmse: 0.000090
2025-06-25 08:26:49.226 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.007431
2025-06-25 08:26:49.226 INFO: val_f_mae: 0.007431
val_f_rmse: 0.010247
2025-06-25 08:26:49.227 INFO: val_f_rmse: 0.010247
##### Step: 182 Learning rate: 1.953125e-05 #####
2025-06-25 08:27:33.343 INFO: ##### Step: 182 Learning rate: 1.953125e-05 #####
Epoch 43, Train Loss: 0.1033, Val Loss: 0.1065
2025-06-25 08:27:33.344 INFO: Epoch 43, Train Loss: 0.1033, Val Loss: 0.1065
train_e/atom_mae: 0.000078
2025-06-25 08:27:33.344 INFO: train_e/atom_mae: 0.000078
train_e/atom_rmse: 0.000104
2025-06-25 08:27:33.345 INFO: train_e/atom_rmse: 0.000104
train_f_mae: 0.007327
2025-06-25 08:27:33.348 INFO: train_f_mae: 0.007327
train_f_rmse: 0.010098
2025-06-25 08:27:33.348 INFO: train_f_rmse: 0.010098
val_e/atom_mae: 0.000062
2025-06-25 08:27:33.351 INFO: val_e/atom_mae: 0.000062
val_e/atom_rmse: 0.000092
2025-06-25 08:27:33.351 INFO: val_e/atom_rmse: 0.000092
val_f_mae: 0.007445
2025-06-25 08:27:33.352 INFO: val_f_mae: 0.007445
val_f_rmse: 0.010271
2025-06-25 08:27:33.352 INFO: val_f_rmse: 0.010271
##### Step: 183 Learning rate: 1.953125e-05 #####
2025-06-25 08:28:17.448 INFO: ##### Step: 183 Learning rate: 1.953125e-05 #####
Epoch 44, Train Loss: 0.1031, Val Loss: 0.1064
2025-06-25 08:28:17.449 INFO: Epoch 44, Train Loss: 0.1031, Val Loss: 0.1064
train_e/atom_mae: 0.000072
2025-06-25 08:28:17.449 INFO: train_e/atom_mae: 0.000072
train_e/atom_rmse: 0.000097
2025-06-25 08:28:17.450 INFO: train_e/atom_rmse: 0.000097
train_f_mae: 0.007326
2025-06-25 08:28:17.453 INFO: train_f_mae: 0.007326
train_f_rmse: 0.010095
2025-06-25 08:28:17.453 INFO: train_f_rmse: 0.010095
val_e/atom_mae: 0.000067
2025-06-25 08:28:17.456 INFO: val_e/atom_mae: 0.000067
val_e/atom_rmse: 0.000094
2025-06-25 08:28:17.456 INFO: val_e/atom_rmse: 0.000094
val_f_mae: 0.007436
2025-06-25 08:28:17.456 INFO: val_f_mae: 0.007436
val_f_rmse: 0.010262
2025-06-25 08:28:17.457 INFO: val_f_rmse: 0.010262
##### Step: 184 Learning rate: 1.953125e-05 #####
2025-06-25 08:29:01.545 INFO: ##### Step: 184 Learning rate: 1.953125e-05 #####
Epoch 45, Train Loss: 0.1033, Val Loss: 0.1064
2025-06-25 08:29:01.545 INFO: Epoch 45, Train Loss: 0.1033, Val Loss: 0.1064
train_e/atom_mae: 0.000081
2025-06-25 08:29:01.546 INFO: train_e/atom_mae: 0.000081
train_e/atom_rmse: 0.000106
2025-06-25 08:29:01.546 INFO: train_e/atom_rmse: 0.000106
train_f_mae: 0.007327
2025-06-25 08:29:01.549 INFO: train_f_mae: 0.007327
train_f_rmse: 0.010095
2025-06-25 08:29:01.550 INFO: train_f_rmse: 0.010095
val_e/atom_mae: 0.000081
2025-06-25 08:29:01.552 INFO: val_e/atom_mae: 0.000081
val_e/atom_rmse: 0.000104
2025-06-25 08:29:01.552 INFO: val_e/atom_rmse: 0.000104
val_f_mae: 0.007434
2025-06-25 08:29:01.553 INFO: val_f_mae: 0.007434
val_f_rmse: 0.010254
2025-06-25 08:29:01.553 INFO: val_f_rmse: 0.010254
##### Step: 185 Learning rate: 1.953125e-05 #####
2025-06-25 08:29:45.687 INFO: ##### Step: 185 Learning rate: 1.953125e-05 #####
Epoch 46, Train Loss: 0.1031, Val Loss: 0.1060
2025-06-25 08:29:45.688 INFO: Epoch 46, Train Loss: 0.1031, Val Loss: 0.1060
train_e/atom_mae: 0.000076
2025-06-25 08:29:45.689 INFO: train_e/atom_mae: 0.000076
train_e/atom_rmse: 0.000102
2025-06-25 08:29:45.689 INFO: train_e/atom_rmse: 0.000102
train_f_mae: 0.007326
2025-06-25 08:29:45.692 INFO: train_f_mae: 0.007326
train_f_rmse: 0.010093
2025-06-25 08:29:45.692 INFO: train_f_rmse: 0.010093
val_e/atom_mae: 0.000071
2025-06-25 08:29:45.695 INFO: val_e/atom_mae: 0.000071
val_e/atom_rmse: 0.000095
2025-06-25 08:29:45.695 INFO: val_e/atom_rmse: 0.000095
val_f_mae: 0.007427
2025-06-25 08:29:45.696 INFO: val_f_mae: 0.007427
val_f_rmse: 0.010242
2025-06-25 08:29:45.696 INFO: val_f_rmse: 0.010242
##### Step: 186 Learning rate: 1.953125e-05 #####
2025-06-25 08:30:29.762 INFO: ##### Step: 186 Learning rate: 1.953125e-05 #####
Epoch 47, Train Loss: 0.1032, Val Loss: 0.1058
2025-06-25 08:30:29.763 INFO: Epoch 47, Train Loss: 0.1032, Val Loss: 0.1058
train_e/atom_mae: 0.000075
2025-06-25 08:30:29.763 INFO: train_e/atom_mae: 0.000075
train_e/atom_rmse: 0.000102
2025-06-25 08:30:29.764 INFO: train_e/atom_rmse: 0.000102
train_f_mae: 0.007327
2025-06-25 08:30:29.767 INFO: train_f_mae: 0.007327
train_f_rmse: 0.010099
2025-06-25 08:30:29.767 INFO: train_f_rmse: 0.010099
val_e/atom_mae: 0.000059
2025-06-25 08:30:29.770 INFO: val_e/atom_mae: 0.000059
val_e/atom_rmse: 0.000083
2025-06-25 08:30:29.770 INFO: val_e/atom_rmse: 0.000083
val_f_mae: 0.007426
2025-06-25 08:30:29.770 INFO: val_f_mae: 0.007426
val_f_rmse: 0.010247
2025-06-25 08:30:29.771 INFO: val_f_rmse: 0.010247
##### Step: 187 Learning rate: 1.953125e-05 #####
2025-06-25 08:31:13.877 INFO: ##### Step: 187 Learning rate: 1.953125e-05 #####
Epoch 48, Train Loss: 0.1033, Val Loss: 0.1058
2025-06-25 08:31:13.878 INFO: Epoch 48, Train Loss: 0.1033, Val Loss: 0.1058
train_e/atom_mae: 0.000083
2025-06-25 08:31:13.879 INFO: train_e/atom_mae: 0.000083
train_e/atom_rmse: 0.000108
2025-06-25 08:31:13.879 INFO: train_e/atom_rmse: 0.000108
train_f_mae: 0.007326
2025-06-25 08:31:13.882 INFO: train_f_mae: 0.007326
train_f_rmse: 0.010096
2025-06-25 08:31:13.882 INFO: train_f_rmse: 0.010096
val_e/atom_mae: 0.000062
2025-06-25 08:31:13.885 INFO: val_e/atom_mae: 0.000062
val_e/atom_rmse: 0.000086
2025-06-25 08:31:13.885 INFO: val_e/atom_rmse: 0.000086
val_f_mae: 0.007424
2025-06-25 08:31:13.886 INFO: val_f_mae: 0.007424
val_f_rmse: 0.010241
2025-06-25 08:31:13.886 INFO: val_f_rmse: 0.010241
##### Step: 188 Learning rate: 1.953125e-05 #####
2025-06-25 08:31:58.004 INFO: ##### Step: 188 Learning rate: 1.953125e-05 #####
Epoch 49, Train Loss: 0.1031, Val Loss: 0.1062
2025-06-25 08:31:58.005 INFO: Epoch 49, Train Loss: 0.1031, Val Loss: 0.1062
train_e/atom_mae: 0.000071
2025-06-25 08:31:58.006 INFO: train_e/atom_mae: 0.000071
train_e/atom_rmse: 0.000097
2025-06-25 08:31:58.006 INFO: train_e/atom_rmse: 0.000097
train_f_mae: 0.007327
2025-06-25 08:31:58.009 INFO: train_f_mae: 0.007327
train_f_rmse: 0.010099
2025-06-25 08:31:58.009 INFO: train_f_rmse: 0.010099
val_e/atom_mae: 0.000057
2025-06-25 08:31:58.012 INFO: val_e/atom_mae: 0.000057
val_e/atom_rmse: 0.000083
2025-06-25 08:31:58.012 INFO: val_e/atom_rmse: 0.000083
val_f_mae: 0.007439
2025-06-25 08:31:58.012 INFO: val_f_mae: 0.007439
val_f_rmse: 0.010267
2025-06-25 08:31:58.013 INFO: val_f_rmse: 0.010267
##### Step: 189 Learning rate: 1.953125e-05 #####
2025-06-25 08:32:42.122 INFO: ##### Step: 189 Learning rate: 1.953125e-05 #####
Epoch 50, Train Loss: 0.1032, Val Loss: 0.1059
2025-06-25 08:32:42.122 INFO: Epoch 50, Train Loss: 0.1032, Val Loss: 0.1059
train_e/atom_mae: 0.000079
2025-06-25 08:32:42.123 INFO: train_e/atom_mae: 0.000079
train_e/atom_rmse: 0.000104
2025-06-25 08:32:42.123 INFO: train_e/atom_rmse: 0.000104
train_f_mae: 0.007325
2025-06-25 08:32:42.126 INFO: train_f_mae: 0.007325
train_f_rmse: 0.010093
2025-06-25 08:32:42.127 INFO: train_f_rmse: 0.010093
val_e/atom_mae: 0.000064
2025-06-25 08:32:42.129 INFO: val_e/atom_mae: 0.000064
val_e/atom_rmse: 0.000091
2025-06-25 08:32:42.129 INFO: val_e/atom_rmse: 0.000091
val_f_mae: 0.007428
2025-06-25 08:32:42.130 INFO: val_f_mae: 0.007428
val_f_rmse: 0.010243
2025-06-25 08:32:42.130 INFO: val_f_rmse: 0.010243
##### Step: 190 Learning rate: 1.953125e-05 #####
2025-06-25 08:33:26.249 INFO: ##### Step: 190 Learning rate: 1.953125e-05 #####
Epoch 51, Train Loss: 0.1032, Val Loss: 0.1061
2025-06-25 08:33:26.250 INFO: Epoch 51, Train Loss: 0.1032, Val Loss: 0.1061
train_e/atom_mae: 0.000077
2025-06-25 08:33:26.250 INFO: train_e/atom_mae: 0.000077
train_e/atom_rmse: 0.000103
2025-06-25 08:33:26.251 INFO: train_e/atom_rmse: 0.000103
train_f_mae: 0.007327
2025-06-25 08:33:26.254 INFO: train_f_mae: 0.007327
train_f_rmse: 0.010096
2025-06-25 08:33:26.254 INFO: train_f_rmse: 0.010096
val_e/atom_mae: 0.000081
2025-06-25 08:33:26.257 INFO: val_e/atom_mae: 0.000081
val_e/atom_rmse: 0.000104
2025-06-25 08:33:26.257 INFO: val_e/atom_rmse: 0.000104
val_f_mae: 0.007421
2025-06-25 08:33:26.257 INFO: val_f_mae: 0.007421
val_f_rmse: 0.010237
2025-06-25 08:33:26.258 INFO: val_f_rmse: 0.010237
##### Step: 191 Learning rate: 1.953125e-05 #####
2025-06-25 08:34:10.334 INFO: ##### Step: 191 Learning rate: 1.953125e-05 #####
Epoch 52, Train Loss: 0.1031, Val Loss: 0.1062
2025-06-25 08:34:10.334 INFO: Epoch 52, Train Loss: 0.1031, Val Loss: 0.1062
train_e/atom_mae: 0.000074
2025-06-25 08:34:10.335 INFO: train_e/atom_mae: 0.000074
train_e/atom_rmse: 0.000099
2025-06-25 08:34:10.335 INFO: train_e/atom_rmse: 0.000099
train_f_mae: 0.007328
2025-06-25 08:34:10.339 INFO: train_f_mae: 0.007328
train_f_rmse: 0.010094
2025-06-25 08:34:10.339 INFO: train_f_rmse: 0.010094
val_e/atom_mae: 0.000069
2025-06-25 08:34:10.341 INFO: val_e/atom_mae: 0.000069
val_e/atom_rmse: 0.000093
2025-06-25 08:34:10.342 INFO: val_e/atom_rmse: 0.000093
val_f_mae: 0.007432
2025-06-25 08:34:10.342 INFO: val_f_mae: 0.007432
val_f_rmse: 0.010254
2025-06-25 08:34:10.342 INFO: val_f_rmse: 0.010254
##### Step: 192 Learning rate: 1.953125e-05 #####
2025-06-25 08:34:54.407 INFO: ##### Step: 192 Learning rate: 1.953125e-05 #####
Epoch 53, Train Loss: 0.1030, Val Loss: 0.1058
2025-06-25 08:34:54.408 INFO: Epoch 53, Train Loss: 0.1030, Val Loss: 0.1058
train_e/atom_mae: 0.000070
2025-06-25 08:34:54.409 INFO: train_e/atom_mae: 0.000070
train_e/atom_rmse: 0.000095
2025-06-25 08:34:54.409 INFO: train_e/atom_rmse: 0.000095
train_f_mae: 0.007325
2025-06-25 08:34:54.412 INFO: train_f_mae: 0.007325
train_f_rmse: 0.010093
2025-06-25 08:34:54.412 INFO: train_f_rmse: 0.010093
val_e/atom_mae: 0.000060
2025-06-25 08:34:54.415 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000086
2025-06-25 08:34:54.415 INFO: val_e/atom_rmse: 0.000086
val_f_mae: 0.007426
2025-06-25 08:34:54.416 INFO: val_f_mae: 0.007426
val_f_rmse: 0.010241
2025-06-25 08:34:54.416 INFO: val_f_rmse: 0.010241
##### Step: 193 Learning rate: 1.953125e-05 #####
2025-06-25 08:35:38.509 INFO: ##### Step: 193 Learning rate: 1.953125e-05 #####
Epoch 54, Train Loss: 0.1030, Val Loss: 0.1062
2025-06-25 08:35:38.509 INFO: Epoch 54, Train Loss: 0.1030, Val Loss: 0.1062
train_e/atom_mae: 0.000070
2025-06-25 08:35:38.510 INFO: train_e/atom_mae: 0.000070
train_e/atom_rmse: 0.000095
2025-06-25 08:35:38.510 INFO: train_e/atom_rmse: 0.000095
train_f_mae: 0.007325
2025-06-25 08:35:38.514 INFO: train_f_mae: 0.007325
train_f_rmse: 0.010093
2025-06-25 08:35:38.514 INFO: train_f_rmse: 0.010093
val_e/atom_mae: 0.000068
2025-06-25 08:35:38.517 INFO: val_e/atom_mae: 0.000068
val_e/atom_rmse: 0.000093
2025-06-25 08:35:38.517 INFO: val_e/atom_rmse: 0.000093
val_f_mae: 0.007433
2025-06-25 08:35:38.517 INFO: val_f_mae: 0.007433
val_f_rmse: 0.010252
2025-06-25 08:35:38.517 INFO: val_f_rmse: 0.010252
##### Step: 194 Learning rate: 1.953125e-05 #####
2025-06-25 08:36:22.643 INFO: ##### Step: 194 Learning rate: 1.953125e-05 #####
Epoch 55, Train Loss: 0.1033, Val Loss: 0.1063
2025-06-25 08:36:22.643 INFO: Epoch 55, Train Loss: 0.1033, Val Loss: 0.1063
train_e/atom_mae: 0.000079
2025-06-25 08:36:22.644 INFO: train_e/atom_mae: 0.000079
train_e/atom_rmse: 0.000104
2025-06-25 08:36:22.644 INFO: train_e/atom_rmse: 0.000104
train_f_mae: 0.007329
2025-06-25 08:36:22.647 INFO: train_f_mae: 0.007329
train_f_rmse: 0.010100
2025-06-25 08:36:22.648 INFO: train_f_rmse: 0.010100
val_e/atom_mae: 0.000080
2025-06-25 08:36:22.650 INFO: val_e/atom_mae: 0.000080
val_e/atom_rmse: 0.000105
2025-06-25 08:36:22.650 INFO: val_e/atom_rmse: 0.000105
val_f_mae: 0.007430
2025-06-25 08:36:22.651 INFO: val_f_mae: 0.007430
val_f_rmse: 0.010247
2025-06-25 08:36:22.651 INFO: val_f_rmse: 0.010247
##### Step: 195 Learning rate: 1.953125e-05 #####
2025-06-25 08:37:06.763 INFO: ##### Step: 195 Learning rate: 1.953125e-05 #####
Epoch 56, Train Loss: 0.1031, Val Loss: 0.1063
2025-06-25 08:37:06.763 INFO: Epoch 56, Train Loss: 0.1031, Val Loss: 0.1063
train_e/atom_mae: 0.000076
2025-06-25 08:37:06.764 INFO: train_e/atom_mae: 0.000076
train_e/atom_rmse: 0.000100
2025-06-25 08:37:06.764 INFO: train_e/atom_rmse: 0.000100
train_f_mae: 0.007324
2025-06-25 08:37:06.767 INFO: train_f_mae: 0.007324
train_f_rmse: 0.010094
2025-06-25 08:37:06.767 INFO: train_f_rmse: 0.010094
val_e/atom_mae: 0.000087
2025-06-25 08:37:06.770 INFO: val_e/atom_mae: 0.000087
val_e/atom_rmse: 0.000111
2025-06-25 08:37:06.770 INFO: val_e/atom_rmse: 0.000111
val_f_mae: 0.007426
2025-06-25 08:37:06.771 INFO: val_f_mae: 0.007426
val_f_rmse: 0.010239
2025-06-25 08:37:06.771 INFO: val_f_rmse: 0.010239
##### Step: 196 Learning rate: 1.953125e-05 #####
2025-06-25 08:37:50.858 INFO: ##### Step: 196 Learning rate: 1.953125e-05 #####
Epoch 57, Train Loss: 0.1032, Val Loss: 0.1058
2025-06-25 08:37:50.858 INFO: Epoch 57, Train Loss: 0.1032, Val Loss: 0.1058
train_e/atom_mae: 0.000081
2025-06-25 08:37:50.859 INFO: train_e/atom_mae: 0.000081
train_e/atom_rmse: 0.000107
2025-06-25 08:37:50.859 INFO: train_e/atom_rmse: 0.000107
train_f_mae: 0.007326
2025-06-25 08:37:50.863 INFO: train_f_mae: 0.007326
train_f_rmse: 0.010092
2025-06-25 08:37:50.863 INFO: train_f_rmse: 0.010092
val_e/atom_mae: 0.000063
2025-06-25 08:37:50.865 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000088
2025-06-25 08:37:50.866 INFO: val_e/atom_rmse: 0.000088
val_f_mae: 0.007425
2025-06-25 08:37:50.866 INFO: val_f_mae: 0.007425
val_f_rmse: 0.010241
2025-06-25 08:37:50.866 INFO: val_f_rmse: 0.010241
##### Step: 197 Learning rate: 1.953125e-05 #####
2025-06-25 08:38:34.894 INFO: ##### Step: 197 Learning rate: 1.953125e-05 #####
Epoch 58, Train Loss: 0.1033, Val Loss: 0.1057
2025-06-25 08:38:34.894 INFO: Epoch 58, Train Loss: 0.1033, Val Loss: 0.1057
train_e/atom_mae: 0.000081
2025-06-25 08:38:34.895 INFO: train_e/atom_mae: 0.000081
train_e/atom_rmse: 0.000107
2025-06-25 08:38:34.895 INFO: train_e/atom_rmse: 0.000107
train_f_mae: 0.007325
2025-06-25 08:38:34.899 INFO: train_f_mae: 0.007325
train_f_rmse: 0.010094
2025-06-25 08:38:34.899 INFO: train_f_rmse: 0.010094
val_e/atom_mae: 0.000063
2025-06-25 08:38:34.902 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000090
2025-06-25 08:38:34.902 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.007419
2025-06-25 08:38:34.902 INFO: val_f_mae: 0.007419
val_f_rmse: 0.010233
2025-06-25 08:38:34.903 INFO: val_f_rmse: 0.010233
##### Step: 198 Learning rate: 1.953125e-05 #####
2025-06-25 08:39:18.979 INFO: ##### Step: 198 Learning rate: 1.953125e-05 #####
Epoch 59, Train Loss: 0.1034, Val Loss: 0.1062
2025-06-25 08:39:18.979 INFO: Epoch 59, Train Loss: 0.1034, Val Loss: 0.1062
train_e/atom_mae: 0.000087
2025-06-25 08:39:18.980 INFO: train_e/atom_mae: 0.000087
train_e/atom_rmse: 0.000114
2025-06-25 08:39:18.980 INFO: train_e/atom_rmse: 0.000114
train_f_mae: 0.007325
2025-06-25 08:39:18.983 INFO: train_f_mae: 0.007325
train_f_rmse: 0.010093
2025-06-25 08:39:18.983 INFO: train_f_rmse: 0.010093
val_e/atom_mae: 0.000062
2025-06-25 08:39:18.986 INFO: val_e/atom_mae: 0.000062
val_e/atom_rmse: 0.000086
2025-06-25 08:39:18.986 INFO: val_e/atom_rmse: 0.000086
val_f_mae: 0.007437
2025-06-25 08:39:18.987 INFO: val_f_mae: 0.007437
val_f_rmse: 0.010261
2025-06-25 08:39:18.987 INFO: val_f_rmse: 0.010261
##### Step: 199 Learning rate: 1.953125e-05 #####
2025-06-25 08:40:03.074 INFO: ##### Step: 199 Learning rate: 1.953125e-05 #####
Epoch 60, Train Loss: 0.1031, Val Loss: 0.1066
2025-06-25 08:40:03.075 INFO: Epoch 60, Train Loss: 0.1031, Val Loss: 0.1066
train_e/atom_mae: 0.000077
2025-06-25 08:40:03.075 INFO: train_e/atom_mae: 0.000077
train_e/atom_rmse: 0.000102
2025-06-25 08:40:03.076 INFO: train_e/atom_rmse: 0.000102
train_f_mae: 0.007323
2025-06-25 08:40:03.079 INFO: train_f_mae: 0.007323
train_f_rmse: 0.010091
2025-06-25 08:40:03.079 INFO: train_f_rmse: 0.010091
val_e/atom_mae: 0.000094
2025-06-25 08:40:03.082 INFO: val_e/atom_mae: 0.000094
val_e/atom_rmse: 0.000120
2025-06-25 08:40:03.082 INFO: val_e/atom_rmse: 0.000120
val_f_mae: 0.007420
2025-06-25 08:40:03.082 INFO: val_f_mae: 0.007420
val_f_rmse: 0.010240
2025-06-25 08:40:03.083 INFO: val_f_rmse: 0.010240
##### Step: 200 Learning rate: 9.765625e-06 #####
2025-06-25 08:40:47.234 INFO: ##### Step: 200 Learning rate: 9.765625e-06 #####
Epoch 61, Train Loss: 0.1030, Val Loss: 0.1060
2025-06-25 08:40:47.234 INFO: Epoch 61, Train Loss: 0.1030, Val Loss: 0.1060
train_e/atom_mae: 0.000075
2025-06-25 08:40:47.235 INFO: train_e/atom_mae: 0.000075
train_e/atom_rmse: 0.000100
2025-06-25 08:40:47.235 INFO: train_e/atom_rmse: 0.000100
train_f_mae: 0.007319
2025-06-25 08:40:47.238 INFO: train_f_mae: 0.007319
train_f_rmse: 0.010087
2025-06-25 08:40:47.238 INFO: train_f_rmse: 0.010087
val_e/atom_mae: 0.000076
2025-06-25 08:40:47.241 INFO: val_e/atom_mae: 0.000076
val_e/atom_rmse: 0.000100
2025-06-25 08:40:47.241 INFO: val_e/atom_rmse: 0.000100
val_f_mae: 0.007420
2025-06-25 08:40:47.242 INFO: val_f_mae: 0.007420
val_f_rmse: 0.010239
2025-06-25 08:40:47.242 INFO: val_f_rmse: 0.010239
##### Step: 201 Learning rate: 9.765625e-06 #####
2025-06-25 08:41:31.350 INFO: ##### Step: 201 Learning rate: 9.765625e-06 #####
Epoch 62, Train Loss: 0.1028, Val Loss: 0.1057
2025-06-25 08:41:31.350 INFO: Epoch 62, Train Loss: 0.1028, Val Loss: 0.1057
train_e/atom_mae: 0.000070
2025-06-25 08:41:31.351 INFO: train_e/atom_mae: 0.000070
train_e/atom_rmse: 0.000095
2025-06-25 08:41:31.351 INFO: train_e/atom_rmse: 0.000095
train_f_mae: 0.007321
2025-06-25 08:41:31.355 INFO: train_f_mae: 0.007321
train_f_rmse: 0.010086
2025-06-25 08:41:31.355 INFO: train_f_rmse: 0.010086
val_e/atom_mae: 0.000069
2025-06-25 08:41:31.357 INFO: val_e/atom_mae: 0.000069
val_e/atom_rmse: 0.000096
2025-06-25 08:41:31.358 INFO: val_e/atom_rmse: 0.000096
val_f_mae: 0.007417
2025-06-25 08:41:31.358 INFO: val_f_mae: 0.007417
val_f_rmse: 0.010228
2025-06-25 08:41:31.358 INFO: val_f_rmse: 0.010228
##### Step: 202 Learning rate: 9.765625e-06 #####
2025-06-25 08:42:15.422 INFO: ##### Step: 202 Learning rate: 9.765625e-06 #####
Epoch 63, Train Loss: 0.1027, Val Loss: 0.1055
2025-06-25 08:42:15.423 INFO: Epoch 63, Train Loss: 0.1027, Val Loss: 0.1055
train_e/atom_mae: 0.000067
2025-06-25 08:42:15.423 INFO: train_e/atom_mae: 0.000067
train_e/atom_rmse: 0.000092
2025-06-25 08:42:15.424 INFO: train_e/atom_rmse: 0.000092
train_f_mae: 0.007318
2025-06-25 08:42:15.427 INFO: train_f_mae: 0.007318
train_f_rmse: 0.010083
2025-06-25 08:42:15.427 INFO: train_f_rmse: 0.010083
val_e/atom_mae: 0.000061
2025-06-25 08:42:15.430 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000086
2025-06-25 08:42:15.430 INFO: val_e/atom_rmse: 0.000086
val_f_mae: 0.007418
2025-06-25 08:42:15.430 INFO: val_f_mae: 0.007418
val_f_rmse: 0.010229
2025-06-25 08:42:15.431 INFO: val_f_rmse: 0.010229
##### Step: 203 Learning rate: 9.765625e-06 #####
2025-06-25 08:42:59.545 INFO: ##### Step: 203 Learning rate: 9.765625e-06 #####
Epoch 64, Train Loss: 0.1027, Val Loss: 0.1060
2025-06-25 08:42:59.546 INFO: Epoch 64, Train Loss: 0.1027, Val Loss: 0.1060
train_e/atom_mae: 0.000066
2025-06-25 08:42:59.546 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000090
2025-06-25 08:42:59.547 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.007321
2025-06-25 08:42:59.550 INFO: train_f_mae: 0.007321
train_f_rmse: 0.010087
2025-06-25 08:42:59.550 INFO: train_f_rmse: 0.010087
val_e/atom_mae: 0.000064
2025-06-25 08:42:59.553 INFO: val_e/atom_mae: 0.000064
val_e/atom_rmse: 0.000088
2025-06-25 08:42:59.553 INFO: val_e/atom_rmse: 0.000088
val_f_mae: 0.007428
2025-06-25 08:42:59.553 INFO: val_f_mae: 0.007428
val_f_rmse: 0.010248
2025-06-25 08:42:59.554 INFO: val_f_rmse: 0.010248
##### Step: 204 Learning rate: 9.765625e-06 #####
2025-06-25 08:43:43.624 INFO: ##### Step: 204 Learning rate: 9.765625e-06 #####
Epoch 65, Train Loss: 0.1030, Val Loss: 0.1056
2025-06-25 08:43:43.624 INFO: Epoch 65, Train Loss: 0.1030, Val Loss: 0.1056
train_e/atom_mae: 0.000078
2025-06-25 08:43:43.625 INFO: train_e/atom_mae: 0.000078
train_e/atom_rmse: 0.000104
2025-06-25 08:43:43.625 INFO: train_e/atom_rmse: 0.000104
train_f_mae: 0.007321
2025-06-25 08:43:43.629 INFO: train_f_mae: 0.007321
train_f_rmse: 0.010085
2025-06-25 08:43:43.629 INFO: train_f_rmse: 0.010085
val_e/atom_mae: 0.000063
2025-06-25 08:43:43.632 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000091
2025-06-25 08:43:43.632 INFO: val_e/atom_rmse: 0.000091
val_f_mae: 0.007418
2025-06-25 08:43:43.632 INFO: val_f_mae: 0.007418
val_f_rmse: 0.010229
2025-06-25 08:43:43.632 INFO: val_f_rmse: 0.010229
##### Step: 205 Learning rate: 9.765625e-06 #####
2025-06-25 08:44:27.716 INFO: ##### Step: 205 Learning rate: 9.765625e-06 #####
Epoch 66, Train Loss: 0.1026, Val Loss: 0.1057
2025-06-25 08:44:27.716 INFO: Epoch 66, Train Loss: 0.1026, Val Loss: 0.1057
train_e/atom_mae: 0.000066
2025-06-25 08:44:27.717 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000091
2025-06-25 08:44:27.717 INFO: train_e/atom_rmse: 0.000091
train_f_mae: 0.007318
2025-06-25 08:44:27.720 INFO: train_f_mae: 0.007318
train_f_rmse: 0.010082
2025-06-25 08:44:27.721 INFO: train_f_rmse: 0.010082
val_e/atom_mae: 0.000060
2025-06-25 08:44:27.723 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000086
2025-06-25 08:44:27.724 INFO: val_e/atom_rmse: 0.000086
val_f_mae: 0.007422
2025-06-25 08:44:27.724 INFO: val_f_mae: 0.007422
val_f_rmse: 0.010238
2025-06-25 08:44:27.724 INFO: val_f_rmse: 0.010238
##### Step: 206 Learning rate: 9.765625e-06 #####
2025-06-25 08:45:11.787 INFO: ##### Step: 206 Learning rate: 9.765625e-06 #####
Epoch 67, Train Loss: 0.1027, Val Loss: 0.1057
2025-06-25 08:45:11.787 INFO: Epoch 67, Train Loss: 0.1027, Val Loss: 0.1057
train_e/atom_mae: 0.000066
2025-06-25 08:45:11.788 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000091
2025-06-25 08:45:11.788 INFO: train_e/atom_rmse: 0.000091
train_f_mae: 0.007318
2025-06-25 08:45:11.792 INFO: train_f_mae: 0.007318
train_f_rmse: 0.010083
2025-06-25 08:45:11.792 INFO: train_f_rmse: 0.010083
val_e/atom_mae: 0.000064
2025-06-25 08:45:11.794 INFO: val_e/atom_mae: 0.000064
val_e/atom_rmse: 0.000089
2025-06-25 08:45:11.795 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.007421
2025-06-25 08:45:11.795 INFO: val_f_mae: 0.007421
val_f_rmse: 0.010236
2025-06-25 08:45:11.795 INFO: val_f_rmse: 0.010236
##### Step: 207 Learning rate: 9.765625e-06 #####
2025-06-25 08:45:55.845 INFO: ##### Step: 207 Learning rate: 9.765625e-06 #####
Epoch 68, Train Loss: 0.1028, Val Loss: 0.1085
2025-06-25 08:45:55.845 INFO: Epoch 68, Train Loss: 0.1028, Val Loss: 0.1085
train_e/atom_mae: 0.000070
2025-06-25 08:45:55.846 INFO: train_e/atom_mae: 0.000070
train_e/atom_rmse: 0.000096
2025-06-25 08:45:55.846 INFO: train_e/atom_rmse: 0.000096
train_f_mae: 0.007319
2025-06-25 08:45:55.850 INFO: train_f_mae: 0.007319
train_f_rmse: 0.010082
2025-06-25 08:45:55.850 INFO: train_f_rmse: 0.010082
val_e/atom_mae: 0.000150
2025-06-25 08:45:55.852 INFO: val_e/atom_mae: 0.000150
val_e/atom_rmse: 0.000176
2025-06-25 08:45:55.853 INFO: val_e/atom_rmse: 0.000176
val_f_mae: 0.007422
2025-06-25 08:45:55.853 INFO: val_f_mae: 0.007422
val_f_rmse: 0.010235
2025-06-25 08:45:55.853 INFO: val_f_rmse: 0.010235
##### Step: 208 Learning rate: 9.765625e-06 #####
2025-06-25 08:46:39.910 INFO: ##### Step: 208 Learning rate: 9.765625e-06 #####
Epoch 69, Train Loss: 0.1027, Val Loss: 0.1056
2025-06-25 08:46:39.910 INFO: Epoch 69, Train Loss: 0.1027, Val Loss: 0.1056
train_e/atom_mae: 0.000069
2025-06-25 08:46:39.911 INFO: train_e/atom_mae: 0.000069
train_e/atom_rmse: 0.000094
2025-06-25 08:46:39.911 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.007318
2025-06-25 08:46:39.914 INFO: train_f_mae: 0.007318
train_f_rmse: 0.010082
2025-06-25 08:46:39.914 INFO: train_f_rmse: 0.010082
val_e/atom_mae: 0.000063
2025-06-25 08:46:39.917 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000088
2025-06-25 08:46:39.917 INFO: val_e/atom_rmse: 0.000088
val_f_mae: 0.007418
2025-06-25 08:46:39.918 INFO: val_f_mae: 0.007418
val_f_rmse: 0.010232
2025-06-25 08:46:39.918 INFO: val_f_rmse: 0.010232
##### Step: 209 Learning rate: 9.765625e-06 #####
2025-06-25 08:47:24.016 INFO: ##### Step: 209 Learning rate: 9.765625e-06 #####
Epoch 70, Train Loss: 0.1028, Val Loss: 0.1060
2025-06-25 08:47:24.016 INFO: Epoch 70, Train Loss: 0.1028, Val Loss: 0.1060
train_e/atom_mae: 0.000073
2025-06-25 08:47:24.017 INFO: train_e/atom_mae: 0.000073
train_e/atom_rmse: 0.000098
2025-06-25 08:47:24.017 INFO: train_e/atom_rmse: 0.000098
train_f_mae: 0.007319
2025-06-25 08:47:24.020 INFO: train_f_mae: 0.007319
train_f_rmse: 0.010082
2025-06-25 08:47:24.020 INFO: train_f_rmse: 0.010082
val_e/atom_mae: 0.000076
2025-06-25 08:47:24.023 INFO: val_e/atom_mae: 0.000076
val_e/atom_rmse: 0.000103
2025-06-25 08:47:24.023 INFO: val_e/atom_rmse: 0.000103
val_f_mae: 0.007416
2025-06-25 08:47:24.024 INFO: val_f_mae: 0.007416
val_f_rmse: 0.010234
2025-06-25 08:47:24.024 INFO: val_f_rmse: 0.010234
##### Step: 210 Learning rate: 9.765625e-06 #####
2025-06-25 08:48:07.951 INFO: ##### Step: 210 Learning rate: 9.765625e-06 #####
Epoch 71, Train Loss: 0.1026, Val Loss: 0.1059
2025-06-25 08:48:07.951 INFO: Epoch 71, Train Loss: 0.1026, Val Loss: 0.1059
train_e/atom_mae: 0.000066
2025-06-25 08:48:07.952 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000091
2025-06-25 08:48:07.952 INFO: train_e/atom_rmse: 0.000091
train_f_mae: 0.007317
2025-06-25 08:48:07.956 INFO: train_f_mae: 0.007317
train_f_rmse: 0.010082
2025-06-25 08:48:07.956 INFO: train_f_rmse: 0.010082
val_e/atom_mae: 0.000080
2025-06-25 08:48:07.958 INFO: val_e/atom_mae: 0.000080
val_e/atom_rmse: 0.000104
2025-06-25 08:48:07.959 INFO: val_e/atom_rmse: 0.000104
val_f_mae: 0.007417
2025-06-25 08:48:07.959 INFO: val_f_mae: 0.007417
val_f_rmse: 0.010229
2025-06-25 08:48:07.959 INFO: val_f_rmse: 0.010229
##### Step: 211 Learning rate: 9.765625e-06 #####
2025-06-25 08:48:51.795 INFO: ##### Step: 211 Learning rate: 9.765625e-06 #####
Epoch 72, Train Loss: 0.1027, Val Loss: 0.1055
2025-06-25 08:48:51.795 INFO: Epoch 72, Train Loss: 0.1027, Val Loss: 0.1055
train_e/atom_mae: 0.000069
2025-06-25 08:48:51.796 INFO: train_e/atom_mae: 0.000069
train_e/atom_rmse: 0.000094
2025-06-25 08:48:51.796 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.007318
2025-06-25 08:48:51.800 INFO: train_f_mae: 0.007318
train_f_rmse: 0.010082
2025-06-25 08:48:51.800 INFO: train_f_rmse: 0.010082
val_e/atom_mae: 0.000058
2025-06-25 08:48:51.802 INFO: val_e/atom_mae: 0.000058
val_e/atom_rmse: 0.000086
2025-06-25 08:48:51.803 INFO: val_e/atom_rmse: 0.000086
val_f_mae: 0.007418
2025-06-25 08:48:51.803 INFO: val_f_mae: 0.007418
val_f_rmse: 0.010230
2025-06-25 08:48:51.803 INFO: val_f_rmse: 0.010230
##### Step: 212 Learning rate: 9.765625e-06 #####
2025-06-25 08:49:35.648 INFO: ##### Step: 212 Learning rate: 9.765625e-06 #####
Epoch 73, Train Loss: 0.1029, Val Loss: 0.1063
2025-06-25 08:49:35.649 INFO: Epoch 73, Train Loss: 0.1029, Val Loss: 0.1063
train_e/atom_mae: 0.000077
2025-06-25 08:49:35.650 INFO: train_e/atom_mae: 0.000077
train_e/atom_rmse: 0.000102
2025-06-25 08:49:35.650 INFO: train_e/atom_rmse: 0.000102
train_f_mae: 0.007318
2025-06-25 08:49:35.653 INFO: train_f_mae: 0.007318
train_f_rmse: 0.010082
2025-06-25 08:49:35.653 INFO: train_f_rmse: 0.010082
val_e/atom_mae: 0.000089
2025-06-25 08:49:35.656 INFO: val_e/atom_mae: 0.000089
val_e/atom_rmse: 0.000114
2025-06-25 08:49:35.656 INFO: val_e/atom_rmse: 0.000114
val_f_mae: 0.007421
2025-06-25 08:49:35.657 INFO: val_f_mae: 0.007421
val_f_rmse: 0.010236
2025-06-25 08:49:35.657 INFO: val_f_rmse: 0.010236
##### Step: 213 Learning rate: 9.765625e-06 #####
2025-06-25 08:50:19.478 INFO: ##### Step: 213 Learning rate: 9.765625e-06 #####
Epoch 74, Train Loss: 0.1027, Val Loss: 0.1065
2025-06-25 08:50:19.479 INFO: Epoch 74, Train Loss: 0.1027, Val Loss: 0.1065
train_e/atom_mae: 0.000073
2025-06-25 08:50:19.479 INFO: train_e/atom_mae: 0.000073
train_e/atom_rmse: 0.000098
2025-06-25 08:50:19.480 INFO: train_e/atom_rmse: 0.000098
train_f_mae: 0.007317
2025-06-25 08:50:19.483 INFO: train_f_mae: 0.007317
train_f_rmse: 0.010078
2025-06-25 08:50:19.483 INFO: train_f_rmse: 0.010078
val_e/atom_mae: 0.000077
2025-06-25 08:50:19.486 INFO: val_e/atom_mae: 0.000077
val_e/atom_rmse: 0.000100
2025-06-25 08:50:19.486 INFO: val_e/atom_rmse: 0.000100
val_f_mae: 0.007435
2025-06-25 08:50:19.487 INFO: val_f_mae: 0.007435
val_f_rmse: 0.010262
2025-06-25 08:50:19.487 INFO: val_f_rmse: 0.010262
##### Step: 214 Learning rate: 9.765625e-06 #####
2025-06-25 08:51:03.316 INFO: ##### Step: 214 Learning rate: 9.765625e-06 #####
Epoch 75, Train Loss: 0.1028, Val Loss: 0.1068
2025-06-25 08:51:03.316 INFO: Epoch 75, Train Loss: 0.1028, Val Loss: 0.1068
train_e/atom_mae: 0.000070
2025-06-25 08:51:03.317 INFO: train_e/atom_mae: 0.000070
train_e/atom_rmse: 0.000096
2025-06-25 08:51:03.317 INFO: train_e/atom_rmse: 0.000096
train_f_mae: 0.007318
2025-06-25 08:51:03.320 INFO: train_f_mae: 0.007318
train_f_rmse: 0.010082
2025-06-25 08:51:03.321 INFO: train_f_rmse: 0.010082
val_e/atom_mae: 0.000105
2025-06-25 08:51:03.323 INFO: val_e/atom_mae: 0.000105
val_e/atom_rmse: 0.000129
2025-06-25 08:51:03.323 INFO: val_e/atom_rmse: 0.000129
val_f_mae: 0.007418
2025-06-25 08:51:03.324 INFO: val_f_mae: 0.007418
val_f_rmse: 0.010234
2025-06-25 08:51:03.324 INFO: val_f_rmse: 0.010234
##### Step: 215 Learning rate: 9.765625e-06 #####
2025-06-25 08:51:47.231 INFO: ##### Step: 215 Learning rate: 9.765625e-06 #####
Epoch 76, Train Loss: 0.1028, Val Loss: 0.1055
2025-06-25 08:51:47.231 INFO: Epoch 76, Train Loss: 0.1028, Val Loss: 0.1055
train_e/atom_mae: 0.000074
2025-06-25 08:51:47.232 INFO: train_e/atom_mae: 0.000074
train_e/atom_rmse: 0.000099
2025-06-25 08:51:47.232 INFO: train_e/atom_rmse: 0.000099
train_f_mae: 0.007315
2025-06-25 08:51:47.235 INFO: train_f_mae: 0.007315
train_f_rmse: 0.010079
2025-06-25 08:51:47.235 INFO: train_f_rmse: 0.010079
val_e/atom_mae: 0.000061
2025-06-25 08:51:47.238 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000086
2025-06-25 08:51:47.238 INFO: val_e/atom_rmse: 0.000086
val_f_mae: 0.007415
2025-06-25 08:51:47.239 INFO: val_f_mae: 0.007415
val_f_rmse: 0.010230
2025-06-25 08:51:47.239 INFO: val_f_rmse: 0.010230
##### Step: 216 Learning rate: 9.765625e-06 #####
2025-06-25 08:52:30.823 INFO: ##### Step: 216 Learning rate: 9.765625e-06 #####
Epoch 77, Train Loss: 0.1027, Val Loss: 0.1060
2025-06-25 08:52:30.823 INFO: Epoch 77, Train Loss: 0.1027, Val Loss: 0.1060
train_e/atom_mae: 0.000068
2025-06-25 08:52:30.824 INFO: train_e/atom_mae: 0.000068
train_e/atom_rmse: 0.000093
2025-06-25 08:52:30.824 INFO: train_e/atom_rmse: 0.000093
train_f_mae: 0.007316
2025-06-25 08:52:30.827 INFO: train_f_mae: 0.007316
train_f_rmse: 0.010081
2025-06-25 08:52:30.828 INFO: train_f_rmse: 0.010081
val_e/atom_mae: 0.000078
2025-06-25 08:52:30.830 INFO: val_e/atom_mae: 0.000078
val_e/atom_rmse: 0.000104
2025-06-25 08:52:30.830 INFO: val_e/atom_rmse: 0.000104
val_f_mae: 0.007417
2025-06-25 08:52:30.831 INFO: val_f_mae: 0.007417
val_f_rmse: 0.010231
2025-06-25 08:52:30.831 INFO: val_f_rmse: 0.010231
##### Step: 217 Learning rate: 9.765625e-06 #####
2025-06-25 08:53:14.647 INFO: ##### Step: 217 Learning rate: 9.765625e-06 #####
Epoch 78, Train Loss: 0.1027, Val Loss: 0.1055
2025-06-25 08:53:14.647 INFO: Epoch 78, Train Loss: 0.1027, Val Loss: 0.1055
train_e/atom_mae: 0.000071
2025-06-25 08:53:14.648 INFO: train_e/atom_mae: 0.000071
train_e/atom_rmse: 0.000096
2025-06-25 08:53:14.648 INFO: train_e/atom_rmse: 0.000096
train_f_mae: 0.007317
2025-06-25 08:53:14.652 INFO: train_f_mae: 0.007317
train_f_rmse: 0.010080
2025-06-25 08:53:14.652 INFO: train_f_rmse: 0.010080
val_e/atom_mae: 0.000058
2025-06-25 08:53:14.654 INFO: val_e/atom_mae: 0.000058
val_e/atom_rmse: 0.000085
2025-06-25 08:53:14.655 INFO: val_e/atom_rmse: 0.000085
val_f_mae: 0.007415
2025-06-25 08:53:14.655 INFO: val_f_mae: 0.007415
val_f_rmse: 0.010230
2025-06-25 08:53:14.655 INFO: val_f_rmse: 0.010230
##### Step: 218 Learning rate: 9.765625e-06 #####
2025-06-25 08:53:58.697 INFO: ##### Step: 218 Learning rate: 9.765625e-06 #####
Epoch 79, Train Loss: 0.1026, Val Loss: 0.1059
2025-06-25 08:53:58.697 INFO: Epoch 79, Train Loss: 0.1026, Val Loss: 0.1059
train_e/atom_mae: 0.000065
2025-06-25 08:53:58.698 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000090
2025-06-25 08:53:58.698 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.007317
2025-06-25 08:53:58.702 INFO: train_f_mae: 0.007317
train_f_rmse: 0.010081
2025-06-25 08:53:58.702 INFO: train_f_rmse: 0.010081
val_e/atom_mae: 0.000074
2025-06-25 08:53:58.704 INFO: val_e/atom_mae: 0.000074
val_e/atom_rmse: 0.000104
2025-06-25 08:53:58.705 INFO: val_e/atom_rmse: 0.000104
val_f_mae: 0.007416
2025-06-25 08:53:58.705 INFO: val_f_mae: 0.007416
val_f_rmse: 0.010229
2025-06-25 08:53:58.705 INFO: val_f_rmse: 0.010229
##### Step: 219 Learning rate: 9.765625e-06 #####
2025-06-25 08:54:42.774 INFO: ##### Step: 219 Learning rate: 9.765625e-06 #####
Epoch 80, Train Loss: 0.1026, Val Loss: 0.1057
2025-06-25 08:54:42.774 INFO: Epoch 80, Train Loss: 0.1026, Val Loss: 0.1057
train_e/atom_mae: 0.000066
2025-06-25 08:54:42.775 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000090
2025-06-25 08:54:42.775 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.007317
2025-06-25 08:54:42.779 INFO: train_f_mae: 0.007317
train_f_rmse: 0.010080
2025-06-25 08:54:42.779 INFO: train_f_rmse: 0.010080
val_e/atom_mae: 0.000057
2025-06-25 08:54:42.781 INFO: val_e/atom_mae: 0.000057
val_e/atom_rmse: 0.000083
2025-06-25 08:54:42.782 INFO: val_e/atom_rmse: 0.000083
val_f_mae: 0.007421
2025-06-25 08:54:42.782 INFO: val_f_mae: 0.007421
val_f_rmse: 0.010239
2025-06-25 08:54:42.782 INFO: val_f_rmse: 0.010239
##### Step: 220 Learning rate: 4.8828125e-06 #####
2025-06-25 08:55:26.833 INFO: ##### Step: 220 Learning rate: 4.8828125e-06 #####
Epoch 81, Train Loss: 0.1025, Val Loss: 0.1055
2025-06-25 08:55:26.833 INFO: Epoch 81, Train Loss: 0.1025, Val Loss: 0.1055
train_e/atom_mae: 0.000063
2025-06-25 08:55:26.834 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000088
2025-06-25 08:55:26.834 INFO: train_e/atom_rmse: 0.000088
train_f_mae: 0.007314
2025-06-25 08:55:26.837 INFO: train_f_mae: 0.007314
train_f_rmse: 0.010077
2025-06-25 08:55:26.837 INFO: train_f_rmse: 0.010077
val_e/atom_mae: 0.000060
2025-06-25 08:55:26.840 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000086
2025-06-25 08:55:26.840 INFO: val_e/atom_rmse: 0.000086
val_f_mae: 0.007415
2025-06-25 08:55:26.841 INFO: val_f_mae: 0.007415
val_f_rmse: 0.010227
2025-06-25 08:55:26.841 INFO: val_f_rmse: 0.010227
##### Step: 221 Learning rate: 4.8828125e-06 #####
2025-06-25 08:56:10.910 INFO: ##### Step: 221 Learning rate: 4.8828125e-06 #####
Epoch 82, Train Loss: 0.1025, Val Loss: 0.1057
2025-06-25 08:56:10.910 INFO: Epoch 82, Train Loss: 0.1025, Val Loss: 0.1057
train_e/atom_mae: 0.000063
2025-06-25 08:56:10.911 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000088
2025-06-25 08:56:10.911 INFO: train_e/atom_rmse: 0.000088
train_f_mae: 0.007314
2025-06-25 08:56:10.915 INFO: train_f_mae: 0.007314
train_f_rmse: 0.010076
2025-06-25 08:56:10.915 INFO: train_f_rmse: 0.010076
val_e/atom_mae: 0.000064
2025-06-25 08:56:10.917 INFO: val_e/atom_mae: 0.000064
val_e/atom_rmse: 0.000095
2025-06-25 08:56:10.918 INFO: val_e/atom_rmse: 0.000095
val_f_mae: 0.007415
2025-06-25 08:56:10.918 INFO: val_f_mae: 0.007415
val_f_rmse: 0.010227
2025-06-25 08:56:10.918 INFO: val_f_rmse: 0.010227
##### Step: 222 Learning rate: 4.8828125e-06 #####
2025-06-25 08:56:54.935 INFO: ##### Step: 222 Learning rate: 4.8828125e-06 #####
Epoch 83, Train Loss: 0.1025, Val Loss: 0.1056
2025-06-25 08:56:54.936 INFO: Epoch 83, Train Loss: 0.1025, Val Loss: 0.1056
train_e/atom_mae: 0.000063
2025-06-25 08:56:54.937 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000087
2025-06-25 08:56:54.937 INFO: train_e/atom_rmse: 0.000087
train_f_mae: 0.007314
2025-06-25 08:56:54.940 INFO: train_f_mae: 0.007314
train_f_rmse: 0.010077
2025-06-25 08:56:54.940 INFO: train_f_rmse: 0.010077
val_e/atom_mae: 0.000063
2025-06-25 08:56:54.943 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000087
2025-06-25 08:56:54.943 INFO: val_e/atom_rmse: 0.000087
val_f_mae: 0.007414
2025-06-25 08:56:54.944 INFO: val_f_mae: 0.007414
val_f_rmse: 0.010230
2025-06-25 08:56:54.944 INFO: val_f_rmse: 0.010230
##### Step: 223 Learning rate: 4.8828125e-06 #####
2025-06-25 08:57:38.987 INFO: ##### Step: 223 Learning rate: 4.8828125e-06 #####
Epoch 84, Train Loss: 0.1024, Val Loss: 0.1059
2025-06-25 08:57:38.987 INFO: Epoch 84, Train Loss: 0.1024, Val Loss: 0.1059
train_e/atom_mae: 0.000062
2025-06-25 08:57:38.988 INFO: train_e/atom_mae: 0.000062
train_e/atom_rmse: 0.000086
2025-06-25 08:57:38.988 INFO: train_e/atom_rmse: 0.000086
train_f_mae: 0.007313
2025-06-25 08:57:38.991 INFO: train_f_mae: 0.007313
train_f_rmse: 0.010077
2025-06-25 08:57:38.991 INFO: train_f_rmse: 0.010077
val_e/atom_mae: 0.000073
2025-06-25 08:57:38.994 INFO: val_e/atom_mae: 0.000073
val_e/atom_rmse: 0.000102
2025-06-25 08:57:38.994 INFO: val_e/atom_rmse: 0.000102
val_f_mae: 0.007416
2025-06-25 08:57:38.995 INFO: val_f_mae: 0.007416
val_f_rmse: 0.010230
2025-06-25 08:57:38.995 INFO: val_f_rmse: 0.010230
##### Step: 224 Learning rate: 4.8828125e-06 #####
2025-06-25 08:58:23.041 INFO: ##### Step: 224 Learning rate: 4.8828125e-06 #####
Epoch 85, Train Loss: 0.1025, Val Loss: 0.1056
2025-06-25 08:58:23.041 INFO: Epoch 85, Train Loss: 0.1025, Val Loss: 0.1056
train_e/atom_mae: 0.000062
2025-06-25 08:58:23.042 INFO: train_e/atom_mae: 0.000062
train_e/atom_rmse: 0.000087
2025-06-25 08:58:23.042 INFO: train_e/atom_rmse: 0.000087
train_f_mae: 0.007314
2025-06-25 08:58:23.045 INFO: train_f_mae: 0.007314
train_f_rmse: 0.010077
2025-06-25 08:58:23.046 INFO: train_f_rmse: 0.010077
val_e/atom_mae: 0.000061
2025-06-25 08:58:23.048 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000086
2025-06-25 08:58:23.049 INFO: val_e/atom_rmse: 0.000086
val_f_mae: 0.007416
2025-06-25 08:58:23.049 INFO: val_f_mae: 0.007416
val_f_rmse: 0.010230
2025-06-25 08:58:23.049 INFO: val_f_rmse: 0.010230
##### Step: 225 Learning rate: 4.8828125e-06 #####
2025-06-25 08:59:07.656 INFO: ##### Step: 225 Learning rate: 4.8828125e-06 #####
Epoch 86, Train Loss: 0.1024, Val Loss: 0.1059
2025-06-25 08:59:07.656 INFO: Epoch 86, Train Loss: 0.1024, Val Loss: 0.1059
train_e/atom_mae: 0.000062
2025-06-25 08:59:07.657 INFO: train_e/atom_mae: 0.000062
train_e/atom_rmse: 0.000086
2025-06-25 08:59:07.657 INFO: train_e/atom_rmse: 0.000086
train_f_mae: 0.007313
2025-06-25 08:59:07.661 INFO: train_f_mae: 0.007313
train_f_rmse: 0.010076
2025-06-25 08:59:07.661 INFO: train_f_rmse: 0.010076
val_e/atom_mae: 0.000074
2025-06-25 08:59:07.663 INFO: val_e/atom_mae: 0.000074
val_e/atom_rmse: 0.000099
2025-06-25 08:59:07.664 INFO: val_e/atom_rmse: 0.000099
val_f_mae: 0.007416
2025-06-25 08:59:07.664 INFO: val_f_mae: 0.007416
val_f_rmse: 0.010232
2025-06-25 08:59:07.664 INFO: val_f_rmse: 0.010232
##### Step: 226 Learning rate: 4.8828125e-06 #####
2025-06-25 08:59:51.703 INFO: ##### Step: 226 Learning rate: 4.8828125e-06 #####
Epoch 87, Train Loss: 0.1025, Val Loss: 0.1056
2025-06-25 08:59:51.704 INFO: Epoch 87, Train Loss: 0.1025, Val Loss: 0.1056
train_e/atom_mae: 0.000064
2025-06-25 08:59:51.705 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000088
2025-06-25 08:59:51.705 INFO: train_e/atom_rmse: 0.000088
train_f_mae: 0.007314
2025-06-25 08:59:51.708 INFO: train_f_mae: 0.007314
train_f_rmse: 0.010076
2025-06-25 08:59:51.708 INFO: train_f_rmse: 0.010076
val_e/atom_mae: 0.000065
2025-06-25 08:59:51.711 INFO: val_e/atom_mae: 0.000065
val_e/atom_rmse: 0.000090
2025-06-25 08:59:51.711 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.007414
2025-06-25 08:59:51.712 INFO: val_f_mae: 0.007414
val_f_rmse: 0.010228
2025-06-25 08:59:51.712 INFO: val_f_rmse: 0.010228
##### Step: 227 Learning rate: 4.8828125e-06 #####
2025-06-25 09:00:35.760 INFO: ##### Step: 227 Learning rate: 4.8828125e-06 #####
Epoch 88, Train Loss: 0.1025, Val Loss: 0.1055
2025-06-25 09:00:35.761 INFO: Epoch 88, Train Loss: 0.1025, Val Loss: 0.1055
train_e/atom_mae: 0.000066
2025-06-25 09:00:35.762 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000090
2025-06-25 09:00:35.762 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.007313
2025-06-25 09:00:35.765 INFO: train_f_mae: 0.007313
train_f_rmse: 0.010076
2025-06-25 09:00:35.765 INFO: train_f_rmse: 0.010076
val_e/atom_mae: 0.000063
2025-06-25 09:00:35.768 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000090
2025-06-25 09:00:35.768 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.007412
2025-06-25 09:00:35.769 INFO: val_f_mae: 0.007412
val_f_rmse: 0.010225
2025-06-25 09:00:35.769 INFO: val_f_rmse: 0.010225
##### Step: 228 Learning rate: 4.8828125e-06 #####
2025-06-25 09:01:19.846 INFO: ##### Step: 228 Learning rate: 4.8828125e-06 #####
Epoch 89, Train Loss: 0.1025, Val Loss: 0.1057
2025-06-25 09:01:19.847 INFO: Epoch 89, Train Loss: 0.1025, Val Loss: 0.1057
train_e/atom_mae: 0.000066
2025-06-25 09:01:19.847 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000090
2025-06-25 09:01:19.848 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.007312
2025-06-25 09:01:19.851 INFO: train_f_mae: 0.007312
train_f_rmse: 0.010075
2025-06-25 09:01:19.851 INFO: train_f_rmse: 0.010075
val_e/atom_mae: 0.000065
2025-06-25 09:01:19.854 INFO: val_e/atom_mae: 0.000065
val_e/atom_rmse: 0.000094
2025-06-25 09:01:19.854 INFO: val_e/atom_rmse: 0.000094
val_f_mae: 0.007416
2025-06-25 09:01:19.855 INFO: val_f_mae: 0.007416
val_f_rmse: 0.010229
2025-06-25 09:01:19.855 INFO: val_f_rmse: 0.010229
##### Step: 229 Learning rate: 4.8828125e-06 #####
2025-06-25 09:02:03.941 INFO: ##### Step: 229 Learning rate: 4.8828125e-06 #####
Epoch 90, Train Loss: 0.1025, Val Loss: 0.1054
2025-06-25 09:02:03.942 INFO: Epoch 90, Train Loss: 0.1025, Val Loss: 0.1054
train_e/atom_mae: 0.000065
2025-06-25 09:02:03.943 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000089
2025-06-25 09:02:03.943 INFO: train_e/atom_rmse: 0.000089
train_f_mae: 0.007313
2025-06-25 09:02:03.946 INFO: train_f_mae: 0.007313
train_f_rmse: 0.010076
2025-06-25 09:02:03.946 INFO: train_f_rmse: 0.010076
val_e/atom_mae: 0.000058
2025-06-25 09:02:03.949 INFO: val_e/atom_mae: 0.000058
val_e/atom_rmse: 0.000085
2025-06-25 09:02:03.949 INFO: val_e/atom_rmse: 0.000085
val_f_mae: 0.007412
2025-06-25 09:02:03.950 INFO: val_f_mae: 0.007412
val_f_rmse: 0.010224
2025-06-25 09:02:03.950 INFO: val_f_rmse: 0.010224
##### Step: 230 Learning rate: 4.8828125e-06 #####
2025-06-25 09:02:48.037 INFO: ##### Step: 230 Learning rate: 4.8828125e-06 #####
Epoch 91, Train Loss: 0.1025, Val Loss: 0.1058
2025-06-25 09:02:48.037 INFO: Epoch 91, Train Loss: 0.1025, Val Loss: 0.1058
train_e/atom_mae: 0.000065
2025-06-25 09:02:48.038 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000089
2025-06-25 09:02:48.038 INFO: train_e/atom_rmse: 0.000089
train_f_mae: 0.007314
2025-06-25 09:02:48.041 INFO: train_f_mae: 0.007314
train_f_rmse: 0.010077
2025-06-25 09:02:48.042 INFO: train_f_rmse: 0.010077
val_e/atom_mae: 0.000068
2025-06-25 09:02:48.044 INFO: val_e/atom_mae: 0.000068
val_e/atom_rmse: 0.000092
2025-06-25 09:02:48.044 INFO: val_e/atom_rmse: 0.000092
val_f_mae: 0.007417
2025-06-25 09:02:48.045 INFO: val_f_mae: 0.007417
val_f_rmse: 0.010234
2025-06-25 09:02:48.045 INFO: val_f_rmse: 0.010234
##### Step: 231 Learning rate: 4.8828125e-06 #####
2025-06-25 09:03:32.108 INFO: ##### Step: 231 Learning rate: 4.8828125e-06 #####
Epoch 92, Train Loss: 0.1025, Val Loss: 0.1055
2025-06-25 09:03:32.108 INFO: Epoch 92, Train Loss: 0.1025, Val Loss: 0.1055
train_e/atom_mae: 0.000064
2025-06-25 09:03:32.109 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000088
2025-06-25 09:03:32.109 INFO: train_e/atom_rmse: 0.000088
train_f_mae: 0.007313
2025-06-25 09:03:32.113 INFO: train_f_mae: 0.007313
train_f_rmse: 0.010076
2025-06-25 09:03:32.113 INFO: train_f_rmse: 0.010076
val_e/atom_mae: 0.000058
2025-06-25 09:03:32.116 INFO: val_e/atom_mae: 0.000058
val_e/atom_rmse: 0.000084
2025-06-25 09:03:32.116 INFO: val_e/atom_rmse: 0.000084
val_f_mae: 0.007415
2025-06-25 09:03:32.116 INFO: val_f_mae: 0.007415
val_f_rmse: 0.010229
2025-06-25 09:03:32.116 INFO: val_f_rmse: 0.010229
##### Step: 232 Learning rate: 4.8828125e-06 #####
2025-06-25 09:04:16.172 INFO: ##### Step: 232 Learning rate: 4.8828125e-06 #####
Epoch 93, Train Loss: 0.1025, Val Loss: 0.1058
2025-06-25 09:04:16.172 INFO: Epoch 93, Train Loss: 0.1025, Val Loss: 0.1058
train_e/atom_mae: 0.000065
2025-06-25 09:04:16.173 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000089
2025-06-25 09:04:16.173 INFO: train_e/atom_rmse: 0.000089
train_f_mae: 0.007313
2025-06-25 09:04:16.176 INFO: train_f_mae: 0.007313
train_f_rmse: 0.010076
2025-06-25 09:04:16.176 INFO: train_f_rmse: 0.010076
val_e/atom_mae: 0.000068
2025-06-25 09:04:16.179 INFO: val_e/atom_mae: 0.000068
val_e/atom_rmse: 0.000096
2025-06-25 09:04:16.179 INFO: val_e/atom_rmse: 0.000096
val_f_mae: 0.007418
2025-06-25 09:04:16.180 INFO: val_f_mae: 0.007418
val_f_rmse: 0.010233
2025-06-25 09:04:16.180 INFO: val_f_rmse: 0.010233
##### Step: 233 Learning rate: 4.8828125e-06 #####
2025-06-25 09:05:00.784 INFO: ##### Step: 233 Learning rate: 4.8828125e-06 #####
Epoch 94, Train Loss: 0.1025, Val Loss: 0.1056
2025-06-25 09:05:00.784 INFO: Epoch 94, Train Loss: 0.1025, Val Loss: 0.1056
train_e/atom_mae: 0.000064
2025-06-25 09:05:00.785 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000088
2025-06-25 09:05:00.785 INFO: train_e/atom_rmse: 0.000088
train_f_mae: 0.007313
2025-06-25 09:05:00.789 INFO: train_f_mae: 0.007313
train_f_rmse: 0.010077
2025-06-25 09:05:00.789 INFO: train_f_rmse: 0.010077
val_e/atom_mae: 0.000060
2025-06-25 09:05:00.791 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000085
2025-06-25 09:05:00.792 INFO: val_e/atom_rmse: 0.000085
val_f_mae: 0.007417
2025-06-25 09:05:00.792 INFO: val_f_mae: 0.007417
val_f_rmse: 0.010232
2025-06-25 09:05:00.792 INFO: val_f_rmse: 0.010232
##### Step: 234 Learning rate: 4.8828125e-06 #####
2025-06-25 09:05:44.919 INFO: ##### Step: 234 Learning rate: 4.8828125e-06 #####
Epoch 95, Train Loss: 0.1024, Val Loss: 0.1055
2025-06-25 09:05:44.920 INFO: Epoch 95, Train Loss: 0.1024, Val Loss: 0.1055
train_e/atom_mae: 0.000062
2025-06-25 09:05:44.921 INFO: train_e/atom_mae: 0.000062
train_e/atom_rmse: 0.000087
2025-06-25 09:05:44.921 INFO: train_e/atom_rmse: 0.000087
train_f_mae: 0.007312
2025-06-25 09:05:44.924 INFO: train_f_mae: 0.007312
train_f_rmse: 0.010074
2025-06-25 09:05:44.924 INFO: train_f_rmse: 0.010074
val_e/atom_mae: 0.000060
2025-06-25 09:05:44.927 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000087
2025-06-25 09:05:44.927 INFO: val_e/atom_rmse: 0.000087
val_f_mae: 0.007413
2025-06-25 09:05:44.928 INFO: val_f_mae: 0.007413
val_f_rmse: 0.010227
2025-06-25 09:05:44.928 INFO: val_f_rmse: 0.010227
##### Step: 235 Learning rate: 4.8828125e-06 #####
2025-06-25 09:06:29.049 INFO: ##### Step: 235 Learning rate: 4.8828125e-06 #####
Epoch 96, Train Loss: 0.1025, Val Loss: 0.1056
2025-06-25 09:06:29.050 INFO: Epoch 96, Train Loss: 0.1025, Val Loss: 0.1056
train_e/atom_mae: 0.000065
2025-06-25 09:06:29.050 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000090
2025-06-25 09:06:29.051 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.007313
2025-06-25 09:06:29.054 INFO: train_f_mae: 0.007313
train_f_rmse: 0.010076
2025-06-25 09:06:29.054 INFO: train_f_rmse: 0.010076
val_e/atom_mae: 0.000062
2025-06-25 09:06:29.057 INFO: val_e/atom_mae: 0.000062
val_e/atom_rmse: 0.000089
2025-06-25 09:06:29.057 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.007414
2025-06-25 09:06:29.058 INFO: val_f_mae: 0.007414
val_f_rmse: 0.010229
2025-06-25 09:06:29.058 INFO: val_f_rmse: 0.010229
##### Step: 236 Learning rate: 4.8828125e-06 #####
2025-06-25 09:07:13.159 INFO: ##### Step: 236 Learning rate: 4.8828125e-06 #####
Epoch 97, Train Loss: 0.1024, Val Loss: 0.1055
2025-06-25 09:07:13.160 INFO: Epoch 97, Train Loss: 0.1024, Val Loss: 0.1055
train_e/atom_mae: 0.000063
2025-06-25 09:07:13.160 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000087
2025-06-25 09:07:13.161 INFO: train_e/atom_rmse: 0.000087
train_f_mae: 0.007312
2025-06-25 09:07:13.164 INFO: train_f_mae: 0.007312
train_f_rmse: 0.010075
2025-06-25 09:07:13.164 INFO: train_f_rmse: 0.010075
val_e/atom_mae: 0.000059
2025-06-25 09:07:13.167 INFO: val_e/atom_mae: 0.000059
val_e/atom_rmse: 0.000085
2025-06-25 09:07:13.167 INFO: val_e/atom_rmse: 0.000085
val_f_mae: 0.007416
2025-06-25 09:07:13.167 INFO: val_f_mae: 0.007416
val_f_rmse: 0.010229
2025-06-25 09:07:13.168 INFO: val_f_rmse: 0.010229
##### Step: 237 Learning rate: 4.8828125e-06 #####
2025-06-25 09:07:57.795 INFO: ##### Step: 237 Learning rate: 4.8828125e-06 #####
Epoch 98, Train Loss: 0.1025, Val Loss: 0.1055
2025-06-25 09:07:57.795 INFO: Epoch 98, Train Loss: 0.1025, Val Loss: 0.1055
train_e/atom_mae: 0.000065
2025-06-25 09:07:57.796 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000090
2025-06-25 09:07:57.796 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.007312
2025-06-25 09:07:57.800 INFO: train_f_mae: 0.007312
train_f_rmse: 0.010075
2025-06-25 09:07:57.800 INFO: train_f_rmse: 0.010075
val_e/atom_mae: 0.000057
2025-06-25 09:07:57.803 INFO: val_e/atom_mae: 0.000057
val_e/atom_rmse: 0.000083
2025-06-25 09:07:57.803 INFO: val_e/atom_rmse: 0.000083
val_f_mae: 0.007416
2025-06-25 09:07:57.803 INFO: val_f_mae: 0.007416
val_f_rmse: 0.010233
2025-06-25 09:07:57.803 INFO: val_f_rmse: 0.010233
##### Step: 238 Learning rate: 4.8828125e-06 #####
2025-06-25 09:08:42.010 INFO: ##### Step: 238 Learning rate: 4.8828125e-06 #####
Epoch 99, Train Loss: 0.1024, Val Loss: 0.1055
2025-06-25 09:08:42.010 INFO: Epoch 99, Train Loss: 0.1024, Val Loss: 0.1055
train_e/atom_mae: 0.000062
2025-06-25 09:08:42.011 INFO: train_e/atom_mae: 0.000062
train_e/atom_rmse: 0.000087
2025-06-25 09:08:42.011 INFO: train_e/atom_rmse: 0.000087
train_f_mae: 0.007313
2025-06-25 09:08:42.014 INFO: train_f_mae: 0.007313
train_f_rmse: 0.010075
2025-06-25 09:08:42.014 INFO: train_f_rmse: 0.010075
val_e/atom_mae: 0.000059
2025-06-25 09:08:42.017 INFO: val_e/atom_mae: 0.000059
val_e/atom_rmse: 0.000084
2025-06-25 09:08:42.017 INFO: val_e/atom_rmse: 0.000084
val_f_mae: 0.007415
2025-06-25 09:08:42.018 INFO: val_f_mae: 0.007415
val_f_rmse: 0.010230
2025-06-25 09:08:42.018 INFO: val_f_rmse: 0.010230
##### Step: 239 Learning rate: 4.8828125e-06 #####
2025-06-25 09:09:26.221 INFO: ##### Step: 239 Learning rate: 4.8828125e-06 #####
Epoch 100, Train Loss: 0.1025, Val Loss: 0.1056
2025-06-25 09:09:26.221 INFO: Epoch 100, Train Loss: 0.1025, Val Loss: 0.1056
train_e/atom_mae: 0.000064
2025-06-25 09:09:26.222 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000089
2025-06-25 09:09:26.222 INFO: train_e/atom_rmse: 0.000089
train_f_mae: 0.007312
2025-06-25 09:09:26.225 INFO: train_f_mae: 0.007312
train_f_rmse: 0.010074
2025-06-25 09:09:26.225 INFO: train_f_rmse: 0.010074
val_e/atom_mae: 0.000058
2025-06-25 09:09:26.228 INFO: val_e/atom_mae: 0.000058
val_e/atom_rmse: 0.000084
2025-06-25 09:09:26.228 INFO: val_e/atom_rmse: 0.000084
val_f_mae: 0.007417
2025-06-25 09:09:26.229 INFO: val_f_mae: 0.007417
val_f_rmse: 0.010232
2025-06-25 09:09:26.229 INFO: val_f_rmse: 0.010232
2025-06-25 09:09:26.241 INFO: Fourth train loop:
##### Step: 240 Learning rate: 2.44140625e-06 #####
2025-06-25 09:10:10.607 INFO: ##### Step: 240 Learning rate: 2.44140625e-06 #####
Epoch 1, Train Loss: 0.1802, Val Loss: 0.1763
2025-06-25 09:10:10.608 INFO: Epoch 1, Train Loss: 0.1802, Val Loss: 0.1763
train_e/atom_mae: 0.000056
2025-06-25 09:10:10.608 INFO: train_e/atom_mae: 0.000056
train_e/atom_rmse: 0.000080
2025-06-25 09:10:10.609 INFO: train_e/atom_rmse: 0.000080
train_f_mae: 0.007320
2025-06-25 09:10:10.612 INFO: train_f_mae: 0.007320
train_f_rmse: 0.010090
2025-06-25 09:10:10.612 INFO: train_f_rmse: 0.010090
val_e/atom_mae: 0.000050
2025-06-25 09:10:10.615 INFO: val_e/atom_mae: 0.000050
val_e/atom_rmse: 0.000076
2025-06-25 09:10:10.615 INFO: val_e/atom_rmse: 0.000076
val_f_mae: 0.007445
2025-06-25 09:10:10.615 INFO: val_f_mae: 0.007445
val_f_rmse: 0.010287
2025-06-25 09:10:10.616 INFO: val_f_rmse: 0.010287
##### Step: 241 Learning rate: 2.44140625e-06 #####
2025-06-25 09:10:54.824 INFO: ##### Step: 241 Learning rate: 2.44140625e-06 #####
Epoch 2, Train Loss: 0.1679, Val Loss: 0.1817
2025-06-25 09:10:54.825 INFO: Epoch 2, Train Loss: 0.1679, Val Loss: 0.1817
train_e/atom_mae: 0.000046
2025-06-25 09:10:54.825 INFO: train_e/atom_mae: 0.000046
train_e/atom_rmse: 0.000073
2025-06-25 09:10:54.826 INFO: train_e/atom_rmse: 0.000073
train_f_mae: 0.007342
2025-06-25 09:10:54.829 INFO: train_f_mae: 0.007342
train_f_rmse: 0.010130
2025-06-25 09:10:54.829 INFO: train_f_rmse: 0.010130
val_e/atom_mae: 0.000047
2025-06-25 09:10:54.832 INFO: val_e/atom_mae: 0.000047
val_e/atom_rmse: 0.000079
2025-06-25 09:10:54.832 INFO: val_e/atom_rmse: 0.000079
val_f_mae: 0.007459
2025-06-25 09:10:54.832 INFO: val_f_mae: 0.007459
val_f_rmse: 0.010316
2025-06-25 09:10:54.833 INFO: val_f_rmse: 0.010316
##### Step: 242 Learning rate: 2.44140625e-06 #####
2025-06-25 09:11:39.058 INFO: ##### Step: 242 Learning rate: 2.44140625e-06 #####
Epoch 3, Train Loss: 0.1651, Val Loss: 0.1751
2025-06-25 09:11:39.058 INFO: Epoch 3, Train Loss: 0.1651, Val Loss: 0.1751
train_e/atom_mae: 0.000042
2025-06-25 09:11:39.059 INFO: train_e/atom_mae: 0.000042
train_e/atom_rmse: 0.000072
2025-06-25 09:11:39.059 INFO: train_e/atom_rmse: 0.000072
train_f_mae: 0.007356
2025-06-25 09:11:39.063 INFO: train_f_mae: 0.007356
train_f_rmse: 0.010154
2025-06-25 09:11:39.063 INFO: train_f_rmse: 0.010154
val_e/atom_mae: 0.000041
2025-06-25 09:11:39.065 INFO: val_e/atom_mae: 0.000041
val_e/atom_rmse: 0.000075
2025-06-25 09:11:39.066 INFO: val_e/atom_rmse: 0.000075
val_f_mae: 0.007474
2025-06-25 09:11:39.066 INFO: val_f_mae: 0.007474
val_f_rmse: 0.010343
2025-06-25 09:11:39.066 INFO: val_f_rmse: 0.010343
##### Step: 243 Learning rate: 2.44140625e-06 #####
2025-06-25 09:12:23.289 INFO: ##### Step: 243 Learning rate: 2.44140625e-06 #####
Epoch 4, Train Loss: 0.1637, Val Loss: 0.1740
2025-06-25 09:12:23.289 INFO: Epoch 4, Train Loss: 0.1637, Val Loss: 0.1740
train_e/atom_mae: 0.000040
2025-06-25 09:12:23.290 INFO: train_e/atom_mae: 0.000040
train_e/atom_rmse: 0.000071
2025-06-25 09:12:23.290 INFO: train_e/atom_rmse: 0.000071
train_f_mae: 0.007366
2025-06-25 09:12:23.293 INFO: train_f_mae: 0.007366
train_f_rmse: 0.010170
2025-06-25 09:12:23.294 INFO: train_f_rmse: 0.010170
val_e/atom_mae: 0.000041
2025-06-25 09:12:23.296 INFO: val_e/atom_mae: 0.000041
val_e/atom_rmse: 0.000074
2025-06-25 09:12:23.296 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007475
2025-06-25 09:12:23.297 INFO: val_f_mae: 0.007475
val_f_rmse: 0.010339
2025-06-25 09:12:23.297 INFO: val_f_rmse: 0.010339
##### Step: 244 Learning rate: 2.44140625e-06 #####
2025-06-25 09:13:07.547 INFO: ##### Step: 244 Learning rate: 2.44140625e-06 #####
Epoch 5, Train Loss: 0.1632, Val Loss: 0.1812
2025-06-25 09:13:07.547 INFO: Epoch 5, Train Loss: 0.1632, Val Loss: 0.1812
train_e/atom_mae: 0.000039
2025-06-25 09:13:07.548 INFO: train_e/atom_mae: 0.000039
train_e/atom_rmse: 0.000070
2025-06-25 09:13:07.548 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.007369
2025-06-25 09:13:07.552 INFO: train_f_mae: 0.007369
train_f_rmse: 0.010171
2025-06-25 09:13:07.552 INFO: train_f_rmse: 0.010171
val_e/atom_mae: 0.000042
2025-06-25 09:13:07.555 INFO: val_e/atom_mae: 0.000042
val_e/atom_rmse: 0.000078
2025-06-25 09:13:07.555 INFO: val_e/atom_rmse: 0.000078
val_f_mae: 0.007481
2025-06-25 09:13:07.555 INFO: val_f_mae: 0.007481
val_f_rmse: 0.010346
2025-06-25 09:13:07.555 INFO: val_f_rmse: 0.010346
##### Step: 245 Learning rate: 2.44140625e-06 #####
2025-06-25 09:13:51.857 INFO: ##### Step: 245 Learning rate: 2.44140625e-06 #####
Epoch 6, Train Loss: 0.1639, Val Loss: 0.1845
2025-06-25 09:13:51.857 INFO: Epoch 6, Train Loss: 0.1639, Val Loss: 0.1845
train_e/atom_mae: 0.000040
2025-06-25 09:13:51.858 INFO: train_e/atom_mae: 0.000040
train_e/atom_rmse: 0.000071
2025-06-25 09:13:51.858 INFO: train_e/atom_rmse: 0.000071
train_f_mae: 0.007373
2025-06-25 09:13:51.862 INFO: train_f_mae: 0.007373
train_f_rmse: 0.010176
2025-06-25 09:13:51.862 INFO: train_f_rmse: 0.010176
val_e/atom_mae: 0.000044
2025-06-25 09:13:51.864 INFO: val_e/atom_mae: 0.000044
val_e/atom_rmse: 0.000080
2025-06-25 09:13:51.865 INFO: val_e/atom_rmse: 0.000080
val_f_mae: 0.007484
2025-06-25 09:13:51.865 INFO: val_f_mae: 0.007484
val_f_rmse: 0.010350
2025-06-25 09:13:51.865 INFO: val_f_rmse: 0.010350
##### Step: 246 Learning rate: 2.44140625e-06 #####
2025-06-25 09:14:36.156 INFO: ##### Step: 246 Learning rate: 2.44140625e-06 #####
Epoch 7, Train Loss: 0.1636, Val Loss: 0.1766
2025-06-25 09:14:36.157 INFO: Epoch 7, Train Loss: 0.1636, Val Loss: 0.1766
train_e/atom_mae: 0.000039
2025-06-25 09:14:36.157 INFO: train_e/atom_mae: 0.000039
train_e/atom_rmse: 0.000070
2025-06-25 09:14:36.158 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.007375
2025-06-25 09:14:36.161 INFO: train_f_mae: 0.007375
train_f_rmse: 0.010180
2025-06-25 09:14:36.161 INFO: train_f_rmse: 0.010180
val_e/atom_mae: 0.000041
2025-06-25 09:14:36.164 INFO: val_e/atom_mae: 0.000041
val_e/atom_rmse: 0.000076
2025-06-25 09:14:36.164 INFO: val_e/atom_rmse: 0.000076
val_f_mae: 0.007485
2025-06-25 09:14:36.165 INFO: val_f_mae: 0.007485
val_f_rmse: 0.010351
2025-06-25 09:14:36.165 INFO: val_f_rmse: 0.010351
##### Step: 247 Learning rate: 2.44140625e-06 #####
2025-06-25 09:15:20.420 INFO: ##### Step: 247 Learning rate: 2.44140625e-06 #####
Epoch 8, Train Loss: 0.1627, Val Loss: 0.1705
2025-06-25 09:15:20.420 INFO: Epoch 8, Train Loss: 0.1627, Val Loss: 0.1705
train_e/atom_mae: 0.000039
2025-06-25 09:15:20.421 INFO: train_e/atom_mae: 0.000039
train_e/atom_rmse: 0.000070
2025-06-25 09:15:20.421 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.007376
2025-06-25 09:15:20.424 INFO: train_f_mae: 0.007376
train_f_rmse: 0.010179
2025-06-25 09:15:20.425 INFO: train_f_rmse: 0.010179
val_e/atom_mae: 0.000038
2025-06-25 09:15:20.427 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000072
2025-06-25 09:15:20.428 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007486
2025-06-25 09:15:20.428 INFO: val_f_mae: 0.007486
val_f_rmse: 0.010351
2025-06-25 09:15:20.428 INFO: val_f_rmse: 0.010351
##### Step: 248 Learning rate: 2.44140625e-06 #####
2025-06-25 09:16:04.657 INFO: ##### Step: 248 Learning rate: 2.44140625e-06 #####
Epoch 9, Train Loss: 0.1639, Val Loss: 0.1729
2025-06-25 09:16:04.658 INFO: Epoch 9, Train Loss: 0.1639, Val Loss: 0.1729
train_e/atom_mae: 0.000040
2025-06-25 09:16:04.659 INFO: train_e/atom_mae: 0.000040
train_e/atom_rmse: 0.000071
2025-06-25 09:16:04.659 INFO: train_e/atom_rmse: 0.000071
train_f_mae: 0.007375
2025-06-25 09:16:04.662 INFO: train_f_mae: 0.007375
train_f_rmse: 0.010177
2025-06-25 09:16:04.662 INFO: train_f_rmse: 0.010177
val_e/atom_mae: 0.000042
2025-06-25 09:16:04.665 INFO: val_e/atom_mae: 0.000042
val_e/atom_rmse: 0.000074
2025-06-25 09:16:04.665 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007489
2025-06-25 09:16:04.666 INFO: val_f_mae: 0.007489
val_f_rmse: 0.010356
2025-06-25 09:16:04.666 INFO: val_f_rmse: 0.010356
##### Step: 249 Learning rate: 2.44140625e-06 #####
2025-06-25 09:16:48.903 INFO: ##### Step: 249 Learning rate: 2.44140625e-06 #####
Epoch 10, Train Loss: 0.1645, Val Loss: 0.1717
2025-06-25 09:16:48.903 INFO: Epoch 10, Train Loss: 0.1645, Val Loss: 0.1717
train_e/atom_mae: 0.000041
2025-06-25 09:16:48.904 INFO: train_e/atom_mae: 0.000041
train_e/atom_rmse: 0.000071
2025-06-25 09:16:48.904 INFO: train_e/atom_rmse: 0.000071
train_f_mae: 0.007379
2025-06-25 09:16:48.908 INFO: train_f_mae: 0.007379
train_f_rmse: 0.010184
2025-06-25 09:16:48.908 INFO: train_f_rmse: 0.010184
val_e/atom_mae: 0.000038
2025-06-25 09:16:48.910 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000073
2025-06-25 09:16:48.911 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007491
2025-06-25 09:16:48.911 INFO: val_f_mae: 0.007491
val_f_rmse: 0.010358
2025-06-25 09:16:48.911 INFO: val_f_rmse: 0.010358
##### Step: 250 Learning rate: 2.44140625e-06 #####
2025-06-25 09:17:33.181 INFO: ##### Step: 250 Learning rate: 2.44140625e-06 #####
Epoch 11, Train Loss: 0.1622, Val Loss: 0.1820
2025-06-25 09:17:33.182 INFO: Epoch 11, Train Loss: 0.1622, Val Loss: 0.1820
train_e/atom_mae: 0.000039
2025-06-25 09:17:33.183 INFO: train_e/atom_mae: 0.000039
train_e/atom_rmse: 0.000070
2025-06-25 09:17:33.183 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.007379
2025-06-25 09:17:33.186 INFO: train_f_mae: 0.007379
train_f_rmse: 0.010182
2025-06-25 09:17:33.186 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000044
2025-06-25 09:17:33.189 INFO: val_e/atom_mae: 0.000044
val_e/atom_rmse: 0.000079
2025-06-25 09:17:33.189 INFO: val_e/atom_rmse: 0.000079
val_f_mae: 0.007490
2025-06-25 09:17:33.190 INFO: val_f_mae: 0.007490
val_f_rmse: 0.010354
2025-06-25 09:17:33.190 INFO: val_f_rmse: 0.010354
##### Step: 251 Learning rate: 2.44140625e-06 #####
2025-06-25 09:18:17.403 INFO: ##### Step: 251 Learning rate: 2.44140625e-06 #####
Epoch 12, Train Loss: 0.1626, Val Loss: 0.1710
2025-06-25 09:18:17.404 INFO: Epoch 12, Train Loss: 0.1626, Val Loss: 0.1710
train_e/atom_mae: 0.000039
2025-06-25 09:18:17.404 INFO: train_e/atom_mae: 0.000039
train_e/atom_rmse: 0.000070
2025-06-25 09:18:17.405 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.007378
2025-06-25 09:18:17.408 INFO: train_f_mae: 0.007378
train_f_rmse: 0.010178
2025-06-25 09:18:17.408 INFO: train_f_rmse: 0.010178
val_e/atom_mae: 0.000037
2025-06-25 09:18:17.411 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:18:17.411 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007488
2025-06-25 09:18:17.411 INFO: val_f_mae: 0.007488
val_f_rmse: 0.010348
2025-06-25 09:18:17.412 INFO: val_f_rmse: 0.010348
##### Step: 252 Learning rate: 2.44140625e-06 #####
2025-06-25 09:19:01.681 INFO: ##### Step: 252 Learning rate: 2.44140625e-06 #####
Epoch 13, Train Loss: 0.1626, Val Loss: 0.1709
2025-06-25 09:19:01.681 INFO: Epoch 13, Train Loss: 0.1626, Val Loss: 0.1709
train_e/atom_mae: 0.000039
2025-06-25 09:19:01.682 INFO: train_e/atom_mae: 0.000039
train_e/atom_rmse: 0.000070
2025-06-25 09:19:01.682 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.007379
2025-06-25 09:19:01.686 INFO: train_f_mae: 0.007379
train_f_rmse: 0.010180
2025-06-25 09:19:01.686 INFO: train_f_rmse: 0.010180
val_e/atom_mae: 0.000038
2025-06-25 09:19:01.688 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000073
2025-06-25 09:19:01.689 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007489
2025-06-25 09:19:01.689 INFO: val_f_mae: 0.007489
val_f_rmse: 0.010351
2025-06-25 09:19:01.689 INFO: val_f_rmse: 0.010351
##### Step: 253 Learning rate: 2.44140625e-06 #####
2025-06-25 09:19:45.917 INFO: ##### Step: 253 Learning rate: 2.44140625e-06 #####
Epoch 14, Train Loss: 0.1618, Val Loss: 0.1894
2025-06-25 09:19:45.917 INFO: Epoch 14, Train Loss: 0.1618, Val Loss: 0.1894
train_e/atom_mae: 0.000039
2025-06-25 09:19:45.918 INFO: train_e/atom_mae: 0.000039
train_e/atom_rmse: 0.000069
2025-06-25 09:19:45.918 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.007379
2025-06-25 09:19:45.922 INFO: train_f_mae: 0.007379
train_f_rmse: 0.010179
2025-06-25 09:19:45.922 INFO: train_f_rmse: 0.010179
val_e/atom_mae: 0.000048
2025-06-25 09:19:45.924 INFO: val_e/atom_mae: 0.000048
val_e/atom_rmse: 0.000082
2025-06-25 09:19:45.925 INFO: val_e/atom_rmse: 0.000082
val_f_mae: 0.007493
2025-06-25 09:19:45.925 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010356
2025-06-25 09:19:45.925 INFO: val_f_rmse: 0.010356
##### Step: 254 Learning rate: 2.44140625e-06 #####
2025-06-25 09:20:30.162 INFO: ##### Step: 254 Learning rate: 2.44140625e-06 #####
Epoch 15, Train Loss: 0.1627, Val Loss: 0.1724
2025-06-25 09:20:30.162 INFO: Epoch 15, Train Loss: 0.1627, Val Loss: 0.1724
train_e/atom_mae: 0.000040
2025-06-25 09:20:30.163 INFO: train_e/atom_mae: 0.000040
train_e/atom_rmse: 0.000070
2025-06-25 09:20:30.163 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.007380
2025-06-25 09:20:30.167 INFO: train_f_mae: 0.007380
train_f_rmse: 0.010180
2025-06-25 09:20:30.167 INFO: train_f_rmse: 0.010180
val_e/atom_mae: 0.000038
2025-06-25 09:20:30.170 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000073
2025-06-25 09:20:30.170 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007490
2025-06-25 09:20:30.170 INFO: val_f_mae: 0.007490
val_f_rmse: 0.010351
2025-06-25 09:20:30.171 INFO: val_f_rmse: 0.010351
##### Step: 255 Learning rate: 2.44140625e-06 #####
2025-06-25 09:21:14.438 INFO: ##### Step: 255 Learning rate: 2.44140625e-06 #####
Epoch 16, Train Loss: 0.1605, Val Loss: 0.1716
2025-06-25 09:21:14.438 INFO: Epoch 16, Train Loss: 0.1605, Val Loss: 0.1716
train_e/atom_mae: 0.000038
2025-06-25 09:21:14.439 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000069
2025-06-25 09:21:14.439 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.007379
2025-06-25 09:21:14.443 INFO: train_f_mae: 0.007379
train_f_rmse: 0.010179
2025-06-25 09:21:14.443 INFO: train_f_rmse: 0.010179
val_e/atom_mae: 0.000038
2025-06-25 09:21:14.446 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000073
2025-06-25 09:21:14.446 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 09:21:14.446 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010359
2025-06-25 09:21:14.446 INFO: val_f_rmse: 0.010359
##### Step: 256 Learning rate: 2.44140625e-06 #####
2025-06-25 09:21:58.703 INFO: ##### Step: 256 Learning rate: 2.44140625e-06 #####
Epoch 17, Train Loss: 0.1619, Val Loss: 0.1707
2025-06-25 09:21:58.703 INFO: Epoch 17, Train Loss: 0.1619, Val Loss: 0.1707
train_e/atom_mae: 0.000039
2025-06-25 09:21:58.704 INFO: train_e/atom_mae: 0.000039
train_e/atom_rmse: 0.000069
2025-06-25 09:21:58.704 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.007382
2025-06-25 09:21:58.708 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010185
2025-06-25 09:21:58.708 INFO: train_f_rmse: 0.010185
val_e/atom_mae: 0.000037
2025-06-25 09:21:58.710 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:21:58.711 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007497
2025-06-25 09:21:58.711 INFO: val_f_mae: 0.007497
val_f_rmse: 0.010365
2025-06-25 09:21:58.711 INFO: val_f_rmse: 0.010365
##### Step: 257 Learning rate: 2.44140625e-06 #####
2025-06-25 09:22:42.925 INFO: ##### Step: 257 Learning rate: 2.44140625e-06 #####
Epoch 18, Train Loss: 0.1605, Val Loss: 0.1722
2025-06-25 09:22:42.926 INFO: Epoch 18, Train Loss: 0.1605, Val Loss: 0.1722
train_e/atom_mae: 0.000038
2025-06-25 09:22:42.927 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000068
2025-06-25 09:22:42.927 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007383
2025-06-25 09:22:42.930 INFO: train_f_mae: 0.007383
train_f_rmse: 0.010186
2025-06-25 09:22:42.930 INFO: train_f_rmse: 0.010186
val_e/atom_mae: 0.000039
2025-06-25 09:22:42.933 INFO: val_e/atom_mae: 0.000039
val_e/atom_rmse: 0.000073
2025-06-25 09:22:42.933 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007498
2025-06-25 09:22:42.934 INFO: val_f_mae: 0.007498
val_f_rmse: 0.010366
2025-06-25 09:22:42.934 INFO: val_f_rmse: 0.010366
##### Step: 258 Learning rate: 2.44140625e-06 #####
2025-06-25 09:23:27.159 INFO: ##### Step: 258 Learning rate: 2.44140625e-06 #####
Epoch 19, Train Loss: 0.1681, Val Loss: 0.1700
2025-06-25 09:23:27.159 INFO: Epoch 19, Train Loss: 0.1681, Val Loss: 0.1700
train_e/atom_mae: 0.000044
2025-06-25 09:23:27.160 INFO: train_e/atom_mae: 0.000044
train_e/atom_rmse: 0.000073
2025-06-25 09:23:27.160 INFO: train_e/atom_rmse: 0.000073
train_f_mae: 0.007383
2025-06-25 09:23:27.164 INFO: train_f_mae: 0.007383
train_f_rmse: 0.010186
2025-06-25 09:23:27.164 INFO: train_f_rmse: 0.010186
val_e/atom_mae: 0.000038
2025-06-25 09:23:27.167 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000072
2025-06-25 09:23:27.167 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007495
2025-06-25 09:23:27.167 INFO: val_f_mae: 0.007495
val_f_rmse: 0.010360
2025-06-25 09:23:27.167 INFO: val_f_rmse: 0.010360
##### Step: 259 Learning rate: 2.44140625e-06 #####
2025-06-25 09:24:11.275 INFO: ##### Step: 259 Learning rate: 2.44140625e-06 #####
Epoch 20, Train Loss: 0.1619, Val Loss: 0.1757
2025-06-25 09:24:11.276 INFO: Epoch 20, Train Loss: 0.1619, Val Loss: 0.1757
train_e/atom_mae: 0.000038
2025-06-25 09:24:11.276 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000069
2025-06-25 09:24:11.277 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.007382
2025-06-25 09:24:11.280 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010185
2025-06-25 09:24:11.280 INFO: train_f_rmse: 0.010185
val_e/atom_mae: 0.000042
2025-06-25 09:24:11.283 INFO: val_e/atom_mae: 0.000042
val_e/atom_rmse: 0.000075
2025-06-25 09:24:11.283 INFO: val_e/atom_rmse: 0.000075
val_f_mae: 0.007496
2025-06-25 09:24:11.283 INFO: val_f_mae: 0.007496
val_f_rmse: 0.010360
2025-06-25 09:24:11.284 INFO: val_f_rmse: 0.010360
##### Step: 260 Learning rate: 1.220703125e-06 #####
2025-06-25 09:24:55.916 INFO: ##### Step: 260 Learning rate: 1.220703125e-06 #####
Epoch 21, Train Loss: 0.1609, Val Loss: 0.1703
2025-06-25 09:24:55.916 INFO: Epoch 21, Train Loss: 0.1609, Val Loss: 0.1703
train_e/atom_mae: 0.000038
2025-06-25 09:24:55.917 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000069
2025-06-25 09:24:55.917 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.007382
2025-06-25 09:24:55.921 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:24:55.921 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:24:55.923 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:24:55.924 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007495
2025-06-25 09:24:55.924 INFO: val_f_mae: 0.007495
val_f_rmse: 0.010358
2025-06-25 09:24:55.924 INFO: val_f_rmse: 0.010358
##### Step: 261 Learning rate: 1.220703125e-06 #####
2025-06-25 09:25:40.141 INFO: ##### Step: 261 Learning rate: 1.220703125e-06 #####
Epoch 22, Train Loss: 0.1593, Val Loss: 0.1735
2025-06-25 09:25:40.141 INFO: Epoch 22, Train Loss: 0.1593, Val Loss: 0.1735
train_e/atom_mae: 0.000037
2025-06-25 09:25:40.142 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:25:40.142 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:25:40.146 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010184
2025-06-25 09:25:40.146 INFO: train_f_rmse: 0.010184
val_e/atom_mae: 0.000038
2025-06-25 09:25:40.148 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000074
2025-06-25 09:25:40.149 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007497
2025-06-25 09:25:40.149 INFO: val_f_mae: 0.007497
val_f_rmse: 0.010362
2025-06-25 09:25:40.149 INFO: val_f_rmse: 0.010362
##### Step: 262 Learning rate: 1.220703125e-06 #####
2025-06-25 09:26:24.369 INFO: ##### Step: 262 Learning rate: 1.220703125e-06 #####
Epoch 23, Train Loss: 0.1609, Val Loss: 0.1733
2025-06-25 09:26:24.370 INFO: Epoch 23, Train Loss: 0.1609, Val Loss: 0.1733
train_e/atom_mae: 0.000037
2025-06-25 09:26:24.371 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000069
2025-06-25 09:26:24.371 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.007382
2025-06-25 09:26:24.374 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010184
2025-06-25 09:26:24.374 INFO: train_f_rmse: 0.010184
val_e/atom_mae: 0.000042
2025-06-25 09:26:24.377 INFO: val_e/atom_mae: 0.000042
val_e/atom_rmse: 0.000074
2025-06-25 09:26:24.377 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007495
2025-06-25 09:26:24.377 INFO: val_f_mae: 0.007495
val_f_rmse: 0.010359
2025-06-25 09:26:24.378 INFO: val_f_rmse: 0.010359
##### Step: 263 Learning rate: 1.220703125e-06 #####
2025-06-25 09:27:08.573 INFO: ##### Step: 263 Learning rate: 1.220703125e-06 #####
Epoch 24, Train Loss: 0.1610, Val Loss: 0.1770
2025-06-25 09:27:08.573 INFO: Epoch 24, Train Loss: 0.1610, Val Loss: 0.1770
train_e/atom_mae: 0.000038
2025-06-25 09:27:08.574 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000069
2025-06-25 09:27:08.574 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.007383
2025-06-25 09:27:08.578 INFO: train_f_mae: 0.007383
train_f_rmse: 0.010183
2025-06-25 09:27:08.578 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000041
2025-06-25 09:27:08.580 INFO: val_e/atom_mae: 0.000041
val_e/atom_rmse: 0.000076
2025-06-25 09:27:08.581 INFO: val_e/atom_rmse: 0.000076
val_f_mae: 0.007495
2025-06-25 09:27:08.581 INFO: val_f_mae: 0.007495
val_f_rmse: 0.010359
2025-06-25 09:27:08.581 INFO: val_f_rmse: 0.010359
##### Step: 264 Learning rate: 1.220703125e-06 #####
2025-06-25 09:27:52.809 INFO: ##### Step: 264 Learning rate: 1.220703125e-06 #####
Epoch 25, Train Loss: 0.1601, Val Loss: 0.1702
2025-06-25 09:27:52.810 INFO: Epoch 25, Train Loss: 0.1601, Val Loss: 0.1702
train_e/atom_mae: 0.000037
2025-06-25 09:27:52.810 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:27:52.811 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007384
2025-06-25 09:27:52.814 INFO: train_f_mae: 0.007384
train_f_rmse: 0.010185
2025-06-25 09:27:52.814 INFO: train_f_rmse: 0.010185
val_e/atom_mae: 0.000038
2025-06-25 09:27:52.817 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000072
2025-06-25 09:27:52.817 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007496
2025-06-25 09:27:52.817 INFO: val_f_mae: 0.007496
val_f_rmse: 0.010360
2025-06-25 09:27:52.817 INFO: val_f_rmse: 0.010360
##### Step: 265 Learning rate: 1.220703125e-06 #####
2025-06-25 09:28:37.002 INFO: ##### Step: 265 Learning rate: 1.220703125e-06 #####
Epoch 26, Train Loss: 0.1612, Val Loss: 0.1754
2025-06-25 09:28:37.003 INFO: Epoch 26, Train Loss: 0.1612, Val Loss: 0.1754
train_e/atom_mae: 0.000038
2025-06-25 09:28:37.003 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000069
2025-06-25 09:28:37.004 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.007383
2025-06-25 09:28:37.007 INFO: train_f_mae: 0.007383
train_f_rmse: 0.010184
2025-06-25 09:28:37.007 INFO: train_f_rmse: 0.010184
val_e/atom_mae: 0.000038
2025-06-25 09:28:37.010 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000075
2025-06-25 09:28:37.010 INFO: val_e/atom_rmse: 0.000075
val_f_mae: 0.007496
2025-06-25 09:28:37.010 INFO: val_f_mae: 0.007496
val_f_rmse: 0.010358
2025-06-25 09:28:37.011 INFO: val_f_rmse: 0.010358
##### Step: 266 Learning rate: 1.220703125e-06 #####
2025-06-25 09:29:21.217 INFO: ##### Step: 266 Learning rate: 1.220703125e-06 #####
Epoch 27, Train Loss: 0.1605, Val Loss: 0.1710
2025-06-25 09:29:21.218 INFO: Epoch 27, Train Loss: 0.1605, Val Loss: 0.1710
train_e/atom_mae: 0.000037
2025-06-25 09:29:21.218 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000069
2025-06-25 09:29:21.219 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.007384
2025-06-25 09:29:21.222 INFO: train_f_mae: 0.007384
train_f_rmse: 0.010184
2025-06-25 09:29:21.222 INFO: train_f_rmse: 0.010184
val_e/atom_mae: 0.000037
2025-06-25 09:29:21.225 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:29:21.225 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007495
2025-06-25 09:29:21.225 INFO: val_f_mae: 0.007495
val_f_rmse: 0.010359
2025-06-25 09:29:21.226 INFO: val_f_rmse: 0.010359
##### Step: 267 Learning rate: 1.220703125e-06 #####
2025-06-25 09:30:05.406 INFO: ##### Step: 267 Learning rate: 1.220703125e-06 #####
Epoch 28, Train Loss: 0.1591, Val Loss: 0.1721
2025-06-25 09:30:05.406 INFO: Epoch 28, Train Loss: 0.1591, Val Loss: 0.1721
train_e/atom_mae: 0.000037
2025-06-25 09:30:05.407 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:30:05.407 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007383
2025-06-25 09:30:05.410 INFO: train_f_mae: 0.007383
train_f_rmse: 0.010183
2025-06-25 09:30:05.410 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000037
2025-06-25 09:30:05.413 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:30:05.413 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007495
2025-06-25 09:30:05.414 INFO: val_f_mae: 0.007495
val_f_rmse: 0.010357
2025-06-25 09:30:05.414 INFO: val_f_rmse: 0.010357
##### Step: 268 Learning rate: 1.220703125e-06 #####
2025-06-25 09:30:49.634 INFO: ##### Step: 268 Learning rate: 1.220703125e-06 #####
Epoch 29, Train Loss: 0.1604, Val Loss: 0.1794
2025-06-25 09:30:49.634 INFO: Epoch 29, Train Loss: 0.1604, Val Loss: 0.1794
train_e/atom_mae: 0.000037
2025-06-25 09:30:49.635 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:30:49.635 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007383
2025-06-25 09:30:49.639 INFO: train_f_mae: 0.007383
train_f_rmse: 0.010183
2025-06-25 09:30:49.639 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000041
2025-06-25 09:30:49.641 INFO: val_e/atom_mae: 0.000041
val_e/atom_rmse: 0.000077
2025-06-25 09:30:49.642 INFO: val_e/atom_rmse: 0.000077
val_f_mae: 0.007494
2025-06-25 09:30:49.642 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010357
2025-06-25 09:30:49.642 INFO: val_f_rmse: 0.010357
##### Step: 269 Learning rate: 1.220703125e-06 #####
2025-06-25 09:31:33.870 INFO: ##### Step: 269 Learning rate: 1.220703125e-06 #####
Epoch 30, Train Loss: 0.1597, Val Loss: 0.1727
2025-06-25 09:31:33.870 INFO: Epoch 30, Train Loss: 0.1597, Val Loss: 0.1727
train_e/atom_mae: 0.000037
2025-06-25 09:31:33.871 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:31:33.871 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:31:33.875 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010181
2025-06-25 09:31:33.875 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000041
2025-06-25 09:31:33.878 INFO: val_e/atom_mae: 0.000041
val_e/atom_rmse: 0.000074
2025-06-25 09:31:33.878 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007494
2025-06-25 09:31:33.878 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 09:31:33.878 INFO: val_f_rmse: 0.010356
##### Step: 270 Learning rate: 1.220703125e-06 #####
2025-06-25 09:32:18.088 INFO: ##### Step: 270 Learning rate: 1.220703125e-06 #####
Epoch 31, Train Loss: 0.1595, Val Loss: 0.1779
2025-06-25 09:32:18.088 INFO: Epoch 31, Train Loss: 0.1595, Val Loss: 0.1779
train_e/atom_mae: 0.000037
2025-06-25 09:32:18.089 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:32:18.089 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:32:18.093 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:32:18.093 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000040
2025-06-25 09:32:18.095 INFO: val_e/atom_mae: 0.000040
val_e/atom_rmse: 0.000076
2025-06-25 09:32:18.096 INFO: val_e/atom_rmse: 0.000076
val_f_mae: 0.007494
2025-06-25 09:32:18.096 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010357
2025-06-25 09:32:18.096 INFO: val_f_rmse: 0.010357
##### Step: 271 Learning rate: 1.220703125e-06 #####
2025-06-25 09:33:02.304 INFO: ##### Step: 271 Learning rate: 1.220703125e-06 #####
Epoch 32, Train Loss: 0.1611, Val Loss: 0.1696
2025-06-25 09:33:02.304 INFO: Epoch 32, Train Loss: 0.1611, Val Loss: 0.1696
train_e/atom_mae: 0.000038
2025-06-25 09:33:02.305 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000069
2025-06-25 09:33:02.305 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.007382
2025-06-25 09:33:02.308 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010181
2025-06-25 09:33:02.309 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000037
2025-06-25 09:33:02.311 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:33:02.311 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 09:33:02.312 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 09:33:02.312 INFO: val_f_rmse: 0.010356
##### Step: 272 Learning rate: 1.220703125e-06 #####
2025-06-25 09:33:46.483 INFO: ##### Step: 272 Learning rate: 1.220703125e-06 #####
Epoch 33, Train Loss: 0.1603, Val Loss: 0.1709
2025-06-25 09:33:46.483 INFO: Epoch 33, Train Loss: 0.1603, Val Loss: 0.1709
train_e/atom_mae: 0.000037
2025-06-25 09:33:46.484 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:33:46.484 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007381
2025-06-25 09:33:46.487 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010181
2025-06-25 09:33:46.487 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000037
2025-06-25 09:33:46.490 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:33:46.490 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007495
2025-06-25 09:33:46.491 INFO: val_f_mae: 0.007495
val_f_rmse: 0.010356
2025-06-25 09:33:46.491 INFO: val_f_rmse: 0.010356
##### Step: 273 Learning rate: 1.220703125e-06 #####
2025-06-25 09:34:30.696 INFO: ##### Step: 273 Learning rate: 1.220703125e-06 #####
Epoch 34, Train Loss: 0.1599, Val Loss: 0.1704
2025-06-25 09:34:30.696 INFO: Epoch 34, Train Loss: 0.1599, Val Loss: 0.1704
train_e/atom_mae: 0.000037
2025-06-25 09:34:30.697 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:34:30.697 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:34:30.700 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010181
2025-06-25 09:34:30.701 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000037
2025-06-25 09:34:30.703 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:34:30.703 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 09:34:30.704 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 09:34:30.704 INFO: val_f_rmse: 0.010356
##### Step: 274 Learning rate: 1.220703125e-06 #####
2025-06-25 09:35:14.933 INFO: ##### Step: 274 Learning rate: 1.220703125e-06 #####
Epoch 35, Train Loss: 0.1601, Val Loss: 0.1751
2025-06-25 09:35:14.933 INFO: Epoch 35, Train Loss: 0.1601, Val Loss: 0.1751
train_e/atom_mae: 0.000037
2025-06-25 09:35:14.934 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:35:14.934 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007381
2025-06-25 09:35:14.937 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010181
2025-06-25 09:35:14.938 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000039
2025-06-25 09:35:14.940 INFO: val_e/atom_mae: 0.000039
val_e/atom_rmse: 0.000075
2025-06-25 09:35:14.940 INFO: val_e/atom_rmse: 0.000075
val_f_mae: 0.007495
2025-06-25 09:35:14.941 INFO: val_f_mae: 0.007495
val_f_rmse: 0.010358
2025-06-25 09:35:14.941 INFO: val_f_rmse: 0.010358
##### Step: 275 Learning rate: 1.220703125e-06 #####
2025-06-25 09:35:59.179 INFO: ##### Step: 275 Learning rate: 1.220703125e-06 #####
Epoch 36, Train Loss: 0.1592, Val Loss: 0.1733
2025-06-25 09:35:59.179 INFO: Epoch 36, Train Loss: 0.1592, Val Loss: 0.1733
train_e/atom_mae: 0.000037
2025-06-25 09:35:59.180 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:35:59.180 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007381
2025-06-25 09:35:59.183 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010181
2025-06-25 09:35:59.184 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000038
2025-06-25 09:35:59.186 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000074
2025-06-25 09:35:59.186 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007491
2025-06-25 09:35:59.187 INFO: val_f_mae: 0.007491
val_f_rmse: 0.010351
2025-06-25 09:35:59.187 INFO: val_f_rmse: 0.010351
##### Step: 276 Learning rate: 1.220703125e-06 #####
2025-06-25 09:36:43.419 INFO: ##### Step: 276 Learning rate: 1.220703125e-06 #####
Epoch 37, Train Loss: 0.1593, Val Loss: 0.1717
2025-06-25 09:36:43.420 INFO: Epoch 37, Train Loss: 0.1593, Val Loss: 0.1717
train_e/atom_mae: 0.000037
2025-06-25 09:36:43.421 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:36:43.421 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007381
2025-06-25 09:36:43.424 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010181
2025-06-25 09:36:43.424 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000038
2025-06-25 09:36:43.427 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000073
2025-06-25 09:36:43.427 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007494
2025-06-25 09:36:43.428 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 09:36:43.428 INFO: val_f_rmse: 0.010356
##### Step: 277 Learning rate: 1.220703125e-06 #####
2025-06-25 09:37:27.673 INFO: ##### Step: 277 Learning rate: 1.220703125e-06 #####
Epoch 38, Train Loss: 0.1598, Val Loss: 0.1728
2025-06-25 09:37:27.673 INFO: Epoch 38, Train Loss: 0.1598, Val Loss: 0.1728
train_e/atom_mae: 0.000037
2025-06-25 09:37:27.674 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:37:27.674 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007381
2025-06-25 09:37:27.678 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 09:37:27.678 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:37:27.681 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000074
2025-06-25 09:37:27.681 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007493
2025-06-25 09:37:27.681 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010356
2025-06-25 09:37:27.681 INFO: val_f_rmse: 0.010356
##### Step: 278 Learning rate: 1.220703125e-06 #####
2025-06-25 09:38:11.922 INFO: ##### Step: 278 Learning rate: 1.220703125e-06 #####
Epoch 39, Train Loss: 0.1589, Val Loss: 0.1741
2025-06-25 09:38:11.922 INFO: Epoch 39, Train Loss: 0.1589, Val Loss: 0.1741
train_e/atom_mae: 0.000037
2025-06-25 09:38:11.923 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:38:11.923 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:38:11.927 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010183
2025-06-25 09:38:11.927 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000038
2025-06-25 09:38:11.929 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000074
2025-06-25 09:38:11.930 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007493
2025-06-25 09:38:11.930 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010356
2025-06-25 09:38:11.930 INFO: val_f_rmse: 0.010356
##### Step: 279 Learning rate: 1.220703125e-06 #####
2025-06-25 09:38:56.202 INFO: ##### Step: 279 Learning rate: 1.220703125e-06 #####
Epoch 40, Train Loss: 0.1603, Val Loss: 0.1696
2025-06-25 09:38:56.202 INFO: Epoch 40, Train Loss: 0.1603, Val Loss: 0.1696
train_e/atom_mae: 0.000037
2025-06-25 09:38:56.203 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:38:56.203 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007381
2025-06-25 09:38:56.207 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 09:38:56.207 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000038
2025-06-25 09:38:56.209 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000072
2025-06-25 09:38:56.210 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 09:38:56.210 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010356
2025-06-25 09:38:56.210 INFO: val_f_rmse: 0.010356
##### Step: 280 Learning rate: 6.103515625e-07 #####
2025-06-25 09:39:40.471 INFO: ##### Step: 280 Learning rate: 6.103515625e-07 #####
Epoch 41, Train Loss: 0.1588, Val Loss: 0.1727
2025-06-25 09:39:40.471 INFO: Epoch 41, Train Loss: 0.1588, Val Loss: 0.1727
train_e/atom_mae: 0.000036
2025-06-25 09:39:40.472 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000068
2025-06-25 09:39:40.472 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007381
2025-06-25 09:39:40.475 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 09:39:40.475 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000038
2025-06-25 09:39:40.478 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000074
2025-06-25 09:39:40.478 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007493
2025-06-25 09:39:40.479 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 09:39:40.479 INFO: val_f_rmse: 0.010355
##### Step: 281 Learning rate: 6.103515625e-07 #####
2025-06-25 09:40:24.665 INFO: ##### Step: 281 Learning rate: 6.103515625e-07 #####
Epoch 42, Train Loss: 0.1587, Val Loss: 0.1698
2025-06-25 09:40:24.665 INFO: Epoch 42, Train Loss: 0.1587, Val Loss: 0.1698
train_e/atom_mae: 0.000036
2025-06-25 09:40:24.666 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:40:24.666 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 09:40:24.670 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010181
2025-06-25 09:40:24.670 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000037
2025-06-25 09:40:24.672 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:40:24.673 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 09:40:24.673 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 09:40:24.673 INFO: val_f_rmse: 0.010354
##### Step: 282 Learning rate: 6.103515625e-07 #####
2025-06-25 09:41:08.873 INFO: ##### Step: 282 Learning rate: 6.103515625e-07 #####
Epoch 43, Train Loss: 0.1587, Val Loss: 0.1755
2025-06-25 09:41:08.874 INFO: Epoch 43, Train Loss: 0.1587, Val Loss: 0.1755
train_e/atom_mae: 0.000036
2025-06-25 09:41:08.874 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:41:08.875 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 09:41:08.878 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010181
2025-06-25 09:41:08.878 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000039
2025-06-25 09:41:08.881 INFO: val_e/atom_mae: 0.000039
val_e/atom_rmse: 0.000075
2025-06-25 09:41:08.881 INFO: val_e/atom_rmse: 0.000075
val_f_mae: 0.007493
2025-06-25 09:41:08.881 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010356
2025-06-25 09:41:08.882 INFO: val_f_rmse: 0.010356
##### Step: 283 Learning rate: 6.103515625e-07 #####
2025-06-25 09:41:53.085 INFO: ##### Step: 283 Learning rate: 6.103515625e-07 #####
Epoch 44, Train Loss: 0.1589, Val Loss: 0.1700
2025-06-25 09:41:53.085 INFO: Epoch 44, Train Loss: 0.1589, Val Loss: 0.1700
train_e/atom_mae: 0.000037
2025-06-25 09:41:53.086 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:41:53.086 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007381
2025-06-25 09:41:53.090 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 09:41:53.090 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000036
2025-06-25 09:41:53.092 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000072
2025-06-25 09:41:53.093 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 09:41:53.093 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010357
2025-06-25 09:41:53.093 INFO: val_f_rmse: 0.010357
##### Step: 284 Learning rate: 6.103515625e-07 #####
2025-06-25 09:42:37.205 INFO: ##### Step: 284 Learning rate: 6.103515625e-07 #####
Epoch 45, Train Loss: 0.1585, Val Loss: 0.1709
2025-06-25 09:42:37.205 INFO: Epoch 45, Train Loss: 0.1585, Val Loss: 0.1709
train_e/atom_mae: 0.000036
2025-06-25 09:42:37.206 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:42:37.206 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 09:42:37.209 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 09:42:37.209 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:42:37.212 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:42:37.212 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007494
2025-06-25 09:42:37.213 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010357
2025-06-25 09:42:37.213 INFO: val_f_rmse: 0.010357
##### Step: 285 Learning rate: 6.103515625e-07 #####
2025-06-25 09:43:21.347 INFO: ##### Step: 285 Learning rate: 6.103515625e-07 #####
Epoch 46, Train Loss: 0.1586, Val Loss: 0.1699
2025-06-25 09:43:21.347 INFO: Epoch 46, Train Loss: 0.1586, Val Loss: 0.1699
train_e/atom_mae: 0.000036
2025-06-25 09:43:21.348 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:43:21.348 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 09:43:21.351 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010183
2025-06-25 09:43:21.352 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000037
2025-06-25 09:43:21.354 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:43:21.355 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 09:43:21.355 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010357
2025-06-25 09:43:21.355 INFO: val_f_rmse: 0.010357
##### Step: 286 Learning rate: 6.103515625e-07 #####
2025-06-25 09:44:05.469 INFO: ##### Step: 286 Learning rate: 6.103515625e-07 #####
Epoch 47, Train Loss: 0.1591, Val Loss: 0.1699
2025-06-25 09:44:05.469 INFO: Epoch 47, Train Loss: 0.1591, Val Loss: 0.1699
train_e/atom_mae: 0.000037
2025-06-25 09:44:05.470 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:44:05.470 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:44:05.474 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010183
2025-06-25 09:44:05.474 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000037
2025-06-25 09:44:05.476 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:44:05.477 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 09:44:05.477 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 09:44:05.477 INFO: val_f_rmse: 0.010355
##### Step: 287 Learning rate: 6.103515625e-07 #####
2025-06-25 09:44:50.149 INFO: ##### Step: 287 Learning rate: 6.103515625e-07 #####
Epoch 48, Train Loss: 0.1586, Val Loss: 0.1707
2025-06-25 09:44:50.149 INFO: Epoch 48, Train Loss: 0.1586, Val Loss: 0.1707
train_e/atom_mae: 0.000036
2025-06-25 09:44:50.150 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:44:50.150 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 09:44:50.153 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:44:50.153 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000038
2025-06-25 09:44:50.156 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000072
2025-06-25 09:44:50.156 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 09:44:50.157 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 09:44:50.157 INFO: val_f_rmse: 0.010356
##### Step: 288 Learning rate: 6.103515625e-07 #####
2025-06-25 09:45:34.326 INFO: ##### Step: 288 Learning rate: 6.103515625e-07 #####
Epoch 49, Train Loss: 0.1594, Val Loss: 0.1706
2025-06-25 09:45:34.326 INFO: Epoch 49, Train Loss: 0.1594, Val Loss: 0.1706
train_e/atom_mae: 0.000037
2025-06-25 09:45:34.327 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:45:34.327 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007381
2025-06-25 09:45:34.331 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010181
2025-06-25 09:45:34.331 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000038
2025-06-25 09:45:34.333 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000072
2025-06-25 09:45:34.334 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 09:45:34.334 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 09:45:34.334 INFO: val_f_rmse: 0.010356
##### Step: 289 Learning rate: 6.103515625e-07 #####
2025-06-25 09:46:18.523 INFO: ##### Step: 289 Learning rate: 6.103515625e-07 #####
Epoch 50, Train Loss: 0.1588, Val Loss: 0.1705
2025-06-25 09:46:18.523 INFO: Epoch 50, Train Loss: 0.1588, Val Loss: 0.1705
train_e/atom_mae: 0.000036
2025-06-25 09:46:18.524 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000068
2025-06-25 09:46:18.524 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:46:18.528 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:46:18.528 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:46:18.530 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:46:18.531 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 09:46:18.531 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 09:46:18.531 INFO: val_f_rmse: 0.010356
##### Step: 290 Learning rate: 6.103515625e-07 #####
2025-06-25 09:47:02.686 INFO: ##### Step: 290 Learning rate: 6.103515625e-07 #####
Epoch 51, Train Loss: 0.1592, Val Loss: 0.1700
2025-06-25 09:47:02.686 INFO: Epoch 51, Train Loss: 0.1592, Val Loss: 0.1700
train_e/atom_mae: 0.000036
2025-06-25 09:47:02.687 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000068
2025-06-25 09:47:02.687 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:47:02.691 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010183
2025-06-25 09:47:02.691 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000037
2025-06-25 09:47:02.693 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:47:02.694 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007495
2025-06-25 09:47:02.694 INFO: val_f_mae: 0.007495
val_f_rmse: 0.010358
2025-06-25 09:47:02.694 INFO: val_f_rmse: 0.010358
##### Step: 291 Learning rate: 6.103515625e-07 #####
2025-06-25 09:47:46.868 INFO: ##### Step: 291 Learning rate: 6.103515625e-07 #####
Epoch 52, Train Loss: 0.1588, Val Loss: 0.1716
2025-06-25 09:47:46.869 INFO: Epoch 52, Train Loss: 0.1588, Val Loss: 0.1716
train_e/atom_mae: 0.000036
2025-06-25 09:47:46.870 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:47:46.870 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 09:47:46.873 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010183
2025-06-25 09:47:46.873 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000037
2025-06-25 09:47:46.876 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:47:46.876 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007494
2025-06-25 09:47:46.877 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010357
2025-06-25 09:47:46.877 INFO: val_f_rmse: 0.010357
##### Step: 292 Learning rate: 6.103515625e-07 #####
2025-06-25 09:48:31.054 INFO: ##### Step: 292 Learning rate: 6.103515625e-07 #####
Epoch 53, Train Loss: 0.1592, Val Loss: 0.1697
2025-06-25 09:48:31.054 INFO: Epoch 53, Train Loss: 0.1592, Val Loss: 0.1697
train_e/atom_mae: 0.000036
2025-06-25 09:48:31.055 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000068
2025-06-25 09:48:31.055 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:48:31.058 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:48:31.059 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:48:31.061 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:48:31.062 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 09:48:31.062 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 09:48:31.062 INFO: val_f_rmse: 0.010355
##### Step: 293 Learning rate: 6.103515625e-07 #####
2025-06-25 09:49:15.214 INFO: ##### Step: 293 Learning rate: 6.103515625e-07 #####
Epoch 54, Train Loss: 0.1594, Val Loss: 0.1696
2025-06-25 09:49:15.214 INFO: Epoch 54, Train Loss: 0.1594, Val Loss: 0.1696
train_e/atom_mae: 0.000037
2025-06-25 09:49:15.215 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:49:15.215 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:49:15.219 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:49:15.219 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:49:15.222 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:49:15.222 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 09:49:15.222 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 09:49:15.222 INFO: val_f_rmse: 0.010356
##### Step: 294 Learning rate: 6.103515625e-07 #####
2025-06-25 09:49:59.359 INFO: ##### Step: 294 Learning rate: 6.103515625e-07 #####
Epoch 55, Train Loss: 0.1589, Val Loss: 0.1721
2025-06-25 09:49:59.359 INFO: Epoch 55, Train Loss: 0.1589, Val Loss: 0.1721
train_e/atom_mae: 0.000036
2025-06-25 09:49:59.360 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000068
2025-06-25 09:49:59.360 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:49:59.364 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:49:59.364 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:49:59.367 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:49:59.367 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 09:49:59.367 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 09:49:59.367 INFO: val_f_rmse: 0.010355
##### Step: 295 Learning rate: 6.103515625e-07 #####
2025-06-25 09:50:43.511 INFO: ##### Step: 295 Learning rate: 6.103515625e-07 #####
Epoch 56, Train Loss: 0.1586, Val Loss: 0.1705
2025-06-25 09:50:43.511 INFO: Epoch 56, Train Loss: 0.1586, Val Loss: 0.1705
train_e/atom_mae: 0.000036
2025-06-25 09:50:43.512 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:50:43.512 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 09:50:43.515 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010184
2025-06-25 09:50:43.515 INFO: train_f_rmse: 0.010184
val_e/atom_mae: 0.000038
2025-06-25 09:50:43.518 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000072
2025-06-25 09:50:43.518 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 09:50:43.519 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010357
2025-06-25 09:50:43.519 INFO: val_f_rmse: 0.010357
##### Step: 296 Learning rate: 6.103515625e-07 #####
2025-06-25 09:51:27.654 INFO: ##### Step: 296 Learning rate: 6.103515625e-07 #####
Epoch 57, Train Loss: 0.1590, Val Loss: 0.1712
2025-06-25 09:51:27.655 INFO: Epoch 57, Train Loss: 0.1590, Val Loss: 0.1712
train_e/atom_mae: 0.000036
2025-06-25 09:51:27.656 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000068
2025-06-25 09:51:27.656 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:51:27.659 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010183
2025-06-25 09:51:27.659 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000037
2025-06-25 09:51:27.662 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:51:27.662 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007494
2025-06-25 09:51:27.663 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010357
2025-06-25 09:51:27.663 INFO: val_f_rmse: 0.010357
##### Step: 297 Learning rate: 6.103515625e-07 #####
2025-06-25 09:52:11.820 INFO: ##### Step: 297 Learning rate: 6.103515625e-07 #####
Epoch 58, Train Loss: 0.1588, Val Loss: 0.1727
2025-06-25 09:52:11.820 INFO: Epoch 58, Train Loss: 0.1588, Val Loss: 0.1727
train_e/atom_mae: 0.000036
2025-06-25 09:52:11.821 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:52:11.821 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 09:52:11.824 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010183
2025-06-25 09:52:11.824 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000037
2025-06-25 09:52:11.827 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000074
2025-06-25 09:52:11.827 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007494
2025-06-25 09:52:11.828 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 09:52:11.828 INFO: val_f_rmse: 0.010356
##### Step: 298 Learning rate: 6.103515625e-07 #####
2025-06-25 09:52:55.960 INFO: ##### Step: 298 Learning rate: 6.103515625e-07 #####
Epoch 59, Train Loss: 0.1589, Val Loss: 0.1734
2025-06-25 09:52:55.961 INFO: Epoch 59, Train Loss: 0.1589, Val Loss: 0.1734
train_e/atom_mae: 0.000037
2025-06-25 09:52:55.962 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:52:55.962 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:52:55.965 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010183
2025-06-25 09:52:55.965 INFO: train_f_rmse: 0.010183
val_e/atom_mae: 0.000038
2025-06-25 09:52:55.968 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000074
2025-06-25 09:52:55.968 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007493
2025-06-25 09:52:55.968 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 09:52:55.969 INFO: val_f_rmse: 0.010355
##### Step: 299 Learning rate: 6.103515625e-07 #####
2025-06-25 09:53:40.297 INFO: ##### Step: 299 Learning rate: 6.103515625e-07 #####
Epoch 60, Train Loss: 0.1602, Val Loss: 0.1709
2025-06-25 09:53:40.297 INFO: Epoch 60, Train Loss: 0.1602, Val Loss: 0.1709
train_e/atom_mae: 0.000037
2025-06-25 09:53:40.298 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000068
2025-06-25 09:53:40.298 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:53:40.302 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:53:40.302 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:53:40.304 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:53:40.305 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007492
2025-06-25 09:53:40.305 INFO: val_f_mae: 0.007492
val_f_rmse: 0.010352
2025-06-25 09:53:40.305 INFO: val_f_rmse: 0.010352
##### Step: 300 Learning rate: 3.0517578125e-07 #####
2025-06-25 09:54:24.449 INFO: ##### Step: 300 Learning rate: 3.0517578125e-07 #####
Epoch 61, Train Loss: 0.1593, Val Loss: 0.1709
2025-06-25 09:54:24.449 INFO: Epoch 61, Train Loss: 0.1593, Val Loss: 0.1709
train_e/atom_mae: 0.000036
2025-06-25 09:54:24.450 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000068
2025-06-25 09:54:24.450 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007381
2025-06-25 09:54:24.454 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010181
2025-06-25 09:54:24.454 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000037
2025-06-25 09:54:24.456 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:54:24.457 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 09:54:24.457 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 09:54:24.457 INFO: val_f_rmse: 0.010354
##### Step: 301 Learning rate: 3.0517578125e-07 #####
2025-06-25 09:55:08.617 INFO: ##### Step: 301 Learning rate: 3.0517578125e-07 #####
Epoch 62, Train Loss: 0.1588, Val Loss: 0.1714
2025-06-25 09:55:08.617 INFO: Epoch 62, Train Loss: 0.1588, Val Loss: 0.1714
train_e/atom_mae: 0.000036
2025-06-25 09:55:08.618 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:55:08.618 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 09:55:08.621 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010181
2025-06-25 09:55:08.621 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000037
2025-06-25 09:55:08.624 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:55:08.624 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 09:55:08.625 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 09:55:08.625 INFO: val_f_rmse: 0.010355
##### Step: 302 Learning rate: 3.0517578125e-07 #####
2025-06-25 09:55:52.769 INFO: ##### Step: 302 Learning rate: 3.0517578125e-07 #####
Epoch 63, Train Loss: 0.1587, Val Loss: 0.1717
2025-06-25 09:55:52.769 INFO: Epoch 63, Train Loss: 0.1587, Val Loss: 0.1717
train_e/atom_mae: 0.000036
2025-06-25 09:55:52.770 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:55:52.770 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 09:55:52.774 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010181
2025-06-25 09:55:52.774 INFO: train_f_rmse: 0.010181
val_e/atom_mae: 0.000037
2025-06-25 09:55:52.776 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:55:52.777 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 09:55:52.777 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 09:55:52.777 INFO: val_f_rmse: 0.010354
##### Step: 303 Learning rate: 3.0517578125e-07 #####
2025-06-25 09:56:36.935 INFO: ##### Step: 303 Learning rate: 3.0517578125e-07 #####
Epoch 64, Train Loss: 0.1589, Val Loss: 0.1697
2025-06-25 09:56:36.936 INFO: Epoch 64, Train Loss: 0.1589, Val Loss: 0.1697
train_e/atom_mae: 0.000036
2025-06-25 09:56:36.937 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000068
2025-06-25 09:56:36.937 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.007382
2025-06-25 09:56:36.940 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:56:36.940 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:56:36.943 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:56:36.943 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 09:56:36.944 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 09:56:36.944 INFO: val_f_rmse: 0.010354
##### Step: 304 Learning rate: 3.0517578125e-07 #####
2025-06-25 09:57:21.093 INFO: ##### Step: 304 Learning rate: 3.0517578125e-07 #####
Epoch 65, Train Loss: 0.1585, Val Loss: 0.1707
2025-06-25 09:57:21.093 INFO: Epoch 65, Train Loss: 0.1585, Val Loss: 0.1707
train_e/atom_mae: 0.000036
2025-06-25 09:57:21.094 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:57:21.094 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 09:57:21.097 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:57:21.098 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:57:21.100 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 09:57:21.100 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 09:57:21.101 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 09:57:21.101 INFO: val_f_rmse: 0.010356
##### Step: 305 Learning rate: 3.0517578125e-07 #####
2025-06-25 09:58:05.258 INFO: ##### Step: 305 Learning rate: 3.0517578125e-07 #####
Epoch 66, Train Loss: 0.1586, Val Loss: 0.1720
2025-06-25 09:58:05.258 INFO: Epoch 66, Train Loss: 0.1586, Val Loss: 0.1720
train_e/atom_mae: 0.000036
2025-06-25 09:58:05.259 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:58:05.259 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 09:58:05.262 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:58:05.263 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:58:05.265 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:58:05.265 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 09:58:05.266 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 09:58:05.266 INFO: val_f_rmse: 0.010355
##### Step: 306 Learning rate: 3.0517578125e-07 #####
2025-06-25 09:58:49.418 INFO: ##### Step: 306 Learning rate: 3.0517578125e-07 #####
Epoch 67, Train Loss: 0.1584, Val Loss: 0.1723
2025-06-25 09:58:49.418 INFO: Epoch 67, Train Loss: 0.1584, Val Loss: 0.1723
train_e/atom_mae: 0.000036
2025-06-25 09:58:49.419 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:58:49.419 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 09:58:49.423 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:58:49.423 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:58:49.425 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:58:49.426 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 09:58:49.426 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 09:58:49.426 INFO: val_f_rmse: 0.010355
##### Step: 307 Learning rate: 3.0517578125e-07 #####
2025-06-25 09:59:33.583 INFO: ##### Step: 307 Learning rate: 3.0517578125e-07 #####
Epoch 68, Train Loss: 0.1586, Val Loss: 0.1714
2025-06-25 09:59:33.583 INFO: Epoch 68, Train Loss: 0.1586, Val Loss: 0.1714
train_e/atom_mae: 0.000036
2025-06-25 09:59:33.584 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 09:59:33.584 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 09:59:33.588 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 09:59:33.588 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 09:59:33.590 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 09:59:33.591 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007494
2025-06-25 09:59:33.591 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010355
2025-06-25 09:59:33.591 INFO: val_f_rmse: 0.010355
##### Step: 308 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:00:17.763 INFO: ##### Step: 308 Learning rate: 3.0517578125e-07 #####
Epoch 69, Train Loss: 0.1585, Val Loss: 0.1734
2025-06-25 10:00:17.763 INFO: Epoch 69, Train Loss: 0.1585, Val Loss: 0.1734
train_e/atom_mae: 0.000036
2025-06-25 10:00:17.764 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:00:17.764 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:00:17.768 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:00:17.768 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000038
2025-06-25 10:00:17.771 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000074
2025-06-25 10:00:17.771 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007493
2025-06-25 10:00:17.771 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:00:17.772 INFO: val_f_rmse: 0.010354
##### Step: 309 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:01:01.922 INFO: ##### Step: 309 Learning rate: 3.0517578125e-07 #####
Epoch 70, Train Loss: 0.1585, Val Loss: 0.1714
2025-06-25 10:01:01.922 INFO: Epoch 70, Train Loss: 0.1585, Val Loss: 0.1714
train_e/atom_mae: 0.000036
2025-06-25 10:01:01.923 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:01:01.923 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 10:01:01.926 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 10:01:01.926 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:01:01.929 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 10:01:01.929 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 10:01:01.930 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 10:01:01.930 INFO: val_f_rmse: 0.010355
##### Step: 310 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:01:46.087 INFO: ##### Step: 310 Learning rate: 3.0517578125e-07 #####
Epoch 71, Train Loss: 0.1584, Val Loss: 0.1697
2025-06-25 10:01:46.087 INFO: Epoch 71, Train Loss: 0.1584, Val Loss: 0.1697
train_e/atom_mae: 0.000036
2025-06-25 10:01:46.088 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:01:46.088 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:01:46.091 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:01:46.091 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:01:46.094 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:01:46.094 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:01:46.095 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:01:46.095 INFO: val_f_rmse: 0.010354
##### Step: 311 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:02:30.266 INFO: ##### Step: 311 Learning rate: 3.0517578125e-07 #####
Epoch 72, Train Loss: 0.1583, Val Loss: 0.1706
2025-06-25 10:02:30.266 INFO: Epoch 72, Train Loss: 0.1583, Val Loss: 0.1706
train_e/atom_mae: 0.000036
2025-06-25 10:02:30.267 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:02:30.267 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:02:30.271 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:02:30.271 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:02:30.273 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:02:30.274 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007494
2025-06-25 10:02:30.274 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010356
2025-06-25 10:02:30.274 INFO: val_f_rmse: 0.010356
##### Step: 312 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:03:14.457 INFO: ##### Step: 312 Learning rate: 3.0517578125e-07 #####
Epoch 73, Train Loss: 0.1585, Val Loss: 0.1708
2025-06-25 10:03:14.457 INFO: Epoch 73, Train Loss: 0.1585, Val Loss: 0.1708
train_e/atom_mae: 0.000036
2025-06-25 10:03:14.458 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:03:14.458 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:03:14.462 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:03:14.462 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:03:14.464 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:03:14.465 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:03:14.465 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:03:14.465 INFO: val_f_rmse: 0.010354
##### Step: 313 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:03:58.656 INFO: ##### Step: 313 Learning rate: 3.0517578125e-07 #####
Epoch 74, Train Loss: 0.1586, Val Loss: 0.1703
2025-06-25 10:03:58.657 INFO: Epoch 74, Train Loss: 0.1586, Val Loss: 0.1703
train_e/atom_mae: 0.000036
2025-06-25 10:03:58.658 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:03:58.658 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:03:58.661 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:03:58.661 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000036
2025-06-25 10:03:58.664 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000072
2025-06-25 10:03:58.664 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:03:58.665 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:03:58.665 INFO: val_f_rmse: 0.010354
##### Step: 314 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:04:42.831 INFO: ##### Step: 314 Learning rate: 3.0517578125e-07 #####
Epoch 75, Train Loss: 0.1583, Val Loss: 0.1705
2025-06-25 10:04:42.831 INFO: Epoch 75, Train Loss: 0.1583, Val Loss: 0.1705
train_e/atom_mae: 0.000036
2025-06-25 10:04:42.832 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:04:42.832 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:04:42.836 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:04:42.836 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:04:42.838 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:04:42.839 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:04:42.839 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 10:04:42.839 INFO: val_f_rmse: 0.010355
##### Step: 315 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:05:27.016 INFO: ##### Step: 315 Learning rate: 3.0517578125e-07 #####
Epoch 76, Train Loss: 0.1586, Val Loss: 0.1700
2025-06-25 10:05:27.017 INFO: Epoch 76, Train Loss: 0.1586, Val Loss: 0.1700
train_e/atom_mae: 0.000036
2025-06-25 10:05:27.017 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:05:27.018 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:05:27.021 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:05:27.021 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000036
2025-06-25 10:05:27.024 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000072
2025-06-25 10:05:27.024 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:05:27.024 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 10:05:27.025 INFO: val_f_rmse: 0.010355
##### Step: 316 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:06:11.183 INFO: ##### Step: 316 Learning rate: 3.0517578125e-07 #####
Epoch 77, Train Loss: 0.1583, Val Loss: 0.1720
2025-06-25 10:06:11.184 INFO: Epoch 77, Train Loss: 0.1583, Val Loss: 0.1720
train_e/atom_mae: 0.000036
2025-06-25 10:06:11.185 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:06:11.185 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:06:11.188 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:06:11.188 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000038
2025-06-25 10:06:11.191 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000073
2025-06-25 10:06:11.191 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007494
2025-06-25 10:06:11.192 INFO: val_f_mae: 0.007494
val_f_rmse: 0.010357
2025-06-25 10:06:11.192 INFO: val_f_rmse: 0.010357
##### Step: 317 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:06:55.360 INFO: ##### Step: 317 Learning rate: 3.0517578125e-07 #####
Epoch 78, Train Loss: 0.1587, Val Loss: 0.1725
2025-06-25 10:06:55.361 INFO: Epoch 78, Train Loss: 0.1587, Val Loss: 0.1725
train_e/atom_mae: 0.000036
2025-06-25 10:06:55.362 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:06:55.362 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:06:55.365 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:06:55.365 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:06:55.368 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 10:06:55.368 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 10:06:55.369 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:06:55.369 INFO: val_f_rmse: 0.010354
##### Step: 318 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:07:39.542 INFO: ##### Step: 318 Learning rate: 3.0517578125e-07 #####
Epoch 79, Train Loss: 0.1588, Val Loss: 0.1706
2025-06-25 10:07:39.542 INFO: Epoch 79, Train Loss: 0.1588, Val Loss: 0.1706
train_e/atom_mae: 0.000036
2025-06-25 10:07:39.543 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:07:39.543 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:07:39.547 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:07:39.547 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:07:39.549 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:07:39.550 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:07:39.550 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:07:39.550 INFO: val_f_rmse: 0.010354
##### Step: 319 Learning rate: 3.0517578125e-07 #####
2025-06-25 10:08:23.706 INFO: ##### Step: 319 Learning rate: 3.0517578125e-07 #####
Epoch 80, Train Loss: 0.1584, Val Loss: 0.1718
2025-06-25 10:08:23.706 INFO: Epoch 80, Train Loss: 0.1584, Val Loss: 0.1718
train_e/atom_mae: 0.000036
2025-06-25 10:08:23.707 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:08:23.707 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:08:23.711 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:08:23.711 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:08:23.714 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 10:08:23.714 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 10:08:23.714 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:08:23.714 INFO: val_f_rmse: 0.010354
##### Step: 320 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:09:07.849 INFO: ##### Step: 320 Learning rate: 1.52587890625e-07 #####
Epoch 81, Train Loss: 0.1583, Val Loss: 0.1703
2025-06-25 10:09:07.850 INFO: Epoch 81, Train Loss: 0.1583, Val Loss: 0.1703
train_e/atom_mae: 0.000036
2025-06-25 10:09:07.850 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:09:07.851 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:09:07.854 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:09:07.854 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000036
2025-06-25 10:09:07.857 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000072
2025-06-25 10:09:07.857 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:09:07.857 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:09:07.858 INFO: val_f_rmse: 0.010354
##### Step: 321 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:09:52.005 INFO: ##### Step: 321 Learning rate: 1.52587890625e-07 #####
Epoch 82, Train Loss: 0.1581, Val Loss: 0.1705
2025-06-25 10:09:52.005 INFO: Epoch 82, Train Loss: 0.1581, Val Loss: 0.1705
train_e/atom_mae: 0.000036
2025-06-25 10:09:52.006 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:09:52.006 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:09:52.010 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:09:52.010 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:09:52.012 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:09:52.013 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:09:52.013 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:09:52.013 INFO: val_f_rmse: 0.010354
##### Step: 322 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:10:36.133 INFO: ##### Step: 322 Learning rate: 1.52587890625e-07 #####
Epoch 83, Train Loss: 0.1582, Val Loss: 0.1706
2025-06-25 10:10:36.133 INFO: Epoch 83, Train Loss: 0.1582, Val Loss: 0.1706
train_e/atom_mae: 0.000036
2025-06-25 10:10:36.134 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:10:36.134 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 10:10:36.138 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 10:10:36.138 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:10:36.140 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:10:36.141 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:10:36.141 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:10:36.141 INFO: val_f_rmse: 0.010354
##### Step: 323 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:11:20.293 INFO: ##### Step: 323 Learning rate: 1.52587890625e-07 #####
Epoch 84, Train Loss: 0.1583, Val Loss: 0.1734
2025-06-25 10:11:20.293 INFO: Epoch 84, Train Loss: 0.1583, Val Loss: 0.1734
train_e/atom_mae: 0.000036
2025-06-25 10:11:20.294 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:11:20.294 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:11:20.298 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:11:20.298 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000038
2025-06-25 10:11:20.301 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000074
2025-06-25 10:11:20.301 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.007493
2025-06-25 10:11:20.301 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:11:20.301 INFO: val_f_rmse: 0.010354
##### Step: 324 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:12:04.403 INFO: ##### Step: 324 Learning rate: 1.52587890625e-07 #####
Epoch 85, Train Loss: 0.1588, Val Loss: 0.1706
2025-06-25 10:12:04.403 INFO: Epoch 85, Train Loss: 0.1588, Val Loss: 0.1706
train_e/atom_mae: 0.000036
2025-06-25 10:12:04.404 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:12:04.404 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:12:04.407 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:12:04.407 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:12:04.410 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:12:04.410 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:12:04.411 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:12:04.411 INFO: val_f_rmse: 0.010354
##### Step: 325 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:12:48.462 INFO: ##### Step: 325 Learning rate: 1.52587890625e-07 #####
Epoch 86, Train Loss: 0.1582, Val Loss: 0.1701
2025-06-25 10:12:48.462 INFO: Epoch 86, Train Loss: 0.1582, Val Loss: 0.1701
train_e/atom_mae: 0.000036
2025-06-25 10:12:48.463 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:12:48.463 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:12:48.467 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:12:48.467 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:12:48.469 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:12:48.470 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:12:48.470 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:12:48.470 INFO: val_f_rmse: 0.010354
##### Step: 326 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:13:32.548 INFO: ##### Step: 326 Learning rate: 1.52587890625e-07 #####
Epoch 87, Train Loss: 0.1583, Val Loss: 0.1716
2025-06-25 10:13:32.548 INFO: Epoch 87, Train Loss: 0.1583, Val Loss: 0.1716
train_e/atom_mae: 0.000036
2025-06-25 10:13:32.549 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:13:32.549 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:13:32.553 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:13:32.553 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:13:32.555 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 10:13:32.556 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 10:13:32.556 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 10:13:32.556 INFO: val_f_rmse: 0.010355
##### Step: 327 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:14:16.632 INFO: ##### Step: 327 Learning rate: 1.52587890625e-07 #####
Epoch 88, Train Loss: 0.1582, Val Loss: 0.1706
2025-06-25 10:14:16.632 INFO: Epoch 88, Train Loss: 0.1582, Val Loss: 0.1706
train_e/atom_mae: 0.000036
2025-06-25 10:14:16.633 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:14:16.633 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:14:16.636 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:14:16.637 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:14:16.639 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:14:16.639 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:14:16.640 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 10:14:16.640 INFO: val_f_rmse: 0.010355
##### Step: 328 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:15:00.711 INFO: ##### Step: 328 Learning rate: 1.52587890625e-07 #####
Epoch 89, Train Loss: 0.1582, Val Loss: 0.1714
2025-06-25 10:15:00.712 INFO: Epoch 89, Train Loss: 0.1582, Val Loss: 0.1714
train_e/atom_mae: 0.000036
2025-06-25 10:15:00.712 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:15:00.713 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:15:00.716 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:15:00.716 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:15:00.719 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 10:15:00.719 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 10:15:00.719 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:15:00.720 INFO: val_f_rmse: 0.010354
##### Step: 329 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:15:44.816 INFO: ##### Step: 329 Learning rate: 1.52587890625e-07 #####
Epoch 90, Train Loss: 0.1581, Val Loss: 0.1710
2025-06-25 10:15:44.816 INFO: Epoch 90, Train Loss: 0.1581, Val Loss: 0.1710
train_e/atom_mae: 0.000036
2025-06-25 10:15:44.817 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:15:44.817 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:15:44.820 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:15:44.821 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:15:44.823 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 10:15:44.824 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 10:15:44.824 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:15:44.824 INFO: val_f_rmse: 0.010354
##### Step: 330 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:16:28.892 INFO: ##### Step: 330 Learning rate: 1.52587890625e-07 #####
Epoch 91, Train Loss: 0.1583, Val Loss: 0.1703
2025-06-25 10:16:28.893 INFO: Epoch 91, Train Loss: 0.1583, Val Loss: 0.1703
train_e/atom_mae: 0.000036
2025-06-25 10:16:28.894 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:16:28.894 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:16:28.897 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:16:28.897 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:16:28.900 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:16:28.900 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:16:28.901 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:16:28.901 INFO: val_f_rmse: 0.010354
##### Step: 331 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:17:12.971 INFO: ##### Step: 331 Learning rate: 1.52587890625e-07 #####
Epoch 92, Train Loss: 0.1582, Val Loss: 0.1701
2025-06-25 10:17:12.971 INFO: Epoch 92, Train Loss: 0.1582, Val Loss: 0.1701
train_e/atom_mae: 0.000036
2025-06-25 10:17:12.972 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:17:12.972 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 10:17:12.976 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 10:17:12.976 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:17:12.979 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:17:12.979 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:17:12.979 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:17:12.979 INFO: val_f_rmse: 0.010354
##### Step: 332 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:17:57.072 INFO: ##### Step: 332 Learning rate: 1.52587890625e-07 #####
Epoch 93, Train Loss: 0.1584, Val Loss: 0.1712
2025-06-25 10:17:57.072 INFO: Epoch 93, Train Loss: 0.1584, Val Loss: 0.1712
train_e/atom_mae: 0.000036
2025-06-25 10:17:57.073 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:17:57.073 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 10:17:57.077 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 10:17:57.077 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:17:57.079 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 10:17:57.080 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 10:17:57.080 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:17:57.080 INFO: val_f_rmse: 0.010354
##### Step: 333 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:18:41.170 INFO: ##### Step: 333 Learning rate: 1.52587890625e-07 #####
Epoch 94, Train Loss: 0.1582, Val Loss: 0.1702
2025-06-25 10:18:41.170 INFO: Epoch 94, Train Loss: 0.1582, Val Loss: 0.1702
train_e/atom_mae: 0.000036
2025-06-25 10:18:41.171 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:18:41.171 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 10:18:41.174 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 10:18:41.174 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000036
2025-06-25 10:18:41.177 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000072
2025-06-25 10:18:41.177 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:18:41.178 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:18:41.178 INFO: val_f_rmse: 0.010354
##### Step: 334 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:19:25.295 INFO: ##### Step: 334 Learning rate: 1.52587890625e-07 #####
Epoch 95, Train Loss: 0.1582, Val Loss: 0.1701
2025-06-25 10:19:25.295 INFO: Epoch 95, Train Loss: 0.1582, Val Loss: 0.1701
train_e/atom_mae: 0.000036
2025-06-25 10:19:25.296 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:19:25.296 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:19:25.300 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:19:25.300 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:19:25.302 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:19:25.303 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:19:25.303 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 10:19:25.303 INFO: val_f_rmse: 0.010355
##### Step: 335 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:20:09.376 INFO: ##### Step: 335 Learning rate: 1.52587890625e-07 #####
Epoch 96, Train Loss: 0.1581, Val Loss: 0.1705
2025-06-25 10:20:09.376 INFO: Epoch 96, Train Loss: 0.1581, Val Loss: 0.1705
train_e/atom_mae: 0.000036
2025-06-25 10:20:09.377 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:20:09.377 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007382
2025-06-25 10:20:09.381 INFO: train_f_mae: 0.007382
train_f_rmse: 0.010182
2025-06-25 10:20:09.381 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:20:09.383 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:20:09.384 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:20:09.384 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 10:20:09.384 INFO: val_f_rmse: 0.010355
##### Step: 336 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:20:53.437 INFO: ##### Step: 336 Learning rate: 1.52587890625e-07 #####
Epoch 97, Train Loss: 0.1582, Val Loss: 0.1701
2025-06-25 10:20:53.437 INFO: Epoch 97, Train Loss: 0.1582, Val Loss: 0.1701
train_e/atom_mae: 0.000036
2025-06-25 10:20:53.438 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:20:53.438 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 10:20:53.441 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 10:20:53.441 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:20:53.444 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:20:53.444 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:20:53.445 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:20:53.445 INFO: val_f_rmse: 0.010354
##### Step: 337 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:21:37.501 INFO: ##### Step: 337 Learning rate: 1.52587890625e-07 #####
Epoch 98, Train Loss: 0.1581, Val Loss: 0.1700
2025-06-25 10:21:37.501 INFO: Epoch 98, Train Loss: 0.1581, Val Loss: 0.1700
train_e/atom_mae: 0.000036
2025-06-25 10:21:37.502 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:21:37.502 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 10:21:37.506 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 10:21:37.506 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:21:37.508 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:21:37.509 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:21:37.509 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:21:37.509 INFO: val_f_rmse: 0.010354
##### Step: 338 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:22:21.585 INFO: ##### Step: 338 Learning rate: 1.52587890625e-07 #####
Epoch 99, Train Loss: 0.1583, Val Loss: 0.1705
2025-06-25 10:22:21.586 INFO: Epoch 99, Train Loss: 0.1583, Val Loss: 0.1705
train_e/atom_mae: 0.000036
2025-06-25 10:22:21.587 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:22:21.587 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 10:22:21.590 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 10:22:21.590 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:22:21.593 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-06-25 10:22:21.593 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.007493
2025-06-25 10:22:21.593 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010355
2025-06-25 10:22:21.594 INFO: val_f_rmse: 0.010355
##### Step: 339 Learning rate: 1.52587890625e-07 #####
2025-06-25 10:23:05.684 INFO: ##### Step: 339 Learning rate: 1.52587890625e-07 #####
Epoch 100, Train Loss: 0.1581, Val Loss: 0.1714
2025-06-25 10:23:05.685 INFO: Epoch 100, Train Loss: 0.1581, Val Loss: 0.1714
train_e/atom_mae: 0.000036
2025-06-25 10:23:05.686 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000067
2025-06-25 10:23:05.686 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.007381
2025-06-25 10:23:05.689 INFO: train_f_mae: 0.007381
train_f_rmse: 0.010182
2025-06-25 10:23:05.689 INFO: train_f_rmse: 0.010182
val_e/atom_mae: 0.000037
2025-06-25 10:23:05.692 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000073
2025-06-25 10:23:05.692 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.007493
2025-06-25 10:23:05.693 INFO: val_f_mae: 0.007493
val_f_rmse: 0.010354
2025-06-25 10:23:05.693 INFO: val_f_rmse: 0.010354
2025-06-25 10:23:05.705 INFO: Finished
2025-06-25 10:23:05.705 INFO: Number of trainable parameters: 42666
