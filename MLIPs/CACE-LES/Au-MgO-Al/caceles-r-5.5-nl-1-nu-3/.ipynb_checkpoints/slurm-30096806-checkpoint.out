2025-04-08 19:38:58.416 INFO: reading data
2025-04-08 19:39:00.950 INFO: Loaded 5000 training configurations from '../Au-MgO-Al.xyz'
2025-04-08 19:39:00.950 INFO: Using random 10.0% of training set for validation
2025-04-08 19:39:18.685 INFO: CUDA version: 12.4, CUDA device: 0
2025-04-08 19:39:18.686 INFO: device: cuda
2025-04-08 19:39:18.686 INFO: building CACE representation
2025-04-08 19:39:18.847 INFO: Representation: Cace(
  (node_onehot): NodeEncoder(num_classes=4)
  (node_embedding_sender): NodeEmbedding(num_classes=4, embedding_dim=4)
  (node_embedding_receiver): NodeEmbedding(num_classes=4, embedding_dim=4)
  (edge_coding): EdgeEncoder(directed=True)
  (radial_basis): BesselRBF(cutoff=5.5, n_rbf=6, trainable=True)
  (cutoff_fn): PolynomialCutoff(p=6.0, cutoff=5.5)
  (angular_basis): AngularComponent(l_max=3)
  (radial_transform): SharedRadialLinearTransform(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 6x12x16 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 6x12x16 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 6x12x16 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 6x12x16 (cuda:0)]
    )
  )
  (symmetrizer): Symmetrizer()
  (message_passing_list): ModuleList()
)
2025-04-08 19:39:18.848 INFO: building CACE NNP
2025-04-08 19:39:18.848 INFO: First train loop:
2025-04-08 19:39:18.849 INFO: creating training task
2025-04-08 19:39:18.849 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-04-08 19:40:50.322 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 67.5922, Val Loss: 18.0303
2025-04-08 19:40:50.323 INFO: Epoch 1, Train Loss: 67.5922, Val Loss: 18.0303
train_e/atom_mae: 0.012378
2025-04-08 19:40:50.326 INFO: train_e/atom_mae: 0.012378
train_e/atom_rmse: 0.020196
2025-04-08 19:40:50.326 INFO: train_e/atom_rmse: 0.020196
train_f_mae: 0.126680
2025-04-08 19:40:50.330 INFO: train_f_mae: 0.126680
train_f_rmse: 0.259034
2025-04-08 19:40:50.330 INFO: train_f_rmse: 0.259034
val_e/atom_mae: 0.005065
2025-04-08 19:40:50.333 INFO: val_e/atom_mae: 0.005065
val_e/atom_rmse: 0.005893
2025-04-08 19:40:50.333 INFO: val_e/atom_rmse: 0.005893
val_f_mae: 0.069968
2025-04-08 19:40:50.333 INFO: val_f_mae: 0.069968
val_f_rmse: 0.134121
2025-04-08 19:40:50.334 INFO: val_f_rmse: 0.134121
##### Step: 1 Learning rate: 0.004 #####
2025-04-08 19:42:18.834 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 9.9928, Val Loss: 7.4241
2025-04-08 19:42:18.835 INFO: Epoch 2, Train Loss: 9.9928, Val Loss: 7.4241
train_e/atom_mae: 0.009378
2025-04-08 19:42:18.836 INFO: train_e/atom_mae: 0.009378
train_e/atom_rmse: 0.011921
2025-04-08 19:42:18.836 INFO: train_e/atom_rmse: 0.011921
train_f_mae: 0.056784
2025-04-08 19:42:18.840 INFO: train_f_mae: 0.056784
train_f_rmse: 0.099100
2025-04-08 19:42:18.840 INFO: train_f_rmse: 0.099100
val_e/atom_mae: 0.016574
2025-04-08 19:42:18.843 INFO: val_e/atom_mae: 0.016574
val_e/atom_rmse: 0.016895
2025-04-08 19:42:18.843 INFO: val_e/atom_rmse: 0.016895
val_f_mae: 0.051453
2025-04-08 19:42:18.844 INFO: val_f_mae: 0.051453
val_f_rmse: 0.084135
2025-04-08 19:42:18.844 INFO: val_f_rmse: 0.084135
##### Step: 2 Learning rate: 0.006 #####
2025-04-08 19:43:47.243 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 5.9581, Val Loss: 5.1699
2025-04-08 19:43:47.244 INFO: Epoch 3, Train Loss: 5.9581, Val Loss: 5.1699
train_e/atom_mae: 0.010540
2025-04-08 19:43:47.245 INFO: train_e/atom_mae: 0.010540
train_e/atom_rmse: 0.013055
2025-04-08 19:43:47.245 INFO: train_e/atom_rmse: 0.013055
train_f_mae: 0.046502
2025-04-08 19:43:47.249 INFO: train_f_mae: 0.046502
train_f_rmse: 0.075841
2025-04-08 19:43:47.250 INFO: train_f_rmse: 0.075841
val_e/atom_mae: 0.013234
2025-04-08 19:43:47.252 INFO: val_e/atom_mae: 0.013234
val_e/atom_rmse: 0.013936
2025-04-08 19:43:47.253 INFO: val_e/atom_rmse: 0.013936
val_f_mae: 0.046104
2025-04-08 19:43:47.253 INFO: val_f_mae: 0.046104
val_f_rmse: 0.070249
2025-04-08 19:43:47.254 INFO: val_f_rmse: 0.070249
##### Step: 3 Learning rate: 0.008 #####
2025-04-08 19:45:15.569 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 4.2693, Val Loss: 5.5906
2025-04-08 19:45:15.569 INFO: Epoch 4, Train Loss: 4.2693, Val Loss: 5.5906
train_e/atom_mae: 0.011636
2025-04-08 19:45:15.570 INFO: train_e/atom_mae: 0.011636
train_e/atom_rmse: 0.013722
2025-04-08 19:45:15.571 INFO: train_e/atom_rmse: 0.013722
train_f_mae: 0.040887
2025-04-08 19:45:15.574 INFO: train_f_mae: 0.040887
train_f_rmse: 0.063573
2025-04-08 19:45:15.574 INFO: train_f_rmse: 0.063573
val_e/atom_mae: 0.030492
2025-04-08 19:45:15.577 INFO: val_e/atom_mae: 0.030492
val_e/atom_rmse: 0.030691
2025-04-08 19:45:15.578 INFO: val_e/atom_rmse: 0.030691
val_f_mae: 0.044613
2025-04-08 19:45:15.578 INFO: val_f_mae: 0.044613
val_f_rmse: 0.066714
2025-04-08 19:45:15.578 INFO: val_f_rmse: 0.066714
##### Step: 4 Learning rate: 0.01 #####
2025-04-08 19:46:43.934 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 3.9093, Val Loss: 3.4731
2025-04-08 19:46:43.935 INFO: Epoch 5, Train Loss: 3.9093, Val Loss: 3.4731
train_e/atom_mae: 0.009821
2025-04-08 19:46:43.936 INFO: train_e/atom_mae: 0.009821
train_e/atom_rmse: 0.012975
2025-04-08 19:46:43.936 INFO: train_e/atom_rmse: 0.012975
train_f_mae: 0.039191
2025-04-08 19:46:43.939 INFO: train_f_mae: 0.039191
train_f_rmse: 0.060874
2025-04-08 19:46:43.939 INFO: train_f_rmse: 0.060874
val_e/atom_mae: 0.007474
2025-04-08 19:46:43.942 INFO: val_e/atom_mae: 0.007474
val_e/atom_rmse: 0.008582
2025-04-08 19:46:43.943 INFO: val_e/atom_rmse: 0.008582
val_f_mae: 0.039335
2025-04-08 19:46:43.943 INFO: val_f_mae: 0.039335
val_f_rmse: 0.058172
2025-04-08 19:46:43.943 INFO: val_f_rmse: 0.058172
##### Step: 5 Learning rate: 0.01 #####
2025-04-08 19:48:12.467 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 3.2915, Val Loss: 2.8514
2025-04-08 19:48:12.468 INFO: Epoch 6, Train Loss: 3.2915, Val Loss: 2.8514
train_e/atom_mae: 0.007196
2025-04-08 19:48:12.469 INFO: train_e/atom_mae: 0.007196
train_e/atom_rmse: 0.009131
2025-04-08 19:48:12.469 INFO: train_e/atom_rmse: 0.009131
train_f_mae: 0.036539
2025-04-08 19:48:12.472 INFO: train_f_mae: 0.036539
train_f_rmse: 0.056486
2025-04-08 19:48:12.473 INFO: train_f_rmse: 0.056486
val_e/atom_mae: 0.005140
2025-04-08 19:48:12.475 INFO: val_e/atom_mae: 0.005140
val_e/atom_rmse: 0.006052
2025-04-08 19:48:12.476 INFO: val_e/atom_rmse: 0.006052
val_f_mae: 0.034444
2025-04-08 19:48:12.477 INFO: val_f_mae: 0.034444
val_f_rmse: 0.052982
2025-04-08 19:48:12.477 INFO: val_f_rmse: 0.052982
##### Step: 6 Learning rate: 0.01 #####
2025-04-08 19:49:40.705 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 3.1353, Val Loss: 2.8664
2025-04-08 19:49:40.706 INFO: Epoch 7, Train Loss: 3.1353, Val Loss: 2.8664
train_e/atom_mae: 0.010509
2025-04-08 19:49:40.707 INFO: train_e/atom_mae: 0.010509
train_e/atom_rmse: 0.013113
2025-04-08 19:49:40.707 INFO: train_e/atom_rmse: 0.013113
train_f_mae: 0.035210
2025-04-08 19:49:40.711 INFO: train_f_mae: 0.035210
train_f_rmse: 0.054104
2025-04-08 19:49:40.711 INFO: train_f_rmse: 0.054104
val_e/atom_mae: 0.003141
2025-04-08 19:49:40.714 INFO: val_e/atom_mae: 0.003141
val_e/atom_rmse: 0.003528
2025-04-08 19:49:40.714 INFO: val_e/atom_rmse: 0.003528
val_f_mae: 0.036567
2025-04-08 19:49:40.714 INFO: val_f_mae: 0.036567
val_f_rmse: 0.053398
2025-04-08 19:49:40.715 INFO: val_f_rmse: 0.053398
##### Step: 7 Learning rate: 0.01 #####
2025-04-08 19:51:08.797 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 3.1934, Val Loss: 2.9114
2025-04-08 19:51:08.798 INFO: Epoch 8, Train Loss: 3.1934, Val Loss: 2.9114
train_e/atom_mae: 0.007220
2025-04-08 19:51:08.799 INFO: train_e/atom_mae: 0.007220
train_e/atom_rmse: 0.009105
2025-04-08 19:51:08.799 INFO: train_e/atom_rmse: 0.009105
train_f_mae: 0.036124
2025-04-08 19:51:08.802 INFO: train_f_mae: 0.036124
train_f_rmse: 0.055615
2025-04-08 19:51:08.803 INFO: train_f_rmse: 0.055615
val_e/atom_mae: 0.008318
2025-04-08 19:51:08.805 INFO: val_e/atom_mae: 0.008318
val_e/atom_rmse: 0.009246
2025-04-08 19:51:08.806 INFO: val_e/atom_rmse: 0.009246
val_f_mae: 0.035178
2025-04-08 19:51:08.806 INFO: val_f_mae: 0.035178
val_f_rmse: 0.052990
2025-04-08 19:51:08.806 INFO: val_f_rmse: 0.052990
##### Step: 8 Learning rate: 0.01 #####
2025-04-08 19:52:36.855 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 2.8235, Val Loss: 2.4985
2025-04-08 19:52:36.856 INFO: Epoch 9, Train Loss: 2.8235, Val Loss: 2.4985
train_e/atom_mae: 0.007464
2025-04-08 19:52:36.857 INFO: train_e/atom_mae: 0.007464
train_e/atom_rmse: 0.009133
2025-04-08 19:52:36.857 INFO: train_e/atom_rmse: 0.009133
train_f_mae: 0.033890
2025-04-08 19:52:36.860 INFO: train_f_mae: 0.033890
train_f_rmse: 0.052178
2025-04-08 19:52:36.860 INFO: train_f_rmse: 0.052178
val_e/atom_mae: 0.007501
2025-04-08 19:52:36.863 INFO: val_e/atom_mae: 0.007501
val_e/atom_rmse: 0.008225
2025-04-08 19:52:36.864 INFO: val_e/atom_rmse: 0.008225
val_f_mae: 0.033105
2025-04-08 19:52:36.864 INFO: val_f_mae: 0.033105
val_f_rmse: 0.049160
2025-04-08 19:52:36.864 INFO: val_f_rmse: 0.049160
##### Step: 9 Learning rate: 0.01 #####
2025-04-08 19:54:04.813 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 2.5870, Val Loss: 2.6519
2025-04-08 19:54:04.814 INFO: Epoch 10, Train Loss: 2.5870, Val Loss: 2.6519
train_e/atom_mae: 0.005543
2025-04-08 19:54:04.814 INFO: train_e/atom_mae: 0.005543
train_e/atom_rmse: 0.006910
2025-04-08 19:54:04.815 INFO: train_e/atom_rmse: 0.006910
train_f_mae: 0.033169
2025-04-08 19:54:04.818 INFO: train_f_mae: 0.033169
train_f_rmse: 0.050291
2025-04-08 19:54:04.818 INFO: train_f_rmse: 0.050291
val_e/atom_mae: 0.007585
2025-04-08 19:54:04.821 INFO: val_e/atom_mae: 0.007585
val_e/atom_rmse: 0.008505
2025-04-08 19:54:04.822 INFO: val_e/atom_rmse: 0.008505
val_f_mae: 0.033365
2025-04-08 19:54:04.822 INFO: val_f_mae: 0.033365
val_f_rmse: 0.050640
2025-04-08 19:54:04.822 INFO: val_f_rmse: 0.050640
##### Step: 10 Learning rate: 0.01 #####
2025-04-08 19:55:32.825 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 2.4170, Val Loss: 1.9682
2025-04-08 19:55:32.826 INFO: Epoch 11, Train Loss: 2.4170, Val Loss: 1.9682
train_e/atom_mae: 0.006252
2025-04-08 19:55:32.827 INFO: train_e/atom_mae: 0.006252
train_e/atom_rmse: 0.007710
2025-04-08 19:55:32.827 INFO: train_e/atom_rmse: 0.007710
train_f_mae: 0.031698
2025-04-08 19:55:32.831 INFO: train_f_mae: 0.031698
train_f_rmse: 0.048426
2025-04-08 19:55:32.831 INFO: train_f_rmse: 0.048426
val_e/atom_mae: 0.003352
2025-04-08 19:55:32.834 INFO: val_e/atom_mae: 0.003352
val_e/atom_rmse: 0.004279
2025-04-08 19:55:32.834 INFO: val_e/atom_rmse: 0.004279
val_f_mae: 0.029436
2025-04-08 19:55:32.835 INFO: val_f_mae: 0.029436
val_f_rmse: 0.044114
2025-04-08 19:55:32.835 INFO: val_f_rmse: 0.044114
##### Step: 11 Learning rate: 0.01 #####
2025-04-08 19:57:00.763 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 2.3837, Val Loss: 1.9033
2025-04-08 19:57:00.764 INFO: Epoch 12, Train Loss: 2.3837, Val Loss: 1.9033
train_e/atom_mae: 0.006402
2025-04-08 19:57:00.765 INFO: train_e/atom_mae: 0.006402
train_e/atom_rmse: 0.007924
2025-04-08 19:57:00.765 INFO: train_e/atom_rmse: 0.007924
train_f_mae: 0.031639
2025-04-08 19:57:00.769 INFO: train_f_mae: 0.031639
train_f_rmse: 0.048039
2025-04-08 19:57:00.769 INFO: train_f_rmse: 0.048039
val_e/atom_mae: 0.003460
2025-04-08 19:57:00.771 INFO: val_e/atom_mae: 0.003460
val_e/atom_rmse: 0.004227
2025-04-08 19:57:00.772 INFO: val_e/atom_rmse: 0.004227
val_f_mae: 0.028844
2025-04-08 19:57:00.772 INFO: val_f_mae: 0.028844
val_f_rmse: 0.043378
2025-04-08 19:57:00.772 INFO: val_f_rmse: 0.043378
##### Step: 12 Learning rate: 0.01 #####
2025-04-08 19:58:28.599 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 2.2275, Val Loss: 2.0340
2025-04-08 19:58:28.600 INFO: Epoch 13, Train Loss: 2.2275, Val Loss: 2.0340
train_e/atom_mae: 0.004932
2025-04-08 19:58:28.601 INFO: train_e/atom_mae: 0.004932
train_e/atom_rmse: 0.006116
2025-04-08 19:58:28.601 INFO: train_e/atom_rmse: 0.006116
train_f_mae: 0.030678
2025-04-08 19:58:28.604 INFO: train_f_mae: 0.030678
train_f_rmse: 0.046715
2025-04-08 19:58:28.604 INFO: train_f_rmse: 0.046715
val_e/atom_mae: 0.005109
2025-04-08 19:58:28.607 INFO: val_e/atom_mae: 0.005109
val_e/atom_rmse: 0.005545
2025-04-08 19:58:28.608 INFO: val_e/atom_rmse: 0.005545
val_f_mae: 0.029991
2025-04-08 19:58:28.608 INFO: val_f_mae: 0.029991
val_f_rmse: 0.044686
2025-04-08 19:58:28.608 INFO: val_f_rmse: 0.044686
##### Step: 13 Learning rate: 0.01 #####
2025-04-08 19:59:56.332 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 2.0984, Val Loss: 1.8108
2025-04-08 19:59:56.332 INFO: Epoch 14, Train Loss: 2.0984, Val Loss: 1.8108
train_e/atom_mae: 0.005040
2025-04-08 19:59:56.333 INFO: train_e/atom_mae: 0.005040
train_e/atom_rmse: 0.006245
2025-04-08 19:59:56.334 INFO: train_e/atom_rmse: 0.006245
train_f_mae: 0.030014
2025-04-08 19:59:56.337 INFO: train_f_mae: 0.030014
train_f_rmse: 0.045291
2025-04-08 19:59:56.337 INFO: train_f_rmse: 0.045291
val_e/atom_mae: 0.003221
2025-04-08 19:59:56.340 INFO: val_e/atom_mae: 0.003221
val_e/atom_rmse: 0.003951
2025-04-08 19:59:56.341 INFO: val_e/atom_rmse: 0.003951
val_f_mae: 0.028155
2025-04-08 19:59:56.341 INFO: val_f_mae: 0.028155
val_f_rmse: 0.042331
2025-04-08 19:59:56.341 INFO: val_f_rmse: 0.042331
##### Step: 14 Learning rate: 0.01 #####
2025-04-08 20:01:24.101 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 2.1512, Val Loss: 1.9503
2025-04-08 20:01:24.101 INFO: Epoch 15, Train Loss: 2.1512, Val Loss: 1.9503
train_e/atom_mae: 0.006836
2025-04-08 20:01:24.102 INFO: train_e/atom_mae: 0.006836
train_e/atom_rmse: 0.008474
2025-04-08 20:01:24.102 INFO: train_e/atom_rmse: 0.008474
train_f_mae: 0.030108
2025-04-08 20:01:24.106 INFO: train_f_mae: 0.030108
train_f_rmse: 0.045435
2025-04-08 20:01:24.106 INFO: train_f_rmse: 0.045435
val_e/atom_mae: 0.006117
2025-04-08 20:01:24.109 INFO: val_e/atom_mae: 0.006117
val_e/atom_rmse: 0.007297
2025-04-08 20:01:24.109 INFO: val_e/atom_rmse: 0.007297
val_f_mae: 0.029366
2025-04-08 20:01:24.110 INFO: val_f_mae: 0.029366
val_f_rmse: 0.043426
2025-04-08 20:01:24.110 INFO: val_f_rmse: 0.043426
##### Step: 15 Learning rate: 0.01 #####
2025-04-08 20:02:51.747 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 2.0779, Val Loss: 2.0933
2025-04-08 20:02:51.748 INFO: Epoch 16, Train Loss: 2.0779, Val Loss: 2.0933
train_e/atom_mae: 0.005091
2025-04-08 20:02:51.749 INFO: train_e/atom_mae: 0.005091
train_e/atom_rmse: 0.006325
2025-04-08 20:02:51.749 INFO: train_e/atom_rmse: 0.006325
train_f_mae: 0.029952
2025-04-08 20:02:51.753 INFO: train_f_mae: 0.029952
train_f_rmse: 0.045050
2025-04-08 20:02:51.753 INFO: train_f_rmse: 0.045050
val_e/atom_mae: 0.004525
2025-04-08 20:02:51.756 INFO: val_e/atom_mae: 0.004525
val_e/atom_rmse: 0.005489
2025-04-08 20:02:51.756 INFO: val_e/atom_rmse: 0.005489
val_f_mae: 0.028836
2025-04-08 20:02:51.757 INFO: val_f_mae: 0.028836
val_f_rmse: 0.045352
2025-04-08 20:02:51.757 INFO: val_f_rmse: 0.045352
##### Step: 16 Learning rate: 0.01 #####
2025-04-08 20:04:19.166 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 2.0046, Val Loss: 2.4793
2025-04-08 20:04:19.167 INFO: Epoch 17, Train Loss: 2.0046, Val Loss: 2.4793
train_e/atom_mae: 0.005605
2025-04-08 20:04:19.168 INFO: train_e/atom_mae: 0.005605
train_e/atom_rmse: 0.006929
2025-04-08 20:04:19.168 INFO: train_e/atom_rmse: 0.006929
train_f_mae: 0.029049
2025-04-08 20:04:19.171 INFO: train_f_mae: 0.029049
train_f_rmse: 0.044119
2025-04-08 20:04:19.172 INFO: train_f_rmse: 0.044119
val_e/atom_mae: 0.012563
2025-04-08 20:04:19.174 INFO: val_e/atom_mae: 0.012563
val_e/atom_rmse: 0.012939
2025-04-08 20:04:19.175 INFO: val_e/atom_rmse: 0.012939
val_f_mae: 0.031766
2025-04-08 20:04:19.175 INFO: val_f_mae: 0.031766
val_f_rmse: 0.047715
2025-04-08 20:04:19.175 INFO: val_f_rmse: 0.047715
##### Step: 17 Learning rate: 0.01 #####
2025-04-08 20:05:46.146 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 1.9101, Val Loss: 1.9131
2025-04-08 20:05:46.146 INFO: Epoch 18, Train Loss: 1.9101, Val Loss: 1.9131
train_e/atom_mae: 0.005381
2025-04-08 20:05:46.147 INFO: train_e/atom_mae: 0.005381
train_e/atom_rmse: 0.006735
2025-04-08 20:05:46.147 INFO: train_e/atom_rmse: 0.006735
train_f_mae: 0.028507
2025-04-08 20:05:46.151 INFO: train_f_mae: 0.028507
train_f_rmse: 0.043072
2025-04-08 20:05:46.151 INFO: train_f_rmse: 0.043072
val_e/atom_mae: 0.007087
2025-04-08 20:05:46.154 INFO: val_e/atom_mae: 0.007087
val_e/atom_rmse: 0.007905
2025-04-08 20:05:46.154 INFO: val_e/atom_rmse: 0.007905
val_f_mae: 0.028956
2025-04-08 20:05:46.154 INFO: val_f_mae: 0.028956
val_f_rmse: 0.042866
2025-04-08 20:05:46.154 INFO: val_f_rmse: 0.042866
##### Step: 18 Learning rate: 0.01 #####
2025-04-08 20:07:13.122 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 2.0646, Val Loss: 1.8474
2025-04-08 20:07:13.123 INFO: Epoch 19, Train Loss: 2.0646, Val Loss: 1.8474
train_e/atom_mae: 0.005686
2025-04-08 20:07:13.123 INFO: train_e/atom_mae: 0.005686
train_e/atom_rmse: 0.007028
2025-04-08 20:07:13.124 INFO: train_e/atom_rmse: 0.007028
train_f_mae: 0.029831
2025-04-08 20:07:13.127 INFO: train_f_mae: 0.029831
train_f_rmse: 0.044776
2025-04-08 20:07:13.127 INFO: train_f_rmse: 0.044776
val_e/atom_mae: 0.003245
2025-04-08 20:07:13.130 INFO: val_e/atom_mae: 0.003245
val_e/atom_rmse: 0.003912
2025-04-08 20:07:13.130 INFO: val_e/atom_rmse: 0.003912
val_f_mae: 0.028825
2025-04-08 20:07:13.131 INFO: val_f_mae: 0.028825
val_f_rmse: 0.042765
2025-04-08 20:07:13.132 INFO: val_f_rmse: 0.042765
##### Step: 19 Learning rate: 0.01 #####
2025-04-08 20:08:40.253 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 1.7940, Val Loss: 1.8908
2025-04-08 20:08:40.254 INFO: Epoch 20, Train Loss: 1.7940, Val Loss: 1.8908
train_e/atom_mae: 0.005595
2025-04-08 20:08:40.254 INFO: train_e/atom_mae: 0.005595
train_e/atom_rmse: 0.006888
2025-04-08 20:08:40.255 INFO: train_e/atom_rmse: 0.006888
train_f_mae: 0.027747
2025-04-08 20:08:40.258 INFO: train_f_mae: 0.027747
train_f_rmse: 0.041673
2025-04-08 20:08:40.258 INFO: train_f_rmse: 0.041673
val_e/atom_mae: 0.010063
2025-04-08 20:08:40.261 INFO: val_e/atom_mae: 0.010063
val_e/atom_rmse: 0.010519
2025-04-08 20:08:40.261 INFO: val_e/atom_rmse: 0.010519
val_f_mae: 0.027364
2025-04-08 20:08:40.262 INFO: val_f_mae: 0.027364
val_f_rmse: 0.041915
2025-04-08 20:08:40.262 INFO: val_f_rmse: 0.041915
##### Step: 20 Learning rate: 0.005 #####
2025-04-08 20:10:07.210 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 1.5050, Val Loss: 1.4251
2025-04-08 20:10:07.210 INFO: Epoch 21, Train Loss: 1.5050, Val Loss: 1.4251
train_e/atom_mae: 0.002968
2025-04-08 20:10:07.211 INFO: train_e/atom_mae: 0.002968
train_e/atom_rmse: 0.003633
2025-04-08 20:10:07.211 INFO: train_e/atom_rmse: 0.003633
train_f_mae: 0.025886
2025-04-08 20:10:07.215 INFO: train_f_mae: 0.025886
train_f_rmse: 0.038588
2025-04-08 20:10:07.215 INFO: train_f_rmse: 0.038588
val_e/atom_mae: 0.003382
2025-04-08 20:10:07.218 INFO: val_e/atom_mae: 0.003382
val_e/atom_rmse: 0.004092
2025-04-08 20:10:07.218 INFO: val_e/atom_rmse: 0.004092
val_f_mae: 0.025700
2025-04-08 20:10:07.218 INFO: val_f_mae: 0.025700
val_f_rmse: 0.037481
2025-04-08 20:10:07.219 INFO: val_f_rmse: 0.037481
##### Step: 21 Learning rate: 0.005 #####
2025-04-08 20:11:34.324 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 1.4959, Val Loss: 1.4929
2025-04-08 20:11:34.324 INFO: Epoch 22, Train Loss: 1.4959, Val Loss: 1.4929
train_e/atom_mae: 0.003678
2025-04-08 20:11:34.325 INFO: train_e/atom_mae: 0.003678
train_e/atom_rmse: 0.004821
2025-04-08 20:11:34.325 INFO: train_e/atom_rmse: 0.004821
train_f_mae: 0.025826
2025-04-08 20:11:34.328 INFO: train_f_mae: 0.025826
train_f_rmse: 0.038311
2025-04-08 20:11:34.328 INFO: train_f_rmse: 0.038311
val_e/atom_mae: 0.010783
2025-04-08 20:11:34.331 INFO: val_e/atom_mae: 0.010783
val_e/atom_rmse: 0.010974
2025-04-08 20:11:34.331 INFO: val_e/atom_rmse: 0.010974
val_f_mae: 0.024996
2025-04-08 20:11:34.332 INFO: val_f_mae: 0.024996
val_f_rmse: 0.036704
2025-04-08 20:11:34.332 INFO: val_f_rmse: 0.036704
##### Step: 22 Learning rate: 0.005 #####
2025-04-08 20:13:01.333 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 1.4152, Val Loss: 1.3237
2025-04-08 20:13:01.333 INFO: Epoch 23, Train Loss: 1.4152, Val Loss: 1.3237
train_e/atom_mae: 0.003820
2025-04-08 20:13:01.334 INFO: train_e/atom_mae: 0.003820
train_e/atom_rmse: 0.004938
2025-04-08 20:13:01.334 INFO: train_e/atom_rmse: 0.004938
train_f_mae: 0.025216
2025-04-08 20:13:01.338 INFO: train_f_mae: 0.025216
train_f_rmse: 0.037225
2025-04-08 20:13:01.338 INFO: train_f_rmse: 0.037225
val_e/atom_mae: 0.001548
2025-04-08 20:13:01.340 INFO: val_e/atom_mae: 0.001548
val_e/atom_rmse: 0.001841
2025-04-08 20:13:01.341 INFO: val_e/atom_rmse: 0.001841
val_f_mae: 0.025190
2025-04-08 20:13:01.341 INFO: val_f_mae: 0.025190
val_f_rmse: 0.036326
2025-04-08 20:13:01.341 INFO: val_f_rmse: 0.036326
##### Step: 23 Learning rate: 0.005 #####
2025-04-08 20:14:28.380 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 1.2711, Val Loss: 1.3688
2025-04-08 20:14:28.380 INFO: Epoch 24, Train Loss: 1.2711, Val Loss: 1.3688
train_e/atom_mae: 0.001809
2025-04-08 20:14:28.381 INFO: train_e/atom_mae: 0.001809
train_e/atom_rmse: 0.002212
2025-04-08 20:14:28.381 INFO: train_e/atom_rmse: 0.002212
train_f_mae: 0.024100
2025-04-08 20:14:28.385 INFO: train_f_mae: 0.024100
train_f_rmse: 0.035570
2025-04-08 20:14:28.385 INFO: train_f_rmse: 0.035570
val_e/atom_mae: 0.003048
2025-04-08 20:14:28.387 INFO: val_e/atom_mae: 0.003048
val_e/atom_rmse: 0.003238
2025-04-08 20:14:28.388 INFO: val_e/atom_rmse: 0.003238
val_f_mae: 0.024555
2025-04-08 20:14:28.388 INFO: val_f_mae: 0.024555
val_f_rmse: 0.036826
2025-04-08 20:14:28.388 INFO: val_f_rmse: 0.036826
##### Step: 24 Learning rate: 0.005 #####
2025-04-08 20:15:55.320 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 1.2794, Val Loss: 1.4132
2025-04-08 20:15:55.321 INFO: Epoch 25, Train Loss: 1.2794, Val Loss: 1.4132
train_e/atom_mae: 0.002618
2025-04-08 20:15:55.322 INFO: train_e/atom_mae: 0.002618
train_e/atom_rmse: 0.003249
2025-04-08 20:15:55.322 INFO: train_e/atom_rmse: 0.003249
train_f_mae: 0.024084
2025-04-08 20:15:55.326 INFO: train_f_mae: 0.024084
train_f_rmse: 0.035590
2025-04-08 20:15:55.326 INFO: train_f_rmse: 0.035590
val_e/atom_mae: 0.004568
2025-04-08 20:15:55.328 INFO: val_e/atom_mae: 0.004568
val_e/atom_rmse: 0.004784
2025-04-08 20:15:55.329 INFO: val_e/atom_rmse: 0.004784
val_f_mae: 0.024685
2025-04-08 20:15:55.329 INFO: val_f_mae: 0.024685
val_f_rmse: 0.037222
2025-04-08 20:15:55.329 INFO: val_f_rmse: 0.037222
##### Step: 25 Learning rate: 0.005 #####
2025-04-08 20:17:22.349 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 1.2114, Val Loss: 1.1827
2025-04-08 20:17:22.350 INFO: Epoch 26, Train Loss: 1.2114, Val Loss: 1.1827
train_e/atom_mae: 0.002202
2025-04-08 20:17:22.351 INFO: train_e/atom_mae: 0.002202
train_e/atom_rmse: 0.002811
2025-04-08 20:17:22.351 INFO: train_e/atom_rmse: 0.002811
train_f_mae: 0.023562
2025-04-08 20:17:22.354 INFO: train_f_mae: 0.023562
train_f_rmse: 0.034668
2025-04-08 20:17:22.354 INFO: train_f_rmse: 0.034668
val_e/atom_mae: 0.003326
2025-04-08 20:17:22.357 INFO: val_e/atom_mae: 0.003326
val_e/atom_rmse: 0.003407
2025-04-08 20:17:22.357 INFO: val_e/atom_rmse: 0.003407
val_f_mae: 0.023635
2025-04-08 20:17:22.358 INFO: val_f_mae: 0.023635
val_f_rmse: 0.034185
2025-04-08 20:17:22.358 INFO: val_f_rmse: 0.034185
##### Step: 26 Learning rate: 0.005 #####
2025-04-08 20:18:49.531 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 1.1763, Val Loss: 1.3508
2025-04-08 20:18:49.532 INFO: Epoch 27, Train Loss: 1.1763, Val Loss: 1.3508
train_e/atom_mae: 0.002043
2025-04-08 20:18:49.533 INFO: train_e/atom_mae: 0.002043
train_e/atom_rmse: 0.002616
2025-04-08 20:18:49.533 INFO: train_e/atom_rmse: 0.002616
train_f_mae: 0.023165
2025-04-08 20:18:49.536 INFO: train_f_mae: 0.023165
train_f_rmse: 0.034176
2025-04-08 20:18:49.536 INFO: train_f_rmse: 0.034176
val_e/atom_mae: 0.001095
2025-04-08 20:18:49.539 INFO: val_e/atom_mae: 0.001095
val_e/atom_rmse: 0.001505
2025-04-08 20:18:49.539 INFO: val_e/atom_rmse: 0.001505
val_f_mae: 0.024861
2025-04-08 20:18:49.540 INFO: val_f_mae: 0.024861
val_f_rmse: 0.036716
2025-04-08 20:18:49.540 INFO: val_f_rmse: 0.036716
##### Step: 27 Learning rate: 0.005 #####
2025-04-08 20:20:17.131 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 1.1685, Val Loss: 1.1096
2025-04-08 20:20:17.131 INFO: Epoch 28, Train Loss: 1.1685, Val Loss: 1.1096
train_e/atom_mae: 0.002164
2025-04-08 20:20:17.132 INFO: train_e/atom_mae: 0.002164
train_e/atom_rmse: 0.002640
2025-04-08 20:20:17.133 INFO: train_e/atom_rmse: 0.002640
train_f_mae: 0.022985
2025-04-08 20:20:17.136 INFO: train_f_mae: 0.022985
train_f_rmse: 0.034060
2025-04-08 20:20:17.136 INFO: train_f_rmse: 0.034060
val_e/atom_mae: 0.003557
2025-04-08 20:20:17.139 INFO: val_e/atom_mae: 0.003557
val_e/atom_rmse: 0.003589
2025-04-08 20:20:17.139 INFO: val_e/atom_rmse: 0.003589
val_f_mae: 0.022594
2025-04-08 20:20:17.140 INFO: val_f_mae: 0.022594
val_f_rmse: 0.033075
2025-04-08 20:20:17.140 INFO: val_f_rmse: 0.033075
##### Step: 28 Learning rate: 0.005 #####
2025-04-08 20:21:44.598 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 1.0672, Val Loss: 1.1637
2025-04-08 20:21:44.599 INFO: Epoch 29, Train Loss: 1.0672, Val Loss: 1.1637
train_e/atom_mae: 0.001800
2025-04-08 20:21:44.600 INFO: train_e/atom_mae: 0.001800
train_e/atom_rmse: 0.002240
2025-04-08 20:21:44.600 INFO: train_e/atom_rmse: 0.002240
train_f_mae: 0.022171
2025-04-08 20:21:44.603 INFO: train_f_mae: 0.022171
train_f_rmse: 0.032575
2025-04-08 20:21:44.603 INFO: train_f_rmse: 0.032575
val_e/atom_mae: 0.001252
2025-04-08 20:21:44.606 INFO: val_e/atom_mae: 0.001252
val_e/atom_rmse: 0.001395
2025-04-08 20:21:44.606 INFO: val_e/atom_rmse: 0.001395
val_f_mae: 0.023726
2025-04-08 20:21:44.607 INFO: val_f_mae: 0.023726
val_f_rmse: 0.034078
2025-04-08 20:21:44.607 INFO: val_f_rmse: 0.034078
##### Step: 29 Learning rate: 0.005 #####
2025-04-08 20:23:12.015 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 1.0606, Val Loss: 1.1795
2025-04-08 20:23:12.015 INFO: Epoch 30, Train Loss: 1.0606, Val Loss: 1.1795
train_e/atom_mae: 0.001794
2025-04-08 20:23:12.016 INFO: train_e/atom_mae: 0.001794
train_e/atom_rmse: 0.002276
2025-04-08 20:23:12.016 INFO: train_e/atom_rmse: 0.002276
train_f_mae: 0.022205
2025-04-08 20:23:12.020 INFO: train_f_mae: 0.022205
train_f_rmse: 0.032471
2025-04-08 20:23:12.020 INFO: train_f_rmse: 0.032471
val_e/atom_mae: 0.001608
2025-04-08 20:23:12.023 INFO: val_e/atom_mae: 0.001608
val_e/atom_rmse: 0.001717
2025-04-08 20:23:12.023 INFO: val_e/atom_rmse: 0.001717
val_f_mae: 0.023140
2025-04-08 20:23:12.023 INFO: val_f_mae: 0.023140
val_f_rmse: 0.034292
2025-04-08 20:23:12.024 INFO: val_f_rmse: 0.034292
##### Step: 30 Learning rate: 0.005 #####
2025-04-08 20:24:39.455 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 1.0780, Val Loss: 1.2042
2025-04-08 20:24:39.456 INFO: Epoch 31, Train Loss: 1.0780, Val Loss: 1.2042
train_e/atom_mae: 0.002310
2025-04-08 20:24:39.457 INFO: train_e/atom_mae: 0.002310
train_e/atom_rmse: 0.003005
2025-04-08 20:24:39.457 INFO: train_e/atom_rmse: 0.003005
train_f_mae: 0.022277
2025-04-08 20:24:39.461 INFO: train_f_mae: 0.022277
train_f_rmse: 0.032667
2025-04-08 20:24:39.461 INFO: train_f_rmse: 0.032667
val_e/atom_mae: 0.004696
2025-04-08 20:24:39.463 INFO: val_e/atom_mae: 0.004696
val_e/atom_rmse: 0.004754
2025-04-08 20:24:39.464 INFO: val_e/atom_rmse: 0.004754
val_f_mae: 0.023524
2025-04-08 20:24:39.464 INFO: val_f_mae: 0.023524
val_f_rmse: 0.034306
2025-04-08 20:24:39.464 INFO: val_f_rmse: 0.034306
##### Step: 31 Learning rate: 0.005 #####
2025-04-08 20:26:06.881 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 1.0401, Val Loss: 0.9417
2025-04-08 20:26:06.882 INFO: Epoch 32, Train Loss: 1.0401, Val Loss: 0.9417
train_e/atom_mae: 0.001782
2025-04-08 20:26:06.882 INFO: train_e/atom_mae: 0.001782
train_e/atom_rmse: 0.002298
2025-04-08 20:26:06.883 INFO: train_e/atom_rmse: 0.002298
train_f_mae: 0.021886
2025-04-08 20:26:06.886 INFO: train_f_mae: 0.021886
train_f_rmse: 0.032151
2025-04-08 20:26:06.886 INFO: train_f_rmse: 0.032151
val_e/atom_mae: 0.003611
2025-04-08 20:26:06.889 INFO: val_e/atom_mae: 0.003611
val_e/atom_rmse: 0.003695
2025-04-08 20:26:06.889 INFO: val_e/atom_rmse: 0.003695
val_f_mae: 0.021010
2025-04-08 20:26:06.890 INFO: val_f_mae: 0.021010
val_f_rmse: 0.030417
2025-04-08 20:26:06.890 INFO: val_f_rmse: 0.030417
##### Step: 32 Learning rate: 0.005 #####
2025-04-08 20:27:34.297 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.9975, Val Loss: 0.9512
2025-04-08 20:27:34.297 INFO: Epoch 33, Train Loss: 0.9975, Val Loss: 0.9512
train_e/atom_mae: 0.002821
2025-04-08 20:27:34.298 INFO: train_e/atom_mae: 0.002821
train_e/atom_rmse: 0.003769
2025-04-08 20:27:34.298 INFO: train_e/atom_rmse: 0.003769
train_f_mae: 0.021421
2025-04-08 20:27:34.302 INFO: train_f_mae: 0.021421
train_f_rmse: 0.031309
2025-04-08 20:27:34.302 INFO: train_f_rmse: 0.031309
val_e/atom_mae: 0.005051
2025-04-08 20:27:34.304 INFO: val_e/atom_mae: 0.005051
val_e/atom_rmse: 0.005093
2025-04-08 20:27:34.305 INFO: val_e/atom_rmse: 0.005093
val_f_mae: 0.020615
2025-04-08 20:27:34.305 INFO: val_f_mae: 0.020615
val_f_rmse: 0.030329
2025-04-08 20:27:34.305 INFO: val_f_rmse: 0.030329
##### Step: 33 Learning rate: 0.005 #####
2025-04-08 20:29:01.764 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.9945, Val Loss: 0.9265
2025-04-08 20:29:01.765 INFO: Epoch 34, Train Loss: 0.9945, Val Loss: 0.9265
train_e/atom_mae: 0.002365
2025-04-08 20:29:01.766 INFO: train_e/atom_mae: 0.002365
train_e/atom_rmse: 0.002888
2025-04-08 20:29:01.766 INFO: train_e/atom_rmse: 0.002888
train_f_mae: 0.021498
2025-04-08 20:29:01.770 INFO: train_f_mae: 0.021498
train_f_rmse: 0.031375
2025-04-08 20:29:01.770 INFO: train_f_rmse: 0.031375
val_e/atom_mae: 0.001429
2025-04-08 20:29:01.772 INFO: val_e/atom_mae: 0.001429
val_e/atom_rmse: 0.001639
2025-04-08 20:29:01.773 INFO: val_e/atom_rmse: 0.001639
val_f_mae: 0.021035
2025-04-08 20:29:01.773 INFO: val_f_mae: 0.021035
val_f_rmse: 0.030386
2025-04-08 20:29:01.773 INFO: val_f_rmse: 0.030386
##### Step: 34 Learning rate: 0.005 #####
2025-04-08 20:30:29.101 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.9385, Val Loss: 1.3860
2025-04-08 20:30:29.102 INFO: Epoch 35, Train Loss: 0.9385, Val Loss: 1.3860
train_e/atom_mae: 0.001633
2025-04-08 20:30:29.102 INFO: train_e/atom_mae: 0.001633
train_e/atom_rmse: 0.002047
2025-04-08 20:30:29.103 INFO: train_e/atom_rmse: 0.002047
train_f_mae: 0.020865
2025-04-08 20:30:29.106 INFO: train_f_mae: 0.020865
train_f_rmse: 0.030553
2025-04-08 20:30:29.106 INFO: train_f_rmse: 0.030553
val_e/atom_mae: 0.000673
2025-04-08 20:30:29.109 INFO: val_e/atom_mae: 0.000673
val_e/atom_rmse: 0.000806
2025-04-08 20:30:29.109 INFO: val_e/atom_rmse: 0.000806
val_f_mae: 0.023944
2025-04-08 20:30:29.110 INFO: val_f_mae: 0.023944
val_f_rmse: 0.037218
2025-04-08 20:30:29.110 INFO: val_f_rmse: 0.037218
##### Step: 35 Learning rate: 0.005 #####
2025-04-08 20:31:56.552 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.9962, Val Loss: 0.8613
2025-04-08 20:31:56.552 INFO: Epoch 36, Train Loss: 0.9962, Val Loss: 0.8613
train_e/atom_mae: 0.002796
2025-04-08 20:31:56.553 INFO: train_e/atom_mae: 0.002796
train_e/atom_rmse: 0.003693
2025-04-08 20:31:56.553 INFO: train_e/atom_rmse: 0.003693
train_f_mae: 0.021277
2025-04-08 20:31:56.557 INFO: train_f_mae: 0.021277
train_f_rmse: 0.031301
2025-04-08 20:31:56.557 INFO: train_f_rmse: 0.031301
val_e/atom_mae: 0.001482
2025-04-08 20:31:56.560 INFO: val_e/atom_mae: 0.001482
val_e/atom_rmse: 0.001651
2025-04-08 20:31:56.560 INFO: val_e/atom_rmse: 0.001651
val_f_mae: 0.020313
2025-04-08 20:31:56.561 INFO: val_f_mae: 0.020313
val_f_rmse: 0.029291
2025-04-08 20:31:56.561 INFO: val_f_rmse: 0.029291
##### Step: 36 Learning rate: 0.005 #####
2025-04-08 20:33:24.196 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.8982, Val Loss: 0.9178
2025-04-08 20:33:24.197 INFO: Epoch 37, Train Loss: 0.8982, Val Loss: 0.9178
train_e/atom_mae: 0.002017
2025-04-08 20:33:24.198 INFO: train_e/atom_mae: 0.002017
train_e/atom_rmse: 0.002533
2025-04-08 20:33:24.198 INFO: train_e/atom_rmse: 0.002533
train_f_mae: 0.020495
2025-04-08 20:33:24.201 INFO: train_f_mae: 0.020495
train_f_rmse: 0.029841
2025-04-08 20:33:24.201 INFO: train_f_rmse: 0.029841
val_e/atom_mae: 0.002579
2025-04-08 20:33:24.204 INFO: val_e/atom_mae: 0.002579
val_e/atom_rmse: 0.002742
2025-04-08 20:33:24.204 INFO: val_e/atom_rmse: 0.002742
val_f_mae: 0.020625
2025-04-08 20:33:24.205 INFO: val_f_mae: 0.020625
val_f_rmse: 0.030145
2025-04-08 20:33:24.205 INFO: val_f_rmse: 0.030145
##### Step: 37 Learning rate: 0.005 #####
2025-04-08 20:34:51.609 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.9417, Val Loss: 0.8708
2025-04-08 20:34:51.610 INFO: Epoch 38, Train Loss: 0.9417, Val Loss: 0.8708
train_e/atom_mae: 0.002494
2025-04-08 20:34:51.611 INFO: train_e/atom_mae: 0.002494
train_e/atom_rmse: 0.003272
2025-04-08 20:34:51.611 INFO: train_e/atom_rmse: 0.003272
train_f_mae: 0.020794
2025-04-08 20:34:51.614 INFO: train_f_mae: 0.020794
train_f_rmse: 0.030476
2025-04-08 20:34:51.614 INFO: train_f_rmse: 0.030476
val_e/atom_mae: 0.002790
2025-04-08 20:34:51.617 INFO: val_e/atom_mae: 0.002790
val_e/atom_rmse: 0.002996
2025-04-08 20:34:51.617 INFO: val_e/atom_rmse: 0.002996
val_f_mae: 0.020516
2025-04-08 20:34:51.618 INFO: val_f_mae: 0.020516
val_f_rmse: 0.029324
2025-04-08 20:34:51.618 INFO: val_f_rmse: 0.029324
##### Step: 38 Learning rate: 0.005 #####
2025-04-08 20:36:19.115 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.9002, Val Loss: 1.0048
2025-04-08 20:36:19.115 INFO: Epoch 39, Train Loss: 0.9002, Val Loss: 1.0048
train_e/atom_mae: 0.002036
2025-04-08 20:36:19.116 INFO: train_e/atom_mae: 0.002036
train_e/atom_rmse: 0.002587
2025-04-08 20:36:19.116 INFO: train_e/atom_rmse: 0.002587
train_f_mae: 0.020507
2025-04-08 20:36:19.120 INFO: train_f_mae: 0.020507
train_f_rmse: 0.029869
2025-04-08 20:36:19.120 INFO: train_f_rmse: 0.029869
val_e/atom_mae: 0.005013
2025-04-08 20:36:19.123 INFO: val_e/atom_mae: 0.005013
val_e/atom_rmse: 0.005173
2025-04-08 20:36:19.123 INFO: val_e/atom_rmse: 0.005173
val_f_mae: 0.020831
2025-04-08 20:36:19.124 INFO: val_f_mae: 0.020831
val_f_rmse: 0.031184
2025-04-08 20:36:19.124 INFO: val_f_rmse: 0.031184
##### Step: 39 Learning rate: 0.005 #####
2025-04-08 20:37:47.002 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.8931, Val Loss: 0.8470
2025-04-08 20:37:47.003 INFO: Epoch 40, Train Loss: 0.8931, Val Loss: 0.8470
train_e/atom_mae: 0.002781
2025-04-08 20:37:47.004 INFO: train_e/atom_mae: 0.002781
train_e/atom_rmse: 0.003425
2025-04-08 20:37:47.004 INFO: train_e/atom_rmse: 0.003425
train_f_mae: 0.020308
2025-04-08 20:37:47.007 INFO: train_f_mae: 0.020308
train_f_rmse: 0.029647
2025-04-08 20:37:47.007 INFO: train_f_rmse: 0.029647
val_e/atom_mae: 0.001944
2025-04-08 20:37:47.010 INFO: val_e/atom_mae: 0.001944
val_e/atom_rmse: 0.002064
2025-04-08 20:37:47.010 INFO: val_e/atom_rmse: 0.002064
val_f_mae: 0.020294
2025-04-08 20:37:47.011 INFO: val_f_mae: 0.020294
val_f_rmse: 0.029015
2025-04-08 20:37:47.011 INFO: val_f_rmse: 0.029015
2025-04-08 20:37:47.030 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-04-08 20:39:14.478 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 7.1874, Val Loss: 2.3836
2025-04-08 20:39:14.479 INFO: Epoch 1, Train Loss: 7.1874, Val Loss: 2.3836
train_e/atom_mae: 0.009786
2025-04-08 20:39:14.480 INFO: train_e/atom_mae: 0.009786
train_e/atom_rmse: 0.017172
2025-04-08 20:39:14.480 INFO: train_e/atom_rmse: 0.017172
train_f_mae: 0.041779
2025-04-08 20:39:14.484 INFO: train_f_mae: 0.041779
train_f_rmse: 0.082648
2025-04-08 20:39:14.484 INFO: train_f_rmse: 0.082648
val_e/atom_mae: 0.005607
2025-04-08 20:39:14.486 INFO: val_e/atom_mae: 0.005607
val_e/atom_rmse: 0.006471
2025-04-08 20:39:14.487 INFO: val_e/atom_rmse: 0.006471
val_f_mae: 0.030560
2025-04-08 20:39:14.487 INFO: val_f_mae: 0.030560
val_f_rmse: 0.048301
2025-04-08 20:39:14.487 INFO: val_f_rmse: 0.048301
##### Step: 1 Learning rate: 0.004 #####
2025-04-08 20:40:42.123 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 2.5807, Val Loss: 2.0285
2025-04-08 20:40:42.124 INFO: Epoch 2, Train Loss: 2.5807, Val Loss: 2.0285
train_e/atom_mae: 0.005361
2025-04-08 20:40:42.125 INFO: train_e/atom_mae: 0.005361
train_e/atom_rmse: 0.007128
2025-04-08 20:40:42.125 INFO: train_e/atom_rmse: 0.007128
train_f_mae: 0.030398
2025-04-08 20:40:42.128 INFO: train_f_mae: 0.030398
train_f_rmse: 0.050192
2025-04-08 20:40:42.128 INFO: train_f_rmse: 0.050192
val_e/atom_mae: 0.009013
2025-04-08 20:40:42.131 INFO: val_e/atom_mae: 0.009013
val_e/atom_rmse: 0.009200
2025-04-08 20:40:42.131 INFO: val_e/atom_rmse: 0.009200
val_f_mae: 0.025666
2025-04-08 20:40:42.132 INFO: val_f_mae: 0.025666
val_f_rmse: 0.043888
2025-04-08 20:40:42.132 INFO: val_f_rmse: 0.043888
##### Step: 2 Learning rate: 0.006 #####
2025-04-08 20:42:09.859 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 2.2069, Val Loss: 1.3644
2025-04-08 20:42:09.859 INFO: Epoch 3, Train Loss: 2.2069, Val Loss: 1.3644
train_e/atom_mae: 0.005798
2025-04-08 20:42:09.860 INFO: train_e/atom_mae: 0.005798
train_e/atom_rmse: 0.007302
2025-04-08 20:42:09.860 INFO: train_e/atom_rmse: 0.007302
train_f_mae: 0.028241
2025-04-08 20:42:09.864 INFO: train_f_mae: 0.028241
train_f_rmse: 0.046286
2025-04-08 20:42:09.864 INFO: train_f_rmse: 0.046286
val_e/atom_mae: 0.001684
2025-04-08 20:42:09.866 INFO: val_e/atom_mae: 0.001684
val_e/atom_rmse: 0.001984
2025-04-08 20:42:09.867 INFO: val_e/atom_rmse: 0.001984
val_f_mae: 0.023913
2025-04-08 20:42:09.867 INFO: val_f_mae: 0.023913
val_f_rmse: 0.036873
2025-04-08 20:42:09.867 INFO: val_f_rmse: 0.036873
##### Step: 3 Learning rate: 0.008 #####
2025-04-08 20:43:37.487 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 1.8742, Val Loss: 1.5504
2025-04-08 20:43:37.487 INFO: Epoch 4, Train Loss: 1.8742, Val Loss: 1.5504
train_e/atom_mae: 0.005375
2025-04-08 20:43:37.488 INFO: train_e/atom_mae: 0.005375
train_e/atom_rmse: 0.006921
2025-04-08 20:43:37.488 INFO: train_e/atom_rmse: 0.006921
train_f_mae: 0.026226
2025-04-08 20:43:37.492 INFO: train_f_mae: 0.026226
train_f_rmse: 0.042617
2025-04-08 20:43:37.492 INFO: train_f_rmse: 0.042617
val_e/atom_mae: 0.001674
2025-04-08 20:43:37.495 INFO: val_e/atom_mae: 0.001674
val_e/atom_rmse: 0.002470
2025-04-08 20:43:37.495 INFO: val_e/atom_rmse: 0.002470
val_f_mae: 0.025650
2025-04-08 20:43:37.496 INFO: val_f_mae: 0.025650
val_f_rmse: 0.039282
2025-04-08 20:43:37.496 INFO: val_f_rmse: 0.039282
##### Step: 4 Learning rate: 0.01 #####
2025-04-08 20:45:04.980 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 1.6733, Val Loss: 1.8368
2025-04-08 20:45:04.980 INFO: Epoch 5, Train Loss: 1.6733, Val Loss: 1.8368
train_e/atom_mae: 0.006508
2025-04-08 20:45:04.981 INFO: train_e/atom_mae: 0.006508
train_e/atom_rmse: 0.008011
2025-04-08 20:45:04.982 INFO: train_e/atom_rmse: 0.008011
train_f_mae: 0.025469
2025-04-08 20:45:04.985 INFO: train_f_mae: 0.025469
train_f_rmse: 0.039945
2025-04-08 20:45:04.985 INFO: train_f_rmse: 0.039945
val_e/atom_mae: 0.002858
2025-04-08 20:45:04.988 INFO: val_e/atom_mae: 0.002858
val_e/atom_rmse: 0.003438
2025-04-08 20:45:04.988 INFO: val_e/atom_rmse: 0.003438
val_f_mae: 0.027520
2025-04-08 20:45:04.989 INFO: val_f_mae: 0.027520
val_f_rmse: 0.042691
2025-04-08 20:45:04.989 INFO: val_f_rmse: 0.042691
##### Step: 5 Learning rate: 0.01 #####
2025-04-08 20:46:32.744 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 1.4242, Val Loss: 1.6642
2025-04-08 20:46:32.745 INFO: Epoch 6, Train Loss: 1.4242, Val Loss: 1.6642
train_e/atom_mae: 0.006219
2025-04-08 20:46:32.746 INFO: train_e/atom_mae: 0.006219
train_e/atom_rmse: 0.007814
2025-04-08 20:46:32.746 INFO: train_e/atom_rmse: 0.007814
train_f_mae: 0.023113
2025-04-08 20:46:32.749 INFO: train_f_mae: 0.023113
train_f_rmse: 0.036747
2025-04-08 20:46:32.749 INFO: train_f_rmse: 0.036747
val_e/atom_mae: 0.002784
2025-04-08 20:46:32.752 INFO: val_e/atom_mae: 0.002784
val_e/atom_rmse: 0.003277
2025-04-08 20:46:32.752 INFO: val_e/atom_rmse: 0.003277
val_f_mae: 0.022795
2025-04-08 20:46:32.753 INFO: val_f_mae: 0.022795
val_f_rmse: 0.040635
2025-04-08 20:46:32.753 INFO: val_f_rmse: 0.040635
##### Step: 6 Learning rate: 0.01 #####
2025-04-08 20:48:00.150 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 1.3919, Val Loss: 0.9709
2025-04-08 20:48:00.150 INFO: Epoch 7, Train Loss: 1.3919, Val Loss: 0.9709
train_e/atom_mae: 0.005854
2025-04-08 20:48:00.151 INFO: train_e/atom_mae: 0.005854
train_e/atom_rmse: 0.007247
2025-04-08 20:48:00.152 INFO: train_e/atom_rmse: 0.007247
train_f_mae: 0.022882
2025-04-08 20:48:00.155 INFO: train_f_mae: 0.022882
train_f_rmse: 0.036446
2025-04-08 20:48:00.155 INFO: train_f_rmse: 0.036446
val_e/atom_mae: 0.008029
2025-04-08 20:48:00.158 INFO: val_e/atom_mae: 0.008029
val_e/atom_rmse: 0.008345
2025-04-08 20:48:00.158 INFO: val_e/atom_rmse: 0.008345
val_f_mae: 0.019700
2025-04-08 20:48:00.159 INFO: val_f_mae: 0.019700
val_f_rmse: 0.029777
2025-04-08 20:48:00.159 INFO: val_f_rmse: 0.029777
##### Step: 7 Learning rate: 0.01 #####
2025-04-08 20:49:27.129 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 1.3760, Val Loss: 1.5250
2025-04-08 20:49:27.129 INFO: Epoch 8, Train Loss: 1.3760, Val Loss: 1.5250
train_e/atom_mae: 0.006540
2025-04-08 20:49:27.130 INFO: train_e/atom_mae: 0.006540
train_e/atom_rmse: 0.008065
2025-04-08 20:49:27.130 INFO: train_e/atom_rmse: 0.008065
train_f_mae: 0.022685
2025-04-08 20:49:27.134 INFO: train_f_mae: 0.022685
train_f_rmse: 0.036019
2025-04-08 20:49:27.134 INFO: train_f_rmse: 0.036019
val_e/atom_mae: 0.007453
2025-04-08 20:49:27.136 INFO: val_e/atom_mae: 0.007453
val_e/atom_rmse: 0.008486
2025-04-08 20:49:27.137 INFO: val_e/atom_rmse: 0.008486
val_f_mae: 0.024116
2025-04-08 20:49:27.137 INFO: val_f_mae: 0.024116
val_f_rmse: 0.037920
2025-04-08 20:49:27.137 INFO: val_f_rmse: 0.037920
##### Step: 8 Learning rate: 0.01 #####
2025-04-08 20:50:54.141 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 1.1919, Val Loss: 1.1069
2025-04-08 20:50:54.142 INFO: Epoch 9, Train Loss: 1.1919, Val Loss: 1.1069
train_e/atom_mae: 0.004813
2025-04-08 20:50:54.143 INFO: train_e/atom_mae: 0.004813
train_e/atom_rmse: 0.006069
2025-04-08 20:50:54.143 INFO: train_e/atom_rmse: 0.006069
train_f_mae: 0.021226
2025-04-08 20:50:54.146 INFO: train_f_mae: 0.021226
train_f_rmse: 0.033872
2025-04-08 20:50:54.147 INFO: train_f_rmse: 0.033872
val_e/atom_mae: 0.004050
2025-04-08 20:50:54.149 INFO: val_e/atom_mae: 0.004050
val_e/atom_rmse: 0.005254
2025-04-08 20:50:54.149 INFO: val_e/atom_rmse: 0.005254
val_f_mae: 0.020249
2025-04-08 20:50:54.150 INFO: val_f_mae: 0.020249
val_f_rmse: 0.032764
2025-04-08 20:50:54.150 INFO: val_f_rmse: 0.032764
##### Step: 9 Learning rate: 0.01 #####
2025-04-08 20:52:21.432 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 1.0232, Val Loss: 0.8744
2025-04-08 20:52:21.433 INFO: Epoch 10, Train Loss: 1.0232, Val Loss: 0.8744
train_e/atom_mae: 0.005471
2025-04-08 20:52:21.434 INFO: train_e/atom_mae: 0.005471
train_e/atom_rmse: 0.006719
2025-04-08 20:52:21.434 INFO: train_e/atom_rmse: 0.006719
train_f_mae: 0.020096
2025-04-08 20:52:21.438 INFO: train_f_mae: 0.020096
train_f_rmse: 0.031122
2025-04-08 20:52:21.438 INFO: train_f_rmse: 0.031122
val_e/atom_mae: 0.002704
2025-04-08 20:52:21.440 INFO: val_e/atom_mae: 0.002704
val_e/atom_rmse: 0.003178
2025-04-08 20:52:21.441 INFO: val_e/atom_rmse: 0.003178
val_f_mae: 0.020386
2025-04-08 20:52:21.441 INFO: val_f_mae: 0.020386
val_f_rmse: 0.029363
2025-04-08 20:52:21.441 INFO: val_f_rmse: 0.029363
##### Step: 10 Learning rate: 0.01 #####
2025-04-08 20:53:48.438 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 1.0624, Val Loss: 1.3624
2025-04-08 20:53:48.438 INFO: Epoch 11, Train Loss: 1.0624, Val Loss: 1.3624
train_e/atom_mae: 0.004783
2025-04-08 20:53:48.439 INFO: train_e/atom_mae: 0.004783
train_e/atom_rmse: 0.006130
2025-04-08 20:53:48.439 INFO: train_e/atom_rmse: 0.006130
train_f_mae: 0.019972
2025-04-08 20:53:48.443 INFO: train_f_mae: 0.019972
train_f_rmse: 0.031890
2025-04-08 20:53:48.443 INFO: train_f_rmse: 0.031890
val_e/atom_mae: 0.020806
2025-04-08 20:53:48.446 INFO: val_e/atom_mae: 0.020806
val_e/atom_rmse: 0.020991
2025-04-08 20:53:48.446 INFO: val_e/atom_rmse: 0.020991
val_f_mae: 0.019237
2025-04-08 20:53:48.447 INFO: val_f_mae: 0.019237
val_f_rmse: 0.028796
2025-04-08 20:53:48.447 INFO: val_f_rmse: 0.028796
##### Step: 11 Learning rate: 0.01 #####
2025-04-08 20:55:15.462 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 0.9678, Val Loss: 0.7513
2025-04-08 20:55:15.462 INFO: Epoch 12, Train Loss: 0.9678, Val Loss: 0.7513
train_e/atom_mae: 0.005348
2025-04-08 20:55:15.463 INFO: train_e/atom_mae: 0.005348
train_e/atom_rmse: 0.006873
2025-04-08 20:55:15.463 INFO: train_e/atom_rmse: 0.006873
train_f_mae: 0.019466
2025-04-08 20:55:15.467 INFO: train_f_mae: 0.019466
train_f_rmse: 0.030177
2025-04-08 20:55:15.467 INFO: train_f_rmse: 0.030177
val_e/atom_mae: 0.009126
2025-04-08 20:55:15.470 INFO: val_e/atom_mae: 0.009126
val_e/atom_rmse: 0.010024
2025-04-08 20:55:15.470 INFO: val_e/atom_rmse: 0.010024
val_f_mae: 0.016485
2025-04-08 20:55:15.471 INFO: val_f_mae: 0.016485
val_f_rmse: 0.025095
2025-04-08 20:55:15.471 INFO: val_f_rmse: 0.025095
##### Step: 12 Learning rate: 0.01 #####
2025-04-08 20:56:42.512 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 1.0502, Val Loss: 0.7672
2025-04-08 20:56:42.513 INFO: Epoch 13, Train Loss: 1.0502, Val Loss: 0.7672
train_e/atom_mae: 0.006065
2025-04-08 20:56:42.513 INFO: train_e/atom_mae: 0.006065
train_e/atom_rmse: 0.007581
2025-04-08 20:56:42.514 INFO: train_e/atom_rmse: 0.007581
train_f_mae: 0.019868
2025-04-08 20:56:42.517 INFO: train_f_mae: 0.019868
train_f_rmse: 0.031316
2025-04-08 20:56:42.517 INFO: train_f_rmse: 0.031316
val_e/atom_mae: 0.004066
2025-04-08 20:56:42.520 INFO: val_e/atom_mae: 0.004066
val_e/atom_rmse: 0.004641
2025-04-08 20:56:42.520 INFO: val_e/atom_rmse: 0.004641
val_f_mae: 0.018624
2025-04-08 20:56:42.521 INFO: val_f_mae: 0.018624
val_f_rmse: 0.027225
2025-04-08 20:56:42.521 INFO: val_f_rmse: 0.027225
##### Step: 13 Learning rate: 0.01 #####
2025-04-08 20:58:09.554 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 0.8285, Val Loss: 0.8564
2025-04-08 20:58:09.555 INFO: Epoch 14, Train Loss: 0.8285, Val Loss: 0.8564
train_e/atom_mae: 0.004342
2025-04-08 20:58:09.556 INFO: train_e/atom_mae: 0.004342
train_e/atom_rmse: 0.005576
2025-04-08 20:58:09.556 INFO: train_e/atom_rmse: 0.005576
train_f_mae: 0.018120
2025-04-08 20:58:09.559 INFO: train_f_mae: 0.018120
train_f_rmse: 0.028123
2025-04-08 20:58:09.559 INFO: train_f_rmse: 0.028123
val_e/atom_mae: 0.009588
2025-04-08 20:58:09.562 INFO: val_e/atom_mae: 0.009588
val_e/atom_rmse: 0.009820
2025-04-08 20:58:09.562 INFO: val_e/atom_rmse: 0.009820
val_f_mae: 0.017395
2025-04-08 20:58:09.563 INFO: val_f_mae: 0.017395
val_f_rmse: 0.027199
2025-04-08 20:58:09.563 INFO: val_f_rmse: 0.027199
##### Step: 14 Learning rate: 0.01 #####
2025-04-08 20:59:36.632 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 0.7137, Val Loss: 0.7104
2025-04-08 20:59:36.632 INFO: Epoch 15, Train Loss: 0.7137, Val Loss: 0.7104
train_e/atom_mae: 0.004662
2025-04-08 20:59:36.633 INFO: train_e/atom_mae: 0.004662
train_e/atom_rmse: 0.005716
2025-04-08 20:59:36.633 INFO: train_e/atom_rmse: 0.005716
train_f_mae: 0.016945
2025-04-08 20:59:36.637 INFO: train_f_mae: 0.016945
train_f_rmse: 0.025964
2025-04-08 20:59:36.637 INFO: train_f_rmse: 0.025964
val_e/atom_mae: 0.008194
2025-04-08 20:59:36.640 INFO: val_e/atom_mae: 0.008194
val_e/atom_rmse: 0.008386
2025-04-08 20:59:36.640 INFO: val_e/atom_rmse: 0.008386
val_f_mae: 0.016679
2025-04-08 20:59:36.640 INFO: val_f_mae: 0.016679
val_f_rmse: 0.025005
2025-04-08 20:59:36.641 INFO: val_f_rmse: 0.025005
##### Step: 15 Learning rate: 0.01 #####
2025-04-08 21:01:03.592 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 0.7815, Val Loss: 0.8745
2025-04-08 21:01:03.592 INFO: Epoch 16, Train Loss: 0.7815, Val Loss: 0.8745
train_e/atom_mae: 0.004816
2025-04-08 21:01:03.593 INFO: train_e/atom_mae: 0.004816
train_e/atom_rmse: 0.005885
2025-04-08 21:01:03.593 INFO: train_e/atom_rmse: 0.005885
train_f_mae: 0.017332
2025-04-08 21:01:03.597 INFO: train_f_mae: 0.017332
train_f_rmse: 0.027196
2025-04-08 21:01:03.597 INFO: train_f_rmse: 0.027196
val_e/atom_mae: 0.002775
2025-04-08 21:01:03.600 INFO: val_e/atom_mae: 0.002775
val_e/atom_rmse: 0.003317
2025-04-08 21:01:03.600 INFO: val_e/atom_rmse: 0.003317
val_f_mae: 0.019183
2025-04-08 21:01:03.600 INFO: val_f_mae: 0.019183
val_f_rmse: 0.029345
2025-04-08 21:01:03.601 INFO: val_f_rmse: 0.029345
##### Step: 16 Learning rate: 0.01 #####
2025-04-08 21:02:30.654 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 0.7322, Val Loss: 0.9856
2025-04-08 21:02:30.654 INFO: Epoch 17, Train Loss: 0.7322, Val Loss: 0.9856
train_e/atom_mae: 0.004386
2025-04-08 21:02:30.655 INFO: train_e/atom_mae: 0.004386
train_e/atom_rmse: 0.005439
2025-04-08 21:02:30.655 INFO: train_e/atom_rmse: 0.005439
train_f_mae: 0.016761
2025-04-08 21:02:30.659 INFO: train_f_mae: 0.016761
train_f_rmse: 0.026389
2025-04-08 21:02:30.659 INFO: train_f_rmse: 0.026389
val_e/atom_mae: 0.002219
2025-04-08 21:02:30.662 INFO: val_e/atom_mae: 0.002219
val_e/atom_rmse: 0.002805
2025-04-08 21:02:30.662 INFO: val_e/atom_rmse: 0.002805
val_f_mae: 0.021434
2025-04-08 21:02:30.662 INFO: val_f_mae: 0.021434
val_f_rmse: 0.031243
2025-04-08 21:02:30.663 INFO: val_f_rmse: 0.031243
##### Step: 17 Learning rate: 0.01 #####
2025-04-08 21:03:57.673 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 0.6733, Val Loss: 0.6512
2025-04-08 21:03:57.673 INFO: Epoch 18, Train Loss: 0.6733, Val Loss: 0.6512
train_e/atom_mae: 0.003505
2025-04-08 21:03:57.674 INFO: train_e/atom_mae: 0.003505
train_e/atom_rmse: 0.004458
2025-04-08 21:03:57.674 INFO: train_e/atom_rmse: 0.004458
train_f_mae: 0.016465
2025-04-08 21:03:57.678 INFO: train_f_mae: 0.016465
train_f_rmse: 0.025480
2025-04-08 21:03:57.678 INFO: train_f_rmse: 0.025480
val_e/atom_mae: 0.005354
2025-04-08 21:03:57.681 INFO: val_e/atom_mae: 0.005354
val_e/atom_rmse: 0.005575
2025-04-08 21:03:57.681 INFO: val_e/atom_rmse: 0.005575
val_f_mae: 0.016999
2025-04-08 21:03:57.682 INFO: val_f_mae: 0.016999
val_f_rmse: 0.024772
2025-04-08 21:03:57.682 INFO: val_f_rmse: 0.024772
##### Step: 18 Learning rate: 0.01 #####
2025-04-08 21:05:24.700 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 0.7120, Val Loss: 0.7409
2025-04-08 21:05:24.701 INFO: Epoch 19, Train Loss: 0.7120, Val Loss: 0.7409
train_e/atom_mae: 0.003597
2025-04-08 21:05:24.702 INFO: train_e/atom_mae: 0.003597
train_e/atom_rmse: 0.004552
2025-04-08 21:05:24.702 INFO: train_e/atom_rmse: 0.004552
train_f_mae: 0.016789
2025-04-08 21:05:24.705 INFO: train_f_mae: 0.016789
train_f_rmse: 0.026208
2025-04-08 21:05:24.705 INFO: train_f_rmse: 0.026208
val_e/atom_mae: 0.003526
2025-04-08 21:05:24.708 INFO: val_e/atom_mae: 0.003526
val_e/atom_rmse: 0.003827
2025-04-08 21:05:24.708 INFO: val_e/atom_rmse: 0.003827
val_f_mae: 0.016208
2025-04-08 21:05:24.709 INFO: val_f_mae: 0.016208
val_f_rmse: 0.026892
2025-04-08 21:05:24.709 INFO: val_f_rmse: 0.026892
##### Step: 19 Learning rate: 0.01 #####
2025-04-08 21:06:51.755 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 0.7050, Val Loss: 0.9779
2025-04-08 21:06:51.755 INFO: Epoch 20, Train Loss: 0.7050, Val Loss: 0.9779
train_e/atom_mae: 0.004350
2025-04-08 21:06:51.756 INFO: train_e/atom_mae: 0.004350
train_e/atom_rmse: 0.005368
2025-04-08 21:06:51.756 INFO: train_e/atom_rmse: 0.005368
train_f_mae: 0.016906
2025-04-08 21:06:51.760 INFO: train_f_mae: 0.016906
train_f_rmse: 0.025887
2025-04-08 21:06:51.760 INFO: train_f_rmse: 0.025887
val_e/atom_mae: 0.001773
2025-04-08 21:06:51.763 INFO: val_e/atom_mae: 0.001773
val_e/atom_rmse: 0.001807
2025-04-08 21:06:51.763 INFO: val_e/atom_rmse: 0.001807
val_f_mae: 0.021377
2025-04-08 21:06:51.763 INFO: val_f_mae: 0.021377
val_f_rmse: 0.031209
2025-04-08 21:06:51.763 INFO: val_f_rmse: 0.031209
##### Step: 20 Learning rate: 0.005 #####
2025-04-08 21:08:18.681 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 0.5109, Val Loss: 0.7009
2025-04-08 21:08:18.681 INFO: Epoch 21, Train Loss: 0.5109, Val Loss: 0.7009
train_e/atom_mae: 0.003721
2025-04-08 21:08:18.682 INFO: train_e/atom_mae: 0.003721
train_e/atom_rmse: 0.004602
2025-04-08 21:08:18.682 INFO: train_e/atom_rmse: 0.004602
train_f_mae: 0.014469
2025-04-08 21:08:18.686 INFO: train_f_mae: 0.014469
train_f_rmse: 0.022029
2025-04-08 21:08:18.686 INFO: train_f_rmse: 0.022029
val_e/atom_mae: 0.005497
2025-04-08 21:08:18.688 INFO: val_e/atom_mae: 0.005497
val_e/atom_rmse: 0.005874
2025-04-08 21:08:18.689 INFO: val_e/atom_rmse: 0.005874
val_f_mae: 0.015257
2025-04-08 21:08:18.689 INFO: val_f_mae: 0.015257
val_f_rmse: 0.025674
2025-04-08 21:08:18.689 INFO: val_f_rmse: 0.025674
##### Step: 21 Learning rate: 0.005 #####
2025-04-08 21:09:45.673 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.3903, Val Loss: 0.4932
2025-04-08 21:09:45.673 INFO: Epoch 22, Train Loss: 0.3903, Val Loss: 0.4932
train_e/atom_mae: 0.003830
2025-04-08 21:09:45.674 INFO: train_e/atom_mae: 0.003830
train_e/atom_rmse: 0.004690
2025-04-08 21:09:45.674 INFO: train_e/atom_rmse: 0.004690
train_f_mae: 0.012597
2025-04-08 21:09:45.678 INFO: train_f_mae: 0.012597
train_f_rmse: 0.019070
2025-04-08 21:09:45.678 INFO: train_f_rmse: 0.019070
val_e/atom_mae: 0.006435
2025-04-08 21:09:45.681 INFO: val_e/atom_mae: 0.006435
val_e/atom_rmse: 0.006947
2025-04-08 21:09:45.681 INFO: val_e/atom_rmse: 0.006947
val_f_mae: 0.013479
2025-04-08 21:09:45.681 INFO: val_f_mae: 0.013479
val_f_rmse: 0.020851
2025-04-08 21:09:45.682 INFO: val_f_rmse: 0.020851
##### Step: 22 Learning rate: 0.005 #####
2025-04-08 21:11:12.564 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.4447, Val Loss: 0.3106
2025-04-08 21:11:12.564 INFO: Epoch 23, Train Loss: 0.4447, Val Loss: 0.3106
train_e/atom_mae: 0.002922
2025-04-08 21:11:12.565 INFO: train_e/atom_mae: 0.002922
train_e/atom_rmse: 0.003607
2025-04-08 21:11:12.565 INFO: train_e/atom_rmse: 0.003607
train_f_mae: 0.013594
2025-04-08 21:11:12.569 INFO: train_f_mae: 0.013594
train_f_rmse: 0.020711
2025-04-08 21:11:12.569 INFO: train_f_rmse: 0.020711
val_e/atom_mae: 0.004703
2025-04-08 21:11:12.572 INFO: val_e/atom_mae: 0.004703
val_e/atom_rmse: 0.004943
2025-04-08 21:11:12.572 INFO: val_e/atom_rmse: 0.004943
val_f_mae: 0.011601
2025-04-08 21:11:12.572 INFO: val_f_mae: 0.011601
val_f_rmse: 0.016765
2025-04-08 21:11:12.573 INFO: val_f_rmse: 0.016765
##### Step: 23 Learning rate: 0.005 #####
2025-04-08 21:12:39.563 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.3623, Val Loss: 0.4334
2025-04-08 21:12:39.564 INFO: Epoch 24, Train Loss: 0.3623, Val Loss: 0.4334
train_e/atom_mae: 0.002507
2025-04-08 21:12:39.565 INFO: train_e/atom_mae: 0.002507
train_e/atom_rmse: 0.003093
2025-04-08 21:12:39.565 INFO: train_e/atom_rmse: 0.003093
train_f_mae: 0.012554
2025-04-08 21:12:39.568 INFO: train_f_mae: 0.012554
train_f_rmse: 0.018728
2025-04-08 21:12:39.568 INFO: train_f_rmse: 0.018728
val_e/atom_mae: 0.002910
2025-04-08 21:12:39.571 INFO: val_e/atom_mae: 0.002910
val_e/atom_rmse: 0.003175
2025-04-08 21:12:39.571 INFO: val_e/atom_rmse: 0.003175
val_f_mae: 0.014327
2025-04-08 21:12:39.572 INFO: val_f_mae: 0.014327
val_f_rmse: 0.020522
2025-04-08 21:12:39.572 INFO: val_f_rmse: 0.020522
##### Step: 24 Learning rate: 0.005 #####
2025-04-08 21:14:06.535 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.3575, Val Loss: 0.4456
2025-04-08 21:14:06.535 INFO: Epoch 25, Train Loss: 0.3575, Val Loss: 0.4456
train_e/atom_mae: 0.002177
2025-04-08 21:14:06.536 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002686
2025-04-08 21:14:06.536 INFO: train_e/atom_rmse: 0.002686
train_f_mae: 0.012346
2025-04-08 21:14:06.540 INFO: train_f_mae: 0.012346
train_f_rmse: 0.018676
2025-04-08 21:14:06.540 INFO: train_f_rmse: 0.018676
val_e/atom_mae: 0.002554
2025-04-08 21:14:06.543 INFO: val_e/atom_mae: 0.002554
val_e/atom_rmse: 0.002872
2025-04-08 21:14:06.543 INFO: val_e/atom_rmse: 0.002872
val_f_mae: 0.013526
2025-04-08 21:14:06.544 INFO: val_f_mae: 0.013526
val_f_rmse: 0.020872
2025-04-08 21:14:06.544 INFO: val_f_rmse: 0.020872
##### Step: 25 Learning rate: 0.005 #####
2025-04-08 21:15:33.536 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.3630, Val Loss: 0.4220
2025-04-08 21:15:33.536 INFO: Epoch 26, Train Loss: 0.3630, Val Loss: 0.4220
train_e/atom_mae: 0.002698
2025-04-08 21:15:33.537 INFO: train_e/atom_mae: 0.002698
train_e/atom_rmse: 0.003354
2025-04-08 21:15:33.537 INFO: train_e/atom_rmse: 0.003354
train_f_mae: 0.012233
2025-04-08 21:15:33.541 INFO: train_f_mae: 0.012233
train_f_rmse: 0.018691
2025-04-08 21:15:33.541 INFO: train_f_rmse: 0.018691
val_e/atom_mae: 0.002209
2025-04-08 21:15:33.543 INFO: val_e/atom_mae: 0.002209
val_e/atom_rmse: 0.002506
2025-04-08 21:15:33.544 INFO: val_e/atom_rmse: 0.002506
val_f_mae: 0.012562
2025-04-08 21:15:33.544 INFO: val_f_mae: 0.012562
val_f_rmse: 0.020356
2025-04-08 21:15:33.544 INFO: val_f_rmse: 0.020356
##### Step: 26 Learning rate: 0.005 #####
2025-04-08 21:17:00.534 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.4271, Val Loss: 0.3300
2025-04-08 21:17:00.534 INFO: Epoch 27, Train Loss: 0.4271, Val Loss: 0.3300
train_e/atom_mae: 0.002747
2025-04-08 21:17:00.535 INFO: train_e/atom_mae: 0.002747
train_e/atom_rmse: 0.003497
2025-04-08 21:17:00.535 INFO: train_e/atom_rmse: 0.003497
train_f_mae: 0.013251
2025-04-08 21:17:00.539 INFO: train_f_mae: 0.013251
train_f_rmse: 0.020306
2025-04-08 21:17:00.539 INFO: train_f_rmse: 0.020306
val_e/atom_mae: 0.001332
2025-04-08 21:17:00.541 INFO: val_e/atom_mae: 0.001332
val_e/atom_rmse: 0.001611
2025-04-08 21:17:00.542 INFO: val_e/atom_rmse: 0.001611
val_f_mae: 0.011729
2025-04-08 21:17:00.542 INFO: val_f_mae: 0.011729
val_f_rmse: 0.018080
2025-04-08 21:17:00.542 INFO: val_f_rmse: 0.018080
##### Step: 27 Learning rate: 0.005 #####
2025-04-08 21:18:27.475 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.3352, Val Loss: 0.3128
2025-04-08 21:18:27.475 INFO: Epoch 28, Train Loss: 0.3352, Val Loss: 0.3128
train_e/atom_mae: 0.003016
2025-04-08 21:18:27.476 INFO: train_e/atom_mae: 0.003016
train_e/atom_rmse: 0.003801
2025-04-08 21:18:27.476 INFO: train_e/atom_rmse: 0.003801
train_f_mae: 0.011752
2025-04-08 21:18:27.480 INFO: train_f_mae: 0.011752
train_f_rmse: 0.017824
2025-04-08 21:18:27.480 INFO: train_f_rmse: 0.017824
val_e/atom_mae: 0.001013
2025-04-08 21:18:27.482 INFO: val_e/atom_mae: 0.001013
val_e/atom_rmse: 0.001037
2025-04-08 21:18:27.483 INFO: val_e/atom_rmse: 0.001037
val_f_mae: 0.011597
2025-04-08 21:18:27.483 INFO: val_f_mae: 0.011597
val_f_rmse: 0.017648
2025-04-08 21:18:27.483 INFO: val_f_rmse: 0.017648
##### Step: 28 Learning rate: 0.005 #####
2025-04-08 21:19:54.421 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.3532, Val Loss: 0.3450
2025-04-08 21:19:54.421 INFO: Epoch 29, Train Loss: 0.3532, Val Loss: 0.3450
train_e/atom_mae: 0.002423
2025-04-08 21:19:54.422 INFO: train_e/atom_mae: 0.002423
train_e/atom_rmse: 0.003034
2025-04-08 21:19:54.422 INFO: train_e/atom_rmse: 0.003034
train_f_mae: 0.012098
2025-04-08 21:19:54.425 INFO: train_f_mae: 0.012098
train_f_rmse: 0.018495
2025-04-08 21:19:54.425 INFO: train_f_rmse: 0.018495
val_e/atom_mae: 0.002493
2025-04-08 21:19:54.428 INFO: val_e/atom_mae: 0.002493
val_e/atom_rmse: 0.002674
2025-04-08 21:19:54.428 INFO: val_e/atom_rmse: 0.002674
val_f_mae: 0.011008
2025-04-08 21:19:54.429 INFO: val_f_mae: 0.011008
val_f_rmse: 0.018341
2025-04-08 21:19:54.429 INFO: val_f_rmse: 0.018341
##### Step: 29 Learning rate: 0.005 #####
2025-04-08 21:21:21.350 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.3485, Val Loss: 0.3524
2025-04-08 21:21:21.351 INFO: Epoch 30, Train Loss: 0.3485, Val Loss: 0.3524
train_e/atom_mae: 0.003370
2025-04-08 21:21:21.352 INFO: train_e/atom_mae: 0.003370
train_e/atom_rmse: 0.004190
2025-04-08 21:21:21.352 INFO: train_e/atom_rmse: 0.004190
train_f_mae: 0.012082
2025-04-08 21:21:21.355 INFO: train_f_mae: 0.012082
train_f_rmse: 0.018090
2025-04-08 21:21:21.355 INFO: train_f_rmse: 0.018090
val_e/atom_mae: 0.004658
2025-04-08 21:21:21.358 INFO: val_e/atom_mae: 0.004658
val_e/atom_rmse: 0.005064
2025-04-08 21:21:21.358 INFO: val_e/atom_rmse: 0.005064
val_f_mae: 0.012084
2025-04-08 21:21:21.359 INFO: val_f_mae: 0.012084
val_f_rmse: 0.017926
2025-04-08 21:21:21.359 INFO: val_f_rmse: 0.017926
##### Step: 30 Learning rate: 0.005 #####
2025-04-08 21:22:48.290 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.3341, Val Loss: 0.3773
2025-04-08 21:22:48.291 INFO: Epoch 31, Train Loss: 0.3341, Val Loss: 0.3773
train_e/atom_mae: 0.002087
2025-04-08 21:22:48.291 INFO: train_e/atom_mae: 0.002087
train_e/atom_rmse: 0.002629
2025-04-08 21:22:48.292 INFO: train_e/atom_rmse: 0.002629
train_f_mae: 0.011953
2025-04-08 21:22:48.295 INFO: train_f_mae: 0.011953
train_f_rmse: 0.018049
2025-04-08 21:22:48.295 INFO: train_f_rmse: 0.018049
val_e/atom_mae: 0.000974
2025-04-08 21:22:48.298 INFO: val_e/atom_mae: 0.000974
val_e/atom_rmse: 0.001308
2025-04-08 21:22:48.298 INFO: val_e/atom_rmse: 0.001308
val_f_mae: 0.011823
2025-04-08 21:22:48.299 INFO: val_f_mae: 0.011823
val_f_rmse: 0.019370
2025-04-08 21:22:48.299 INFO: val_f_rmse: 0.019370
##### Step: 31 Learning rate: 0.005 #####
2025-04-08 21:24:15.263 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.3004, Val Loss: 0.2777
2025-04-08 21:24:15.263 INFO: Epoch 32, Train Loss: 0.3004, Val Loss: 0.2777
train_e/atom_mae: 0.002111
2025-04-08 21:24:15.264 INFO: train_e/atom_mae: 0.002111
train_e/atom_rmse: 0.002590
2025-04-08 21:24:15.264 INFO: train_e/atom_rmse: 0.002590
train_f_mae: 0.011241
2025-04-08 21:24:15.268 INFO: train_f_mae: 0.011241
train_f_rmse: 0.017097
2025-04-08 21:24:15.268 INFO: train_f_rmse: 0.017097
val_e/atom_mae: 0.001422
2025-04-08 21:24:15.271 INFO: val_e/atom_mae: 0.001422
val_e/atom_rmse: 0.001636
2025-04-08 21:24:15.271 INFO: val_e/atom_rmse: 0.001636
val_f_mae: 0.010766
2025-04-08 21:24:15.272 INFO: val_f_mae: 0.010766
val_f_rmse: 0.016566
2025-04-08 21:24:15.272 INFO: val_f_rmse: 0.016566
##### Step: 32 Learning rate: 0.005 #####
2025-04-08 21:25:42.284 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.3148, Val Loss: 0.3744
2025-04-08 21:25:42.285 INFO: Epoch 33, Train Loss: 0.3148, Val Loss: 0.3744
train_e/atom_mae: 0.002374
2025-04-08 21:25:42.285 INFO: train_e/atom_mae: 0.002374
train_e/atom_rmse: 0.002918
2025-04-08 21:25:42.286 INFO: train_e/atom_rmse: 0.002918
train_f_mae: 0.011522
2025-04-08 21:25:42.289 INFO: train_f_mae: 0.011522
train_f_rmse: 0.017451
2025-04-08 21:25:42.289 INFO: train_f_rmse: 0.017451
val_e/atom_mae: 0.001567
2025-04-08 21:25:42.292 INFO: val_e/atom_mae: 0.001567
val_e/atom_rmse: 0.001692
2025-04-08 21:25:42.292 INFO: val_e/atom_rmse: 0.001692
val_f_mae: 0.013208
2025-04-08 21:25:42.293 INFO: val_f_mae: 0.013208
val_f_rmse: 0.019260
2025-04-08 21:25:42.293 INFO: val_f_rmse: 0.019260
##### Step: 33 Learning rate: 0.005 #####
2025-04-08 21:27:09.330 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.3043, Val Loss: 0.3506
2025-04-08 21:27:09.331 INFO: Epoch 34, Train Loss: 0.3043, Val Loss: 0.3506
train_e/atom_mae: 0.002597
2025-04-08 21:27:09.332 INFO: train_e/atom_mae: 0.002597
train_e/atom_rmse: 0.003207
2025-04-08 21:27:09.332 INFO: train_e/atom_rmse: 0.003207
train_f_mae: 0.011370
2025-04-08 21:27:09.335 INFO: train_f_mae: 0.011370
train_f_rmse: 0.017083
2025-04-08 21:27:09.335 INFO: train_f_rmse: 0.017083
val_e/atom_mae: 0.004523
2025-04-08 21:27:09.338 INFO: val_e/atom_mae: 0.004523
val_e/atom_rmse: 0.004931
2025-04-08 21:27:09.338 INFO: val_e/atom_rmse: 0.004931
val_f_mae: 0.012013
2025-04-08 21:27:09.339 INFO: val_f_mae: 0.012013
val_f_rmse: 0.017921
2025-04-08 21:27:09.339 INFO: val_f_rmse: 0.017921
##### Step: 34 Learning rate: 0.005 #####
2025-04-08 21:28:36.344 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.3257, Val Loss: 0.3153
2025-04-08 21:28:36.345 INFO: Epoch 35, Train Loss: 0.3257, Val Loss: 0.3153
train_e/atom_mae: 0.002563
2025-04-08 21:28:36.346 INFO: train_e/atom_mae: 0.002563
train_e/atom_rmse: 0.003190
2025-04-08 21:28:36.346 INFO: train_e/atom_rmse: 0.003190
train_f_mae: 0.011463
2025-04-08 21:28:36.349 INFO: train_f_mae: 0.011463
train_f_rmse: 0.017702
2025-04-08 21:28:36.350 INFO: train_f_rmse: 0.017702
val_e/atom_mae: 0.001053
2025-04-08 21:28:36.352 INFO: val_e/atom_mae: 0.001053
val_e/atom_rmse: 0.001129
2025-04-08 21:28:36.353 INFO: val_e/atom_rmse: 0.001129
val_f_mae: 0.011108
2025-04-08 21:28:36.353 INFO: val_f_mae: 0.011108
val_f_rmse: 0.017714
2025-04-08 21:28:36.353 INFO: val_f_rmse: 0.017714
##### Step: 35 Learning rate: 0.005 #####
2025-04-08 21:30:03.333 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.3164, Val Loss: 0.2802
2025-04-08 21:30:03.333 INFO: Epoch 36, Train Loss: 0.3164, Val Loss: 0.2802
train_e/atom_mae: 0.002713
2025-04-08 21:30:03.334 INFO: train_e/atom_mae: 0.002713
train_e/atom_rmse: 0.003423
2025-04-08 21:30:03.334 INFO: train_e/atom_rmse: 0.003423
train_f_mae: 0.011495
2025-04-08 21:30:03.338 INFO: train_f_mae: 0.011495
train_f_rmse: 0.017385
2025-04-08 21:30:03.338 INFO: train_f_rmse: 0.017385
val_e/atom_mae: 0.002181
2025-04-08 21:30:03.341 INFO: val_e/atom_mae: 0.002181
val_e/atom_rmse: 0.002493
2025-04-08 21:30:03.341 INFO: val_e/atom_rmse: 0.002493
val_f_mae: 0.010911
2025-04-08 21:30:03.341 INFO: val_f_mae: 0.010911
val_f_rmse: 0.016513
2025-04-08 21:30:03.342 INFO: val_f_rmse: 0.016513
##### Step: 36 Learning rate: 0.005 #####
2025-04-08 21:31:30.311 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.3540, Val Loss: 0.3001
2025-04-08 21:31:30.312 INFO: Epoch 37, Train Loss: 0.3540, Val Loss: 0.3001
train_e/atom_mae: 0.002932
2025-04-08 21:31:30.312 INFO: train_e/atom_mae: 0.002932
train_e/atom_rmse: 0.003813
2025-04-08 21:31:30.313 INFO: train_e/atom_rmse: 0.003813
train_f_mae: 0.011810
2025-04-08 21:31:30.316 INFO: train_f_mae: 0.011810
train_f_rmse: 0.018343
2025-04-08 21:31:30.316 INFO: train_f_rmse: 0.018343
val_e/atom_mae: 0.001069
2025-04-08 21:31:30.319 INFO: val_e/atom_mae: 0.001069
val_e/atom_rmse: 0.001335
2025-04-08 21:31:30.319 INFO: val_e/atom_rmse: 0.001335
val_f_mae: 0.011823
2025-04-08 21:31:30.320 INFO: val_f_mae: 0.011823
val_f_rmse: 0.017261
2025-04-08 21:31:30.320 INFO: val_f_rmse: 0.017261
##### Step: 37 Learning rate: 0.005 #####
2025-04-08 21:32:57.336 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.3072, Val Loss: 0.2685
2025-04-08 21:32:57.337 INFO: Epoch 38, Train Loss: 0.3072, Val Loss: 0.2685
train_e/atom_mae: 0.002340
2025-04-08 21:32:57.338 INFO: train_e/atom_mae: 0.002340
train_e/atom_rmse: 0.002965
2025-04-08 21:32:57.338 INFO: train_e/atom_rmse: 0.002965
train_f_mae: 0.011310
2025-04-08 21:32:57.341 INFO: train_f_mae: 0.011310
train_f_rmse: 0.017221
2025-04-08 21:32:57.341 INFO: train_f_rmse: 0.017221
val_e/atom_mae: 0.001245
2025-04-08 21:32:57.344 INFO: val_e/atom_mae: 0.001245
val_e/atom_rmse: 0.001439
2025-04-08 21:32:57.344 INFO: val_e/atom_rmse: 0.001439
val_f_mae: 0.010698
2025-04-08 21:32:57.345 INFO: val_f_mae: 0.010698
val_f_rmse: 0.016309
2025-04-08 21:32:57.345 INFO: val_f_rmse: 0.016309
##### Step: 38 Learning rate: 0.005 #####
2025-04-08 21:34:24.387 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.3168, Val Loss: 0.3053
2025-04-08 21:34:24.387 INFO: Epoch 39, Train Loss: 0.3168, Val Loss: 0.3053
train_e/atom_mae: 0.002668
2025-04-08 21:34:24.388 INFO: train_e/atom_mae: 0.002668
train_e/atom_rmse: 0.003250
2025-04-08 21:34:24.388 INFO: train_e/atom_rmse: 0.003250
train_f_mae: 0.011376
2025-04-08 21:34:24.392 INFO: train_f_mae: 0.011376
train_f_rmse: 0.017437
2025-04-08 21:34:24.392 INFO: train_f_rmse: 0.017437
val_e/atom_mae: 0.001657
2025-04-08 21:34:24.395 INFO: val_e/atom_mae: 0.001657
val_e/atom_rmse: 0.001975
2025-04-08 21:34:24.395 INFO: val_e/atom_rmse: 0.001975
val_f_mae: 0.011371
2025-04-08 21:34:24.395 INFO: val_f_mae: 0.011371
val_f_rmse: 0.017336
2025-04-08 21:34:24.395 INFO: val_f_rmse: 0.017336
##### Step: 39 Learning rate: 0.005 #####
2025-04-08 21:35:51.352 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.3121, Val Loss: 0.2973
2025-04-08 21:35:51.352 INFO: Epoch 40, Train Loss: 0.3121, Val Loss: 0.2973
train_e/atom_mae: 0.002634
2025-04-08 21:35:51.353 INFO: train_e/atom_mae: 0.002634
train_e/atom_rmse: 0.003197
2025-04-08 21:35:51.353 INFO: train_e/atom_rmse: 0.003197
train_f_mae: 0.011347
2025-04-08 21:35:51.357 INFO: train_f_mae: 0.011347
train_f_rmse: 0.017313
2025-04-08 21:35:51.357 INFO: train_f_rmse: 0.017313
val_e/atom_mae: 0.005885
2025-04-08 21:35:51.360 INFO: val_e/atom_mae: 0.005885
val_e/atom_rmse: 0.005994
2025-04-08 21:35:51.360 INFO: val_e/atom_rmse: 0.005994
val_f_mae: 0.010479
2025-04-08 21:35:51.360 INFO: val_f_mae: 0.010479
val_f_rmse: 0.015933
2025-04-08 21:35:51.361 INFO: val_f_rmse: 0.015933
2025-04-08 21:35:51.369 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-04-08 21:37:18.382 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 0.2118, Val Loss: 0.1732
2025-04-08 21:37:18.383 INFO: Epoch 1, Train Loss: 0.2118, Val Loss: 0.1732
train_e/atom_mae: 0.002021
2025-04-08 21:37:18.384 INFO: train_e/atom_mae: 0.002021
train_e/atom_rmse: 0.002555
2025-04-08 21:37:18.384 INFO: train_e/atom_rmse: 0.002555
train_f_mae: 0.009677
2025-04-08 21:37:18.387 INFO: train_f_mae: 0.009677
train_f_rmse: 0.014278
2025-04-08 21:37:18.387 INFO: train_f_rmse: 0.014278
val_e/atom_mae: 0.001279
2025-04-08 21:37:18.390 INFO: val_e/atom_mae: 0.001279
val_e/atom_rmse: 0.001580
2025-04-08 21:37:18.390 INFO: val_e/atom_rmse: 0.001580
val_f_mae: 0.008897
2025-04-08 21:37:18.391 INFO: val_f_mae: 0.008897
val_f_rmse: 0.013046
2025-04-08 21:37:18.391 INFO: val_f_rmse: 0.013046
##### Step: 1 Learning rate: 0.004 #####
2025-04-08 21:38:45.397 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 0.2663, Val Loss: 0.2603
2025-04-08 21:38:45.398 INFO: Epoch 2, Train Loss: 0.2663, Val Loss: 0.2603
train_e/atom_mae: 0.002259
2025-04-08 21:38:45.398 INFO: train_e/atom_mae: 0.002259
train_e/atom_rmse: 0.002870
2025-04-08 21:38:45.399 INFO: train_e/atom_rmse: 0.002870
train_f_mae: 0.010626
2025-04-08 21:38:45.403 INFO: train_f_mae: 0.010626
train_f_rmse: 0.016009
2025-04-08 21:38:45.403 INFO: train_f_rmse: 0.016009
val_e/atom_mae: 0.003424
2025-04-08 21:38:45.405 INFO: val_e/atom_mae: 0.003424
val_e/atom_rmse: 0.003557
2025-04-08 21:38:45.406 INFO: val_e/atom_rmse: 0.003557
val_f_mae: 0.010394
2025-04-08 21:38:45.406 INFO: val_f_mae: 0.010394
val_f_rmse: 0.015653
2025-04-08 21:38:45.406 INFO: val_f_rmse: 0.015653
##### Step: 2 Learning rate: 0.006 #####
2025-04-08 21:40:12.617 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 0.2916, Val Loss: 0.3557
2025-04-08 21:40:12.617 INFO: Epoch 3, Train Loss: 0.2916, Val Loss: 0.3557
train_e/atom_mae: 0.002273
2025-04-08 21:40:12.618 INFO: train_e/atom_mae: 0.002273
train_e/atom_rmse: 0.002877
2025-04-08 21:40:12.618 INFO: train_e/atom_rmse: 0.002877
train_f_mae: 0.011119
2025-04-08 21:40:12.622 INFO: train_f_mae: 0.011119
train_f_rmse: 0.016780
2025-04-08 21:40:12.622 INFO: train_f_rmse: 0.016780
val_e/atom_mae: 0.001866
2025-04-08 21:40:12.625 INFO: val_e/atom_mae: 0.001866
val_e/atom_rmse: 0.002199
2025-04-08 21:40:12.625 INFO: val_e/atom_rmse: 0.002199
val_f_mae: 0.012078
2025-04-08 21:40:12.625 INFO: val_f_mae: 0.012078
val_f_rmse: 0.018705
2025-04-08 21:40:12.625 INFO: val_f_rmse: 0.018705
##### Step: 3 Learning rate: 0.008 #####
2025-04-08 21:41:39.562 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 0.4416, Val Loss: 0.3776
2025-04-08 21:41:39.562 INFO: Epoch 4, Train Loss: 0.4416, Val Loss: 0.3776
train_e/atom_mae: 0.002978
2025-04-08 21:41:39.563 INFO: train_e/atom_mae: 0.002978
train_e/atom_rmse: 0.003756
2025-04-08 21:41:39.563 INFO: train_e/atom_rmse: 0.003756
train_f_mae: 0.012812
2025-04-08 21:41:39.567 INFO: train_f_mae: 0.012812
train_f_rmse: 0.020604
2025-04-08 21:41:39.567 INFO: train_f_rmse: 0.020604
val_e/atom_mae: 0.004208
2025-04-08 21:41:39.569 INFO: val_e/atom_mae: 0.004208
val_e/atom_rmse: 0.004785
2025-04-08 21:41:39.570 INFO: val_e/atom_rmse: 0.004785
val_f_mae: 0.011992
2025-04-08 21:41:39.570 INFO: val_f_mae: 0.011992
val_f_rmse: 0.018704
2025-04-08 21:41:39.570 INFO: val_f_rmse: 0.018704
##### Step: 4 Learning rate: 0.01 #####
2025-04-08 21:43:06.558 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 0.5924, Val Loss: 0.5135
2025-04-08 21:43:06.559 INFO: Epoch 5, Train Loss: 0.5924, Val Loss: 0.5135
train_e/atom_mae: 0.004319
2025-04-08 21:43:06.560 INFO: train_e/atom_mae: 0.004319
train_e/atom_rmse: 0.005539
2025-04-08 21:43:06.560 INFO: train_e/atom_rmse: 0.005539
train_f_mae: 0.014836
2025-04-08 21:43:06.563 INFO: train_f_mae: 0.014836
train_f_rmse: 0.023564
2025-04-08 21:43:06.563 INFO: train_f_rmse: 0.023564
val_e/atom_mae: 0.004085
2025-04-08 21:43:06.566 INFO: val_e/atom_mae: 0.004085
val_e/atom_rmse: 0.004381
2025-04-08 21:43:06.566 INFO: val_e/atom_rmse: 0.004381
val_f_mae: 0.014398
2025-04-08 21:43:06.567 INFO: val_f_mae: 0.014398
val_f_rmse: 0.022143
2025-04-08 21:43:06.567 INFO: val_f_rmse: 0.022143
##### Step: 5 Learning rate: 0.01 #####
2025-04-08 21:44:33.481 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 0.5937, Val Loss: 0.6829
2025-04-08 21:44:33.482 INFO: Epoch 6, Train Loss: 0.5937, Val Loss: 0.6829
train_e/atom_mae: 0.004443
2025-04-08 21:44:33.482 INFO: train_e/atom_mae: 0.004443
train_e/atom_rmse: 0.005650
2025-04-08 21:44:33.483 INFO: train_e/atom_rmse: 0.005650
train_f_mae: 0.015118
2025-04-08 21:44:33.486 INFO: train_f_mae: 0.015118
train_f_rmse: 0.023560
2025-04-08 21:44:33.486 INFO: train_f_rmse: 0.023560
val_e/atom_mae: 0.010034
2025-04-08 21:44:33.489 INFO: val_e/atom_mae: 0.010034
val_e/atom_rmse: 0.010191
2025-04-08 21:44:33.489 INFO: val_e/atom_rmse: 0.010191
val_f_mae: 0.014535
2025-04-08 21:44:33.490 INFO: val_f_mae: 0.014535
val_f_rmse: 0.023607
2025-04-08 21:44:33.490 INFO: val_f_rmse: 0.023607
##### Step: 6 Learning rate: 0.01 #####
2025-04-08 21:46:00.410 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 0.6232, Val Loss: 0.3779
2025-04-08 21:46:00.411 INFO: Epoch 7, Train Loss: 0.6232, Val Loss: 0.3779
train_e/atom_mae: 0.004157
2025-04-08 21:46:00.411 INFO: train_e/atom_mae: 0.004157
train_e/atom_rmse: 0.005313
2025-04-08 21:46:00.412 INFO: train_e/atom_rmse: 0.005313
train_f_mae: 0.014851
2025-04-08 21:46:00.415 INFO: train_f_mae: 0.014851
train_f_rmse: 0.024270
2025-04-08 21:46:00.415 INFO: train_f_rmse: 0.024270
val_e/atom_mae: 0.001231
2025-04-08 21:46:00.418 INFO: val_e/atom_mae: 0.001231
val_e/atom_rmse: 0.001252
2025-04-08 21:46:00.418 INFO: val_e/atom_rmse: 0.001252
val_f_mae: 0.012979
2025-04-08 21:46:00.419 INFO: val_f_mae: 0.012979
val_f_rmse: 0.019390
2025-04-08 21:46:00.419 INFO: val_f_rmse: 0.019390
##### Step: 7 Learning rate: 0.01 #####
2025-04-08 21:47:27.393 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 0.5363, Val Loss: 0.4033
2025-04-08 21:47:27.393 INFO: Epoch 8, Train Loss: 0.5363, Val Loss: 0.4033
train_e/atom_mae: 0.004782
2025-04-08 21:47:27.394 INFO: train_e/atom_mae: 0.004782
train_e/atom_rmse: 0.005868
2025-04-08 21:47:27.394 INFO: train_e/atom_rmse: 0.005868
train_f_mae: 0.014307
2025-04-08 21:47:27.398 INFO: train_f_mae: 0.014307
train_f_rmse: 0.022241
2025-04-08 21:47:27.398 INFO: train_f_rmse: 0.022241
val_e/atom_mae: 0.001540
2025-04-08 21:47:27.401 INFO: val_e/atom_mae: 0.001540
val_e/atom_rmse: 0.001691
2025-04-08 21:47:27.401 INFO: val_e/atom_rmse: 0.001691
val_f_mae: 0.012996
2025-04-08 21:47:27.401 INFO: val_f_mae: 0.012996
val_f_rmse: 0.019997
2025-04-08 21:47:27.402 INFO: val_f_rmse: 0.019997
##### Step: 8 Learning rate: 0.01 #####
2025-04-08 21:48:54.366 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 0.6794, Val Loss: 0.5598
2025-04-08 21:48:54.366 INFO: Epoch 9, Train Loss: 0.6794, Val Loss: 0.5598
train_e/atom_mae: 0.003373
2025-04-08 21:48:54.367 INFO: train_e/atom_mae: 0.003373
train_e/atom_rmse: 0.004284
2025-04-08 21:48:54.367 INFO: train_e/atom_rmse: 0.004284
train_f_mae: 0.015738
2025-04-08 21:48:54.371 INFO: train_f_mae: 0.015738
train_f_rmse: 0.025635
2025-04-08 21:48:54.371 INFO: train_f_rmse: 0.025635
val_e/atom_mae: 0.002540
2025-04-08 21:48:54.373 INFO: val_e/atom_mae: 0.002540
val_e/atom_rmse: 0.002912
2025-04-08 21:48:54.374 INFO: val_e/atom_rmse: 0.002912
val_f_mae: 0.015994
2025-04-08 21:48:54.374 INFO: val_f_mae: 0.015994
val_f_rmse: 0.023442
2025-04-08 21:48:54.374 INFO: val_f_rmse: 0.023442
##### Step: 9 Learning rate: 0.01 #####
2025-04-08 21:50:21.360 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 0.5815, Val Loss: 0.3683
2025-04-08 21:50:21.361 INFO: Epoch 10, Train Loss: 0.5815, Val Loss: 0.3683
train_e/atom_mae: 0.004307
2025-04-08 21:50:21.362 INFO: train_e/atom_mae: 0.004307
train_e/atom_rmse: 0.005498
2025-04-08 21:50:21.362 INFO: train_e/atom_rmse: 0.005498
train_f_mae: 0.014585
2025-04-08 21:50:21.365 INFO: train_f_mae: 0.014585
train_f_rmse: 0.023344
2025-04-08 21:50:21.366 INFO: train_f_rmse: 0.023344
val_e/atom_mae: 0.003665
2025-04-08 21:50:21.368 INFO: val_e/atom_mae: 0.003665
val_e/atom_rmse: 0.003857
2025-04-08 21:50:21.369 INFO: val_e/atom_rmse: 0.003857
val_f_mae: 0.011940
2025-04-08 21:50:21.369 INFO: val_f_mae: 0.011940
val_f_rmse: 0.018717
2025-04-08 21:50:21.369 INFO: val_f_rmse: 0.018717
##### Step: 10 Learning rate: 0.01 #####
2025-04-08 21:51:48.304 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 0.5286, Val Loss: 0.2897
2025-04-08 21:51:48.305 INFO: Epoch 11, Train Loss: 0.5286, Val Loss: 0.2897
train_e/atom_mae: 0.004026
2025-04-08 21:51:48.306 INFO: train_e/atom_mae: 0.004026
train_e/atom_rmse: 0.005496
2025-04-08 21:51:48.306 INFO: train_e/atom_rmse: 0.005496
train_f_mae: 0.014247
2025-04-08 21:51:48.309 INFO: train_f_mae: 0.014247
train_f_rmse: 0.022182
2025-04-08 21:51:48.309 INFO: train_f_rmse: 0.022182
val_e/atom_mae: 0.001082
2025-04-08 21:51:48.312 INFO: val_e/atom_mae: 0.001082
val_e/atom_rmse: 0.001395
2025-04-08 21:51:48.312 INFO: val_e/atom_rmse: 0.001395
val_f_mae: 0.011221
2025-04-08 21:51:48.313 INFO: val_f_mae: 0.011221
val_f_rmse: 0.016952
2025-04-08 21:51:48.313 INFO: val_f_rmse: 0.016952
##### Step: 11 Learning rate: 0.01 #####
2025-04-08 21:53:15.266 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 0.4879, Val Loss: 0.5720
2025-04-08 21:53:15.266 INFO: Epoch 12, Train Loss: 0.4879, Val Loss: 0.5720
train_e/atom_mae: 0.003161
2025-04-08 21:53:15.267 INFO: train_e/atom_mae: 0.003161
train_e/atom_rmse: 0.004003
2025-04-08 21:53:15.267 INFO: train_e/atom_rmse: 0.004003
train_f_mae: 0.013845
2025-04-08 21:53:15.271 INFO: train_f_mae: 0.013845
train_f_rmse: 0.021646
2025-04-08 21:53:15.271 INFO: train_f_rmse: 0.021646
val_e/atom_mae: 0.001981
2025-04-08 21:53:15.274 INFO: val_e/atom_mae: 0.001981
val_e/atom_rmse: 0.002403
2025-04-08 21:53:15.274 INFO: val_e/atom_rmse: 0.002403
val_f_mae: 0.013749
2025-04-08 21:53:15.275 INFO: val_f_mae: 0.013749
val_f_rmse: 0.023770
2025-04-08 21:53:15.275 INFO: val_f_rmse: 0.023770
##### Step: 12 Learning rate: 0.01 #####
2025-04-08 21:54:42.233 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 0.4987, Val Loss: 0.4763
2025-04-08 21:54:42.233 INFO: Epoch 13, Train Loss: 0.4987, Val Loss: 0.4763
train_e/atom_mae: 0.003914
2025-04-08 21:54:42.234 INFO: train_e/atom_mae: 0.003914
train_e/atom_rmse: 0.004958
2025-04-08 21:54:42.234 INFO: train_e/atom_rmse: 0.004958
train_f_mae: 0.014156
2025-04-08 21:54:42.238 INFO: train_f_mae: 0.014156
train_f_rmse: 0.021654
2025-04-08 21:54:42.238 INFO: train_f_rmse: 0.021654
val_e/atom_mae: 0.001989
2025-04-08 21:54:42.241 INFO: val_e/atom_mae: 0.001989
val_e/atom_rmse: 0.002218
2025-04-08 21:54:42.241 INFO: val_e/atom_rmse: 0.002218
val_f_mae: 0.014184
2025-04-08 21:54:42.241 INFO: val_f_mae: 0.014184
val_f_rmse: 0.021687
2025-04-08 21:54:42.241 INFO: val_f_rmse: 0.021687
##### Step: 13 Learning rate: 0.01 #####
2025-04-08 21:56:09.226 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 0.5172, Val Loss: 0.6306
2025-04-08 21:56:09.227 INFO: Epoch 14, Train Loss: 0.5172, Val Loss: 0.6306
train_e/atom_mae: 0.003816
2025-04-08 21:56:09.228 INFO: train_e/atom_mae: 0.003816
train_e/atom_rmse: 0.004899
2025-04-08 21:56:09.228 INFO: train_e/atom_rmse: 0.004899
train_f_mae: 0.014099
2025-04-08 21:56:09.231 INFO: train_f_mae: 0.014099
train_f_rmse: 0.022094
2025-04-08 21:56:09.231 INFO: train_f_rmse: 0.022094
val_e/atom_mae: 0.003675
2025-04-08 21:56:09.234 INFO: val_e/atom_mae: 0.003675
val_e/atom_rmse: 0.004064
2025-04-08 21:56:09.234 INFO: val_e/atom_rmse: 0.004064
val_f_mae: 0.017018
2025-04-08 21:56:09.235 INFO: val_f_mae: 0.017018
val_f_rmse: 0.024710
2025-04-08 21:56:09.235 INFO: val_f_rmse: 0.024710
##### Step: 14 Learning rate: 0.01 #####
2025-04-08 21:57:36.204 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 0.4684, Val Loss: 0.7071
2025-04-08 21:57:36.205 INFO: Epoch 15, Train Loss: 0.4684, Val Loss: 0.7071
train_e/atom_mae: 0.004044
2025-04-08 21:57:36.206 INFO: train_e/atom_mae: 0.004044
train_e/atom_rmse: 0.004978
2025-04-08 21:57:36.206 INFO: train_e/atom_rmse: 0.004978
train_f_mae: 0.013390
2025-04-08 21:57:36.209 INFO: train_f_mae: 0.013390
train_f_rmse: 0.020940
2025-04-08 21:57:36.210 INFO: train_f_rmse: 0.020940
val_e/atom_mae: 0.005503
2025-04-08 21:57:36.212 INFO: val_e/atom_mae: 0.005503
val_e/atom_rmse: 0.005668
2025-04-08 21:57:36.213 INFO: val_e/atom_rmse: 0.005668
val_f_mae: 0.016314
2025-04-08 21:57:36.213 INFO: val_f_mae: 0.016314
val_f_rmse: 0.025851
2025-04-08 21:57:36.213 INFO: val_f_rmse: 0.025851
##### Step: 15 Learning rate: 0.01 #####
2025-04-08 21:59:03.063 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 0.5703, Val Loss: 0.6836
2025-04-08 21:59:03.064 INFO: Epoch 16, Train Loss: 0.5703, Val Loss: 0.6836
train_e/atom_mae: 0.004116
2025-04-08 21:59:03.065 INFO: train_e/atom_mae: 0.004116
train_e/atom_rmse: 0.005233
2025-04-08 21:59:03.065 INFO: train_e/atom_rmse: 0.005233
train_f_mae: 0.014553
2025-04-08 21:59:03.068 INFO: train_f_mae: 0.014553
train_f_rmse: 0.023177
2025-04-08 21:59:03.068 INFO: train_f_rmse: 0.023177
val_e/atom_mae: 0.000927
2025-04-08 21:59:03.071 INFO: val_e/atom_mae: 0.000927
val_e/atom_rmse: 0.001031
2025-04-08 21:59:03.071 INFO: val_e/atom_rmse: 0.001031
val_f_mae: 0.016768
2025-04-08 21:59:03.072 INFO: val_f_mae: 0.016768
val_f_rmse: 0.026122
2025-04-08 21:59:03.072 INFO: val_f_rmse: 0.026122
##### Step: 16 Learning rate: 0.01 #####
2025-04-08 22:00:30.034 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 0.5471, Val Loss: 0.6718
2025-04-08 22:00:30.035 INFO: Epoch 17, Train Loss: 0.5471, Val Loss: 0.6718
train_e/atom_mae: 0.003980
2025-04-08 22:00:30.035 INFO: train_e/atom_mae: 0.003980
train_e/atom_rmse: 0.005430
2025-04-08 22:00:30.036 INFO: train_e/atom_rmse: 0.005430
train_f_mae: 0.014372
2025-04-08 22:00:30.039 INFO: train_f_mae: 0.014372
train_f_rmse: 0.022614
2025-04-08 22:00:30.039 INFO: train_f_rmse: 0.022614
val_e/atom_mae: 0.003385
2025-04-08 22:00:30.042 INFO: val_e/atom_mae: 0.003385
val_e/atom_rmse: 0.003974
2025-04-08 22:00:30.042 INFO: val_e/atom_rmse: 0.003974
val_f_mae: 0.018377
2025-04-08 22:00:30.043 INFO: val_f_mae: 0.018377
val_f_rmse: 0.025548
2025-04-08 22:00:30.043 INFO: val_f_rmse: 0.025548
##### Step: 17 Learning rate: 0.01 #####
2025-04-08 22:01:57.012 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 0.5215, Val Loss: 1.5709
2025-04-08 22:01:57.013 INFO: Epoch 18, Train Loss: 0.5215, Val Loss: 1.5709
train_e/atom_mae: 0.003849
2025-04-08 22:01:57.014 INFO: train_e/atom_mae: 0.003849
train_e/atom_rmse: 0.005018
2025-04-08 22:01:57.014 INFO: train_e/atom_rmse: 0.005018
train_f_mae: 0.014237
2025-04-08 22:01:57.017 INFO: train_f_mae: 0.014237
train_f_rmse: 0.022158
2025-04-08 22:01:57.017 INFO: train_f_rmse: 0.022158
val_e/atom_mae: 0.004244
2025-04-08 22:01:57.020 INFO: val_e/atom_mae: 0.004244
val_e/atom_rmse: 0.005182
2025-04-08 22:01:57.021 INFO: val_e/atom_rmse: 0.005182
val_f_mae: 0.023371
2025-04-08 22:01:57.021 INFO: val_f_mae: 0.023371
val_f_rmse: 0.039222
2025-04-08 22:01:57.021 INFO: val_f_rmse: 0.039222
##### Step: 18 Learning rate: 0.01 #####
2025-04-08 22:03:23.972 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 0.5018, Val Loss: 0.5077
2025-04-08 22:03:23.973 INFO: Epoch 19, Train Loss: 0.5018, Val Loss: 0.5077
train_e/atom_mae: 0.004350
2025-04-08 22:03:23.974 INFO: train_e/atom_mae: 0.004350
train_e/atom_rmse: 0.005363
2025-04-08 22:03:23.974 INFO: train_e/atom_rmse: 0.005363
train_f_mae: 0.013577
2025-04-08 22:03:23.977 INFO: train_f_mae: 0.013577
train_f_rmse: 0.021610
2025-04-08 22:03:23.977 INFO: train_f_rmse: 0.021610
val_e/atom_mae: 0.007526
2025-04-08 22:03:23.980 INFO: val_e/atom_mae: 0.007526
val_e/atom_rmse: 0.007683
2025-04-08 22:03:23.980 INFO: val_e/atom_rmse: 0.007683
val_f_mae: 0.013501
2025-04-08 22:03:23.981 INFO: val_f_mae: 0.013501
val_f_rmse: 0.020886
2025-04-08 22:03:23.981 INFO: val_f_rmse: 0.020886
##### Step: 19 Learning rate: 0.01 #####
2025-04-08 22:04:50.947 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 0.4585, Val Loss: 0.4734
2025-04-08 22:04:50.948 INFO: Epoch 20, Train Loss: 0.4585, Val Loss: 0.4734
train_e/atom_mae: 0.003294
2025-04-08 22:04:50.949 INFO: train_e/atom_mae: 0.003294
train_e/atom_rmse: 0.004192
2025-04-08 22:04:50.949 INFO: train_e/atom_rmse: 0.004192
train_f_mae: 0.013253
2025-04-08 22:04:50.953 INFO: train_f_mae: 0.013253
train_f_rmse: 0.020910
2025-04-08 22:04:50.953 INFO: train_f_rmse: 0.020910
val_e/atom_mae: 0.004249
2025-04-08 22:04:50.955 INFO: val_e/atom_mae: 0.004249
val_e/atom_rmse: 0.004471
2025-04-08 22:04:50.956 INFO: val_e/atom_rmse: 0.004471
val_f_mae: 0.014345
2025-04-08 22:04:50.956 INFO: val_f_mae: 0.014345
val_f_rmse: 0.021194
2025-04-08 22:04:50.956 INFO: val_f_rmse: 0.021194
##### Step: 20 Learning rate: 0.005 #####
2025-04-08 22:06:17.907 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 0.3033, Val Loss: 0.2162
2025-04-08 22:06:17.907 INFO: Epoch 21, Train Loss: 0.3033, Val Loss: 0.2162
train_e/atom_mae: 0.003313
2025-04-08 22:06:17.908 INFO: train_e/atom_mae: 0.003313
train_e/atom_rmse: 0.003979
2025-04-08 22:06:17.908 INFO: train_e/atom_rmse: 0.003979
train_f_mae: 0.011092
2025-04-08 22:06:17.912 INFO: train_f_mae: 0.011092
train_f_rmse: 0.016856
2025-04-08 22:06:17.912 INFO: train_f_rmse: 0.016856
val_e/atom_mae: 0.003211
2025-04-08 22:06:17.915 INFO: val_e/atom_mae: 0.003211
val_e/atom_rmse: 0.003531
2025-04-08 22:06:17.915 INFO: val_e/atom_rmse: 0.003531
val_f_mae: 0.009625
2025-04-08 22:06:17.915 INFO: val_f_mae: 0.009625
val_f_rmse: 0.014181
2025-04-08 22:06:17.916 INFO: val_f_rmse: 0.014181
##### Step: 21 Learning rate: 0.005 #####
2025-04-08 22:07:44.840 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.2468, Val Loss: 0.2778
2025-04-08 22:07:44.841 INFO: Epoch 22, Train Loss: 0.2468, Val Loss: 0.2778
train_e/atom_mae: 0.002358
2025-04-08 22:07:44.842 INFO: train_e/atom_mae: 0.002358
train_e/atom_rmse: 0.002878
2025-04-08 22:07:44.842 INFO: train_e/atom_rmse: 0.002878
train_f_mae: 0.010266
2025-04-08 22:07:44.845 INFO: train_f_mae: 0.010266
train_f_rmse: 0.015387
2025-04-08 22:07:44.846 INFO: train_f_rmse: 0.015387
val_e/atom_mae: 0.003009
2025-04-08 22:07:44.848 INFO: val_e/atom_mae: 0.003009
val_e/atom_rmse: 0.003166
2025-04-08 22:07:44.849 INFO: val_e/atom_rmse: 0.003166
val_f_mae: 0.011236
2025-04-08 22:07:44.849 INFO: val_f_mae: 0.011236
val_f_rmse: 0.016300
2025-04-08 22:07:44.849 INFO: val_f_rmse: 0.016300
##### Step: 22 Learning rate: 0.005 #####
2025-04-08 22:09:11.770 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.2655, Val Loss: 0.2738
2025-04-08 22:09:11.771 INFO: Epoch 23, Train Loss: 0.2655, Val Loss: 0.2738
train_e/atom_mae: 0.002454
2025-04-08 22:09:11.771 INFO: train_e/atom_mae: 0.002454
train_e/atom_rmse: 0.003115
2025-04-08 22:09:11.772 INFO: train_e/atom_rmse: 0.003115
train_f_mae: 0.010606
2025-04-08 22:09:11.775 INFO: train_f_mae: 0.010606
train_f_rmse: 0.015929
2025-04-08 22:09:11.775 INFO: train_f_rmse: 0.015929
val_e/atom_mae: 0.001058
2025-04-08 22:09:11.778 INFO: val_e/atom_mae: 0.001058
val_e/atom_rmse: 0.001218
2025-04-08 22:09:11.778 INFO: val_e/atom_rmse: 0.001218
val_f_mae: 0.011048
2025-04-08 22:09:11.779 INFO: val_f_mae: 0.011048
val_f_rmse: 0.016492
2025-04-08 22:09:11.779 INFO: val_f_rmse: 0.016492
##### Step: 23 Learning rate: 0.005 #####
2025-04-08 22:10:38.710 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.2545, Val Loss: 0.2723
2025-04-08 22:10:38.710 INFO: Epoch 24, Train Loss: 0.2545, Val Loss: 0.2723
train_e/atom_mae: 0.002035
2025-04-08 22:10:38.711 INFO: train_e/atom_mae: 0.002035
train_e/atom_rmse: 0.002592
2025-04-08 22:10:38.711 INFO: train_e/atom_rmse: 0.002592
train_f_mae: 0.010381
2025-04-08 22:10:38.715 INFO: train_f_mae: 0.010381
train_f_rmse: 0.015696
2025-04-08 22:10:38.715 INFO: train_f_rmse: 0.015696
val_e/atom_mae: 0.001241
2025-04-08 22:10:38.718 INFO: val_e/atom_mae: 0.001241
val_e/atom_rmse: 0.001502
2025-04-08 22:10:38.718 INFO: val_e/atom_rmse: 0.001502
val_f_mae: 0.011400
2025-04-08 22:10:38.719 INFO: val_f_mae: 0.011400
val_f_rmse: 0.016418
2025-04-08 22:10:38.719 INFO: val_f_rmse: 0.016418
##### Step: 24 Learning rate: 0.005 #####
2025-04-08 22:12:05.642 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.2315, Val Loss: 0.1838
2025-04-08 22:12:05.642 INFO: Epoch 25, Train Loss: 0.2315, Val Loss: 0.1838
train_e/atom_mae: 0.002137
2025-04-08 22:12:05.643 INFO: train_e/atom_mae: 0.002137
train_e/atom_rmse: 0.002650
2025-04-08 22:12:05.643 INFO: train_e/atom_rmse: 0.002650
train_f_mae: 0.009953
2025-04-08 22:12:05.647 INFO: train_f_mae: 0.009953
train_f_rmse: 0.014933
2025-04-08 22:12:05.647 INFO: train_f_rmse: 0.014933
val_e/atom_mae: 0.000914
2025-04-08 22:12:05.649 INFO: val_e/atom_mae: 0.000914
val_e/atom_rmse: 0.001208
2025-04-08 22:12:05.650 INFO: val_e/atom_rmse: 0.001208
val_f_mae: 0.009276
2025-04-08 22:12:05.650 INFO: val_f_mae: 0.009276
val_f_rmse: 0.013490
2025-04-08 22:12:05.650 INFO: val_f_rmse: 0.013490
##### Step: 25 Learning rate: 0.005 #####
2025-04-08 22:13:32.546 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.2304, Val Loss: 0.2103
2025-04-08 22:13:32.547 INFO: Epoch 26, Train Loss: 0.2304, Val Loss: 0.2103
train_e/atom_mae: 0.002153
2025-04-08 22:13:32.548 INFO: train_e/atom_mae: 0.002153
train_e/atom_rmse: 0.002674
2025-04-08 22:13:32.548 INFO: train_e/atom_rmse: 0.002674
train_f_mae: 0.009900
2025-04-08 22:13:32.551 INFO: train_f_mae: 0.009900
train_f_rmse: 0.014890
2025-04-08 22:13:32.551 INFO: train_f_rmse: 0.014890
val_e/atom_mae: 0.000889
2025-04-08 22:13:32.554 INFO: val_e/atom_mae: 0.000889
val_e/atom_rmse: 0.001036
2025-04-08 22:13:32.554 INFO: val_e/atom_rmse: 0.001036
val_f_mae: 0.009471
2025-04-08 22:13:32.555 INFO: val_f_mae: 0.009471
val_f_rmse: 0.014457
2025-04-08 22:13:32.555 INFO: val_f_rmse: 0.014457
##### Step: 26 Learning rate: 0.005 #####
2025-04-08 22:14:59.585 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.2330, Val Loss: 0.2737
2025-04-08 22:14:59.586 INFO: Epoch 27, Train Loss: 0.2330, Val Loss: 0.2737
train_e/atom_mae: 0.002671
2025-04-08 22:14:59.587 INFO: train_e/atom_mae: 0.002671
train_e/atom_rmse: 0.003266
2025-04-08 22:14:59.587 INFO: train_e/atom_rmse: 0.003266
train_f_mae: 0.009823
2025-04-08 22:14:59.590 INFO: train_f_mae: 0.009823
train_f_rmse: 0.014836
2025-04-08 22:14:59.591 INFO: train_f_rmse: 0.014836
val_e/atom_mae: 0.001023
2025-04-08 22:14:59.593 INFO: val_e/atom_mae: 0.001023
val_e/atom_rmse: 0.001371
2025-04-08 22:14:59.594 INFO: val_e/atom_rmse: 0.001371
val_f_mae: 0.010595
2025-04-08 22:14:59.594 INFO: val_f_mae: 0.010595
val_f_rmse: 0.016475
2025-04-08 22:14:59.594 INFO: val_f_rmse: 0.016475
##### Step: 27 Learning rate: 0.005 #####
2025-04-08 22:16:26.542 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.2348, Val Loss: 0.2367
2025-04-08 22:16:26.542 INFO: Epoch 28, Train Loss: 0.2348, Val Loss: 0.2367
train_e/atom_mae: 0.002287
2025-04-08 22:16:26.543 INFO: train_e/atom_mae: 0.002287
train_e/atom_rmse: 0.002840
2025-04-08 22:16:26.543 INFO: train_e/atom_rmse: 0.002840
train_f_mae: 0.009918
2025-04-08 22:16:26.547 INFO: train_f_mae: 0.009918
train_f_rmse: 0.015000
2025-04-08 22:16:26.547 INFO: train_f_rmse: 0.015000
val_e/atom_mae: 0.000691
2025-04-08 22:16:26.549 INFO: val_e/atom_mae: 0.000691
val_e/atom_rmse: 0.000799
2025-04-08 22:16:26.550 INFO: val_e/atom_rmse: 0.000799
val_f_mae: 0.010351
2025-04-08 22:16:26.551 INFO: val_f_mae: 0.010351
val_f_rmse: 0.015361
2025-04-08 22:16:26.551 INFO: val_f_rmse: 0.015361
##### Step: 28 Learning rate: 0.005 #####
2025-04-08 22:17:53.560 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.2566, Val Loss: 0.2338
2025-04-08 22:17:53.560 INFO: Epoch 29, Train Loss: 0.2566, Val Loss: 0.2338
train_e/atom_mae: 0.002035
2025-04-08 22:17:53.561 INFO: train_e/atom_mae: 0.002035
train_e/atom_rmse: 0.002566
2025-04-08 22:17:53.561 INFO: train_e/atom_rmse: 0.002566
train_f_mae: 0.010299
2025-04-08 22:17:53.565 INFO: train_f_mae: 0.010299
train_f_rmse: 0.015768
2025-04-08 22:17:53.565 INFO: train_f_rmse: 0.015768
val_e/atom_mae: 0.001087
2025-04-08 22:17:53.567 INFO: val_e/atom_mae: 0.001087
val_e/atom_rmse: 0.001303
2025-04-08 22:17:53.568 INFO: val_e/atom_rmse: 0.001303
val_f_mae: 0.009829
2025-04-08 22:17:53.568 INFO: val_f_mae: 0.009829
val_f_rmse: 0.015223
2025-04-08 22:17:53.568 INFO: val_f_rmse: 0.015223
##### Step: 29 Learning rate: 0.005 #####
2025-04-08 22:19:20.482 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.2751, Val Loss: 0.1956
2025-04-08 22:19:20.483 INFO: Epoch 30, Train Loss: 0.2751, Val Loss: 0.1956
train_e/atom_mae: 0.002397
2025-04-08 22:19:20.483 INFO: train_e/atom_mae: 0.002397
train_e/atom_rmse: 0.002951
2025-04-08 22:19:20.484 INFO: train_e/atom_rmse: 0.002951
train_f_mae: 0.010589
2025-04-08 22:19:20.487 INFO: train_f_mae: 0.010589
train_f_rmse: 0.016266
2025-04-08 22:19:20.487 INFO: train_f_rmse: 0.016266
val_e/atom_mae: 0.001554
2025-04-08 22:19:20.490 INFO: val_e/atom_mae: 0.001554
val_e/atom_rmse: 0.001812
2025-04-08 22:19:20.490 INFO: val_e/atom_rmse: 0.001812
val_f_mae: 0.009185
2025-04-08 22:19:20.491 INFO: val_f_mae: 0.009185
val_f_rmse: 0.013842
2025-04-08 22:19:20.491 INFO: val_f_rmse: 0.013842
##### Step: 30 Learning rate: 0.005 #####
2025-04-08 22:20:47.441 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.2335, Val Loss: 0.5454
2025-04-08 22:20:47.441 INFO: Epoch 31, Train Loss: 0.2335, Val Loss: 0.5454
train_e/atom_mae: 0.002941
2025-04-08 22:20:47.442 INFO: train_e/atom_mae: 0.002941
train_e/atom_rmse: 0.003719
2025-04-08 22:20:47.442 INFO: train_e/atom_rmse: 0.003719
train_f_mae: 0.009645
2025-04-08 22:20:47.446 INFO: train_f_mae: 0.009645
train_f_rmse: 0.014724
2025-04-08 22:20:47.446 INFO: train_f_rmse: 0.014724
val_e/atom_mae: 0.002896
2025-04-08 22:20:47.449 INFO: val_e/atom_mae: 0.002896
val_e/atom_rmse: 0.003538
2025-04-08 22:20:47.449 INFO: val_e/atom_rmse: 0.003538
val_f_mae: 0.013945
2025-04-08 22:20:47.450 INFO: val_f_mae: 0.013945
val_f_rmse: 0.023028
2025-04-08 22:20:47.450 INFO: val_f_rmse: 0.023028
##### Step: 31 Learning rate: 0.005 #####
2025-04-08 22:22:14.405 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.2630, Val Loss: 0.2024
2025-04-08 22:22:14.406 INFO: Epoch 32, Train Loss: 0.2630, Val Loss: 0.2024
train_e/atom_mae: 0.003525
2025-04-08 22:22:14.407 INFO: train_e/atom_mae: 0.003525
train_e/atom_rmse: 0.004309
2025-04-08 22:22:14.407 INFO: train_e/atom_rmse: 0.004309
train_f_mae: 0.010169
2025-04-08 22:22:14.410 INFO: train_f_mae: 0.010169
train_f_rmse: 0.015508
2025-04-08 22:22:14.410 INFO: train_f_rmse: 0.015508
val_e/atom_mae: 0.001873
2025-04-08 22:22:14.413 INFO: val_e/atom_mae: 0.001873
val_e/atom_rmse: 0.002061
2025-04-08 22:22:14.413 INFO: val_e/atom_rmse: 0.002061
val_f_mae: 0.009300
2025-04-08 22:22:14.414 INFO: val_f_mae: 0.009300
val_f_rmse: 0.014045
2025-04-08 22:22:14.414 INFO: val_f_rmse: 0.014045
##### Step: 32 Learning rate: 0.005 #####
2025-04-08 22:23:41.362 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.2370, Val Loss: 0.2101
2025-04-08 22:23:41.362 INFO: Epoch 33, Train Loss: 0.2370, Val Loss: 0.2101
train_e/atom_mae: 0.002597
2025-04-08 22:23:41.363 INFO: train_e/atom_mae: 0.002597
train_e/atom_rmse: 0.003395
2025-04-08 22:23:41.363 INFO: train_e/atom_rmse: 0.003395
train_f_mae: 0.009792
2025-04-08 22:23:41.367 INFO: train_f_mae: 0.009792
train_f_rmse: 0.014933
2025-04-08 22:23:41.367 INFO: train_f_rmse: 0.014933
val_e/atom_mae: 0.001703
2025-04-08 22:23:41.369 INFO: val_e/atom_mae: 0.001703
val_e/atom_rmse: 0.001867
2025-04-08 22:23:41.370 INFO: val_e/atom_rmse: 0.001867
val_f_mae: 0.009629
2025-04-08 22:23:41.370 INFO: val_f_mae: 0.009629
val_f_rmse: 0.014348
2025-04-08 22:23:41.370 INFO: val_f_rmse: 0.014348
##### Step: 33 Learning rate: 0.005 #####
2025-04-08 22:25:08.387 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.2385, Val Loss: 0.2470
2025-04-08 22:25:08.387 INFO: Epoch 34, Train Loss: 0.2385, Val Loss: 0.2470
train_e/atom_mae: 0.001983
2025-04-08 22:25:08.388 INFO: train_e/atom_mae: 0.001983
train_e/atom_rmse: 0.002521
2025-04-08 22:25:08.388 INFO: train_e/atom_rmse: 0.002521
train_f_mae: 0.010016
2025-04-08 22:25:08.392 INFO: train_f_mae: 0.010016
train_f_rmse: 0.015191
2025-04-08 22:25:08.392 INFO: train_f_rmse: 0.015191
val_e/atom_mae: 0.001930
2025-04-08 22:25:08.395 INFO: val_e/atom_mae: 0.001930
val_e/atom_rmse: 0.002236
2025-04-08 22:25:08.395 INFO: val_e/atom_rmse: 0.002236
val_f_mae: 0.009906
2025-04-08 22:25:08.396 INFO: val_f_mae: 0.009906
val_f_rmse: 0.015522
2025-04-08 22:25:08.396 INFO: val_f_rmse: 0.015522
##### Step: 34 Learning rate: 0.005 #####
2025-04-08 22:26:35.454 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.2153, Val Loss: 0.1862
2025-04-08 22:26:35.454 INFO: Epoch 35, Train Loss: 0.2153, Val Loss: 0.1862
train_e/atom_mae: 0.002432
2025-04-08 22:26:35.455 INFO: train_e/atom_mae: 0.002432
train_e/atom_rmse: 0.002992
2025-04-08 22:26:35.455 INFO: train_e/atom_rmse: 0.002992
train_f_mae: 0.009494
2025-04-08 22:26:35.459 INFO: train_f_mae: 0.009494
train_f_rmse: 0.014299
2025-04-08 22:26:35.459 INFO: train_f_rmse: 0.014299
val_e/atom_mae: 0.001633
2025-04-08 22:26:35.461 INFO: val_e/atom_mae: 0.001633
val_e/atom_rmse: 0.001907
2025-04-08 22:26:35.462 INFO: val_e/atom_rmse: 0.001907
val_f_mae: 0.009287
2025-04-08 22:26:35.462 INFO: val_f_mae: 0.009287
val_f_rmse: 0.013483
2025-04-08 22:26:35.462 INFO: val_f_rmse: 0.013483
##### Step: 35 Learning rate: 0.005 #####
2025-04-08 22:28:02.526 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.2152, Val Loss: 0.2519
2025-04-08 22:28:02.526 INFO: Epoch 36, Train Loss: 0.2152, Val Loss: 0.2519
train_e/atom_mae: 0.001867
2025-04-08 22:28:02.527 INFO: train_e/atom_mae: 0.001867
train_e/atom_rmse: 0.002375
2025-04-08 22:28:02.527 INFO: train_e/atom_rmse: 0.002375
train_f_mae: 0.009560
2025-04-08 22:28:02.531 INFO: train_f_mae: 0.009560
train_f_rmse: 0.014434
2025-04-08 22:28:02.531 INFO: train_f_rmse: 0.014434
val_e/atom_mae: 0.005051
2025-04-08 22:28:02.533 INFO: val_e/atom_mae: 0.005051
val_e/atom_rmse: 0.005158
2025-04-08 22:28:02.534 INFO: val_e/atom_rmse: 0.005158
val_f_mae: 0.008793
2025-04-08 22:28:02.534 INFO: val_f_mae: 0.008793
val_f_rmse: 0.014822
2025-04-08 22:28:02.534 INFO: val_f_rmse: 0.014822
##### Step: 36 Learning rate: 0.005 #####
2025-04-08 22:29:29.558 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.2244, Val Loss: 0.2059
2025-04-08 22:29:29.558 INFO: Epoch 37, Train Loss: 0.2244, Val Loss: 0.2059
train_e/atom_mae: 0.002586
2025-04-08 22:29:29.559 INFO: train_e/atom_mae: 0.002586
train_e/atom_rmse: 0.003205
2025-04-08 22:29:29.559 INFO: train_e/atom_rmse: 0.003205
train_f_mae: 0.009523
2025-04-08 22:29:29.563 INFO: train_f_mae: 0.009523
train_f_rmse: 0.014559
2025-04-08 22:29:29.563 INFO: train_f_rmse: 0.014559
val_e/atom_mae: 0.002431
2025-04-08 22:29:29.566 INFO: val_e/atom_mae: 0.002431
val_e/atom_rmse: 0.002687
2025-04-08 22:29:29.566 INFO: val_e/atom_rmse: 0.002687
val_f_mae: 0.009091
2025-04-08 22:29:29.567 INFO: val_f_mae: 0.009091
val_f_rmse: 0.014043
2025-04-08 22:29:29.567 INFO: val_f_rmse: 0.014043
##### Step: 37 Learning rate: 0.005 #####
2025-04-08 22:30:56.594 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.2211, Val Loss: 0.2001
2025-04-08 22:30:56.595 INFO: Epoch 38, Train Loss: 0.2211, Val Loss: 0.2001
train_e/atom_mae: 0.002559
2025-04-08 22:30:56.596 INFO: train_e/atom_mae: 0.002559
train_e/atom_rmse: 0.003154
2025-04-08 22:30:56.596 INFO: train_e/atom_rmse: 0.003154
train_f_mae: 0.009685
2025-04-08 22:30:56.599 INFO: train_f_mae: 0.009685
train_f_rmse: 0.014458
2025-04-08 22:30:56.600 INFO: train_f_rmse: 0.014458
val_e/atom_mae: 0.001126
2025-04-08 22:30:56.602 INFO: val_e/atom_mae: 0.001126
val_e/atom_rmse: 0.001436
2025-04-08 22:30:56.603 INFO: val_e/atom_rmse: 0.001436
val_f_mae: 0.009699
2025-04-08 22:30:56.604 INFO: val_f_mae: 0.009699
val_f_rmse: 0.014056
2025-04-08 22:30:56.604 INFO: val_f_rmse: 0.014056
##### Step: 38 Learning rate: 0.005 #####
2025-04-08 22:32:23.661 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.2369, Val Loss: 0.2396
2025-04-08 22:32:23.661 INFO: Epoch 39, Train Loss: 0.2369, Val Loss: 0.2396
train_e/atom_mae: 0.002209
2025-04-08 22:32:23.662 INFO: train_e/atom_mae: 0.002209
train_e/atom_rmse: 0.002797
2025-04-08 22:32:23.662 INFO: train_e/atom_rmse: 0.002797
train_f_mae: 0.009899
2025-04-08 22:32:23.666 INFO: train_f_mae: 0.009899
train_f_rmse: 0.015081
2025-04-08 22:32:23.666 INFO: train_f_rmse: 0.015081
val_e/atom_mae: 0.001422
2025-04-08 22:32:23.668 INFO: val_e/atom_mae: 0.001422
val_e/atom_rmse: 0.001714
2025-04-08 22:32:23.669 INFO: val_e/atom_rmse: 0.001714
val_f_mae: 0.009904
2025-04-08 22:32:23.669 INFO: val_f_mae: 0.009904
val_f_rmse: 0.015363
2025-04-08 22:32:23.669 INFO: val_f_rmse: 0.015363
##### Step: 39 Learning rate: 0.005 #####
2025-04-08 22:33:50.639 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.2549, Val Loss: 0.3739
2025-04-08 22:33:50.640 INFO: Epoch 40, Train Loss: 0.2549, Val Loss: 0.3739
train_e/atom_mae: 0.002651
2025-04-08 22:33:50.641 INFO: train_e/atom_mae: 0.002651
train_e/atom_rmse: 0.003377
2025-04-08 22:33:50.641 INFO: train_e/atom_rmse: 0.003377
train_f_mae: 0.010279
2025-04-08 22:33:50.644 INFO: train_f_mae: 0.010279
train_f_rmse: 0.015528
2025-04-08 22:33:50.644 INFO: train_f_rmse: 0.015528
val_e/atom_mae: 0.006058
2025-04-08 22:33:50.647 INFO: val_e/atom_mae: 0.006058
val_e/atom_rmse: 0.006115
2025-04-08 22:33:50.647 INFO: val_e/atom_rmse: 0.006115
val_f_mae: 0.012197
2025-04-08 22:33:50.648 INFO: val_f_mae: 0.012197
val_f_rmse: 0.018130
2025-04-08 22:33:50.648 INFO: val_f_rmse: 0.018130
2025-04-08 22:33:50.656 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-04-08 22:35:17.681 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 0.1561, Val Loss: 0.1534
2025-04-08 22:35:17.682 INFO: Epoch 1, Train Loss: 0.1561, Val Loss: 0.1534
train_e/atom_mae: 0.001606
2025-04-08 22:35:17.683 INFO: train_e/atom_mae: 0.001606
train_e/atom_rmse: 0.002173
2025-04-08 22:35:17.683 INFO: train_e/atom_rmse: 0.002173
train_f_mae: 0.008170
2025-04-08 22:35:17.687 INFO: train_f_mae: 0.008170
train_f_rmse: 0.012264
2025-04-08 22:35:17.687 INFO: train_f_rmse: 0.012264
val_e/atom_mae: 0.000801
2025-04-08 22:35:17.689 INFO: val_e/atom_mae: 0.000801
val_e/atom_rmse: 0.001030
2025-04-08 22:35:17.690 INFO: val_e/atom_rmse: 0.001030
val_f_mae: 0.008624
2025-04-08 22:35:17.691 INFO: val_f_mae: 0.008624
val_f_rmse: 0.012333
2025-04-08 22:35:17.691 INFO: val_f_rmse: 0.012333
##### Step: 1 Learning rate: 0.004 #####
2025-04-08 22:36:44.582 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 0.2052, Val Loss: 0.2530
2025-04-08 22:36:44.583 INFO: Epoch 2, Train Loss: 0.2052, Val Loss: 0.2530
train_e/atom_mae: 0.002087
2025-04-08 22:36:44.583 INFO: train_e/atom_mae: 0.002087
train_e/atom_rmse: 0.002684
2025-04-08 22:36:44.584 INFO: train_e/atom_rmse: 0.002684
train_f_mae: 0.009250
2025-04-08 22:36:44.587 INFO: train_f_mae: 0.009250
train_f_rmse: 0.014018
2025-04-08 22:36:44.587 INFO: train_f_rmse: 0.014018
val_e/atom_mae: 0.001250
2025-04-08 22:36:44.590 INFO: val_e/atom_mae: 0.001250
val_e/atom_rmse: 0.001522
2025-04-08 22:36:44.590 INFO: val_e/atom_rmse: 0.001522
val_f_mae: 0.011138
2025-04-08 22:36:44.591 INFO: val_f_mae: 0.011138
val_f_rmse: 0.015818
2025-04-08 22:36:44.591 INFO: val_f_rmse: 0.015818
##### Step: 2 Learning rate: 0.006 #####
2025-04-08 22:38:11.558 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 0.2494, Val Loss: 0.2241
2025-04-08 22:38:11.559 INFO: Epoch 3, Train Loss: 0.2494, Val Loss: 0.2241
train_e/atom_mae: 0.001999
2025-04-08 22:38:11.560 INFO: train_e/atom_mae: 0.001999
train_e/atom_rmse: 0.002595
2025-04-08 22:38:11.560 INFO: train_e/atom_rmse: 0.002595
train_f_mae: 0.010155
2025-04-08 22:38:11.564 INFO: train_f_mae: 0.010155
train_f_rmse: 0.015532
2025-04-08 22:38:11.564 INFO: train_f_rmse: 0.015532
val_e/atom_mae: 0.001051
2025-04-08 22:38:11.567 INFO: val_e/atom_mae: 0.001051
val_e/atom_rmse: 0.001273
2025-04-08 22:38:11.567 INFO: val_e/atom_rmse: 0.001273
val_f_mae: 0.010408
2025-04-08 22:38:11.567 INFO: val_f_mae: 0.010408
val_f_rmse: 0.014906
2025-04-08 22:38:11.568 INFO: val_f_rmse: 0.014906
##### Step: 3 Learning rate: 0.008 #####
2025-04-08 22:39:38.468 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 0.3030, Val Loss: 0.2629
2025-04-08 22:39:38.469 INFO: Epoch 4, Train Loss: 0.3030, Val Loss: 0.2629
train_e/atom_mae: 0.003761
2025-04-08 22:39:38.470 INFO: train_e/atom_mae: 0.003761
train_e/atom_rmse: 0.004859
2025-04-08 22:39:38.470 INFO: train_e/atom_rmse: 0.004859
train_f_mae: 0.010714
2025-04-08 22:39:38.473 INFO: train_f_mae: 0.010714
train_f_rmse: 0.016566
2025-04-08 22:39:38.473 INFO: train_f_rmse: 0.016566
val_e/atom_mae: 0.001594
2025-04-08 22:39:38.476 INFO: val_e/atom_mae: 0.001594
val_e/atom_rmse: 0.001832
2025-04-08 22:39:38.476 INFO: val_e/atom_rmse: 0.001832
val_f_mae: 0.009743
2025-04-08 22:39:38.477 INFO: val_f_mae: 0.009743
val_f_rmse: 0.016088
2025-04-08 22:39:38.477 INFO: val_f_rmse: 0.016088
##### Step: 4 Learning rate: 0.01 #####
2025-04-08 22:41:05.440 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 0.4267, Val Loss: 0.5169
2025-04-08 22:41:05.441 INFO: Epoch 5, Train Loss: 0.4267, Val Loss: 0.5169
train_e/atom_mae: 0.003284
2025-04-08 22:41:05.442 INFO: train_e/atom_mae: 0.003284
train_e/atom_rmse: 0.004182
2025-04-08 22:41:05.442 INFO: train_e/atom_rmse: 0.004182
train_f_mae: 0.012886
2025-04-08 22:41:05.446 INFO: train_f_mae: 0.012886
train_f_rmse: 0.020137
2025-04-08 22:41:05.446 INFO: train_f_rmse: 0.020137
val_e/atom_mae: 0.004309
2025-04-08 22:41:05.449 INFO: val_e/atom_mae: 0.004309
val_e/atom_rmse: 0.004401
2025-04-08 22:41:05.449 INFO: val_e/atom_rmse: 0.004401
val_f_mae: 0.012965
2025-04-08 22:41:05.449 INFO: val_f_mae: 0.012965
val_f_rmse: 0.022213
2025-04-08 22:41:05.449 INFO: val_f_rmse: 0.022213
##### Step: 5 Learning rate: 0.01 #####
2025-04-08 22:42:32.496 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 0.4603, Val Loss: 0.6457
2025-04-08 22:42:32.497 INFO: Epoch 6, Train Loss: 0.4603, Val Loss: 0.6457
train_e/atom_mae: 0.004087
2025-04-08 22:42:32.498 INFO: train_e/atom_mae: 0.004087
train_e/atom_rmse: 0.005113
2025-04-08 22:42:32.498 INFO: train_e/atom_rmse: 0.005113
train_f_mae: 0.013130
2025-04-08 22:42:32.502 INFO: train_f_mae: 0.013130
train_f_rmse: 0.020704
2025-04-08 22:42:32.502 INFO: train_f_rmse: 0.020704
val_e/atom_mae: 0.001207
2025-04-08 22:42:32.504 INFO: val_e/atom_mae: 0.001207
val_e/atom_rmse: 0.001480
2025-04-08 22:42:32.505 INFO: val_e/atom_rmse: 0.001480
val_f_mae: 0.015554
2025-04-08 22:42:32.505 INFO: val_f_mae: 0.015554
val_f_rmse: 0.025358
2025-04-08 22:42:32.505 INFO: val_f_rmse: 0.025358
##### Step: 6 Learning rate: 0.01 #####
2025-04-08 22:43:59.729 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 0.4402, Val Loss: 0.8339
2025-04-08 22:43:59.730 INFO: Epoch 7, Train Loss: 0.4402, Val Loss: 0.8339
train_e/atom_mae: 0.004348
2025-04-08 22:43:59.731 INFO: train_e/atom_mae: 0.004348
train_e/atom_rmse: 0.005704
2025-04-08 22:43:59.731 INFO: train_e/atom_rmse: 0.005704
train_f_mae: 0.012641
2025-04-08 22:43:59.735 INFO: train_f_mae: 0.012641
train_f_rmse: 0.020022
2025-04-08 22:43:59.735 INFO: train_f_rmse: 0.020022
val_e/atom_mae: 0.003776
2025-04-08 22:43:59.737 INFO: val_e/atom_mae: 0.003776
val_e/atom_rmse: 0.004378
2025-04-08 22:43:59.738 INFO: val_e/atom_rmse: 0.004378
val_f_mae: 0.016869
2025-04-08 22:43:59.738 INFO: val_f_mae: 0.016869
val_f_rmse: 0.028474
2025-04-08 22:43:59.738 INFO: val_f_rmse: 0.028474
##### Step: 7 Learning rate: 0.01 #####
2025-04-08 22:45:26.949 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 0.4284, Val Loss: 0.4422
2025-04-08 22:45:26.950 INFO: Epoch 8, Train Loss: 0.4284, Val Loss: 0.4422
train_e/atom_mae: 0.004004
2025-04-08 22:45:26.951 INFO: train_e/atom_mae: 0.004004
train_e/atom_rmse: 0.005019
2025-04-08 22:45:26.951 INFO: train_e/atom_rmse: 0.005019
train_f_mae: 0.012809
2025-04-08 22:45:26.954 INFO: train_f_mae: 0.012809
train_f_rmse: 0.019949
2025-04-08 22:45:26.955 INFO: train_f_rmse: 0.019949
val_e/atom_mae: 0.001808
2025-04-08 22:45:26.957 INFO: val_e/atom_mae: 0.001808
val_e/atom_rmse: 0.002251
2025-04-08 22:45:26.958 INFO: val_e/atom_rmse: 0.002251
val_f_mae: 0.014045
2025-04-08 22:45:26.958 INFO: val_f_mae: 0.014045
val_f_rmse: 0.020883
2025-04-08 22:45:26.958 INFO: val_f_rmse: 0.020883
##### Step: 8 Learning rate: 0.01 #####
2025-04-08 22:46:54.169 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 0.3827, Val Loss: 0.4854
2025-04-08 22:46:54.170 INFO: Epoch 9, Train Loss: 0.3827, Val Loss: 0.4854
train_e/atom_mae: 0.003484
2025-04-08 22:46:54.170 INFO: train_e/atom_mae: 0.003484
train_e/atom_rmse: 0.004248
2025-04-08 22:46:54.171 INFO: train_e/atom_rmse: 0.004248
train_f_mae: 0.012303
2025-04-08 22:46:54.174 INFO: train_f_mae: 0.012303
train_f_rmse: 0.018996
2025-04-08 22:46:54.174 INFO: train_f_rmse: 0.018996
val_e/atom_mae: 0.002701
2025-04-08 22:46:54.177 INFO: val_e/atom_mae: 0.002701
val_e/atom_rmse: 0.002938
2025-04-08 22:46:54.177 INFO: val_e/atom_rmse: 0.002938
val_f_mae: 0.014004
2025-04-08 22:46:54.179 INFO: val_f_mae: 0.014004
val_f_rmse: 0.021793
2025-04-08 22:46:54.179 INFO: val_f_rmse: 0.021793
##### Step: 9 Learning rate: 0.01 #####
2025-04-08 22:48:21.309 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 0.4058, Val Loss: 0.3415
2025-04-08 22:48:21.309 INFO: Epoch 10, Train Loss: 0.4058, Val Loss: 0.3415
train_e/atom_mae: 0.004378
2025-04-08 22:48:21.310 INFO: train_e/atom_mae: 0.004378
train_e/atom_rmse: 0.005351
2025-04-08 22:48:21.310 INFO: train_e/atom_rmse: 0.005351
train_f_mae: 0.012176
2025-04-08 22:48:21.314 INFO: train_f_mae: 0.012176
train_f_rmse: 0.019266
2025-04-08 22:48:21.314 INFO: train_f_rmse: 0.019266
val_e/atom_mae: 0.001525
2025-04-08 22:48:21.317 INFO: val_e/atom_mae: 0.001525
val_e/atom_rmse: 0.002077
2025-04-08 22:48:21.317 INFO: val_e/atom_rmse: 0.002077
val_f_mae: 0.011241
2025-04-08 22:48:21.317 INFO: val_f_mae: 0.011241
val_f_rmse: 0.018338
2025-04-08 22:48:21.318 INFO: val_f_rmse: 0.018338
##### Step: 10 Learning rate: 0.01 #####
2025-04-08 22:49:48.478 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 0.3713, Val Loss: 0.4423
2025-04-08 22:49:48.479 INFO: Epoch 11, Train Loss: 0.3713, Val Loss: 0.4423
train_e/atom_mae: 0.002927
2025-04-08 22:49:48.480 INFO: train_e/atom_mae: 0.002927
train_e/atom_rmse: 0.003614
2025-04-08 22:49:48.480 INFO: train_e/atom_rmse: 0.003614
train_f_mae: 0.012184
2025-04-08 22:49:48.484 INFO: train_f_mae: 0.012184
train_f_rmse: 0.018856
2025-04-08 22:49:48.484 INFO: train_f_rmse: 0.018856
val_e/atom_mae: 0.002082
2025-04-08 22:49:48.486 INFO: val_e/atom_mae: 0.002082
val_e/atom_rmse: 0.002447
2025-04-08 22:49:48.487 INFO: val_e/atom_rmse: 0.002447
val_f_mae: 0.013021
2025-04-08 22:49:48.487 INFO: val_f_mae: 0.013021
val_f_rmse: 0.020857
2025-04-08 22:49:48.487 INFO: val_f_rmse: 0.020857
##### Step: 11 Learning rate: 0.01 #####
2025-04-08 22:51:15.627 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 0.3887, Val Loss: 0.3303
2025-04-08 22:51:15.628 INFO: Epoch 12, Train Loss: 0.3887, Val Loss: 0.3303
train_e/atom_mae: 0.003947
2025-04-08 22:51:15.629 INFO: train_e/atom_mae: 0.003947
train_e/atom_rmse: 0.004770
2025-04-08 22:51:15.629 INFO: train_e/atom_rmse: 0.004770
train_f_mae: 0.012164
2025-04-08 22:51:15.632 INFO: train_f_mae: 0.012164
train_f_rmse: 0.019003
2025-04-08 22:51:15.632 INFO: train_f_rmse: 0.019003
val_e/atom_mae: 0.003055
2025-04-08 22:51:15.635 INFO: val_e/atom_mae: 0.003055
val_e/atom_rmse: 0.003314
2025-04-08 22:51:15.635 INFO: val_e/atom_rmse: 0.003314
val_f_mae: 0.011788
2025-04-08 22:51:15.636 INFO: val_f_mae: 0.011788
val_f_rmse: 0.017804
2025-04-08 22:51:15.636 INFO: val_f_rmse: 0.017804
##### Step: 12 Learning rate: 0.01 #####
2025-04-08 22:52:42.796 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 0.4377, Val Loss: 0.2923
2025-04-08 22:52:42.797 INFO: Epoch 13, Train Loss: 0.4377, Val Loss: 0.2923
train_e/atom_mae: 0.003038
2025-04-08 22:52:42.798 INFO: train_e/atom_mae: 0.003038
train_e/atom_rmse: 0.004116
2025-04-08 22:52:42.798 INFO: train_e/atom_rmse: 0.004116
train_f_mae: 0.012549
2025-04-08 22:52:42.801 INFO: train_f_mae: 0.012549
train_f_rmse: 0.020426
2025-04-08 22:52:42.801 INFO: train_f_rmse: 0.020426
val_e/atom_mae: 0.001963
2025-04-08 22:52:42.804 INFO: val_e/atom_mae: 0.001963
val_e/atom_rmse: 0.002238
2025-04-08 22:52:42.804 INFO: val_e/atom_rmse: 0.002238
val_f_mae: 0.011329
2025-04-08 22:52:42.805 INFO: val_f_mae: 0.011329
val_f_rmse: 0.016918
2025-04-08 22:52:42.805 INFO: val_f_rmse: 0.016918
##### Step: 13 Learning rate: 0.01 #####
2025-04-08 22:54:10.024 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 0.3858, Val Loss: 0.2993
2025-04-08 22:54:10.024 INFO: Epoch 14, Train Loss: 0.3858, Val Loss: 0.2993
train_e/atom_mae: 0.003073
2025-04-08 22:54:10.025 INFO: train_e/atom_mae: 0.003073
train_e/atom_rmse: 0.004037
2025-04-08 22:54:10.025 INFO: train_e/atom_rmse: 0.004037
train_f_mae: 0.012183
2025-04-08 22:54:10.029 INFO: train_f_mae: 0.012183
train_f_rmse: 0.019134
2025-04-08 22:54:10.029 INFO: train_f_rmse: 0.019134
val_e/atom_mae: 0.001654
2025-04-08 22:54:10.031 INFO: val_e/atom_mae: 0.001654
val_e/atom_rmse: 0.002156
2025-04-08 22:54:10.032 INFO: val_e/atom_rmse: 0.002156
val_f_mae: 0.011906
2025-04-08 22:54:10.033 INFO: val_f_mae: 0.011906
val_f_rmse: 0.017136
2025-04-08 22:54:10.033 INFO: val_f_rmse: 0.017136
##### Step: 14 Learning rate: 0.01 #####
2025-04-08 22:55:37.213 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 0.3430, Val Loss: 0.3627
2025-04-08 22:55:37.213 INFO: Epoch 15, Train Loss: 0.3430, Val Loss: 0.3627
train_e/atom_mae: 0.002430
2025-04-08 22:55:37.214 INFO: train_e/atom_mae: 0.002430
train_e/atom_rmse: 0.003047
2025-04-08 22:55:37.214 INFO: train_e/atom_rmse: 0.003047
train_f_mae: 0.011886
2025-04-08 22:55:37.218 INFO: train_f_mae: 0.011886
train_f_rmse: 0.018213
2025-04-08 22:55:37.218 INFO: train_f_rmse: 0.018213
val_e/atom_mae: 0.001841
2025-04-08 22:55:37.221 INFO: val_e/atom_mae: 0.001841
val_e/atom_rmse: 0.002011
2025-04-08 22:55:37.221 INFO: val_e/atom_rmse: 0.002011
val_f_mae: 0.012086
2025-04-08 22:55:37.221 INFO: val_f_mae: 0.012086
val_f_rmse: 0.018917
2025-04-08 22:55:37.221 INFO: val_f_rmse: 0.018917
##### Step: 15 Learning rate: 0.01 #####
2025-04-08 22:57:04.402 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 0.4052, Val Loss: 0.3029
2025-04-08 22:57:04.403 INFO: Epoch 16, Train Loss: 0.4052, Val Loss: 0.3029
train_e/atom_mae: 0.003415
2025-04-08 22:57:04.404 INFO: train_e/atom_mae: 0.003415
train_e/atom_rmse: 0.004540
2025-04-08 22:57:04.404 INFO: train_e/atom_rmse: 0.004540
train_f_mae: 0.012173
2025-04-08 22:57:04.407 INFO: train_f_mae: 0.012173
train_f_rmse: 0.019500
2025-04-08 22:57:04.408 INFO: train_f_rmse: 0.019500
val_e/atom_mae: 0.001923
2025-04-08 22:57:04.410 INFO: val_e/atom_mae: 0.001923
val_e/atom_rmse: 0.002216
2025-04-08 22:57:04.411 INFO: val_e/atom_rmse: 0.002216
val_f_mae: 0.011532
2025-04-08 22:57:04.411 INFO: val_f_mae: 0.011532
val_f_rmse: 0.017232
2025-04-08 22:57:04.411 INFO: val_f_rmse: 0.017232
##### Step: 16 Learning rate: 0.01 #####
2025-04-08 22:58:31.565 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 0.4363, Val Loss: 0.4195
2025-04-08 22:58:31.566 INFO: Epoch 17, Train Loss: 0.4363, Val Loss: 0.4195
train_e/atom_mae: 0.003586
2025-04-08 22:58:31.567 INFO: train_e/atom_mae: 0.003586
train_e/atom_rmse: 0.004579
2025-04-08 22:58:31.567 INFO: train_e/atom_rmse: 0.004579
train_f_mae: 0.013026
2025-04-08 22:58:31.570 INFO: train_f_mae: 0.013026
train_f_rmse: 0.020271
2025-04-08 22:58:31.570 INFO: train_f_rmse: 0.020271
val_e/atom_mae: 0.004351
2025-04-08 22:58:31.573 INFO: val_e/atom_mae: 0.004351
val_e/atom_rmse: 0.004500
2025-04-08 22:58:31.573 INFO: val_e/atom_rmse: 0.004500
val_f_mae: 0.012629
2025-04-08 22:58:31.574 INFO: val_f_mae: 0.012629
val_f_rmse: 0.019874
2025-04-08 22:58:31.574 INFO: val_f_rmse: 0.019874
##### Step: 17 Learning rate: 0.01 #####
2025-04-08 22:59:58.759 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 0.3854, Val Loss: 0.4863
2025-04-08 22:59:58.760 INFO: Epoch 18, Train Loss: 0.3854, Val Loss: 0.4863
train_e/atom_mae: 0.003395
2025-04-08 22:59:58.761 INFO: train_e/atom_mae: 0.003395
train_e/atom_rmse: 0.004393
2025-04-08 22:59:58.761 INFO: train_e/atom_rmse: 0.004393
train_f_mae: 0.012114
2025-04-08 22:59:58.764 INFO: train_f_mae: 0.012114
train_f_rmse: 0.019028
2025-04-08 22:59:58.764 INFO: train_f_rmse: 0.019028
val_e/atom_mae: 0.004781
2025-04-08 22:59:58.767 INFO: val_e/atom_mae: 0.004781
val_e/atom_rmse: 0.005035
2025-04-08 22:59:58.767 INFO: val_e/atom_rmse: 0.005035
val_f_mae: 0.013243
2025-04-08 22:59:58.768 INFO: val_f_mae: 0.013243
val_f_rmse: 0.021346
2025-04-08 22:59:58.768 INFO: val_f_rmse: 0.021346
##### Step: 18 Learning rate: 0.01 #####
2025-04-08 23:01:25.973 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 0.4287, Val Loss: 0.5903
2025-04-08 23:01:25.974 INFO: Epoch 19, Train Loss: 0.4287, Val Loss: 0.5903
train_e/atom_mae: 0.004393
2025-04-08 23:01:25.975 INFO: train_e/atom_mae: 0.004393
train_e/atom_rmse: 0.005340
2025-04-08 23:01:25.975 INFO: train_e/atom_rmse: 0.005340
train_f_mae: 0.012426
2025-04-08 23:01:25.978 INFO: train_f_mae: 0.012426
train_f_rmse: 0.019853
2025-04-08 23:01:25.978 INFO: train_f_rmse: 0.019853
val_e/atom_mae: 0.006435
2025-04-08 23:01:25.981 INFO: val_e/atom_mae: 0.006435
val_e/atom_rmse: 0.006542
2025-04-08 23:01:25.981 INFO: val_e/atom_rmse: 0.006542
val_f_mae: 0.015893
2025-04-08 23:01:25.982 INFO: val_f_mae: 0.015893
val_f_rmse: 0.023205
2025-04-08 23:01:25.982 INFO: val_f_rmse: 0.023205
##### Step: 19 Learning rate: 0.01 #####
2025-04-08 23:02:53.239 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 0.4888, Val Loss: 0.4272
2025-04-08 23:02:53.240 INFO: Epoch 20, Train Loss: 0.4888, Val Loss: 0.4272
train_e/atom_mae: 0.004374
2025-04-08 23:02:53.241 INFO: train_e/atom_mae: 0.004374
train_e/atom_rmse: 0.005280
2025-04-08 23:02:53.241 INFO: train_e/atom_rmse: 0.005280
train_f_mae: 0.013267
2025-04-08 23:02:53.245 INFO: train_f_mae: 0.013267
train_f_rmse: 0.021332
2025-04-08 23:02:53.245 INFO: train_f_rmse: 0.021332
val_e/atom_mae: 0.004617
2025-04-08 23:02:53.247 INFO: val_e/atom_mae: 0.004617
val_e/atom_rmse: 0.005141
2025-04-08 23:02:53.248 INFO: val_e/atom_rmse: 0.005141
val_f_mae: 0.013079
2025-04-08 23:02:53.248 INFO: val_f_mae: 0.013079
val_f_rmse: 0.019880
2025-04-08 23:02:53.248 INFO: val_f_rmse: 0.019880
##### Step: 20 Learning rate: 0.005 #####
2025-04-08 23:04:20.410 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 0.2393, Val Loss: 0.1807
2025-04-08 23:04:20.410 INFO: Epoch 21, Train Loss: 0.2393, Val Loss: 0.1807
train_e/atom_mae: 0.001953
2025-04-08 23:04:20.411 INFO: train_e/atom_mae: 0.001953
train_e/atom_rmse: 0.002500
2025-04-08 23:04:20.411 INFO: train_e/atom_rmse: 0.002500
train_f_mae: 0.009817
2025-04-08 23:04:20.415 INFO: train_f_mae: 0.009817
train_f_rmse: 0.015223
2025-04-08 23:04:20.415 INFO: train_f_rmse: 0.015223
val_e/atom_mae: 0.001471
2025-04-08 23:04:20.418 INFO: val_e/atom_mae: 0.001471
val_e/atom_rmse: 0.001783
2025-04-08 23:04:20.418 INFO: val_e/atom_rmse: 0.001783
val_f_mae: 0.008927
2025-04-08 23:04:20.418 INFO: val_f_mae: 0.008927
val_f_rmse: 0.013300
2025-04-08 23:04:20.419 INFO: val_f_rmse: 0.013300
##### Step: 21 Learning rate: 0.005 #####
2025-04-08 23:05:47.775 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.2058, Val Loss: 0.2661
2025-04-08 23:05:47.776 INFO: Epoch 22, Train Loss: 0.2058, Val Loss: 0.2661
train_e/atom_mae: 0.001631
2025-04-08 23:05:47.777 INFO: train_e/atom_mae: 0.001631
train_e/atom_rmse: 0.002144
2025-04-08 23:05:47.777 INFO: train_e/atom_rmse: 0.002144
train_f_mae: 0.009246
2025-04-08 23:05:47.781 INFO: train_f_mae: 0.009246
train_f_rmse: 0.014151
2025-04-08 23:05:47.781 INFO: train_f_rmse: 0.014151
val_e/atom_mae: 0.000694
2025-04-08 23:05:47.783 INFO: val_e/atom_mae: 0.000694
val_e/atom_rmse: 0.000746
2025-04-08 23:05:47.784 INFO: val_e/atom_rmse: 0.000746
val_f_mae: 0.010000
2025-04-08 23:05:47.784 INFO: val_f_mae: 0.010000
val_f_rmse: 0.016293
2025-04-08 23:05:47.784 INFO: val_f_rmse: 0.016293
##### Step: 22 Learning rate: 0.005 #####
2025-04-08 23:07:14.990 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.2080, Val Loss: 0.1694
2025-04-08 23:07:14.991 INFO: Epoch 23, Train Loss: 0.2080, Val Loss: 0.1694
train_e/atom_mae: 0.002173
2025-04-08 23:07:14.992 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002896
2025-04-08 23:07:14.992 INFO: train_e/atom_rmse: 0.002896
train_f_mae: 0.009225
2025-04-08 23:07:14.995 INFO: train_f_mae: 0.009225
train_f_rmse: 0.014065
2025-04-08 23:07:14.995 INFO: train_f_rmse: 0.014065
val_e/atom_mae: 0.003810
2025-04-08 23:07:14.998 INFO: val_e/atom_mae: 0.003810
val_e/atom_rmse: 0.003922
2025-04-08 23:07:14.998 INFO: val_e/atom_rmse: 0.003922
val_f_mae: 0.008194
2025-04-08 23:07:14.999 INFO: val_f_mae: 0.008194
val_f_rmse: 0.012278
2025-04-08 23:07:14.999 INFO: val_f_rmse: 0.012278
##### Step: 23 Learning rate: 0.005 #####
2025-04-08 23:08:42.242 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.1904, Val Loss: 0.1792
2025-04-08 23:08:42.243 INFO: Epoch 24, Train Loss: 0.1904, Val Loss: 0.1792
train_e/atom_mae: 0.002016
2025-04-08 23:08:42.244 INFO: train_e/atom_mae: 0.002016
train_e/atom_rmse: 0.002527
2025-04-08 23:08:42.244 INFO: train_e/atom_rmse: 0.002527
train_f_mae: 0.008929
2025-04-08 23:08:42.248 INFO: train_f_mae: 0.008929
train_f_rmse: 0.013517
2025-04-08 23:08:42.248 INFO: train_f_rmse: 0.013517
val_e/atom_mae: 0.000820
2025-04-08 23:08:42.250 INFO: val_e/atom_mae: 0.000820
val_e/atom_rmse: 0.000975
2025-04-08 23:08:42.251 INFO: val_e/atom_rmse: 0.000975
val_f_mae: 0.008956
2025-04-08 23:08:42.251 INFO: val_f_mae: 0.008956
val_f_rmse: 0.013344
2025-04-08 23:08:42.251 INFO: val_f_rmse: 0.013344
##### Step: 24 Learning rate: 0.005 #####
2025-04-08 23:10:09.478 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.1998, Val Loss: 0.1696
2025-04-08 23:10:09.479 INFO: Epoch 25, Train Loss: 0.1998, Val Loss: 0.1696
train_e/atom_mae: 0.002340
2025-04-08 23:10:09.480 INFO: train_e/atom_mae: 0.002340
train_e/atom_rmse: 0.002994
2025-04-08 23:10:09.480 INFO: train_e/atom_rmse: 0.002994
train_f_mae: 0.009115
2025-04-08 23:10:09.483 INFO: train_f_mae: 0.009115
train_f_rmse: 0.013747
2025-04-08 23:10:09.484 INFO: train_f_rmse: 0.013747
val_e/atom_mae: 0.001191
2025-04-08 23:10:09.486 INFO: val_e/atom_mae: 0.001191
val_e/atom_rmse: 0.001413
2025-04-08 23:10:09.487 INFO: val_e/atom_rmse: 0.001413
val_f_mae: 0.008581
2025-04-08 23:10:09.487 INFO: val_f_mae: 0.008581
val_f_rmse: 0.012930
2025-04-08 23:10:09.487 INFO: val_f_rmse: 0.012930
##### Step: 25 Learning rate: 0.005 #####
2025-04-08 23:11:36.622 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.2030, Val Loss: 0.2558
2025-04-08 23:11:36.623 INFO: Epoch 26, Train Loss: 0.2030, Val Loss: 0.2558
train_e/atom_mae: 0.001976
2025-04-08 23:11:36.624 INFO: train_e/atom_mae: 0.001976
train_e/atom_rmse: 0.002545
2025-04-08 23:11:36.624 INFO: train_e/atom_rmse: 0.002545
train_f_mae: 0.009144
2025-04-08 23:11:36.627 INFO: train_f_mae: 0.009144
train_f_rmse: 0.013970
2025-04-08 23:11:36.628 INFO: train_f_rmse: 0.013970
val_e/atom_mae: 0.000531
2025-04-08 23:11:36.630 INFO: val_e/atom_mae: 0.000531
val_e/atom_rmse: 0.000844
2025-04-08 23:11:36.631 INFO: val_e/atom_rmse: 0.000844
val_f_mae: 0.010616
2025-04-08 23:11:36.631 INFO: val_f_mae: 0.010616
val_f_rmse: 0.015968
2025-04-08 23:11:36.631 INFO: val_f_rmse: 0.015968
##### Step: 26 Learning rate: 0.005 #####
2025-04-08 23:13:03.840 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.2051, Val Loss: 0.1806
2025-04-08 23:13:03.841 INFO: Epoch 27, Train Loss: 0.2051, Val Loss: 0.1806
train_e/atom_mae: 0.001919
2025-04-08 23:13:03.842 INFO: train_e/atom_mae: 0.001919
train_e/atom_rmse: 0.002514
2025-04-08 23:13:03.842 INFO: train_e/atom_rmse: 0.002514
train_f_mae: 0.009325
2025-04-08 23:13:03.846 INFO: train_f_mae: 0.009325
train_f_rmse: 0.014051
2025-04-08 23:13:03.846 INFO: train_f_rmse: 0.014051
val_e/atom_mae: 0.003535
2025-04-08 23:13:03.849 INFO: val_e/atom_mae: 0.003535
val_e/atom_rmse: 0.003639
2025-04-08 23:13:03.849 INFO: val_e/atom_rmse: 0.003639
val_f_mae: 0.008799
2025-04-08 23:13:03.849 INFO: val_f_mae: 0.008799
val_f_rmse: 0.012827
2025-04-08 23:13:03.849 INFO: val_f_rmse: 0.012827
##### Step: 27 Learning rate: 0.005 #####
2025-04-08 23:14:31.082 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.2154, Val Loss: 0.1811
2025-04-08 23:14:31.083 INFO: Epoch 28, Train Loss: 0.2154, Val Loss: 0.1811
train_e/atom_mae: 0.003150
2025-04-08 23:14:31.084 INFO: train_e/atom_mae: 0.003150
train_e/atom_rmse: 0.003794
2025-04-08 23:14:31.084 INFO: train_e/atom_rmse: 0.003794
train_f_mae: 0.009151
2025-04-08 23:14:31.088 INFO: train_f_mae: 0.009151
train_f_rmse: 0.014072
2025-04-08 23:14:31.088 INFO: train_f_rmse: 0.014072
val_e/atom_mae: 0.001013
2025-04-08 23:14:31.090 INFO: val_e/atom_mae: 0.001013
val_e/atom_rmse: 0.001261
2025-04-08 23:14:31.091 INFO: val_e/atom_rmse: 0.001261
val_f_mae: 0.008856
2025-04-08 23:14:31.091 INFO: val_f_mae: 0.008856
val_f_rmse: 0.013387
2025-04-08 23:14:31.091 INFO: val_f_rmse: 0.013387
##### Step: 28 Learning rate: 0.005 #####
2025-04-08 23:15:58.279 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.2086, Val Loss: 0.1417
2025-04-08 23:15:58.280 INFO: Epoch 29, Train Loss: 0.2086, Val Loss: 0.1417
train_e/atom_mae: 0.002770
2025-04-08 23:15:58.281 INFO: train_e/atom_mae: 0.002770
train_e/atom_rmse: 0.003424
2025-04-08 23:15:58.281 INFO: train_e/atom_rmse: 0.003424
train_f_mae: 0.009200
2025-04-08 23:15:58.285 INFO: train_f_mae: 0.009200
train_f_rmse: 0.013942
2025-04-08 23:15:58.285 INFO: train_f_rmse: 0.013942
val_e/atom_mae: 0.000979
2025-04-08 23:15:58.287 INFO: val_e/atom_mae: 0.000979
val_e/atom_rmse: 0.001169
2025-04-08 23:15:58.288 INFO: val_e/atom_rmse: 0.001169
val_f_mae: 0.008201
2025-04-08 23:15:58.289 INFO: val_f_mae: 0.008201
val_f_rmse: 0.011835
2025-04-08 23:15:58.289 INFO: val_f_rmse: 0.011835
##### Step: 29 Learning rate: 0.005 #####
2025-04-08 23:17:25.475 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.1885, Val Loss: 0.1950
2025-04-08 23:17:25.476 INFO: Epoch 30, Train Loss: 0.1885, Val Loss: 0.1950
train_e/atom_mae: 0.001984
2025-04-08 23:17:25.477 INFO: train_e/atom_mae: 0.001984
train_e/atom_rmse: 0.002606
2025-04-08 23:17:25.477 INFO: train_e/atom_rmse: 0.002606
train_f_mae: 0.008821
2025-04-08 23:17:25.481 INFO: train_f_mae: 0.008821
train_f_rmse: 0.013427
2025-04-08 23:17:25.481 INFO: train_f_rmse: 0.013427
val_e/atom_mae: 0.001229
2025-04-08 23:17:25.483 INFO: val_e/atom_mae: 0.001229
val_e/atom_rmse: 0.001357
2025-04-08 23:17:25.484 INFO: val_e/atom_rmse: 0.001357
val_f_mae: 0.008919
2025-04-08 23:17:25.484 INFO: val_f_mae: 0.008919
val_f_rmse: 0.013885
2025-04-08 23:17:25.484 INFO: val_f_rmse: 0.013885
##### Step: 30 Learning rate: 0.005 #####
2025-04-08 23:18:52.627 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.2030, Val Loss: 0.1784
2025-04-08 23:18:52.628 INFO: Epoch 31, Train Loss: 0.2030, Val Loss: 0.1784
train_e/atom_mae: 0.001709
2025-04-08 23:18:52.629 INFO: train_e/atom_mae: 0.001709
train_e/atom_rmse: 0.002229
2025-04-08 23:18:52.629 INFO: train_e/atom_rmse: 0.002229
train_f_mae: 0.009130
2025-04-08 23:18:52.633 INFO: train_f_mae: 0.009130
train_f_rmse: 0.014037
2025-04-08 23:18:52.633 INFO: train_f_rmse: 0.014037
val_e/atom_mae: 0.003065
2025-04-08 23:18:52.635 INFO: val_e/atom_mae: 0.003065
val_e/atom_rmse: 0.003140
2025-04-08 23:18:52.636 INFO: val_e/atom_rmse: 0.003140
val_f_mae: 0.008661
2025-04-08 23:18:52.636 INFO: val_f_mae: 0.008661
val_f_rmse: 0.012901
2025-04-08 23:18:52.636 INFO: val_f_rmse: 0.012901
##### Step: 31 Learning rate: 0.005 #####
2025-04-08 23:20:19.718 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.2061, Val Loss: 0.2156
2025-04-08 23:20:19.718 INFO: Epoch 32, Train Loss: 0.2061, Val Loss: 0.2156
train_e/atom_mae: 0.002403
2025-04-08 23:20:19.719 INFO: train_e/atom_mae: 0.002403
train_e/atom_rmse: 0.003063
2025-04-08 23:20:19.719 INFO: train_e/atom_rmse: 0.003063
train_f_mae: 0.009197
2025-04-08 23:20:19.723 INFO: train_f_mae: 0.009197
train_f_rmse: 0.013955
2025-04-08 23:20:19.723 INFO: train_f_rmse: 0.013955
val_e/atom_mae: 0.000674
2025-04-08 23:20:19.726 INFO: val_e/atom_mae: 0.000674
val_e/atom_rmse: 0.000738
2025-04-08 23:20:19.726 INFO: val_e/atom_rmse: 0.000738
val_f_mae: 0.009922
2025-04-08 23:20:19.727 INFO: val_f_mae: 0.009922
val_f_rmse: 0.014659
2025-04-08 23:20:19.727 INFO: val_f_rmse: 0.014659
##### Step: 32 Learning rate: 0.005 #####
2025-04-08 23:21:46.916 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.2149, Val Loss: 0.2125
2025-04-08 23:21:46.917 INFO: Epoch 33, Train Loss: 0.2149, Val Loss: 0.2125
train_e/atom_mae: 0.002217
2025-04-08 23:21:46.918 INFO: train_e/atom_mae: 0.002217
train_e/atom_rmse: 0.002845
2025-04-08 23:21:46.918 INFO: train_e/atom_rmse: 0.002845
train_f_mae: 0.009315
2025-04-08 23:21:46.921 INFO: train_f_mae: 0.009315
train_f_rmse: 0.014321
2025-04-08 23:21:46.922 INFO: train_f_rmse: 0.014321
val_e/atom_mae: 0.000937
2025-04-08 23:21:46.924 INFO: val_e/atom_mae: 0.000937
val_e/atom_rmse: 0.001065
2025-04-08 23:21:46.925 INFO: val_e/atom_rmse: 0.001065
val_f_mae: 0.009714
2025-04-08 23:21:46.926 INFO: val_f_mae: 0.009714
val_f_rmse: 0.014529
2025-04-08 23:21:46.926 INFO: val_f_rmse: 0.014529
##### Step: 33 Learning rate: 0.005 #####
2025-04-08 23:23:14.126 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.2023, Val Loss: 0.1584
2025-04-08 23:23:14.127 INFO: Epoch 34, Train Loss: 0.2023, Val Loss: 0.1584
train_e/atom_mae: 0.002423
2025-04-08 23:23:14.128 INFO: train_e/atom_mae: 0.002423
train_e/atom_rmse: 0.002996
2025-04-08 23:23:14.128 INFO: train_e/atom_rmse: 0.002996
train_f_mae: 0.009083
2025-04-08 23:23:14.131 INFO: train_f_mae: 0.009083
train_f_rmse: 0.013834
2025-04-08 23:23:14.132 INFO: train_f_rmse: 0.013834
val_e/atom_mae: 0.001472
2025-04-08 23:23:14.134 INFO: val_e/atom_mae: 0.001472
val_e/atom_rmse: 0.001769
2025-04-08 23:23:14.135 INFO: val_e/atom_rmse: 0.001769
val_f_mae: 0.008212
2025-04-08 23:23:14.136 INFO: val_f_mae: 0.008212
val_f_rmse: 0.012434
2025-04-08 23:23:14.136 INFO: val_f_rmse: 0.012434
##### Step: 34 Learning rate: 0.005 #####
2025-04-08 23:24:41.253 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.1808, Val Loss: 0.2188
2025-04-08 23:24:41.253 INFO: Epoch 35, Train Loss: 0.1808, Val Loss: 0.2188
train_e/atom_mae: 0.001882
2025-04-08 23:24:41.254 INFO: train_e/atom_mae: 0.001882
train_e/atom_rmse: 0.002339
2025-04-08 23:24:41.254 INFO: train_e/atom_rmse: 0.002339
train_f_mae: 0.008776
2025-04-08 23:24:41.258 INFO: train_f_mae: 0.008776
train_f_rmse: 0.013199
2025-04-08 23:24:41.258 INFO: train_f_rmse: 0.013199
val_e/atom_mae: 0.002170
2025-04-08 23:24:41.261 INFO: val_e/atom_mae: 0.002170
val_e/atom_rmse: 0.002311
2025-04-08 23:24:41.261 INFO: val_e/atom_rmse: 0.002311
val_f_mae: 0.009633
2025-04-08 23:24:41.262 INFO: val_f_mae: 0.009633
val_f_rmse: 0.014571
2025-04-08 23:24:41.262 INFO: val_f_rmse: 0.014571
##### Step: 35 Learning rate: 0.005 #####
2025-04-08 23:26:08.415 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.1923, Val Loss: 0.1682
2025-04-08 23:26:08.416 INFO: Epoch 36, Train Loss: 0.1923, Val Loss: 0.1682
train_e/atom_mae: 0.002705
2025-04-08 23:26:08.417 INFO: train_e/atom_mae: 0.002705
train_e/atom_rmse: 0.003421
2025-04-08 23:26:08.417 INFO: train_e/atom_rmse: 0.003421
train_f_mae: 0.008812
2025-04-08 23:26:08.420 INFO: train_f_mae: 0.008812
train_f_rmse: 0.013346
2025-04-08 23:26:08.421 INFO: train_f_rmse: 0.013346
val_e/atom_mae: 0.000807
2025-04-08 23:26:08.423 INFO: val_e/atom_mae: 0.000807
val_e/atom_rmse: 0.000957
2025-04-08 23:26:08.424 INFO: val_e/atom_rmse: 0.000957
val_f_mae: 0.008530
2025-04-08 23:26:08.424 INFO: val_f_mae: 0.008530
val_f_rmse: 0.012925
2025-04-08 23:26:08.424 INFO: val_f_rmse: 0.012925
##### Step: 36 Learning rate: 0.005 #####
2025-04-08 23:27:35.670 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.1800, Val Loss: 0.1716
2025-04-08 23:27:35.670 INFO: Epoch 37, Train Loss: 0.1800, Val Loss: 0.1716
train_e/atom_mae: 0.001844
2025-04-08 23:27:35.671 INFO: train_e/atom_mae: 0.001844
train_e/atom_rmse: 0.002437
2025-04-08 23:27:35.671 INFO: train_e/atom_rmse: 0.002437
train_f_mae: 0.008785
2025-04-08 23:27:35.675 INFO: train_f_mae: 0.008785
train_f_rmse: 0.013147
2025-04-08 23:27:35.675 INFO: train_f_rmse: 0.013147
val_e/atom_mae: 0.001159
2025-04-08 23:27:35.678 INFO: val_e/atom_mae: 0.001159
val_e/atom_rmse: 0.001318
2025-04-08 23:27:35.678 INFO: val_e/atom_rmse: 0.001318
val_f_mae: 0.008889
2025-04-08 23:27:35.678 INFO: val_f_mae: 0.008889
val_f_rmse: 0.013020
2025-04-08 23:27:35.678 INFO: val_f_rmse: 0.013020
##### Step: 37 Learning rate: 0.005 #####
2025-04-08 23:29:02.925 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.2013, Val Loss: 0.2028
2025-04-08 23:29:02.925 INFO: Epoch 38, Train Loss: 0.2013, Val Loss: 0.2028
train_e/atom_mae: 0.002445
2025-04-08 23:29:02.926 INFO: train_e/atom_mae: 0.002445
train_e/atom_rmse: 0.002974
2025-04-08 23:29:02.926 INFO: train_e/atom_rmse: 0.002974
train_f_mae: 0.009041
2025-04-08 23:29:02.930 INFO: train_f_mae: 0.009041
train_f_rmse: 0.013805
2025-04-08 23:29:02.930 INFO: train_f_rmse: 0.013805
val_e/atom_mae: 0.004214
2025-04-08 23:29:02.933 INFO: val_e/atom_mae: 0.004214
val_e/atom_rmse: 0.004451
2025-04-08 23:29:02.933 INFO: val_e/atom_rmse: 0.004451
val_f_mae: 0.008787
2025-04-08 23:29:02.933 INFO: val_f_mae: 0.008787
val_f_rmse: 0.013372
2025-04-08 23:29:02.933 INFO: val_f_rmse: 0.013372
##### Step: 38 Learning rate: 0.005 #####
2025-04-08 23:30:30.104 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.1718, Val Loss: 0.2679
2025-04-08 23:30:30.105 INFO: Epoch 39, Train Loss: 0.1718, Val Loss: 0.2679
train_e/atom_mae: 0.001730
2025-04-08 23:30:30.106 INFO: train_e/atom_mae: 0.001730
train_e/atom_rmse: 0.002160
2025-04-08 23:30:30.106 INFO: train_e/atom_rmse: 0.002160
train_f_mae: 0.008509
2025-04-08 23:30:30.110 INFO: train_f_mae: 0.008509
train_f_rmse: 0.012892
2025-04-08 23:30:30.110 INFO: train_f_rmse: 0.012892
val_e/atom_mae: 0.002972
2025-04-08 23:30:30.113 INFO: val_e/atom_mae: 0.002972
val_e/atom_rmse: 0.003050
2025-04-08 23:30:30.113 INFO: val_e/atom_rmse: 0.003050
val_f_mae: 0.010750
2025-04-08 23:30:30.113 INFO: val_f_mae: 0.010750
val_f_rmse: 0.016021
2025-04-08 23:30:30.113 INFO: val_f_rmse: 0.016021
##### Step: 39 Learning rate: 0.005 #####
2025-04-08 23:31:57.210 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.1806, Val Loss: 0.2061
2025-04-08 23:31:57.211 INFO: Epoch 40, Train Loss: 0.1806, Val Loss: 0.2061
train_e/atom_mae: 0.002030
2025-04-08 23:31:57.212 INFO: train_e/atom_mae: 0.002030
train_e/atom_rmse: 0.002652
2025-04-08 23:31:57.212 INFO: train_e/atom_rmse: 0.002652
train_f_mae: 0.008633
2025-04-08 23:31:57.216 INFO: train_f_mae: 0.008633
train_f_rmse: 0.013118
2025-04-08 23:31:57.216 INFO: train_f_rmse: 0.013118
val_e/atom_mae: 0.001242
2025-04-08 23:31:57.218 INFO: val_e/atom_mae: 0.001242
val_e/atom_rmse: 0.001453
2025-04-08 23:31:57.219 INFO: val_e/atom_rmse: 0.001453
val_f_mae: 0.009164
2025-04-08 23:31:57.219 INFO: val_f_mae: 0.009164
val_f_rmse: 0.014266
2025-04-08 23:31:57.219 INFO: val_f_rmse: 0.014266
2025-04-08 23:31:57.228 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-04-08 23:33:24.396 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 0.1164, Val Loss: 0.1231
2025-04-08 23:33:24.397 INFO: Epoch 1, Train Loss: 0.1164, Val Loss: 0.1231
train_e/atom_mae: 0.001001
2025-04-08 23:33:24.397 INFO: train_e/atom_mae: 0.001001
train_e/atom_rmse: 0.001262
2025-04-08 23:33:24.398 INFO: train_e/atom_rmse: 0.001262
train_f_mae: 0.007306
2025-04-08 23:33:24.401 INFO: train_f_mae: 0.007306
train_f_rmse: 0.010699
2025-04-08 23:33:24.401 INFO: train_f_rmse: 0.010699
val_e/atom_mae: 0.000644
2025-04-08 23:33:24.404 INFO: val_e/atom_mae: 0.000644
val_e/atom_rmse: 0.000693
2025-04-08 23:33:24.404 INFO: val_e/atom_rmse: 0.000693
val_f_mae: 0.007556
2025-04-08 23:33:24.405 INFO: val_f_mae: 0.007556
val_f_rmse: 0.011069
2025-04-08 23:33:24.405 INFO: val_f_rmse: 0.011069
##### Step: 1 Learning rate: 0.004 #####
2025-04-08 23:34:51.606 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 0.1656, Val Loss: 0.1242
2025-04-08 23:34:51.607 INFO: Epoch 2, Train Loss: 0.1656, Val Loss: 0.1242
train_e/atom_mae: 0.002649
2025-04-08 23:34:51.608 INFO: train_e/atom_mae: 0.002649
train_e/atom_rmse: 0.003343
2025-04-08 23:34:51.608 INFO: train_e/atom_rmse: 0.003343
train_f_mae: 0.008211
2025-04-08 23:34:51.612 INFO: train_f_mae: 0.008211
train_f_rmse: 0.012330
2025-04-08 23:34:51.612 INFO: train_f_rmse: 0.012330
val_e/atom_mae: 0.002098
2025-04-08 23:34:51.614 INFO: val_e/atom_mae: 0.002098
val_e/atom_rmse: 0.002170
2025-04-08 23:34:51.615 INFO: val_e/atom_rmse: 0.002170
val_f_mae: 0.007461
2025-04-08 23:34:51.615 INFO: val_f_mae: 0.007461
val_f_rmse: 0.010888
2025-04-08 23:34:51.615 INFO: val_f_rmse: 0.010888
##### Step: 2 Learning rate: 0.006 #####
2025-04-08 23:36:18.683 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 0.2233, Val Loss: 0.1833
2025-04-08 23:36:18.684 INFO: Epoch 3, Train Loss: 0.2233, Val Loss: 0.1833
train_e/atom_mae: 0.002130
2025-04-08 23:36:18.685 INFO: train_e/atom_mae: 0.002130
train_e/atom_rmse: 0.002747
2025-04-08 23:36:18.685 INFO: train_e/atom_rmse: 0.002747
train_f_mae: 0.009380
2025-04-08 23:36:18.689 INFO: train_f_mae: 0.009380
train_f_rmse: 0.014633
2025-04-08 23:36:18.689 INFO: train_f_rmse: 0.014633
val_e/atom_mae: 0.000829
2025-04-08 23:36:18.691 INFO: val_e/atom_mae: 0.000829
val_e/atom_rmse: 0.000961
2025-04-08 23:36:18.692 INFO: val_e/atom_rmse: 0.000961
val_f_mae: 0.008668
2025-04-08 23:36:18.692 INFO: val_f_mae: 0.008668
val_f_rmse: 0.013498
2025-04-08 23:36:18.692 INFO: val_f_rmse: 0.013498
##### Step: 3 Learning rate: 0.008 #####
2025-04-08 23:37:45.809 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 0.2809, Val Loss: 0.3114
2025-04-08 23:37:45.810 INFO: Epoch 4, Train Loss: 0.2809, Val Loss: 0.3114
train_e/atom_mae: 0.002318
2025-04-08 23:37:45.810 INFO: train_e/atom_mae: 0.002318
train_e/atom_rmse: 0.002923
2025-04-08 23:37:45.811 INFO: train_e/atom_rmse: 0.002923
train_f_mae: 0.010604
2025-04-08 23:37:45.814 INFO: train_f_mae: 0.010604
train_f_rmse: 0.016449
2025-04-08 23:37:45.814 INFO: train_f_rmse: 0.016449
val_e/atom_mae: 0.002477
2025-04-08 23:37:45.817 INFO: val_e/atom_mae: 0.002477
val_e/atom_rmse: 0.002668
2025-04-08 23:37:45.817 INFO: val_e/atom_rmse: 0.002668
val_f_mae: 0.010592
2025-04-08 23:37:45.818 INFO: val_f_mae: 0.010592
val_f_rmse: 0.017402
2025-04-08 23:37:45.818 INFO: val_f_rmse: 0.017402
##### Step: 4 Learning rate: 0.01 #####
2025-04-08 23:39:12.933 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 0.3791, Val Loss: 0.5108
2025-04-08 23:39:12.933 INFO: Epoch 5, Train Loss: 0.3791, Val Loss: 0.5108
train_e/atom_mae: 0.003162
2025-04-08 23:39:12.934 INFO: train_e/atom_mae: 0.003162
train_e/atom_rmse: 0.003959
2025-04-08 23:39:12.934 INFO: train_e/atom_rmse: 0.003959
train_f_mae: 0.012151
2025-04-08 23:39:12.938 INFO: train_f_mae: 0.012151
train_f_rmse: 0.018976
2025-04-08 23:39:12.938 INFO: train_f_rmse: 0.018976
val_e/atom_mae: 0.002816
2025-04-08 23:39:12.941 INFO: val_e/atom_mae: 0.002816
val_e/atom_rmse: 0.003024
2025-04-08 23:39:12.941 INFO: val_e/atom_rmse: 0.003024
val_f_mae: 0.013922
2025-04-08 23:39:12.942 INFO: val_f_mae: 0.013922
val_f_rmse: 0.022356
2025-04-08 23:39:12.942 INFO: val_f_rmse: 0.022356
##### Step: 5 Learning rate: 0.01 #####
2025-04-08 23:40:40.173 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 0.3819, Val Loss: 0.3365
2025-04-08 23:40:40.173 INFO: Epoch 6, Train Loss: 0.3819, Val Loss: 0.3365
train_e/atom_mae: 0.003456
2025-04-08 23:40:40.174 INFO: train_e/atom_mae: 0.003456
train_e/atom_rmse: 0.004457
2025-04-08 23:40:40.174 INFO: train_e/atom_rmse: 0.004457
train_f_mae: 0.011990
2025-04-08 23:40:40.178 INFO: train_f_mae: 0.011990
train_f_rmse: 0.018917
2025-04-08 23:40:40.178 INFO: train_f_rmse: 0.018917
val_e/atom_mae: 0.006333
2025-04-08 23:40:40.181 INFO: val_e/atom_mae: 0.006333
val_e/atom_rmse: 0.006396
2025-04-08 23:40:40.181 INFO: val_e/atom_rmse: 0.006396
val_f_mae: 0.010949
2025-04-08 23:40:40.181 INFO: val_f_mae: 0.010949
val_f_rmse: 0.016942
2025-04-08 23:40:40.182 INFO: val_f_rmse: 0.016942
##### Step: 6 Learning rate: 0.01 #####
2025-04-08 23:42:07.316 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 0.4304, Val Loss: 0.3041
2025-04-08 23:42:07.317 INFO: Epoch 7, Train Loss: 0.4304, Val Loss: 0.3041
train_e/atom_mae: 0.003898
2025-04-08 23:42:07.318 INFO: train_e/atom_mae: 0.003898
train_e/atom_rmse: 0.004848
2025-04-08 23:42:07.318 INFO: train_e/atom_rmse: 0.004848
train_f_mae: 0.012077
2025-04-08 23:42:07.322 INFO: train_f_mae: 0.012077
train_f_rmse: 0.020049
2025-04-08 23:42:07.322 INFO: train_f_rmse: 0.020049
val_e/atom_mae: 0.001121
2025-04-08 23:42:07.324 INFO: val_e/atom_mae: 0.001121
val_e/atom_rmse: 0.001421
2025-04-08 23:42:07.325 INFO: val_e/atom_rmse: 0.001421
val_f_mae: 0.011170
2025-04-08 23:42:07.325 INFO: val_f_mae: 0.011170
val_f_rmse: 0.017369
2025-04-08 23:42:07.325 INFO: val_f_rmse: 0.017369
##### Step: 7 Learning rate: 0.01 #####
2025-04-08 23:43:34.503 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 0.4076, Val Loss: 0.8402
2025-04-08 23:43:34.504 INFO: Epoch 8, Train Loss: 0.4076, Val Loss: 0.8402
train_e/atom_mae: 0.003281
2025-04-08 23:43:34.505 INFO: train_e/atom_mae: 0.003281
train_e/atom_rmse: 0.004113
2025-04-08 23:43:34.505 INFO: train_e/atom_rmse: 0.004113
train_f_mae: 0.012392
2025-04-08 23:43:34.509 INFO: train_f_mae: 0.012392
train_f_rmse: 0.019677
2025-04-08 23:43:34.509 INFO: train_f_rmse: 0.019677
val_e/atom_mae: 0.001733
2025-04-08 23:43:34.512 INFO: val_e/atom_mae: 0.001733
val_e/atom_rmse: 0.002089
2025-04-08 23:43:34.512 INFO: val_e/atom_rmse: 0.002089
val_f_mae: 0.018274
2025-04-08 23:43:34.512 INFO: val_f_mae: 0.018274
val_f_rmse: 0.028895
2025-04-08 23:43:34.513 INFO: val_f_rmse: 0.028895
##### Step: 8 Learning rate: 0.01 #####
2025-04-08 23:45:01.668 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 0.4961, Val Loss: 0.3250
2025-04-08 23:45:01.669 INFO: Epoch 9, Train Loss: 0.4961, Val Loss: 0.3250
train_e/atom_mae: 0.003870
2025-04-08 23:45:01.670 INFO: train_e/atom_mae: 0.003870
train_e/atom_rmse: 0.004914
2025-04-08 23:45:01.670 INFO: train_e/atom_rmse: 0.004914
train_f_mae: 0.012552
2025-04-08 23:45:01.673 INFO: train_f_mae: 0.012552
train_f_rmse: 0.021608
2025-04-08 23:45:01.673 INFO: train_f_rmse: 0.021608
val_e/atom_mae: 0.005042
2025-04-08 23:45:01.676 INFO: val_e/atom_mae: 0.005042
val_e/atom_rmse: 0.005099
2025-04-08 23:45:01.676 INFO: val_e/atom_rmse: 0.005099
val_f_mae: 0.011760
2025-04-08 23:45:01.677 INFO: val_f_mae: 0.011760
val_f_rmse: 0.017135
2025-04-08 23:45:01.677 INFO: val_f_rmse: 0.017135
##### Step: 9 Learning rate: 0.01 #####
2025-04-08 23:46:28.820 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 0.3424, Val Loss: 0.2275
2025-04-08 23:46:28.821 INFO: Epoch 10, Train Loss: 0.3424, Val Loss: 0.2275
train_e/atom_mae: 0.003225
2025-04-08 23:46:28.822 INFO: train_e/atom_mae: 0.003225
train_e/atom_rmse: 0.003990
2025-04-08 23:46:28.822 INFO: train_e/atom_rmse: 0.003990
train_f_mae: 0.011653
2025-04-08 23:46:28.825 INFO: train_f_mae: 0.011653
train_f_rmse: 0.017977
2025-04-08 23:46:28.825 INFO: train_f_rmse: 0.017977
val_e/atom_mae: 0.001564
2025-04-08 23:46:28.828 INFO: val_e/atom_mae: 0.001564
val_e/atom_rmse: 0.001787
2025-04-08 23:46:28.828 INFO: val_e/atom_rmse: 0.001787
val_f_mae: 0.010278
2025-04-08 23:46:28.829 INFO: val_f_mae: 0.010278
val_f_rmse: 0.014955
2025-04-08 23:46:28.829 INFO: val_f_rmse: 0.014955
##### Step: 10 Learning rate: 0.01 #####
2025-04-08 23:47:55.978 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 0.3901, Val Loss: 0.4278
2025-04-08 23:47:55.978 INFO: Epoch 11, Train Loss: 0.3901, Val Loss: 0.4278
train_e/atom_mae: 0.003303
2025-04-08 23:47:55.979 INFO: train_e/atom_mae: 0.003303
train_e/atom_rmse: 0.004303
2025-04-08 23:47:55.979 INFO: train_e/atom_rmse: 0.004303
train_f_mae: 0.011809
2025-04-08 23:47:55.983 INFO: train_f_mae: 0.011809
train_f_rmse: 0.019176
2025-04-08 23:47:55.983 INFO: train_f_rmse: 0.019176
val_e/atom_mae: 0.008026
2025-04-08 23:47:55.986 INFO: val_e/atom_mae: 0.008026
val_e/atom_rmse: 0.008127
2025-04-08 23:47:55.986 INFO: val_e/atom_rmse: 0.008127
val_f_mae: 0.012166
2025-04-08 23:47:55.986 INFO: val_f_mae: 0.012166
val_f_rmse: 0.018651
2025-04-08 23:47:55.987 INFO: val_f_rmse: 0.018651
##### Step: 11 Learning rate: 0.01 #####
2025-04-08 23:49:23.200 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 0.3807, Val Loss: 0.3609
2025-04-08 23:49:23.202 INFO: Epoch 12, Train Loss: 0.3807, Val Loss: 0.3609
train_e/atom_mae: 0.004196
2025-04-08 23:49:23.203 INFO: train_e/atom_mae: 0.004196
train_e/atom_rmse: 0.004992
2025-04-08 23:49:23.203 INFO: train_e/atom_rmse: 0.004992
train_f_mae: 0.011373
2025-04-08 23:49:23.207 INFO: train_f_mae: 0.011373
train_f_rmse: 0.018722
2025-04-08 23:49:23.207 INFO: train_f_rmse: 0.018722
val_e/atom_mae: 0.000816
2025-04-08 23:49:23.210 INFO: val_e/atom_mae: 0.000816
val_e/atom_rmse: 0.001105
2025-04-08 23:49:23.210 INFO: val_e/atom_rmse: 0.001105
val_f_mae: 0.012048
2025-04-08 23:49:23.210 INFO: val_f_mae: 0.012048
val_f_rmse: 0.018959
2025-04-08 23:49:23.210 INFO: val_f_rmse: 0.018959
##### Step: 12 Learning rate: 0.01 #####
2025-04-08 23:50:50.378 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 0.3731, Val Loss: 0.3925
2025-04-08 23:50:50.379 INFO: Epoch 13, Train Loss: 0.3731, Val Loss: 0.3925
train_e/atom_mae: 0.003610
2025-04-08 23:50:50.380 INFO: train_e/atom_mae: 0.003610
train_e/atom_rmse: 0.004657
2025-04-08 23:50:50.380 INFO: train_e/atom_rmse: 0.004657
train_f_mae: 0.011862
2025-04-08 23:50:50.383 INFO: train_f_mae: 0.011862
train_f_rmse: 0.018623
2025-04-08 23:50:50.383 INFO: train_f_rmse: 0.018623
val_e/atom_mae: 0.001026
2025-04-08 23:50:50.386 INFO: val_e/atom_mae: 0.001026
val_e/atom_rmse: 0.001245
2025-04-08 23:50:50.386 INFO: val_e/atom_rmse: 0.001245
val_f_mae: 0.012726
2025-04-08 23:50:50.387 INFO: val_f_mae: 0.012726
val_f_rmse: 0.019764
2025-04-08 23:50:50.387 INFO: val_f_rmse: 0.019764
##### Step: 13 Learning rate: 0.01 #####
2025-04-08 23:52:17.542 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 0.3546, Val Loss: 0.4689
2025-04-08 23:52:17.542 INFO: Epoch 14, Train Loss: 0.3546, Val Loss: 0.4689
train_e/atom_mae: 0.003629
2025-04-08 23:52:17.543 INFO: train_e/atom_mae: 0.003629
train_e/atom_rmse: 0.004401
2025-04-08 23:52:17.544 INFO: train_e/atom_rmse: 0.004401
train_f_mae: 0.011438
2025-04-08 23:52:17.547 INFO: train_f_mae: 0.011438
train_f_rmse: 0.018198
2025-04-08 23:52:17.547 INFO: train_f_rmse: 0.018198
val_e/atom_mae: 0.001249
2025-04-08 23:52:17.550 INFO: val_e/atom_mae: 0.001249
val_e/atom_rmse: 0.001693
2025-04-08 23:52:17.550 INFO: val_e/atom_rmse: 0.001693
val_f_mae: 0.013645
2025-04-08 23:52:17.551 INFO: val_f_mae: 0.013645
val_f_rmse: 0.021575
2025-04-08 23:52:17.551 INFO: val_f_rmse: 0.021575
##### Step: 14 Learning rate: 0.01 #####
2025-04-08 23:53:44.795 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 0.3507, Val Loss: 0.3636
2025-04-08 23:53:44.795 INFO: Epoch 15, Train Loss: 0.3507, Val Loss: 0.3636
train_e/atom_mae: 0.002811
2025-04-08 23:53:44.796 INFO: train_e/atom_mae: 0.002811
train_e/atom_rmse: 0.003432
2025-04-08 23:53:44.796 INFO: train_e/atom_rmse: 0.003432
train_f_mae: 0.011585
2025-04-08 23:53:44.800 INFO: train_f_mae: 0.011585
train_f_rmse: 0.018343
2025-04-08 23:53:44.800 INFO: train_f_rmse: 0.018343
val_e/atom_mae: 0.005165
2025-04-08 23:53:44.803 INFO: val_e/atom_mae: 0.005165
val_e/atom_rmse: 0.005257
2025-04-08 23:53:44.803 INFO: val_e/atom_rmse: 0.005257
val_f_mae: 0.012073
2025-04-08 23:53:44.804 INFO: val_f_mae: 0.012073
val_f_rmse: 0.018169
2025-04-08 23:53:44.804 INFO: val_f_rmse: 0.018169
##### Step: 15 Learning rate: 0.01 #####
2025-04-08 23:55:12.049 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 0.3271, Val Loss: 0.3746
2025-04-08 23:55:12.050 INFO: Epoch 16, Train Loss: 0.3271, Val Loss: 0.3746
train_e/atom_mae: 0.003461
2025-04-08 23:55:12.051 INFO: train_e/atom_mae: 0.003461
train_e/atom_rmse: 0.004315
2025-04-08 23:55:12.051 INFO: train_e/atom_rmse: 0.004315
train_f_mae: 0.011138
2025-04-08 23:55:12.054 INFO: train_f_mae: 0.011138
train_f_rmse: 0.017451
2025-04-08 23:55:12.054 INFO: train_f_rmse: 0.017451
val_e/atom_mae: 0.008140
2025-04-08 23:55:12.057 INFO: val_e/atom_mae: 0.008140
val_e/atom_rmse: 0.008305
2025-04-08 23:55:12.057 INFO: val_e/atom_rmse: 0.008305
val_f_mae: 0.010828
2025-04-08 23:55:12.058 INFO: val_f_mae: 0.010828
val_f_rmse: 0.017062
2025-04-08 23:55:12.058 INFO: val_f_rmse: 0.017062
##### Step: 16 Learning rate: 0.01 #####
2025-04-08 23:56:39.265 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 0.3498, Val Loss: 0.2795
2025-04-08 23:56:39.266 INFO: Epoch 17, Train Loss: 0.3498, Val Loss: 0.2795
train_e/atom_mae: 0.004149
2025-04-08 23:56:39.266 INFO: train_e/atom_mae: 0.004149
train_e/atom_rmse: 0.005088
2025-04-08 23:56:39.267 INFO: train_e/atom_rmse: 0.005088
train_f_mae: 0.011515
2025-04-08 23:56:39.270 INFO: train_f_mae: 0.011515
train_f_rmse: 0.017846
2025-04-08 23:56:39.270 INFO: train_f_rmse: 0.017846
val_e/atom_mae: 0.005291
2025-04-08 23:56:39.273 INFO: val_e/atom_mae: 0.005291
val_e/atom_rmse: 0.005397
2025-04-08 23:56:39.273 INFO: val_e/atom_rmse: 0.005397
val_f_mae: 0.009935
2025-04-08 23:56:39.274 INFO: val_f_mae: 0.009935
val_f_rmse: 0.015630
2025-04-08 23:56:39.274 INFO: val_f_rmse: 0.015630
##### Step: 17 Learning rate: 0.01 #####
2025-04-08 23:58:05.358 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 0.3863, Val Loss: 0.2676
2025-04-08 23:58:05.359 INFO: Epoch 18, Train Loss: 0.3863, Val Loss: 0.2676
train_e/atom_mae: 0.003825
2025-04-08 23:58:05.360 INFO: train_e/atom_mae: 0.003825
train_e/atom_rmse: 0.004701
2025-04-08 23:58:05.360 INFO: train_e/atom_rmse: 0.004701
train_f_mae: 0.011960
2025-04-08 23:58:05.364 INFO: train_f_mae: 0.011960
train_f_rmse: 0.018963
2025-04-08 23:58:05.364 INFO: train_f_rmse: 0.018963
val_e/atom_mae: 0.001875
2025-04-08 23:58:05.366 INFO: val_e/atom_mae: 0.001875
val_e/atom_rmse: 0.002387
2025-04-08 23:58:05.367 INFO: val_e/atom_rmse: 0.002387
val_f_mae: 0.010665
2025-04-08 23:58:05.367 INFO: val_f_mae: 0.010665
val_f_rmse: 0.016148
2025-04-08 23:58:05.367 INFO: val_f_rmse: 0.016148
##### Step: 18 Learning rate: 0.01 #####
2025-04-08 23:59:31.097 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 0.3583, Val Loss: 0.3903
2025-04-08 23:59:31.098 INFO: Epoch 19, Train Loss: 0.3583, Val Loss: 0.3903
train_e/atom_mae: 0.003413
2025-04-08 23:59:31.099 INFO: train_e/atom_mae: 0.003413
train_e/atom_rmse: 0.004311
2025-04-08 23:59:31.099 INFO: train_e/atom_rmse: 0.004311
train_f_mae: 0.011879
2025-04-08 23:59:31.102 INFO: train_f_mae: 0.011879
train_f_rmse: 0.018325
2025-04-08 23:59:31.102 INFO: train_f_rmse: 0.018325
val_e/atom_mae: 0.001752
2025-04-08 23:59:31.105 INFO: val_e/atom_mae: 0.001752
val_e/atom_rmse: 0.002072
2025-04-08 23:59:31.105 INFO: val_e/atom_rmse: 0.002072
val_f_mae: 0.012994
2025-04-08 23:59:31.106 INFO: val_f_mae: 0.012994
val_f_rmse: 0.019624
2025-04-08 23:59:31.106 INFO: val_f_rmse: 0.019624
##### Step: 19 Learning rate: 0.01 #####
2025-04-09 00:00:56.803 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 0.4206, Val Loss: 0.5641
2025-04-09 00:00:56.803 INFO: Epoch 20, Train Loss: 0.4206, Val Loss: 0.5641
train_e/atom_mae: 0.003053
2025-04-09 00:00:56.804 INFO: train_e/atom_mae: 0.003053
train_e/atom_rmse: 0.004095
2025-04-09 00:00:56.804 INFO: train_e/atom_rmse: 0.004095
train_f_mae: 0.012408
2025-04-09 00:00:56.808 INFO: train_f_mae: 0.012408
train_f_rmse: 0.020007
2025-04-09 00:00:56.808 INFO: train_f_rmse: 0.020007
val_e/atom_mae: 0.008666
2025-04-09 00:00:56.811 INFO: val_e/atom_mae: 0.008666
val_e/atom_rmse: 0.008760
2025-04-09 00:00:56.811 INFO: val_e/atom_rmse: 0.008760
val_f_mae: 0.013566
2025-04-09 00:00:56.811 INFO: val_f_mae: 0.013566
val_f_rmse: 0.021708
2025-04-09 00:00:56.812 INFO: val_f_rmse: 0.021708
##### Step: 20 Learning rate: 0.005 #####
2025-04-09 00:02:22.519 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 0.2187, Val Loss: 0.1472
2025-04-09 00:02:22.519 INFO: Epoch 21, Train Loss: 0.2187, Val Loss: 0.1472
train_e/atom_mae: 0.002623
2025-04-09 00:02:22.520 INFO: train_e/atom_mae: 0.002623
train_e/atom_rmse: 0.003255
2025-04-09 00:02:22.521 INFO: train_e/atom_rmse: 0.003255
train_f_mae: 0.009479
2025-04-09 00:02:22.524 INFO: train_f_mae: 0.009479
train_f_rmse: 0.014347
2025-04-09 00:02:22.524 INFO: train_f_rmse: 0.014347
val_e/atom_mae: 0.001328
2025-04-09 00:02:22.527 INFO: val_e/atom_mae: 0.001328
val_e/atom_rmse: 0.001500
2025-04-09 00:02:22.527 INFO: val_e/atom_rmse: 0.001500
val_f_mae: 0.007996
2025-04-09 00:02:22.528 INFO: val_f_mae: 0.007996
val_f_rmse: 0.012021
2025-04-09 00:02:22.528 INFO: val_f_rmse: 0.012021
##### Step: 21 Learning rate: 0.005 #####
2025-04-09 00:03:48.200 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.1869, Val Loss: 0.2950
2025-04-09 00:03:48.201 INFO: Epoch 22, Train Loss: 0.1869, Val Loss: 0.2950
train_e/atom_mae: 0.002175
2025-04-09 00:03:48.202 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002698
2025-04-09 00:03:48.202 INFO: train_e/atom_rmse: 0.002698
train_f_mae: 0.008838
2025-04-09 00:03:48.205 INFO: train_f_mae: 0.008838
train_f_rmse: 0.013345
2025-04-09 00:03:48.205 INFO: train_f_rmse: 0.013345
val_e/atom_mae: 0.001953
2025-04-09 00:03:48.208 INFO: val_e/atom_mae: 0.001953
val_e/atom_rmse: 0.002320
2025-04-09 00:03:48.208 INFO: val_e/atom_rmse: 0.002320
val_f_mae: 0.010851
2025-04-09 00:03:48.209 INFO: val_f_mae: 0.010851
val_f_rmse: 0.016985
2025-04-09 00:03:48.209 INFO: val_f_rmse: 0.016985
##### Step: 22 Learning rate: 0.005 #####
2025-04-09 00:05:13.810 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.1863, Val Loss: 0.1747
2025-04-09 00:05:13.811 INFO: Epoch 23, Train Loss: 0.1863, Val Loss: 0.1747
train_e/atom_mae: 0.001987
2025-04-09 00:05:13.811 INFO: train_e/atom_mae: 0.001987
train_e/atom_rmse: 0.002487
2025-04-09 00:05:13.812 INFO: train_e/atom_rmse: 0.002487
train_f_mae: 0.008885
2025-04-09 00:05:13.815 INFO: train_f_mae: 0.008885
train_f_rmse: 0.013372
2025-04-09 00:05:13.815 INFO: train_f_rmse: 0.013372
val_e/atom_mae: 0.001544
2025-04-09 00:05:13.818 INFO: val_e/atom_mae: 0.001544
val_e/atom_rmse: 0.001678
2025-04-09 00:05:13.818 INFO: val_e/atom_rmse: 0.001678
val_f_mae: 0.008747
2025-04-09 00:05:13.818 INFO: val_f_mae: 0.008747
val_f_rmse: 0.013089
2025-04-09 00:05:13.819 INFO: val_f_rmse: 0.013089
##### Step: 23 Learning rate: 0.005 #####
2025-04-09 00:06:39.473 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.1849, Val Loss: 0.2495
2025-04-09 00:06:39.474 INFO: Epoch 24, Train Loss: 0.1849, Val Loss: 0.2495
train_e/atom_mae: 0.002395
2025-04-09 00:06:39.474 INFO: train_e/atom_mae: 0.002395
train_e/atom_rmse: 0.002858
2025-04-09 00:06:39.475 INFO: train_e/atom_rmse: 0.002858
train_f_mae: 0.008553
2025-04-09 00:06:39.478 INFO: train_f_mae: 0.008553
train_f_rmse: 0.013231
2025-04-09 00:06:39.478 INFO: train_f_rmse: 0.013231
val_e/atom_mae: 0.000876
2025-04-09 00:06:39.481 INFO: val_e/atom_mae: 0.000876
val_e/atom_rmse: 0.001112
2025-04-09 00:06:39.481 INFO: val_e/atom_rmse: 0.001112
val_f_mae: 0.010078
2025-04-09 00:06:39.481 INFO: val_f_mae: 0.010078
val_f_rmse: 0.015747
2025-04-09 00:06:39.482 INFO: val_f_rmse: 0.015747
##### Step: 24 Learning rate: 0.005 #####
2025-04-09 00:08:05.187 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.2088, Val Loss: 0.2151
2025-04-09 00:08:05.188 INFO: Epoch 25, Train Loss: 0.2088, Val Loss: 0.2151
train_e/atom_mae: 0.002277
2025-04-09 00:08:05.189 INFO: train_e/atom_mae: 0.002277
train_e/atom_rmse: 0.003167
2025-04-09 00:08:05.189 INFO: train_e/atom_rmse: 0.003167
train_f_mae: 0.009203
2025-04-09 00:08:05.192 INFO: train_f_mae: 0.009203
train_f_rmse: 0.014022
2025-04-09 00:08:05.192 INFO: train_f_rmse: 0.014022
val_e/atom_mae: 0.006498
2025-04-09 00:08:05.195 INFO: val_e/atom_mae: 0.006498
val_e/atom_rmse: 0.006543
2025-04-09 00:08:05.195 INFO: val_e/atom_rmse: 0.006543
val_f_mae: 0.009284
2025-04-09 00:08:05.196 INFO: val_f_mae: 0.009284
val_f_rmse: 0.012779
2025-04-09 00:08:05.196 INFO: val_f_rmse: 0.012779
##### Step: 25 Learning rate: 0.005 #####
2025-04-09 00:09:30.925 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.1795, Val Loss: 0.1360
2025-04-09 00:09:30.926 INFO: Epoch 26, Train Loss: 0.1795, Val Loss: 0.1360
train_e/atom_mae: 0.002695
2025-04-09 00:09:30.927 INFO: train_e/atom_mae: 0.002695
train_e/atom_rmse: 0.003379
2025-04-09 00:09:30.927 INFO: train_e/atom_rmse: 0.003379
train_f_mae: 0.008458
2025-04-09 00:09:30.930 INFO: train_f_mae: 0.008458
train_f_rmse: 0.012871
2025-04-09 00:09:30.930 INFO: train_f_rmse: 0.012871
val_e/atom_mae: 0.000643
2025-04-09 00:09:30.933 INFO: val_e/atom_mae: 0.000643
val_e/atom_rmse: 0.000793
2025-04-09 00:09:30.933 INFO: val_e/atom_rmse: 0.000793
val_f_mae: 0.007879
2025-04-09 00:09:30.934 INFO: val_f_mae: 0.007879
val_f_rmse: 0.011630
2025-04-09 00:09:30.934 INFO: val_f_rmse: 0.011630
##### Step: 26 Learning rate: 0.005 #####
2025-04-09 00:10:56.621 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.1609, Val Loss: 0.1496
2025-04-09 00:10:56.622 INFO: Epoch 27, Train Loss: 0.1609, Val Loss: 0.1496
train_e/atom_mae: 0.002205
2025-04-09 00:10:56.623 INFO: train_e/atom_mae: 0.002205
train_e/atom_rmse: 0.002758
2025-04-09 00:10:56.623 INFO: train_e/atom_rmse: 0.002758
train_f_mae: 0.008194
2025-04-09 00:10:56.626 INFO: train_f_mae: 0.008194
train_f_rmse: 0.012316
2025-04-09 00:10:56.626 INFO: train_f_rmse: 0.012316
val_e/atom_mae: 0.000660
2025-04-09 00:10:56.629 INFO: val_e/atom_mae: 0.000660
val_e/atom_rmse: 0.000808
2025-04-09 00:10:56.629 INFO: val_e/atom_rmse: 0.000808
val_f_mae: 0.008303
2025-04-09 00:10:56.631 INFO: val_f_mae: 0.008303
val_f_rmse: 0.012201
2025-04-09 00:10:56.631 INFO: val_f_rmse: 0.012201
##### Step: 27 Learning rate: 0.005 #####
2025-04-09 00:12:22.299 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.1790, Val Loss: 0.1657
2025-04-09 00:12:22.300 INFO: Epoch 28, Train Loss: 0.1790, Val Loss: 0.1657
train_e/atom_mae: 0.002093
2025-04-09 00:12:22.301 INFO: train_e/atom_mae: 0.002093
train_e/atom_rmse: 0.002858
2025-04-09 00:12:22.301 INFO: train_e/atom_rmse: 0.002858
train_f_mae: 0.008462
2025-04-09 00:12:22.305 INFO: train_f_mae: 0.008462
train_f_rmse: 0.013003
2025-04-09 00:12:22.305 INFO: train_f_rmse: 0.013003
val_e/atom_mae: 0.001225
2025-04-09 00:12:22.307 INFO: val_e/atom_mae: 0.001225
val_e/atom_rmse: 0.001380
2025-04-09 00:12:22.308 INFO: val_e/atom_rmse: 0.001380
val_f_mae: 0.008343
2025-04-09 00:12:22.308 INFO: val_f_mae: 0.008343
val_f_rmse: 0.012784
2025-04-09 00:12:22.308 INFO: val_f_rmse: 0.012784
##### Step: 28 Learning rate: 0.005 #####
2025-04-09 00:13:47.995 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.1695, Val Loss: 0.1863
2025-04-09 00:13:47.995 INFO: Epoch 29, Train Loss: 0.1695, Val Loss: 0.1863
train_e/atom_mae: 0.002005
2025-04-09 00:13:47.996 INFO: train_e/atom_mae: 0.002005
train_e/atom_rmse: 0.002564
2025-04-09 00:13:47.996 INFO: train_e/atom_rmse: 0.002564
train_f_mae: 0.008420
2025-04-09 00:13:48.000 INFO: train_f_mae: 0.008420
train_f_rmse: 0.012711
2025-04-09 00:13:48.000 INFO: train_f_rmse: 0.012711
val_e/atom_mae: 0.000674
2025-04-09 00:13:48.003 INFO: val_e/atom_mae: 0.000674
val_e/atom_rmse: 0.000802
2025-04-09 00:13:48.003 INFO: val_e/atom_rmse: 0.000802
val_f_mae: 0.008607
2025-04-09 00:13:48.004 INFO: val_f_mae: 0.008607
val_f_rmse: 0.013621
2025-04-09 00:13:48.004 INFO: val_f_rmse: 0.013621
##### Step: 29 Learning rate: 0.005 #####
2025-04-09 00:15:13.681 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.1904, Val Loss: 0.1863
2025-04-09 00:15:13.682 INFO: Epoch 30, Train Loss: 0.1904, Val Loss: 0.1863
train_e/atom_mae: 0.002052
2025-04-09 00:15:13.683 INFO: train_e/atom_mae: 0.002052
train_e/atom_rmse: 0.002673
2025-04-09 00:15:13.683 INFO: train_e/atom_rmse: 0.002673
train_f_mae: 0.008657
2025-04-09 00:15:13.686 INFO: train_f_mae: 0.008657
train_f_rmse: 0.013481
2025-04-09 00:15:13.687 INFO: train_f_rmse: 0.013481
val_e/atom_mae: 0.002868
2025-04-09 00:15:13.689 INFO: val_e/atom_mae: 0.002868
val_e/atom_rmse: 0.003037
2025-04-09 00:15:13.690 INFO: val_e/atom_rmse: 0.003037
val_f_mae: 0.009073
2025-04-09 00:15:13.690 INFO: val_f_mae: 0.009073
val_f_rmse: 0.013233
2025-04-09 00:15:13.690 INFO: val_f_rmse: 0.013233
##### Step: 30 Learning rate: 0.005 #####
2025-04-09 00:16:39.427 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.1966, Val Loss: 0.1634
2025-04-09 00:16:39.428 INFO: Epoch 31, Train Loss: 0.1966, Val Loss: 0.1634
train_e/atom_mae: 0.002797
2025-04-09 00:16:39.429 INFO: train_e/atom_mae: 0.002797
train_e/atom_rmse: 0.003633
2025-04-09 00:16:39.429 INFO: train_e/atom_rmse: 0.003633
train_f_mae: 0.008625
2025-04-09 00:16:39.432 INFO: train_f_mae: 0.008625
train_f_rmse: 0.013438
2025-04-09 00:16:39.432 INFO: train_f_rmse: 0.013438
val_e/atom_mae: 0.001707
2025-04-09 00:16:39.435 INFO: val_e/atom_mae: 0.001707
val_e/atom_rmse: 0.001937
2025-04-09 00:16:39.435 INFO: val_e/atom_rmse: 0.001937
val_f_mae: 0.008653
2025-04-09 00:16:39.436 INFO: val_f_mae: 0.008653
val_f_rmse: 0.012605
2025-04-09 00:16:39.436 INFO: val_f_rmse: 0.012605
##### Step: 31 Learning rate: 0.005 #####
2025-04-09 00:18:05.114 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.1852, Val Loss: 0.2944
2025-04-09 00:18:05.115 INFO: Epoch 32, Train Loss: 0.1852, Val Loss: 0.2944
train_e/atom_mae: 0.001756
2025-04-09 00:18:05.116 INFO: train_e/atom_mae: 0.001756
train_e/atom_rmse: 0.002267
2025-04-09 00:18:05.116 INFO: train_e/atom_rmse: 0.002267
train_f_mae: 0.008871
2025-04-09 00:18:05.119 INFO: train_f_mae: 0.008871
train_f_rmse: 0.013379
2025-04-09 00:18:05.119 INFO: train_f_rmse: 0.013379
val_e/atom_mae: 0.004289
2025-04-09 00:18:05.122 INFO: val_e/atom_mae: 0.004289
val_e/atom_rmse: 0.004401
2025-04-09 00:18:05.122 INFO: val_e/atom_rmse: 0.004401
val_f_mae: 0.010732
2025-04-09 00:18:05.123 INFO: val_f_mae: 0.010732
val_f_rmse: 0.016461
2025-04-09 00:18:05.123 INFO: val_f_rmse: 0.016461
##### Step: 32 Learning rate: 0.005 #####
2025-04-09 00:19:30.813 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.1725, Val Loss: 0.1924
2025-04-09 00:19:30.814 INFO: Epoch 33, Train Loss: 0.1725, Val Loss: 0.1924
train_e/atom_mae: 0.001758
2025-04-09 00:19:30.815 INFO: train_e/atom_mae: 0.001758
train_e/atom_rmse: 0.002235
2025-04-09 00:19:30.815 INFO: train_e/atom_rmse: 0.002235
train_f_mae: 0.008488
2025-04-09 00:19:30.818 INFO: train_f_mae: 0.008488
train_f_rmse: 0.012902
2025-04-09 00:19:30.818 INFO: train_f_rmse: 0.012902
val_e/atom_mae: 0.000584
2025-04-09 00:19:30.821 INFO: val_e/atom_mae: 0.000584
val_e/atom_rmse: 0.000649
2025-04-09 00:19:30.821 INFO: val_e/atom_rmse: 0.000649
val_f_mae: 0.009002
2025-04-09 00:19:30.822 INFO: val_f_mae: 0.009002
val_f_rmse: 0.013853
2025-04-09 00:19:30.822 INFO: val_f_rmse: 0.013853
##### Step: 33 Learning rate: 0.005 #####
2025-04-09 00:20:56.695 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.1607, Val Loss: 0.2030
2025-04-09 00:20:56.696 INFO: Epoch 34, Train Loss: 0.1607, Val Loss: 0.2030
train_e/atom_mae: 0.001826
2025-04-09 00:20:56.697 INFO: train_e/atom_mae: 0.001826
train_e/atom_rmse: 0.002511
2025-04-09 00:20:56.697 INFO: train_e/atom_rmse: 0.002511
train_f_mae: 0.008268
2025-04-09 00:20:56.700 INFO: train_f_mae: 0.008268
train_f_rmse: 0.012374
2025-04-09 00:20:56.700 INFO: train_f_rmse: 0.012374
val_e/atom_mae: 0.005558
2025-04-09 00:20:56.703 INFO: val_e/atom_mae: 0.005558
val_e/atom_rmse: 0.005604
2025-04-09 00:20:56.703 INFO: val_e/atom_rmse: 0.005604
val_f_mae: 0.008430
2025-04-09 00:20:56.704 INFO: val_f_mae: 0.008430
val_f_rmse: 0.012845
2025-04-09 00:20:56.704 INFO: val_f_rmse: 0.012845
##### Step: 34 Learning rate: 0.005 #####
2025-04-09 00:22:22.335 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.1794, Val Loss: 0.1998
2025-04-09 00:22:22.336 INFO: Epoch 35, Train Loss: 0.1794, Val Loss: 0.1998
train_e/atom_mae: 0.002218
2025-04-09 00:22:22.337 INFO: train_e/atom_mae: 0.002218
train_e/atom_rmse: 0.002765
2025-04-09 00:22:22.337 INFO: train_e/atom_rmse: 0.002765
train_f_mae: 0.008476
2025-04-09 00:22:22.340 INFO: train_f_mae: 0.008476
train_f_rmse: 0.013043
2025-04-09 00:22:22.340 INFO: train_f_rmse: 0.013043
val_e/atom_mae: 0.004549
2025-04-09 00:22:22.343 INFO: val_e/atom_mae: 0.004549
val_e/atom_rmse: 0.004586
2025-04-09 00:22:22.343 INFO: val_e/atom_rmse: 0.004586
val_f_mae: 0.008784
2025-04-09 00:22:22.344 INFO: val_f_mae: 0.008784
val_f_rmse: 0.013204
2025-04-09 00:22:22.344 INFO: val_f_rmse: 0.013204
##### Step: 35 Learning rate: 0.005 #####
2025-04-09 00:23:48.057 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.1827, Val Loss: 0.1990
2025-04-09 00:23:48.058 INFO: Epoch 36, Train Loss: 0.1827, Val Loss: 0.1990
train_e/atom_mae: 0.002291
2025-04-09 00:23:48.058 INFO: train_e/atom_mae: 0.002291
train_e/atom_rmse: 0.002880
2025-04-09 00:23:48.059 INFO: train_e/atom_rmse: 0.002880
train_f_mae: 0.008613
2025-04-09 00:23:48.062 INFO: train_f_mae: 0.008613
train_f_rmse: 0.013138
2025-04-09 00:23:48.062 INFO: train_f_rmse: 0.013138
val_e/atom_mae: 0.000525
2025-04-09 00:23:48.065 INFO: val_e/atom_mae: 0.000525
val_e/atom_rmse: 0.000600
2025-04-09 00:23:48.065 INFO: val_e/atom_rmse: 0.000600
val_f_mae: 0.009416
2025-04-09 00:23:48.066 INFO: val_f_mae: 0.009416
val_f_rmse: 0.014090
2025-04-09 00:23:48.066 INFO: val_f_rmse: 0.014090
##### Step: 36 Learning rate: 0.005 #####
2025-04-09 00:25:13.760 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.1921, Val Loss: 0.2612
2025-04-09 00:25:13.760 INFO: Epoch 37, Train Loss: 0.1921, Val Loss: 0.2612
train_e/atom_mae: 0.002003
2025-04-09 00:25:13.761 INFO: train_e/atom_mae: 0.002003
train_e/atom_rmse: 0.002497
2025-04-09 00:25:13.762 INFO: train_e/atom_rmse: 0.002497
train_f_mae: 0.008890
2025-04-09 00:25:13.765 INFO: train_f_mae: 0.008890
train_f_rmse: 0.013585
2025-04-09 00:25:13.765 INFO: train_f_rmse: 0.013585
val_e/atom_mae: 0.004869
2025-04-09 00:25:13.768 INFO: val_e/atom_mae: 0.004869
val_e/atom_rmse: 0.004950
2025-04-09 00:25:13.768 INFO: val_e/atom_rmse: 0.004950
val_f_mae: 0.010055
2025-04-09 00:25:13.768 INFO: val_f_mae: 0.010055
val_f_rmse: 0.015216
2025-04-09 00:25:13.769 INFO: val_f_rmse: 0.015216
##### Step: 37 Learning rate: 0.005 #####
2025-04-09 00:26:39.454 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.1535, Val Loss: 0.2074
2025-04-09 00:26:39.455 INFO: Epoch 38, Train Loss: 0.1535, Val Loss: 0.2074
train_e/atom_mae: 0.001832
2025-04-09 00:26:39.455 INFO: train_e/atom_mae: 0.001832
train_e/atom_rmse: 0.002342
2025-04-09 00:26:39.456 INFO: train_e/atom_rmse: 0.002342
train_f_mae: 0.008043
2025-04-09 00:26:39.459 INFO: train_f_mae: 0.008043
train_f_rmse: 0.012120
2025-04-09 00:26:39.459 INFO: train_f_rmse: 0.012120
val_e/atom_mae: 0.001680
2025-04-09 00:26:39.462 INFO: val_e/atom_mae: 0.001680
val_e/atom_rmse: 0.001885
2025-04-09 00:26:39.462 INFO: val_e/atom_rmse: 0.001885
val_f_mae: 0.009302
2025-04-09 00:26:39.462 INFO: val_f_mae: 0.009302
val_f_rmse: 0.014250
2025-04-09 00:26:39.463 INFO: val_f_rmse: 0.014250
##### Step: 38 Learning rate: 0.005 #####
2025-04-09 00:28:05.161 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.1711, Val Loss: 0.1427
2025-04-09 00:28:05.162 INFO: Epoch 39, Train Loss: 0.1711, Val Loss: 0.1427
train_e/atom_mae: 0.002203
2025-04-09 00:28:05.163 INFO: train_e/atom_mae: 0.002203
train_e/atom_rmse: 0.002842
2025-04-09 00:28:05.163 INFO: train_e/atom_rmse: 0.002842
train_f_mae: 0.008400
2025-04-09 00:28:05.166 INFO: train_f_mae: 0.008400
train_f_rmse: 0.012703
2025-04-09 00:28:05.166 INFO: train_f_rmse: 0.012703
val_e/atom_mae: 0.003743
2025-04-09 00:28:05.169 INFO: val_e/atom_mae: 0.003743
val_e/atom_rmse: 0.003780
2025-04-09 00:28:05.169 INFO: val_e/atom_rmse: 0.003780
val_f_mae: 0.007626
2025-04-09 00:28:05.170 INFO: val_f_mae: 0.007626
val_f_rmse: 0.011197
2025-04-09 00:28:05.170 INFO: val_f_rmse: 0.011197
##### Step: 39 Learning rate: 0.005 #####
2025-04-09 00:29:30.894 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.1672, Val Loss: 0.1761
2025-04-09 00:29:30.895 INFO: Epoch 40, Train Loss: 0.1672, Val Loss: 0.1761
train_e/atom_mae: 0.002075
2025-04-09 00:29:30.896 INFO: train_e/atom_mae: 0.002075
train_e/atom_rmse: 0.002568
2025-04-09 00:29:30.896 INFO: train_e/atom_rmse: 0.002568
train_f_mae: 0.008396
2025-04-09 00:29:30.899 INFO: train_f_mae: 0.008396
train_f_rmse: 0.012620
2025-04-09 00:29:30.899 INFO: train_f_rmse: 0.012620
val_e/atom_mae: 0.001779
2025-04-09 00:29:30.902 INFO: val_e/atom_mae: 0.001779
val_e/atom_rmse: 0.001957
2025-04-09 00:29:30.902 INFO: val_e/atom_rmse: 0.001957
val_f_mae: 0.008438
2025-04-09 00:29:30.903 INFO: val_f_mae: 0.008438
val_f_rmse: 0.013093
2025-04-09 00:29:30.903 INFO: val_f_rmse: 0.013093
2025-04-09 00:29:30.917 INFO: Second train loop:
2025-04-09 00:29:30.917 INFO: training
##### Step: 40 Learning rate: 0.0025 #####
2025-04-09 00:30:56.638 INFO: ##### Step: 40 Learning rate: 0.0025 #####
Epoch 1, Train Loss: 0.1272, Val Loss: 0.1090
2025-04-09 00:30:56.639 INFO: Epoch 1, Train Loss: 0.1272, Val Loss: 0.1090
train_e/atom_mae: 0.000889
2025-04-09 00:30:56.640 INFO: train_e/atom_mae: 0.000889
train_e/atom_rmse: 0.001114
2025-04-09 00:30:56.640 INFO: train_e/atom_rmse: 0.001114
train_f_mae: 0.007174
2025-04-09 00:30:56.643 INFO: train_f_mae: 0.007174
train_f_rmse: 0.010591
2025-04-09 00:30:56.643 INFO: train_f_rmse: 0.010591
val_e/atom_mae: 0.000488
2025-04-09 00:30:56.646 INFO: val_e/atom_mae: 0.000488
val_e/atom_rmse: 0.000508
2025-04-09 00:30:56.646 INFO: val_e/atom_rmse: 0.000508
val_f_mae: 0.006994
2025-04-09 00:30:56.647 INFO: val_f_mae: 0.006994
val_f_rmse: 0.010290
2025-04-09 00:30:56.647 INFO: val_f_rmse: 0.010290
##### Step: 41 Learning rate: 0.0025 #####
2025-04-09 00:32:22.238 INFO: ##### Step: 41 Learning rate: 0.0025 #####
Epoch 2, Train Loss: 0.1080, Val Loss: 0.0975
2025-04-09 00:32:22.239 INFO: Epoch 2, Train Loss: 0.1080, Val Loss: 0.0975
train_e/atom_mae: 0.000653
2025-04-09 00:32:22.239 INFO: train_e/atom_mae: 0.000653
train_e/atom_rmse: 0.000814
2025-04-09 00:32:22.240 INFO: train_e/atom_rmse: 0.000814
train_f_mae: 0.006807
2025-04-09 00:32:22.243 INFO: train_f_mae: 0.006807
train_f_rmse: 0.010001
2025-04-09 00:32:22.243 INFO: train_f_rmse: 0.010001
val_e/atom_mae: 0.000396
2025-04-09 00:32:22.246 INFO: val_e/atom_mae: 0.000396
val_e/atom_rmse: 0.000533
2025-04-09 00:32:22.246 INFO: val_e/atom_rmse: 0.000533
val_f_mae: 0.006749
2025-04-09 00:32:22.247 INFO: val_f_mae: 0.006749
val_f_rmse: 0.009696
2025-04-09 00:32:22.247 INFO: val_f_rmse: 0.009696
##### Step: 42 Learning rate: 0.0025 #####
2025-04-09 00:33:47.985 INFO: ##### Step: 42 Learning rate: 0.0025 #####
Epoch 3, Train Loss: 0.1210, Val Loss: 0.1737
2025-04-09 00:33:47.985 INFO: Epoch 3, Train Loss: 0.1210, Val Loss: 0.1737
train_e/atom_mae: 0.000668
2025-04-09 00:33:47.986 INFO: train_e/atom_mae: 0.000668
train_e/atom_rmse: 0.000835
2025-04-09 00:33:47.986 INFO: train_e/atom_rmse: 0.000835
train_f_mae: 0.007150
2025-04-09 00:33:47.990 INFO: train_f_mae: 0.007150
train_f_rmse: 0.010611
2025-04-09 00:33:47.990 INFO: train_f_rmse: 0.010611
val_e/atom_mae: 0.000983
2025-04-09 00:33:47.993 INFO: val_e/atom_mae: 0.000983
val_e/atom_rmse: 0.001034
2025-04-09 00:33:47.993 INFO: val_e/atom_rmse: 0.001034
val_f_mae: 0.008428
2025-04-09 00:33:47.993 INFO: val_f_mae: 0.008428
val_f_rmse: 0.012678
2025-04-09 00:33:47.994 INFO: val_f_rmse: 0.012678
##### Step: 43 Learning rate: 0.0025 #####
2025-04-09 00:35:13.630 INFO: ##### Step: 43 Learning rate: 0.0025 #####
Epoch 4, Train Loss: 0.1258, Val Loss: 0.1154
2025-04-09 00:35:13.631 INFO: Epoch 4, Train Loss: 0.1258, Val Loss: 0.1154
train_e/atom_mae: 0.000677
2025-04-09 00:35:13.631 INFO: train_e/atom_mae: 0.000677
train_e/atom_rmse: 0.000838
2025-04-09 00:35:13.632 INFO: train_e/atom_rmse: 0.000838
train_f_mae: 0.007323
2025-04-09 00:35:13.635 INFO: train_f_mae: 0.007323
train_f_rmse: 0.010828
2025-04-09 00:35:13.635 INFO: train_f_rmse: 0.010828
val_e/atom_mae: 0.000393
2025-04-09 00:35:13.638 INFO: val_e/atom_mae: 0.000393
val_e/atom_rmse: 0.000446
2025-04-09 00:35:13.638 INFO: val_e/atom_rmse: 0.000446
val_f_mae: 0.007436
2025-04-09 00:35:13.639 INFO: val_f_mae: 0.007436
val_f_rmse: 0.010629
2025-04-09 00:35:13.639 INFO: val_f_rmse: 0.010629
##### Step: 44 Learning rate: 0.0025 #####
2025-04-09 00:36:39.373 INFO: ##### Step: 44 Learning rate: 0.0025 #####
Epoch 5, Train Loss: 0.1151, Val Loss: 0.1197
2025-04-09 00:36:39.373 INFO: Epoch 5, Train Loss: 0.1151, Val Loss: 0.1197
train_e/atom_mae: 0.000721
2025-04-09 00:36:39.374 INFO: train_e/atom_mae: 0.000721
train_e/atom_rmse: 0.000890
2025-04-09 00:36:39.374 INFO: train_e/atom_rmse: 0.000890
train_f_mae: 0.006974
2025-04-09 00:36:39.377 INFO: train_f_mae: 0.006974
train_f_rmse: 0.010270
2025-04-09 00:36:39.378 INFO: train_f_rmse: 0.010270
val_e/atom_mae: 0.000897
2025-04-09 00:36:39.380 INFO: val_e/atom_mae: 0.000897
val_e/atom_rmse: 0.001041
2025-04-09 00:36:39.381 INFO: val_e/atom_rmse: 0.001041
val_f_mae: 0.007081
2025-04-09 00:36:39.381 INFO: val_f_mae: 0.007081
val_f_rmse: 0.010324
2025-04-09 00:36:39.381 INFO: val_f_rmse: 0.010324
##### Step: 45 Learning rate: 0.0025 #####
2025-04-09 00:38:05.094 INFO: ##### Step: 45 Learning rate: 0.0025 #####
Epoch 6, Train Loss: 0.1097, Val Loss: 0.1128
2025-04-09 00:38:05.095 INFO: Epoch 6, Train Loss: 0.1097, Val Loss: 0.1128
train_e/atom_mae: 0.000728
2025-04-09 00:38:05.096 INFO: train_e/atom_mae: 0.000728
train_e/atom_rmse: 0.000911
2025-04-09 00:38:05.096 INFO: train_e/atom_rmse: 0.000911
train_f_mae: 0.006843
2025-04-09 00:38:05.099 INFO: train_f_mae: 0.006843
train_f_rmse: 0.009981
2025-04-09 00:38:05.099 INFO: train_f_rmse: 0.009981
val_e/atom_mae: 0.000277
2025-04-09 00:38:05.102 INFO: val_e/atom_mae: 0.000277
val_e/atom_rmse: 0.000354
2025-04-09 00:38:05.102 INFO: val_e/atom_rmse: 0.000354
val_f_mae: 0.007240
2025-04-09 00:38:05.103 INFO: val_f_mae: 0.007240
val_f_rmse: 0.010548
2025-04-09 00:38:05.103 INFO: val_f_rmse: 0.010548
##### Step: 46 Learning rate: 0.0025 #####
2025-04-09 00:39:30.802 INFO: ##### Step: 46 Learning rate: 0.0025 #####
Epoch 7, Train Loss: 0.1231, Val Loss: 0.1076
2025-04-09 00:39:30.803 INFO: Epoch 7, Train Loss: 0.1231, Val Loss: 0.1076
train_e/atom_mae: 0.000731
2025-04-09 00:39:30.804 INFO: train_e/atom_mae: 0.000731
train_e/atom_rmse: 0.000916
2025-04-09 00:39:30.804 INFO: train_e/atom_rmse: 0.000916
train_f_mae: 0.007139
2025-04-09 00:39:30.807 INFO: train_f_mae: 0.007139
train_f_rmse: 0.010630
2025-04-09 00:39:30.807 INFO: train_f_rmse: 0.010630
val_e/atom_mae: 0.000773
2025-04-09 00:39:30.810 INFO: val_e/atom_mae: 0.000773
val_e/atom_rmse: 0.000953
2025-04-09 00:39:30.811 INFO: val_e/atom_rmse: 0.000953
val_f_mae: 0.006802
2025-04-09 00:39:30.811 INFO: val_f_mae: 0.006802
val_f_rmse: 0.009827
2025-04-09 00:39:30.811 INFO: val_f_rmse: 0.009827
##### Step: 47 Learning rate: 0.0025 #####
2025-04-09 00:40:56.538 INFO: ##### Step: 47 Learning rate: 0.0025 #####
Epoch 8, Train Loss: 0.1161, Val Loss: 0.1014
2025-04-09 00:40:56.539 INFO: Epoch 8, Train Loss: 0.1161, Val Loss: 0.1014
train_e/atom_mae: 0.000609
2025-04-09 00:40:56.540 INFO: train_e/atom_mae: 0.000609
train_e/atom_rmse: 0.000754
2025-04-09 00:40:56.540 INFO: train_e/atom_rmse: 0.000754
train_f_mae: 0.007050
2025-04-09 00:40:56.544 INFO: train_f_mae: 0.007050
train_f_rmse: 0.010453
2025-04-09 00:40:56.544 INFO: train_f_rmse: 0.010453
val_e/atom_mae: 0.000439
2025-04-09 00:40:56.546 INFO: val_e/atom_mae: 0.000439
val_e/atom_rmse: 0.000579
2025-04-09 00:40:56.547 INFO: val_e/atom_rmse: 0.000579
val_f_mae: 0.006900
2025-04-09 00:40:56.547 INFO: val_f_mae: 0.006900
val_f_rmse: 0.009868
2025-04-09 00:40:56.547 INFO: val_f_rmse: 0.009868
##### Step: 48 Learning rate: 0.0025 #####
2025-04-09 00:42:22.271 INFO: ##### Step: 48 Learning rate: 0.0025 #####
Epoch 9, Train Loss: 0.1237, Val Loss: 0.1114
2025-04-09 00:42:22.271 INFO: Epoch 9, Train Loss: 0.1237, Val Loss: 0.1114
train_e/atom_mae: 0.000741
2025-04-09 00:42:22.272 INFO: train_e/atom_mae: 0.000741
train_e/atom_rmse: 0.000958
2025-04-09 00:42:22.272 INFO: train_e/atom_rmse: 0.000958
train_f_mae: 0.007160
2025-04-09 00:42:22.276 INFO: train_f_mae: 0.007160
train_f_rmse: 0.010611
2025-04-09 00:42:22.276 INFO: train_f_rmse: 0.010611
val_e/atom_mae: 0.001024
2025-04-09 00:42:22.279 INFO: val_e/atom_mae: 0.001024
val_e/atom_rmse: 0.001077
2025-04-09 00:42:22.279 INFO: val_e/atom_rmse: 0.001077
val_f_mae: 0.006823
2025-04-09 00:42:22.279 INFO: val_f_mae: 0.006823
val_f_rmse: 0.009865
2025-04-09 00:42:22.279 INFO: val_f_rmse: 0.009865
##### Step: 49 Learning rate: 0.0025 #####
2025-04-09 00:43:47.984 INFO: ##### Step: 49 Learning rate: 0.0025 #####
Epoch 10, Train Loss: 0.1276, Val Loss: 0.1310
2025-04-09 00:43:47.985 INFO: Epoch 10, Train Loss: 0.1276, Val Loss: 0.1310
train_e/atom_mae: 0.001097
2025-04-09 00:43:47.986 INFO: train_e/atom_mae: 0.001097
train_e/atom_rmse: 0.001317
2025-04-09 00:43:47.986 INFO: train_e/atom_rmse: 0.001317
train_f_mae: 0.006920
2025-04-09 00:43:47.990 INFO: train_f_mae: 0.006920
train_f_rmse: 0.010327
2025-04-09 00:43:47.990 INFO: train_f_rmse: 0.010327
val_e/atom_mae: 0.000406
2025-04-09 00:43:47.992 INFO: val_e/atom_mae: 0.000406
val_e/atom_rmse: 0.000445
2025-04-09 00:43:47.993 INFO: val_e/atom_rmse: 0.000445
val_f_mae: 0.007626
2025-04-09 00:43:47.993 INFO: val_f_mae: 0.007626
val_f_rmse: 0.011339
2025-04-09 00:43:47.993 INFO: val_f_rmse: 0.011339
##### Step: 50 Learning rate: 0.0025 #####
2025-04-09 00:45:13.734 INFO: ##### Step: 50 Learning rate: 0.0025 #####
Epoch 11, Train Loss: 0.1152, Val Loss: 0.1423
2025-04-09 00:45:13.735 INFO: Epoch 11, Train Loss: 0.1152, Val Loss: 0.1423
train_e/atom_mae: 0.000690
2025-04-09 00:45:13.736 INFO: train_e/atom_mae: 0.000690
train_e/atom_rmse: 0.000862
2025-04-09 00:45:13.736 INFO: train_e/atom_rmse: 0.000862
train_f_mae: 0.007024
2025-04-09 00:45:13.740 INFO: train_f_mae: 0.007024
train_f_rmse: 0.010306
2025-04-09 00:45:13.740 INFO: train_f_rmse: 0.010306
val_e/atom_mae: 0.000754
2025-04-09 00:45:13.742 INFO: val_e/atom_mae: 0.000754
val_e/atom_rmse: 0.000850
2025-04-09 00:45:13.743 INFO: val_e/atom_rmse: 0.000850
val_f_mae: 0.007741
2025-04-09 00:45:13.743 INFO: val_f_mae: 0.007741
val_f_rmse: 0.011555
2025-04-09 00:45:13.743 INFO: val_f_rmse: 0.011555
##### Step: 51 Learning rate: 0.0025 #####
2025-04-09 00:46:39.404 INFO: ##### Step: 51 Learning rate: 0.0025 #####
Epoch 12, Train Loss: 0.1220, Val Loss: 0.1228
2025-04-09 00:46:39.405 INFO: Epoch 12, Train Loss: 0.1220, Val Loss: 0.1228
train_e/atom_mae: 0.000575
2025-04-09 00:46:39.406 INFO: train_e/atom_mae: 0.000575
train_e/atom_rmse: 0.000721
2025-04-09 00:46:39.406 INFO: train_e/atom_rmse: 0.000721
train_f_mae: 0.007218
2025-04-09 00:46:39.409 INFO: train_f_mae: 0.007218
train_f_rmse: 0.010758
2025-04-09 00:46:39.410 INFO: train_f_rmse: 0.010758
val_e/atom_mae: 0.000318
2025-04-09 00:46:39.412 INFO: val_e/atom_mae: 0.000318
val_e/atom_rmse: 0.000416
2025-04-09 00:46:39.412 INFO: val_e/atom_rmse: 0.000416
val_f_mae: 0.007302
2025-04-09 00:46:39.413 INFO: val_f_mae: 0.007302
val_f_rmse: 0.010987
2025-04-09 00:46:39.413 INFO: val_f_rmse: 0.010987
##### Step: 52 Learning rate: 0.0025 #####
2025-04-09 00:48:05.127 INFO: ##### Step: 52 Learning rate: 0.0025 #####
Epoch 13, Train Loss: 0.1112, Val Loss: 0.0987
2025-04-09 00:48:05.128 INFO: Epoch 13, Train Loss: 0.1112, Val Loss: 0.0987
train_e/atom_mae: 0.000686
2025-04-09 00:48:05.129 INFO: train_e/atom_mae: 0.000686
train_e/atom_rmse: 0.000844
2025-04-09 00:48:05.129 INFO: train_e/atom_rmse: 0.000844
train_f_mae: 0.006890
2025-04-09 00:48:05.132 INFO: train_f_mae: 0.006890
train_f_rmse: 0.010126
2025-04-09 00:48:05.132 INFO: train_f_rmse: 0.010126
val_e/atom_mae: 0.000328
2025-04-09 00:48:05.135 INFO: val_e/atom_mae: 0.000328
val_e/atom_rmse: 0.000395
2025-04-09 00:48:05.135 INFO: val_e/atom_rmse: 0.000395
val_f_mae: 0.006913
2025-04-09 00:48:05.136 INFO: val_f_mae: 0.006913
val_f_rmse: 0.009838
2025-04-09 00:48:05.136 INFO: val_f_rmse: 0.009838
##### Step: 53 Learning rate: 0.0025 #####
2025-04-09 00:49:30.694 INFO: ##### Step: 53 Learning rate: 0.0025 #####
Epoch 14, Train Loss: 0.1166, Val Loss: 0.1081
2025-04-09 00:49:30.695 INFO: Epoch 14, Train Loss: 0.1166, Val Loss: 0.1081
train_e/atom_mae: 0.000786
2025-04-09 00:49:30.696 INFO: train_e/atom_mae: 0.000786
train_e/atom_rmse: 0.000995
2025-04-09 00:49:30.696 INFO: train_e/atom_rmse: 0.000995
train_f_mae: 0.006875
2025-04-09 00:49:30.699 INFO: train_f_mae: 0.006875
train_f_rmse: 0.010226
2025-04-09 00:49:30.699 INFO: train_f_rmse: 0.010226
val_e/atom_mae: 0.000841
2025-04-09 00:49:30.702 INFO: val_e/atom_mae: 0.000841
val_e/atom_rmse: 0.000889
2025-04-09 00:49:30.702 INFO: val_e/atom_rmse: 0.000889
val_f_mae: 0.006741
2025-04-09 00:49:30.703 INFO: val_f_mae: 0.006741
val_f_rmse: 0.009926
2025-04-09 00:49:30.703 INFO: val_f_rmse: 0.009926
##### Step: 54 Learning rate: 0.0025 #####
2025-04-09 00:50:56.407 INFO: ##### Step: 54 Learning rate: 0.0025 #####
Epoch 15, Train Loss: 0.1112, Val Loss: 0.1217
2025-04-09 00:50:56.408 INFO: Epoch 15, Train Loss: 0.1112, Val Loss: 0.1217
train_e/atom_mae: 0.000683
2025-04-09 00:50:56.408 INFO: train_e/atom_mae: 0.000683
train_e/atom_rmse: 0.000850
2025-04-09 00:50:56.409 INFO: train_e/atom_rmse: 0.000850
train_f_mae: 0.006830
2025-04-09 00:50:56.412 INFO: train_f_mae: 0.006830
train_f_rmse: 0.010124
2025-04-09 00:50:56.412 INFO: train_f_rmse: 0.010124
val_e/atom_mae: 0.000545
2025-04-09 00:50:56.415 INFO: val_e/atom_mae: 0.000545
val_e/atom_rmse: 0.000594
2025-04-09 00:50:56.415 INFO: val_e/atom_rmse: 0.000594
val_f_mae: 0.007405
2025-04-09 00:50:56.415 INFO: val_f_mae: 0.007405
val_f_rmse: 0.010838
2025-04-09 00:50:56.415 INFO: val_f_rmse: 0.010838
##### Step: 55 Learning rate: 0.0025 #####
2025-04-09 00:52:21.988 INFO: ##### Step: 55 Learning rate: 0.0025 #####
Epoch 16, Train Loss: 0.1131, Val Loss: 0.1166
2025-04-09 00:52:21.989 INFO: Epoch 16, Train Loss: 0.1131, Val Loss: 0.1166
train_e/atom_mae: 0.000587
2025-04-09 00:52:21.990 INFO: train_e/atom_mae: 0.000587
train_e/atom_rmse: 0.000726
2025-04-09 00:52:21.990 INFO: train_e/atom_rmse: 0.000726
train_f_mae: 0.007001
2025-04-09 00:52:21.994 INFO: train_f_mae: 0.007001
train_f_rmse: 0.010331
2025-04-09 00:52:21.994 INFO: train_f_rmse: 0.010331
val_e/atom_mae: 0.000757
2025-04-09 00:52:21.996 INFO: val_e/atom_mae: 0.000757
val_e/atom_rmse: 0.000909
2025-04-09 00:52:21.997 INFO: val_e/atom_rmse: 0.000909
val_f_mae: 0.007229
2025-04-09 00:52:21.997 INFO: val_f_mae: 0.007229
val_f_rmse: 0.010323
2025-04-09 00:52:21.997 INFO: val_f_rmse: 0.010323
##### Step: 56 Learning rate: 0.0025 #####
2025-04-09 00:53:47.689 INFO: ##### Step: 56 Learning rate: 0.0025 #####
Epoch 17, Train Loss: 0.1109, Val Loss: 0.1059
2025-04-09 00:53:47.689 INFO: Epoch 17, Train Loss: 0.1109, Val Loss: 0.1059
train_e/atom_mae: 0.000540
2025-04-09 00:53:47.690 INFO: train_e/atom_mae: 0.000540
train_e/atom_rmse: 0.000684
2025-04-09 00:53:47.690 INFO: train_e/atom_rmse: 0.000684
train_f_mae: 0.006903
2025-04-09 00:53:47.694 INFO: train_f_mae: 0.006903
train_f_rmse: 0.010257
2025-04-09 00:53:47.694 INFO: train_f_rmse: 0.010257
val_e/atom_mae: 0.001254
2025-04-09 00:53:47.696 INFO: val_e/atom_mae: 0.001254
val_e/atom_rmse: 0.001298
2025-04-09 00:53:47.697 INFO: val_e/atom_rmse: 0.001298
val_f_mae: 0.006447
2025-04-09 00:53:47.697 INFO: val_f_mae: 0.006447
val_f_rmse: 0.009247
2025-04-09 00:53:47.697 INFO: val_f_rmse: 0.009247
##### Step: 57 Learning rate: 0.0025 #####
2025-04-09 00:55:13.399 INFO: ##### Step: 57 Learning rate: 0.0025 #####
Epoch 18, Train Loss: 0.1105, Val Loss: 0.1083
2025-04-09 00:55:13.400 INFO: Epoch 18, Train Loss: 0.1105, Val Loss: 0.1083
train_e/atom_mae: 0.000931
2025-04-09 00:55:13.401 INFO: train_e/atom_mae: 0.000931
train_e/atom_rmse: 0.001182
2025-04-09 00:55:13.401 INFO: train_e/atom_rmse: 0.001182
train_f_mae: 0.006619
2025-04-09 00:55:13.405 INFO: train_f_mae: 0.006619
train_f_rmse: 0.009674
2025-04-09 00:55:13.405 INFO: train_f_rmse: 0.009674
val_e/atom_mae: 0.000903
2025-04-09 00:55:13.407 INFO: val_e/atom_mae: 0.000903
val_e/atom_rmse: 0.000973
2025-04-09 00:55:13.408 INFO: val_e/atom_rmse: 0.000973
val_f_mae: 0.006598
2025-04-09 00:55:13.408 INFO: val_f_mae: 0.006598
val_f_rmse: 0.009843
2025-04-09 00:55:13.408 INFO: val_f_rmse: 0.009843
##### Step: 58 Learning rate: 0.0025 #####
2025-04-09 00:56:39.069 INFO: ##### Step: 58 Learning rate: 0.0025 #####
Epoch 19, Train Loss: 0.1248, Val Loss: 0.1018
2025-04-09 00:56:39.070 INFO: Epoch 19, Train Loss: 0.1248, Val Loss: 0.1018
train_e/atom_mae: 0.000867
2025-04-09 00:56:39.071 INFO: train_e/atom_mae: 0.000867
train_e/atom_rmse: 0.001072
2025-04-09 00:56:39.071 INFO: train_e/atom_rmse: 0.001072
train_f_mae: 0.007044
2025-04-09 00:56:39.075 INFO: train_f_mae: 0.007044
train_f_rmse: 0.010530
2025-04-09 00:56:39.075 INFO: train_f_rmse: 0.010530
val_e/atom_mae: 0.000303
2025-04-09 00:56:39.077 INFO: val_e/atom_mae: 0.000303
val_e/atom_rmse: 0.000355
2025-04-09 00:56:39.078 INFO: val_e/atom_rmse: 0.000355
val_f_mae: 0.006831
2025-04-09 00:56:39.078 INFO: val_f_mae: 0.006831
val_f_rmse: 0.010016
2025-04-09 00:56:39.078 INFO: val_f_rmse: 0.010016
##### Step: 59 Learning rate: 0.0025 #####
2025-04-09 00:58:04.805 INFO: ##### Step: 59 Learning rate: 0.0025 #####
Epoch 20, Train Loss: 0.1106, Val Loss: 0.1231
2025-04-09 00:58:04.806 INFO: Epoch 20, Train Loss: 0.1106, Val Loss: 0.1231
train_e/atom_mae: 0.000584
2025-04-09 00:58:04.807 INFO: train_e/atom_mae: 0.000584
train_e/atom_rmse: 0.000723
2025-04-09 00:58:04.807 INFO: train_e/atom_rmse: 0.000723
train_f_mae: 0.006966
2025-04-09 00:58:04.811 INFO: train_f_mae: 0.006966
train_f_rmse: 0.010213
2025-04-09 00:58:04.811 INFO: train_f_rmse: 0.010213
val_e/atom_mae: 0.000517
2025-04-09 00:58:04.813 INFO: val_e/atom_mae: 0.000517
val_e/atom_rmse: 0.000569
2025-04-09 00:58:04.814 INFO: val_e/atom_rmse: 0.000569
val_f_mae: 0.007613
2025-04-09 00:58:04.814 INFO: val_f_mae: 0.007613
val_f_rmse: 0.010917
2025-04-09 00:58:04.814 INFO: val_f_rmse: 0.010917
##### Step: 60 Learning rate: 0.00125 #####
2025-04-09 00:59:30.513 INFO: ##### Step: 60 Learning rate: 0.00125 #####
Epoch 21, Train Loss: 0.0887, Val Loss: 0.0846
2025-04-09 00:59:30.513 INFO: Epoch 21, Train Loss: 0.0887, Val Loss: 0.0846
train_e/atom_mae: 0.000418
2025-04-09 00:59:30.514 INFO: train_e/atom_mae: 0.000418
train_e/atom_rmse: 0.000529
2025-04-09 00:59:30.515 INFO: train_e/atom_rmse: 0.000529
train_f_mae: 0.006353
2025-04-09 00:59:30.518 INFO: train_f_mae: 0.006353
train_f_rmse: 0.009237
2025-04-09 00:59:30.518 INFO: train_f_rmse: 0.009237
val_e/atom_mae: 0.000669
2025-04-09 00:59:30.521 INFO: val_e/atom_mae: 0.000669
val_e/atom_rmse: 0.000715
2025-04-09 00:59:30.521 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.006165
2025-04-09 00:59:30.522 INFO: val_f_mae: 0.006165
val_f_rmse: 0.008857
2025-04-09 00:59:30.522 INFO: val_f_rmse: 0.008857
##### Step: 61 Learning rate: 0.00125 #####
2025-04-09 01:00:56.271 INFO: ##### Step: 61 Learning rate: 0.00125 #####
Epoch 22, Train Loss: 0.0846, Val Loss: 0.0858
2025-04-09 01:00:56.272 INFO: Epoch 22, Train Loss: 0.0846, Val Loss: 0.0858
train_e/atom_mae: 0.000418
2025-04-09 01:00:56.273 INFO: train_e/atom_mae: 0.000418
train_e/atom_rmse: 0.000520
2025-04-09 01:00:56.273 INFO: train_e/atom_rmse: 0.000520
train_f_mae: 0.006223
2025-04-09 01:00:56.276 INFO: train_f_mae: 0.006223
train_f_rmse: 0.009019
2025-04-09 01:00:56.276 INFO: train_f_rmse: 0.009019
val_e/atom_mae: 0.000448
2025-04-09 01:00:56.279 INFO: val_e/atom_mae: 0.000448
val_e/atom_rmse: 0.000530
2025-04-09 01:00:56.279 INFO: val_e/atom_rmse: 0.000530
val_f_mae: 0.006217
2025-04-09 01:00:56.280 INFO: val_f_mae: 0.006217
val_f_rmse: 0.009078
2025-04-09 01:00:56.281 INFO: val_f_rmse: 0.009078
##### Step: 62 Learning rate: 0.00125 #####
2025-04-09 01:02:21.998 INFO: ##### Step: 62 Learning rate: 0.00125 #####
Epoch 23, Train Loss: 0.0842, Val Loss: 0.1014
2025-04-09 01:02:21.998 INFO: Epoch 23, Train Loss: 0.0842, Val Loss: 0.1014
train_e/atom_mae: 0.000407
2025-04-09 01:02:21.999 INFO: train_e/atom_mae: 0.000407
train_e/atom_rmse: 0.000504
2025-04-09 01:02:21.999 INFO: train_e/atom_rmse: 0.000504
train_f_mae: 0.006195
2025-04-09 01:02:22.003 INFO: train_f_mae: 0.006195
train_f_rmse: 0.009007
2025-04-09 01:02:22.003 INFO: train_f_rmse: 0.009007
val_e/atom_mae: 0.000492
2025-04-09 01:02:22.006 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000592
2025-04-09 01:02:22.006 INFO: val_e/atom_rmse: 0.000592
val_f_mae: 0.006658
2025-04-09 01:02:22.007 INFO: val_f_mae: 0.006658
val_f_rmse: 0.009856
2025-04-09 01:02:22.007 INFO: val_f_rmse: 0.009856
##### Step: 63 Learning rate: 0.00125 #####
2025-04-09 01:03:47.723 INFO: ##### Step: 63 Learning rate: 0.00125 #####
Epoch 24, Train Loss: 0.0894, Val Loss: 0.0940
2025-04-09 01:03:47.724 INFO: Epoch 24, Train Loss: 0.0894, Val Loss: 0.0940
train_e/atom_mae: 0.000572
2025-04-09 01:03:47.724 INFO: train_e/atom_mae: 0.000572
train_e/atom_rmse: 0.000714
2025-04-09 01:03:47.725 INFO: train_e/atom_rmse: 0.000714
train_f_mae: 0.006289
2025-04-09 01:03:47.728 INFO: train_f_mae: 0.006289
train_f_rmse: 0.009124
2025-04-09 01:03:47.728 INFO: train_f_rmse: 0.009124
val_e/atom_mae: 0.000364
2025-04-09 01:03:47.731 INFO: val_e/atom_mae: 0.000364
val_e/atom_rmse: 0.000456
2025-04-09 01:03:47.731 INFO: val_e/atom_rmse: 0.000456
val_f_mae: 0.006468
2025-04-09 01:03:47.731 INFO: val_f_mae: 0.006468
val_f_rmse: 0.009565
2025-04-09 01:03:47.732 INFO: val_f_rmse: 0.009565
##### Step: 64 Learning rate: 0.00125 #####
2025-04-09 01:05:13.491 INFO: ##### Step: 64 Learning rate: 0.00125 #####
Epoch 25, Train Loss: 0.0838, Val Loss: 0.1361
2025-04-09 01:05:13.492 INFO: Epoch 25, Train Loss: 0.0838, Val Loss: 0.1361
train_e/atom_mae: 0.000437
2025-04-09 01:05:13.493 INFO: train_e/atom_mae: 0.000437
train_e/atom_rmse: 0.000550
2025-04-09 01:05:13.493 INFO: train_e/atom_rmse: 0.000550
train_f_mae: 0.006192
2025-04-09 01:05:13.496 INFO: train_f_mae: 0.006192
train_f_rmse: 0.008951
2025-04-09 01:05:13.496 INFO: train_f_rmse: 0.008951
val_e/atom_mae: 0.001140
2025-04-09 01:05:13.499 INFO: val_e/atom_mae: 0.001140
val_e/atom_rmse: 0.001174
2025-04-09 01:05:13.499 INFO: val_e/atom_rmse: 0.001174
val_f_mae: 0.007462
2025-04-09 01:05:13.500 INFO: val_f_mae: 0.007462
val_f_rmse: 0.010930
2025-04-09 01:05:13.500 INFO: val_f_rmse: 0.010930
##### Step: 65 Learning rate: 0.00125 #####
2025-04-09 01:06:39.218 INFO: ##### Step: 65 Learning rate: 0.00125 #####
Epoch 26, Train Loss: 0.0899, Val Loss: 0.0940
2025-04-09 01:06:39.219 INFO: Epoch 26, Train Loss: 0.0899, Val Loss: 0.0940
train_e/atom_mae: 0.000520
2025-04-09 01:06:39.220 INFO: train_e/atom_mae: 0.000520
train_e/atom_rmse: 0.000657
2025-04-09 01:06:39.220 INFO: train_e/atom_rmse: 0.000657
train_f_mae: 0.006293
2025-04-09 01:06:39.223 INFO: train_f_mae: 0.006293
train_f_rmse: 0.009201
2025-04-09 01:06:39.223 INFO: train_f_rmse: 0.009201
val_e/atom_mae: 0.000354
2025-04-09 01:06:39.226 INFO: val_e/atom_mae: 0.000354
val_e/atom_rmse: 0.000417
2025-04-09 01:06:39.226 INFO: val_e/atom_rmse: 0.000417
val_f_mae: 0.006375
2025-04-09 01:06:39.227 INFO: val_f_mae: 0.006375
val_f_rmse: 0.009589
2025-04-09 01:06:39.227 INFO: val_f_rmse: 0.009589
##### Step: 66 Learning rate: 0.00125 #####
2025-04-09 01:08:04.945 INFO: ##### Step: 66 Learning rate: 0.00125 #####
Epoch 27, Train Loss: 0.0778, Val Loss: 0.0950
2025-04-09 01:08:04.946 INFO: Epoch 27, Train Loss: 0.0778, Val Loss: 0.0950
train_e/atom_mae: 0.000352
2025-04-09 01:08:04.947 INFO: train_e/atom_mae: 0.000352
train_e/atom_rmse: 0.000445
2025-04-09 01:08:04.947 INFO: train_e/atom_rmse: 0.000445
train_f_mae: 0.006039
2025-04-09 01:08:04.950 INFO: train_f_mae: 0.006039
train_f_rmse: 0.008686
2025-04-09 01:08:04.950 INFO: train_f_rmse: 0.008686
val_e/atom_mae: 0.000426
2025-04-09 01:08:04.953 INFO: val_e/atom_mae: 0.000426
val_e/atom_rmse: 0.000472
2025-04-09 01:08:04.953 INFO: val_e/atom_rmse: 0.000472
val_f_mae: 0.006667
2025-04-09 01:08:04.954 INFO: val_f_mae: 0.006667
val_f_rmse: 0.009608
2025-04-09 01:08:04.954 INFO: val_f_rmse: 0.009608
##### Step: 67 Learning rate: 0.00125 #####
2025-04-09 01:09:30.578 INFO: ##### Step: 67 Learning rate: 0.00125 #####
Epoch 28, Train Loss: 0.0814, Val Loss: 0.0812
2025-04-09 01:09:30.579 INFO: Epoch 28, Train Loss: 0.0814, Val Loss: 0.0812
train_e/atom_mae: 0.000343
2025-04-09 01:09:30.579 INFO: train_e/atom_mae: 0.000343
train_e/atom_rmse: 0.000430
2025-04-09 01:09:30.580 INFO: train_e/atom_rmse: 0.000430
train_f_mae: 0.006142
2025-04-09 01:09:30.583 INFO: train_f_mae: 0.006142
train_f_rmse: 0.008899
2025-04-09 01:09:30.583 INFO: train_f_rmse: 0.008899
val_e/atom_mae: 0.000528
2025-04-09 01:09:30.586 INFO: val_e/atom_mae: 0.000528
val_e/atom_rmse: 0.000593
2025-04-09 01:09:30.586 INFO: val_e/atom_rmse: 0.000593
val_f_mae: 0.006126
2025-04-09 01:09:30.587 INFO: val_f_mae: 0.006126
val_f_rmse: 0.008774
2025-04-09 01:09:30.587 INFO: val_f_rmse: 0.008774
##### Step: 68 Learning rate: 0.00125 #####
2025-04-09 01:10:56.342 INFO: ##### Step: 68 Learning rate: 0.00125 #####
Epoch 29, Train Loss: 0.0825, Val Loss: 0.0844
2025-04-09 01:10:56.342 INFO: Epoch 29, Train Loss: 0.0825, Val Loss: 0.0844
train_e/atom_mae: 0.000355
2025-04-09 01:10:56.343 INFO: train_e/atom_mae: 0.000355
train_e/atom_rmse: 0.000445
2025-04-09 01:10:56.343 INFO: train_e/atom_rmse: 0.000445
train_f_mae: 0.006177
2025-04-09 01:10:56.347 INFO: train_f_mae: 0.006177
train_f_rmse: 0.008949
2025-04-09 01:10:56.347 INFO: train_f_rmse: 0.008949
val_e/atom_mae: 0.000501
2025-04-09 01:10:56.350 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000540
2025-04-09 01:10:56.350 INFO: val_e/atom_rmse: 0.000540
val_f_mae: 0.006297
2025-04-09 01:10:56.351 INFO: val_f_mae: 0.006297
val_f_rmse: 0.008995
2025-04-09 01:10:56.351 INFO: val_f_rmse: 0.008995
##### Step: 69 Learning rate: 0.00125 #####
2025-04-09 01:12:22.010 INFO: ##### Step: 69 Learning rate: 0.00125 #####
Epoch 30, Train Loss: 0.0831, Val Loss: 0.0847
2025-04-09 01:12:22.011 INFO: Epoch 30, Train Loss: 0.0831, Val Loss: 0.0847
train_e/atom_mae: 0.000418
2025-04-09 01:12:22.012 INFO: train_e/atom_mae: 0.000418
train_e/atom_rmse: 0.000539
2025-04-09 01:12:22.012 INFO: train_e/atom_rmse: 0.000539
train_f_mae: 0.006164
2025-04-09 01:12:22.015 INFO: train_f_mae: 0.006164
train_f_rmse: 0.008919
2025-04-09 01:12:22.016 INFO: train_f_rmse: 0.008919
val_e/atom_mae: 0.000227
2025-04-09 01:12:22.018 INFO: val_e/atom_mae: 0.000227
val_e/atom_rmse: 0.000279
2025-04-09 01:12:22.019 INFO: val_e/atom_rmse: 0.000279
val_f_mae: 0.006268
2025-04-09 01:12:22.019 INFO: val_f_mae: 0.006268
val_f_rmse: 0.009154
2025-04-09 01:12:22.019 INFO: val_f_rmse: 0.009154
##### Step: 70 Learning rate: 0.00125 #####
2025-04-09 01:13:47.758 INFO: ##### Step: 70 Learning rate: 0.00125 #####
Epoch 31, Train Loss: 0.0790, Val Loss: 0.0921
2025-04-09 01:13:47.758 INFO: Epoch 31, Train Loss: 0.0790, Val Loss: 0.0921
train_e/atom_mae: 0.000457
2025-04-09 01:13:47.759 INFO: train_e/atom_mae: 0.000457
train_e/atom_rmse: 0.000581
2025-04-09 01:13:47.759 INFO: train_e/atom_rmse: 0.000581
train_f_mae: 0.006028
2025-04-09 01:13:47.763 INFO: train_f_mae: 0.006028
train_f_rmse: 0.008655
2025-04-09 01:13:47.763 INFO: train_f_rmse: 0.008655
val_e/atom_mae: 0.000485
2025-04-09 01:13:47.766 INFO: val_e/atom_mae: 0.000485
val_e/atom_rmse: 0.000533
2025-04-09 01:13:47.766 INFO: val_e/atom_rmse: 0.000533
val_f_mae: 0.006400
2025-04-09 01:13:47.767 INFO: val_f_mae: 0.006400
val_f_rmse: 0.009415
2025-04-09 01:13:47.767 INFO: val_f_rmse: 0.009415
##### Step: 71 Learning rate: 0.00125 #####
2025-04-09 01:15:13.521 INFO: ##### Step: 71 Learning rate: 0.00125 #####
Epoch 32, Train Loss: 0.0802, Val Loss: 0.0708
2025-04-09 01:15:13.522 INFO: Epoch 32, Train Loss: 0.0802, Val Loss: 0.0708
train_e/atom_mae: 0.000466
2025-04-09 01:15:13.523 INFO: train_e/atom_mae: 0.000466
train_e/atom_rmse: 0.000593
2025-04-09 01:15:13.523 INFO: train_e/atom_rmse: 0.000593
train_f_mae: 0.006029
2025-04-09 01:15:13.526 INFO: train_f_mae: 0.006029
train_f_rmse: 0.008715
2025-04-09 01:15:13.527 INFO: train_f_rmse: 0.008715
val_e/atom_mae: 0.000230
2025-04-09 01:15:13.529 INFO: val_e/atom_mae: 0.000230
val_e/atom_rmse: 0.000272
2025-04-09 01:15:13.530 INFO: val_e/atom_rmse: 0.000272
val_f_mae: 0.005831
2025-04-09 01:15:13.530 INFO: val_f_mae: 0.005831
val_f_rmse: 0.008363
2025-04-09 01:15:13.530 INFO: val_f_rmse: 0.008363
##### Step: 72 Learning rate: 0.00125 #####
2025-04-09 01:16:39.286 INFO: ##### Step: 72 Learning rate: 0.00125 #####
Epoch 33, Train Loss: 0.0780, Val Loss: 0.0734
2025-04-09 01:16:39.288 INFO: Epoch 33, Train Loss: 0.0780, Val Loss: 0.0734
train_e/atom_mae: 0.000479
2025-04-09 01:16:39.289 INFO: train_e/atom_mae: 0.000479
train_e/atom_rmse: 0.000592
2025-04-09 01:16:39.289 INFO: train_e/atom_rmse: 0.000592
train_f_mae: 0.005977
2025-04-09 01:16:39.293 INFO: train_f_mae: 0.005977
train_f_rmse: 0.008588
2025-04-09 01:16:39.293 INFO: train_f_rmse: 0.008588
val_e/atom_mae: 0.000270
2025-04-09 01:16:39.296 INFO: val_e/atom_mae: 0.000270
val_e/atom_rmse: 0.000315
2025-04-09 01:16:39.296 INFO: val_e/atom_rmse: 0.000315
val_f_mae: 0.005945
2025-04-09 01:16:39.296 INFO: val_f_mae: 0.005945
val_f_rmse: 0.008496
2025-04-09 01:16:39.296 INFO: val_f_rmse: 0.008496
##### Step: 73 Learning rate: 0.00125 #####
2025-04-09 01:18:05.062 INFO: ##### Step: 73 Learning rate: 0.00125 #####
Epoch 34, Train Loss: 0.0804, Val Loss: 0.0769
2025-04-09 01:18:05.062 INFO: Epoch 34, Train Loss: 0.0804, Val Loss: 0.0769
train_e/atom_mae: 0.000469
2025-04-09 01:18:05.063 INFO: train_e/atom_mae: 0.000469
train_e/atom_rmse: 0.000591
2025-04-09 01:18:05.063 INFO: train_e/atom_rmse: 0.000591
train_f_mae: 0.006051
2025-04-09 01:18:05.067 INFO: train_f_mae: 0.006051
train_f_rmse: 0.008729
2025-04-09 01:18:05.067 INFO: train_f_rmse: 0.008729
val_e/atom_mae: 0.000261
2025-04-09 01:18:05.069 INFO: val_e/atom_mae: 0.000261
val_e/atom_rmse: 0.000316
2025-04-09 01:18:05.070 INFO: val_e/atom_rmse: 0.000316
val_f_mae: 0.006015
2025-04-09 01:18:05.070 INFO: val_f_mae: 0.006015
val_f_rmse: 0.008701
2025-04-09 01:18:05.070 INFO: val_f_rmse: 0.008701
##### Step: 74 Learning rate: 0.00125 #####
2025-04-09 01:19:30.753 INFO: ##### Step: 74 Learning rate: 0.00125 #####
Epoch 35, Train Loss: 0.0783, Val Loss: 0.0854
2025-04-09 01:19:30.754 INFO: Epoch 35, Train Loss: 0.0783, Val Loss: 0.0854
train_e/atom_mae: 0.000390
2025-04-09 01:19:30.755 INFO: train_e/atom_mae: 0.000390
train_e/atom_rmse: 0.000483
2025-04-09 01:19:30.755 INFO: train_e/atom_rmse: 0.000483
train_f_mae: 0.006015
2025-04-09 01:19:30.759 INFO: train_f_mae: 0.006015
train_f_rmse: 0.008687
2025-04-09 01:19:30.759 INFO: train_f_rmse: 0.008687
val_e/atom_mae: 0.000529
2025-04-09 01:19:30.761 INFO: val_e/atom_mae: 0.000529
val_e/atom_rmse: 0.000581
2025-04-09 01:19:30.762 INFO: val_e/atom_rmse: 0.000581
val_f_mae: 0.006162
2025-04-09 01:19:30.762 INFO: val_f_mae: 0.006162
val_f_rmse: 0.009019
2025-04-09 01:19:30.762 INFO: val_f_rmse: 0.009019
##### Step: 75 Learning rate: 0.00125 #####
2025-04-09 01:20:56.388 INFO: ##### Step: 75 Learning rate: 0.00125 #####
Epoch 36, Train Loss: 0.0844, Val Loss: 0.1088
2025-04-09 01:20:56.389 INFO: Epoch 36, Train Loss: 0.0844, Val Loss: 0.1088
train_e/atom_mae: 0.000309
2025-04-09 01:20:56.390 INFO: train_e/atom_mae: 0.000309
train_e/atom_rmse: 0.000387
2025-04-09 01:20:56.390 INFO: train_e/atom_rmse: 0.000387
train_f_mae: 0.006213
2025-04-09 01:20:56.393 INFO: train_f_mae: 0.006213
train_f_rmse: 0.009086
2025-04-09 01:20:56.394 INFO: train_f_rmse: 0.009086
val_e/atom_mae: 0.000555
2025-04-09 01:20:56.396 INFO: val_e/atom_mae: 0.000555
val_e/atom_rmse: 0.000658
2025-04-09 01:20:56.397 INFO: val_e/atom_rmse: 0.000658
val_f_mae: 0.006861
2025-04-09 01:20:56.397 INFO: val_f_mae: 0.006861
val_f_rmse: 0.010176
2025-04-09 01:20:56.397 INFO: val_f_rmse: 0.010176
##### Step: 76 Learning rate: 0.00125 #####
2025-04-09 01:22:22.047 INFO: ##### Step: 76 Learning rate: 0.00125 #####
Epoch 37, Train Loss: 0.0753, Val Loss: 0.0735
2025-04-09 01:22:22.048 INFO: Epoch 37, Train Loss: 0.0753, Val Loss: 0.0735
train_e/atom_mae: 0.000412
2025-04-09 01:22:22.049 INFO: train_e/atom_mae: 0.000412
train_e/atom_rmse: 0.000510
2025-04-09 01:22:22.049 INFO: train_e/atom_rmse: 0.000510
train_f_mae: 0.005884
2025-04-09 01:22:22.052 INFO: train_f_mae: 0.005884
train_f_rmse: 0.008497
2025-04-09 01:22:22.053 INFO: train_f_rmse: 0.008497
val_e/atom_mae: 0.000357
2025-04-09 01:22:22.055 INFO: val_e/atom_mae: 0.000357
val_e/atom_rmse: 0.000401
2025-04-09 01:22:22.056 INFO: val_e/atom_rmse: 0.000401
val_f_mae: 0.005946
2025-04-09 01:22:22.056 INFO: val_f_mae: 0.005946
val_f_rmse: 0.008459
2025-04-09 01:22:22.056 INFO: val_f_rmse: 0.008459
##### Step: 77 Learning rate: 0.00125 #####
2025-04-09 01:23:47.700 INFO: ##### Step: 77 Learning rate: 0.00125 #####
Epoch 38, Train Loss: 0.0780, Val Loss: 0.1025
2025-04-09 01:23:47.705 INFO: Epoch 38, Train Loss: 0.0780, Val Loss: 0.1025
train_e/atom_mae: 0.000431
2025-04-09 01:23:47.706 INFO: train_e/atom_mae: 0.000431
train_e/atom_rmse: 0.000562
2025-04-09 01:23:47.706 INFO: train_e/atom_rmse: 0.000562
train_f_mae: 0.005982
2025-04-09 01:23:47.710 INFO: train_f_mae: 0.005982
train_f_rmse: 0.008616
2025-04-09 01:23:47.710 INFO: train_f_rmse: 0.008616
val_e/atom_mae: 0.001065
2025-04-09 01:23:47.712 INFO: val_e/atom_mae: 0.001065
val_e/atom_rmse: 0.001092
2025-04-09 01:23:47.713 INFO: val_e/atom_rmse: 0.001092
val_f_mae: 0.006443
2025-04-09 01:23:47.713 INFO: val_f_mae: 0.006443
val_f_rmse: 0.009385
2025-04-09 01:23:47.713 INFO: val_f_rmse: 0.009385
##### Step: 78 Learning rate: 0.00125 #####
2025-04-09 01:25:13.378 INFO: ##### Step: 78 Learning rate: 0.00125 #####
Epoch 39, Train Loss: 0.0797, Val Loss: 0.0774
2025-04-09 01:25:13.379 INFO: Epoch 39, Train Loss: 0.0797, Val Loss: 0.0774
train_e/atom_mae: 0.000453
2025-04-09 01:25:13.380 INFO: train_e/atom_mae: 0.000453
train_e/atom_rmse: 0.000595
2025-04-09 01:25:13.380 INFO: train_e/atom_rmse: 0.000595
train_f_mae: 0.005995
2025-04-09 01:25:13.383 INFO: train_f_mae: 0.005995
train_f_rmse: 0.008682
2025-04-09 01:25:13.384 INFO: train_f_rmse: 0.008682
val_e/atom_mae: 0.000200
2025-04-09 01:25:13.386 INFO: val_e/atom_mae: 0.000200
val_e/atom_rmse: 0.000241
2025-04-09 01:25:13.387 INFO: val_e/atom_rmse: 0.000241
val_f_mae: 0.006037
2025-04-09 01:25:13.387 INFO: val_f_mae: 0.006037
val_f_rmse: 0.008755
2025-04-09 01:25:13.387 INFO: val_f_rmse: 0.008755
##### Step: 79 Learning rate: 0.00125 #####
2025-04-09 01:26:38.950 INFO: ##### Step: 79 Learning rate: 0.00125 #####
Epoch 40, Train Loss: 0.0834, Val Loss: 0.0932
2025-04-09 01:26:38.950 INFO: Epoch 40, Train Loss: 0.0834, Val Loss: 0.0932
train_e/atom_mae: 0.000375
2025-04-09 01:26:38.951 INFO: train_e/atom_mae: 0.000375
train_e/atom_rmse: 0.000477
2025-04-09 01:26:38.952 INFO: train_e/atom_rmse: 0.000477
train_f_mae: 0.006170
2025-04-09 01:26:38.955 INFO: train_f_mae: 0.006170
train_f_rmse: 0.008982
2025-04-09 01:26:38.955 INFO: train_f_rmse: 0.008982
val_e/atom_mae: 0.000886
2025-04-09 01:26:38.958 INFO: val_e/atom_mae: 0.000886
val_e/atom_rmse: 0.000911
2025-04-09 01:26:38.958 INFO: val_e/atom_rmse: 0.000911
val_f_mae: 0.006196
2025-04-09 01:26:38.958 INFO: val_f_mae: 0.006196
val_f_rmse: 0.009120
2025-04-09 01:26:38.958 INFO: val_f_rmse: 0.009120
##### Step: 80 Learning rate: 0.000625 #####
2025-04-09 01:28:04.625 INFO: ##### Step: 80 Learning rate: 0.000625 #####
Epoch 41, Train Loss: 0.0673, Val Loss: 0.0764
2025-04-09 01:28:04.626 INFO: Epoch 41, Train Loss: 0.0673, Val Loss: 0.0764
train_e/atom_mae: 0.000261
2025-04-09 01:28:04.626 INFO: train_e/atom_mae: 0.000261
train_e/atom_rmse: 0.000332
2025-04-09 01:28:04.627 INFO: train_e/atom_rmse: 0.000332
train_f_mae: 0.005669
2025-04-09 01:28:04.630 INFO: train_f_mae: 0.005669
train_f_rmse: 0.008120
2025-04-09 01:28:04.630 INFO: train_f_rmse: 0.008120
val_e/atom_mae: 0.000417
2025-04-09 01:28:04.633 INFO: val_e/atom_mae: 0.000417
val_e/atom_rmse: 0.000442
2025-04-09 01:28:04.633 INFO: val_e/atom_rmse: 0.000442
val_f_mae: 0.006024
2025-04-09 01:28:04.633 INFO: val_f_mae: 0.006024
val_f_rmse: 0.008605
2025-04-09 01:28:04.634 INFO: val_f_rmse: 0.008605
##### Step: 81 Learning rate: 0.000625 #####
2025-04-09 01:29:30.272 INFO: ##### Step: 81 Learning rate: 0.000625 #####
Epoch 42, Train Loss: 0.0692, Val Loss: 0.0723
2025-04-09 01:29:30.273 INFO: Epoch 42, Train Loss: 0.0692, Val Loss: 0.0723
train_e/atom_mae: 0.000242
2025-04-09 01:29:30.274 INFO: train_e/atom_mae: 0.000242
train_e/atom_rmse: 0.000304
2025-04-09 01:29:30.274 INFO: train_e/atom_rmse: 0.000304
train_f_mae: 0.005763
2025-04-09 01:29:30.278 INFO: train_f_mae: 0.005763
train_f_rmse: 0.008253
2025-04-09 01:29:30.278 INFO: train_f_rmse: 0.008253
val_e/atom_mae: 0.000332
2025-04-09 01:29:30.280 INFO: val_e/atom_mae: 0.000332
val_e/atom_rmse: 0.000369
2025-04-09 01:29:30.281 INFO: val_e/atom_rmse: 0.000369
val_f_mae: 0.005861
2025-04-09 01:29:30.281 INFO: val_f_mae: 0.005861
val_f_rmse: 0.008408
2025-04-09 01:29:30.281 INFO: val_f_rmse: 0.008408
##### Step: 82 Learning rate: 0.000625 #####
2025-04-09 01:30:55.859 INFO: ##### Step: 82 Learning rate: 0.000625 #####
Epoch 43, Train Loss: 0.0686, Val Loss: 0.0733
2025-04-09 01:30:55.860 INFO: Epoch 43, Train Loss: 0.0686, Val Loss: 0.0733
train_e/atom_mae: 0.000397
2025-04-09 01:30:55.861 INFO: train_e/atom_mae: 0.000397
train_e/atom_rmse: 0.000495
2025-04-09 01:30:55.861 INFO: train_e/atom_rmse: 0.000495
train_f_mae: 0.005683
2025-04-09 01:30:55.865 INFO: train_f_mae: 0.005683
train_f_rmse: 0.008104
2025-04-09 01:30:55.865 INFO: train_f_rmse: 0.008104
val_e/atom_mae: 0.000612
2025-04-09 01:30:55.867 INFO: val_e/atom_mae: 0.000612
val_e/atom_rmse: 0.000643
2025-04-09 01:30:55.868 INFO: val_e/atom_rmse: 0.000643
val_f_mae: 0.005783
2025-04-09 01:30:55.868 INFO: val_f_mae: 0.005783
val_f_rmse: 0.008266
2025-04-09 01:30:55.868 INFO: val_f_rmse: 0.008266
##### Step: 83 Learning rate: 0.000625 #####
2025-04-09 01:32:21.435 INFO: ##### Step: 83 Learning rate: 0.000625 #####
Epoch 44, Train Loss: 0.0683, Val Loss: 0.0705
2025-04-09 01:32:21.436 INFO: Epoch 44, Train Loss: 0.0683, Val Loss: 0.0705
train_e/atom_mae: 0.000294
2025-04-09 01:32:21.437 INFO: train_e/atom_mae: 0.000294
train_e/atom_rmse: 0.000371
2025-04-09 01:32:21.437 INFO: train_e/atom_rmse: 0.000371
train_f_mae: 0.005711
2025-04-09 01:32:21.441 INFO: train_f_mae: 0.005711
train_f_rmse: 0.008160
2025-04-09 01:32:21.441 INFO: train_f_rmse: 0.008160
val_e/atom_mae: 0.000183
2025-04-09 01:32:21.443 INFO: val_e/atom_mae: 0.000183
val_e/atom_rmse: 0.000234
2025-04-09 01:32:21.444 INFO: val_e/atom_rmse: 0.000234
val_f_mae: 0.005842
2025-04-09 01:32:21.444 INFO: val_f_mae: 0.005842
val_f_rmse: 0.008359
2025-04-09 01:32:21.444 INFO: val_f_rmse: 0.008359
##### Step: 84 Learning rate: 0.000625 #####
2025-04-09 01:33:47.127 INFO: ##### Step: 84 Learning rate: 0.000625 #####
Epoch 45, Train Loss: 0.0652, Val Loss: 0.0667
2025-04-09 01:33:47.128 INFO: Epoch 45, Train Loss: 0.0652, Val Loss: 0.0667
train_e/atom_mae: 0.000256
2025-04-09 01:33:47.129 INFO: train_e/atom_mae: 0.000256
train_e/atom_rmse: 0.000323
2025-04-09 01:33:47.129 INFO: train_e/atom_rmse: 0.000323
train_f_mae: 0.005622
2025-04-09 01:33:47.132 INFO: train_f_mae: 0.005622
train_f_rmse: 0.007994
2025-04-09 01:33:47.132 INFO: train_f_rmse: 0.007994
val_e/atom_mae: 0.000448
2025-04-09 01:33:47.135 INFO: val_e/atom_mae: 0.000448
val_e/atom_rmse: 0.000474
2025-04-09 01:33:47.135 INFO: val_e/atom_rmse: 0.000474
val_f_mae: 0.005616
2025-04-09 01:33:47.136 INFO: val_f_mae: 0.005616
val_f_rmse: 0.008000
2025-04-09 01:33:47.136 INFO: val_f_rmse: 0.008000
##### Step: 85 Learning rate: 0.000625 #####
2025-04-09 01:35:12.830 INFO: ##### Step: 85 Learning rate: 0.000625 #####
Epoch 46, Train Loss: 0.0678, Val Loss: 0.0675
2025-04-09 01:35:12.831 INFO: Epoch 46, Train Loss: 0.0678, Val Loss: 0.0675
train_e/atom_mae: 0.000289
2025-04-09 01:35:12.832 INFO: train_e/atom_mae: 0.000289
train_e/atom_rmse: 0.000371
2025-04-09 01:35:12.832 INFO: train_e/atom_rmse: 0.000371
train_f_mae: 0.005699
2025-04-09 01:35:12.835 INFO: train_f_mae: 0.005699
train_f_rmse: 0.008130
2025-04-09 01:35:12.836 INFO: train_f_rmse: 0.008130
val_e/atom_mae: 0.000185
2025-04-09 01:35:12.838 INFO: val_e/atom_mae: 0.000185
val_e/atom_rmse: 0.000236
2025-04-09 01:35:12.839 INFO: val_e/atom_rmse: 0.000236
val_f_mae: 0.005737
2025-04-09 01:35:12.839 INFO: val_f_mae: 0.005737
val_f_rmse: 0.008176
2025-04-09 01:35:12.839 INFO: val_f_rmse: 0.008176
##### Step: 86 Learning rate: 0.000625 #####
2025-04-09 01:36:38.418 INFO: ##### Step: 86 Learning rate: 0.000625 #####
Epoch 47, Train Loss: 0.0671, Val Loss: 0.0673
2025-04-09 01:36:38.419 INFO: Epoch 47, Train Loss: 0.0671, Val Loss: 0.0673
train_e/atom_mae: 0.000253
2025-04-09 01:36:38.420 INFO: train_e/atom_mae: 0.000253
train_e/atom_rmse: 0.000319
2025-04-09 01:36:38.420 INFO: train_e/atom_rmse: 0.000319
train_f_mae: 0.005683
2025-04-09 01:36:38.423 INFO: train_f_mae: 0.005683
train_f_rmse: 0.008113
2025-04-09 01:36:38.423 INFO: train_f_rmse: 0.008113
val_e/atom_mae: 0.000236
2025-04-09 01:36:38.426 INFO: val_e/atom_mae: 0.000236
val_e/atom_rmse: 0.000286
2025-04-09 01:36:38.426 INFO: val_e/atom_rmse: 0.000286
val_f_mae: 0.005726
2025-04-09 01:36:38.427 INFO: val_f_mae: 0.005726
val_f_rmse: 0.008143
2025-04-09 01:36:38.427 INFO: val_f_rmse: 0.008143
##### Step: 87 Learning rate: 0.000625 #####
2025-04-09 01:38:04.101 INFO: ##### Step: 87 Learning rate: 0.000625 #####
Epoch 48, Train Loss: 0.0644, Val Loss: 0.0745
2025-04-09 01:38:04.101 INFO: Epoch 48, Train Loss: 0.0644, Val Loss: 0.0745
train_e/atom_mae: 0.000240
2025-04-09 01:38:04.102 INFO: train_e/atom_mae: 0.000240
train_e/atom_rmse: 0.000304
2025-04-09 01:38:04.102 INFO: train_e/atom_rmse: 0.000304
train_f_mae: 0.005578
2025-04-09 01:38:04.106 INFO: train_f_mae: 0.005578
train_f_rmse: 0.007957
2025-04-09 01:38:04.106 INFO: train_f_rmse: 0.007957
val_e/atom_mae: 0.000180
2025-04-09 01:38:04.108 INFO: val_e/atom_mae: 0.000180
val_e/atom_rmse: 0.000218
2025-04-09 01:38:04.109 INFO: val_e/atom_rmse: 0.000218
val_f_mae: 0.005975
2025-04-09 01:38:04.109 INFO: val_f_mae: 0.005975
val_f_rmse: 0.008598
2025-04-09 01:38:04.109 INFO: val_f_rmse: 0.008598
##### Step: 88 Learning rate: 0.000625 #####
2025-04-09 01:39:29.665 INFO: ##### Step: 88 Learning rate: 0.000625 #####
Epoch 49, Train Loss: 0.0683, Val Loss: 0.0715
2025-04-09 01:39:29.665 INFO: Epoch 49, Train Loss: 0.0683, Val Loss: 0.0715
train_e/atom_mae: 0.000281
2025-04-09 01:39:29.666 INFO: train_e/atom_mae: 0.000281
train_e/atom_rmse: 0.000351
2025-04-09 01:39:29.666 INFO: train_e/atom_rmse: 0.000351
train_f_mae: 0.005703
2025-04-09 01:39:29.669 INFO: train_f_mae: 0.005703
train_f_rmse: 0.008175
2025-04-09 01:39:29.669 INFO: train_f_rmse: 0.008175
val_e/atom_mae: 0.000141
2025-04-09 01:39:29.672 INFO: val_e/atom_mae: 0.000141
val_e/atom_rmse: 0.000169
2025-04-09 01:39:29.672 INFO: val_e/atom_rmse: 0.000169
val_f_mae: 0.005909
2025-04-09 01:39:29.673 INFO: val_f_mae: 0.005909
val_f_rmse: 0.008437
2025-04-09 01:39:29.673 INFO: val_f_rmse: 0.008437
##### Step: 89 Learning rate: 0.000625 #####
2025-04-09 01:40:55.017 INFO: ##### Step: 89 Learning rate: 0.000625 #####
Epoch 50, Train Loss: 0.0675, Val Loss: 0.0708
2025-04-09 01:40:55.018 INFO: Epoch 50, Train Loss: 0.0675, Val Loss: 0.0708
train_e/atom_mae: 0.000270
2025-04-09 01:40:55.019 INFO: train_e/atom_mae: 0.000270
train_e/atom_rmse: 0.000338
2025-04-09 01:40:55.019 INFO: train_e/atom_rmse: 0.000338
train_f_mae: 0.005685
2025-04-09 01:40:55.022 INFO: train_f_mae: 0.005685
train_f_rmse: 0.008134
2025-04-09 01:40:55.022 INFO: train_f_rmse: 0.008134
val_e/atom_mae: 0.000560
2025-04-09 01:40:55.024 INFO: val_e/atom_mae: 0.000560
val_e/atom_rmse: 0.000611
2025-04-09 01:40:55.025 INFO: val_e/atom_rmse: 0.000611
val_f_mae: 0.005737
2025-04-09 01:40:55.025 INFO: val_f_mae: 0.005737
val_f_rmse: 0.008142
2025-04-09 01:40:55.025 INFO: val_f_rmse: 0.008142
##### Step: 90 Learning rate: 0.000625 #####
2025-04-09 01:42:20.196 INFO: ##### Step: 90 Learning rate: 0.000625 #####
Epoch 51, Train Loss: 0.0658, Val Loss: 0.0734
2025-04-09 01:42:20.197 INFO: Epoch 51, Train Loss: 0.0658, Val Loss: 0.0734
train_e/atom_mae: 0.000231
2025-04-09 01:42:20.198 INFO: train_e/atom_mae: 0.000231
train_e/atom_rmse: 0.000291
2025-04-09 01:42:20.198 INFO: train_e/atom_rmse: 0.000291
train_f_mae: 0.005634
2025-04-09 01:42:20.201 INFO: train_f_mae: 0.005634
train_f_rmse: 0.008051
2025-04-09 01:42:20.201 INFO: train_f_rmse: 0.008051
val_e/atom_mae: 0.000256
2025-04-09 01:42:20.203 INFO: val_e/atom_mae: 0.000256
val_e/atom_rmse: 0.000296
2025-04-09 01:42:20.203 INFO: val_e/atom_rmse: 0.000296
val_f_mae: 0.005896
2025-04-09 01:42:20.204 INFO: val_f_mae: 0.005896
val_f_rmse: 0.008508
2025-04-09 01:42:20.204 INFO: val_f_rmse: 0.008508
##### Step: 91 Learning rate: 0.000625 #####
2025-04-09 01:43:45.410 INFO: ##### Step: 91 Learning rate: 0.000625 #####
Epoch 52, Train Loss: 0.0665, Val Loss: 0.0668
2025-04-09 01:43:45.410 INFO: Epoch 52, Train Loss: 0.0665, Val Loss: 0.0668
train_e/atom_mae: 0.000244
2025-04-09 01:43:45.411 INFO: train_e/atom_mae: 0.000244
train_e/atom_rmse: 0.000306
2025-04-09 01:43:45.411 INFO: train_e/atom_rmse: 0.000306
train_f_mae: 0.005647
2025-04-09 01:43:45.414 INFO: train_f_mae: 0.005647
train_f_rmse: 0.008082
2025-04-09 01:43:45.414 INFO: train_f_rmse: 0.008082
val_e/atom_mae: 0.000125
2025-04-09 01:43:45.416 INFO: val_e/atom_mae: 0.000125
val_e/atom_rmse: 0.000160
2025-04-09 01:43:45.417 INFO: val_e/atom_rmse: 0.000160
val_f_mae: 0.005704
2025-04-09 01:43:45.417 INFO: val_f_mae: 0.005704
val_f_rmse: 0.008154
2025-04-09 01:43:45.417 INFO: val_f_rmse: 0.008154
##### Step: 92 Learning rate: 0.000625 #####
2025-04-09 01:45:10.549 INFO: ##### Step: 92 Learning rate: 0.000625 #####
Epoch 53, Train Loss: 0.0651, Val Loss: 0.0663
2025-04-09 01:45:10.549 INFO: Epoch 53, Train Loss: 0.0651, Val Loss: 0.0663
train_e/atom_mae: 0.000299
2025-04-09 01:45:10.550 INFO: train_e/atom_mae: 0.000299
train_e/atom_rmse: 0.000370
2025-04-09 01:45:10.550 INFO: train_e/atom_rmse: 0.000370
train_f_mae: 0.005590
2025-04-09 01:45:10.553 INFO: train_f_mae: 0.005590
train_f_rmse: 0.007966
2025-04-09 01:45:10.553 INFO: train_f_rmse: 0.007966
val_e/atom_mae: 0.000262
2025-04-09 01:45:10.555 INFO: val_e/atom_mae: 0.000262
val_e/atom_rmse: 0.000310
2025-04-09 01:45:10.556 INFO: val_e/atom_rmse: 0.000310
val_f_mae: 0.005688
2025-04-09 01:45:10.557 INFO: val_f_mae: 0.005688
val_f_rmse: 0.008071
2025-04-09 01:45:10.557 INFO: val_f_rmse: 0.008071
##### Step: 93 Learning rate: 0.000625 #####
2025-04-09 01:46:35.771 INFO: ##### Step: 93 Learning rate: 0.000625 #####
Epoch 54, Train Loss: 0.0659, Val Loss: 0.0682
2025-04-09 01:46:35.772 INFO: Epoch 54, Train Loss: 0.0659, Val Loss: 0.0682
train_e/atom_mae: 0.000268
2025-04-09 01:46:35.773 INFO: train_e/atom_mae: 0.000268
train_e/atom_rmse: 0.000337
2025-04-09 01:46:35.773 INFO: train_e/atom_rmse: 0.000337
train_f_mae: 0.005633
2025-04-09 01:46:35.776 INFO: train_f_mae: 0.005633
train_f_rmse: 0.008032
2025-04-09 01:46:35.776 INFO: train_f_rmse: 0.008032
val_e/atom_mae: 0.000526
2025-04-09 01:46:35.778 INFO: val_e/atom_mae: 0.000526
val_e/atom_rmse: 0.000548
2025-04-09 01:46:35.778 INFO: val_e/atom_rmse: 0.000548
val_f_mae: 0.005668
2025-04-09 01:46:35.779 INFO: val_f_mae: 0.005668
val_f_rmse: 0.008037
2025-04-09 01:46:35.779 INFO: val_f_rmse: 0.008037
##### Step: 94 Learning rate: 0.000625 #####
2025-04-09 01:48:00.857 INFO: ##### Step: 94 Learning rate: 0.000625 #####
Epoch 55, Train Loss: 0.0662, Val Loss: 0.0723
2025-04-09 01:48:00.857 INFO: Epoch 55, Train Loss: 0.0662, Val Loss: 0.0723
train_e/atom_mae: 0.000271
2025-04-09 01:48:00.858 INFO: train_e/atom_mae: 0.000271
train_e/atom_rmse: 0.000343
2025-04-09 01:48:00.858 INFO: train_e/atom_rmse: 0.000343
train_f_mae: 0.005639
2025-04-09 01:48:00.861 INFO: train_f_mae: 0.005639
train_f_rmse: 0.008049
2025-04-09 01:48:00.861 INFO: train_f_rmse: 0.008049
val_e/atom_mae: 0.000464
2025-04-09 01:48:00.863 INFO: val_e/atom_mae: 0.000464
val_e/atom_rmse: 0.000497
2025-04-09 01:48:00.864 INFO: val_e/atom_rmse: 0.000497
val_f_mae: 0.005886
2025-04-09 01:48:00.864 INFO: val_f_mae: 0.005886
val_f_rmse: 0.008326
2025-04-09 01:48:00.864 INFO: val_f_rmse: 0.008326
##### Step: 95 Learning rate: 0.000625 #####
2025-04-09 01:49:25.460 INFO: ##### Step: 95 Learning rate: 0.000625 #####
Epoch 56, Train Loss: 0.0664, Val Loss: 0.0840
2025-04-09 01:49:25.461 INFO: Epoch 56, Train Loss: 0.0664, Val Loss: 0.0840
train_e/atom_mae: 0.000262
2025-04-09 01:49:25.462 INFO: train_e/atom_mae: 0.000262
train_e/atom_rmse: 0.000324
2025-04-09 01:49:25.462 INFO: train_e/atom_rmse: 0.000324
train_f_mae: 0.005637
2025-04-09 01:49:25.465 INFO: train_f_mae: 0.005637
train_f_rmse: 0.008068
2025-04-09 01:49:25.465 INFO: train_f_rmse: 0.008068
val_e/atom_mae: 0.000272
2025-04-09 01:49:25.467 INFO: val_e/atom_mae: 0.000272
val_e/atom_rmse: 0.000314
2025-04-09 01:49:25.468 INFO: val_e/atom_rmse: 0.000314
val_f_mae: 0.006247
2025-04-09 01:49:25.468 INFO: val_f_mae: 0.006247
val_f_rmse: 0.009098
2025-04-09 01:49:25.468 INFO: val_f_rmse: 0.009098
##### Step: 96 Learning rate: 0.000625 #####
2025-04-09 01:50:49.951 INFO: ##### Step: 96 Learning rate: 0.000625 #####
Epoch 57, Train Loss: 0.0667, Val Loss: 0.0641
2025-04-09 01:50:49.952 INFO: Epoch 57, Train Loss: 0.0667, Val Loss: 0.0641
train_e/atom_mae: 0.000310
2025-04-09 01:50:49.952 INFO: train_e/atom_mae: 0.000310
train_e/atom_rmse: 0.000410
2025-04-09 01:50:49.953 INFO: train_e/atom_rmse: 0.000410
train_f_mae: 0.005633
2025-04-09 01:50:49.955 INFO: train_f_mae: 0.005633
train_f_rmse: 0.008040
2025-04-09 01:50:49.955 INFO: train_f_rmse: 0.008040
val_e/atom_mae: 0.000343
2025-04-09 01:50:49.958 INFO: val_e/atom_mae: 0.000343
val_e/atom_rmse: 0.000384
2025-04-09 01:50:49.958 INFO: val_e/atom_rmse: 0.000384
val_f_mae: 0.005554
2025-04-09 01:50:49.958 INFO: val_f_mae: 0.005554
val_f_rmse: 0.007893
2025-04-09 01:50:49.958 INFO: val_f_rmse: 0.007893
##### Step: 97 Learning rate: 0.000625 #####
2025-04-09 01:52:14.446 INFO: ##### Step: 97 Learning rate: 0.000625 #####
Epoch 58, Train Loss: 0.0656, Val Loss: 0.0751
2025-04-09 01:52:14.447 INFO: Epoch 58, Train Loss: 0.0656, Val Loss: 0.0751
train_e/atom_mae: 0.000235
2025-04-09 01:52:14.447 INFO: train_e/atom_mae: 0.000235
train_e/atom_rmse: 0.000299
2025-04-09 01:52:14.448 INFO: train_e/atom_rmse: 0.000299
train_f_mae: 0.005632
2025-04-09 01:52:14.450 INFO: train_f_mae: 0.005632
train_f_rmse: 0.008033
2025-04-09 01:52:14.450 INFO: train_f_rmse: 0.008033
val_e/atom_mae: 0.000481
2025-04-09 01:52:14.453 INFO: val_e/atom_mae: 0.000481
val_e/atom_rmse: 0.000521
2025-04-09 01:52:14.453 INFO: val_e/atom_rmse: 0.000521
val_f_mae: 0.005864
2025-04-09 01:52:14.453 INFO: val_f_mae: 0.005864
val_f_rmse: 0.008472
2025-04-09 01:52:14.454 INFO: val_f_rmse: 0.008472
##### Step: 98 Learning rate: 0.000625 #####
2025-04-09 01:53:38.933 INFO: ##### Step: 98 Learning rate: 0.000625 #####
Epoch 59, Train Loss: 0.0676, Val Loss: 0.0670
2025-04-09 01:53:38.934 INFO: Epoch 59, Train Loss: 0.0676, Val Loss: 0.0670
train_e/atom_mae: 0.000261
2025-04-09 01:53:38.935 INFO: train_e/atom_mae: 0.000261
train_e/atom_rmse: 0.000329
2025-04-09 01:53:38.935 INFO: train_e/atom_rmse: 0.000329
train_f_mae: 0.005688
2025-04-09 01:53:38.937 INFO: train_f_mae: 0.005688
train_f_rmse: 0.008145
2025-04-09 01:53:38.938 INFO: train_f_rmse: 0.008145
val_e/atom_mae: 0.000166
2025-04-09 01:53:38.940 INFO: val_e/atom_mae: 0.000166
val_e/atom_rmse: 0.000201
2025-04-09 01:53:38.940 INFO: val_e/atom_rmse: 0.000201
val_f_mae: 0.005757
2025-04-09 01:53:38.941 INFO: val_f_mae: 0.005757
val_f_rmse: 0.008155
2025-04-09 01:53:38.941 INFO: val_f_rmse: 0.008155
##### Step: 99 Learning rate: 0.000625 #####
2025-04-09 01:55:03.369 INFO: ##### Step: 99 Learning rate: 0.000625 #####
Epoch 60, Train Loss: 0.0650, Val Loss: 0.0665
2025-04-09 01:55:03.370 INFO: Epoch 60, Train Loss: 0.0650, Val Loss: 0.0665
train_e/atom_mae: 0.000216
2025-04-09 01:55:03.371 INFO: train_e/atom_mae: 0.000216
train_e/atom_rmse: 0.000271
2025-04-09 01:55:03.371 INFO: train_e/atom_rmse: 0.000271
train_f_mae: 0.005608
2025-04-09 01:55:03.374 INFO: train_f_mae: 0.005608
train_f_rmse: 0.008006
2025-04-09 01:55:03.374 INFO: train_f_rmse: 0.008006
val_e/atom_mae: 0.000211
2025-04-09 01:55:03.376 INFO: val_e/atom_mae: 0.000211
val_e/atom_rmse: 0.000262
2025-04-09 01:55:03.377 INFO: val_e/atom_rmse: 0.000262
val_f_mae: 0.005717
2025-04-09 01:55:03.377 INFO: val_f_mae: 0.005717
val_f_rmse: 0.008101
2025-04-09 01:55:03.377 INFO: val_f_rmse: 0.008101
##### Step: 100 Learning rate: 0.0003125 #####
2025-04-09 01:56:27.837 INFO: ##### Step: 100 Learning rate: 0.0003125 #####
Epoch 61, Train Loss: 0.0596, Val Loss: 0.0641
2025-04-09 01:56:27.838 INFO: Epoch 61, Train Loss: 0.0596, Val Loss: 0.0641
train_e/atom_mae: 0.000174
2025-04-09 01:56:27.839 INFO: train_e/atom_mae: 0.000174
train_e/atom_rmse: 0.000221
2025-04-09 01:56:27.839 INFO: train_e/atom_rmse: 0.000221
train_f_mae: 0.005415
2025-04-09 01:56:27.842 INFO: train_f_mae: 0.005415
train_f_rmse: 0.007684
2025-04-09 01:56:27.842 INFO: train_f_rmse: 0.007684
val_e/atom_mae: 0.000141
2025-04-09 01:56:27.844 INFO: val_e/atom_mae: 0.000141
val_e/atom_rmse: 0.000176
2025-04-09 01:56:27.844 INFO: val_e/atom_rmse: 0.000176
val_f_mae: 0.005582
2025-04-09 01:56:27.845 INFO: val_f_mae: 0.005582
val_f_rmse: 0.007981
2025-04-09 01:56:27.845 INFO: val_f_rmse: 0.007981
##### Step: 101 Learning rate: 0.0003125 #####
2025-04-09 01:57:52.354 INFO: ##### Step: 101 Learning rate: 0.0003125 #####
Epoch 62, Train Loss: 0.0604, Val Loss: 0.0665
2025-04-09 01:57:52.354 INFO: Epoch 62, Train Loss: 0.0604, Val Loss: 0.0665
train_e/atom_mae: 0.000180
2025-04-09 01:57:52.355 INFO: train_e/atom_mae: 0.000180
train_e/atom_rmse: 0.000226
2025-04-09 01:57:52.355 INFO: train_e/atom_rmse: 0.000226
train_f_mae: 0.005443
2025-04-09 01:57:52.358 INFO: train_f_mae: 0.005443
train_f_rmse: 0.007732
2025-04-09 01:57:52.358 INFO: train_f_rmse: 0.007732
val_e/atom_mae: 0.000184
2025-04-09 01:57:52.360 INFO: val_e/atom_mae: 0.000184
val_e/atom_rmse: 0.000223
2025-04-09 01:57:52.361 INFO: val_e/atom_rmse: 0.000223
val_f_mae: 0.005673
2025-04-09 01:57:52.361 INFO: val_f_mae: 0.005673
val_f_rmse: 0.008116
2025-04-09 01:57:52.362 INFO: val_f_rmse: 0.008116
##### Step: 102 Learning rate: 0.0003125 #####
2025-04-09 01:59:16.835 INFO: ##### Step: 102 Learning rate: 0.0003125 #####
Epoch 63, Train Loss: 0.0614, Val Loss: 0.0610
2025-04-09 01:59:16.836 INFO: Epoch 63, Train Loss: 0.0614, Val Loss: 0.0610
train_e/atom_mae: 0.000191
2025-04-09 01:59:16.836 INFO: train_e/atom_mae: 0.000191
train_e/atom_rmse: 0.000239
2025-04-09 01:59:16.837 INFO: train_e/atom_rmse: 0.000239
train_f_mae: 0.005482
2025-04-09 01:59:16.839 INFO: train_f_mae: 0.005482
train_f_rmse: 0.007794
2025-04-09 01:59:16.839 INFO: train_f_rmse: 0.007794
val_e/atom_mae: 0.000168
2025-04-09 01:59:16.842 INFO: val_e/atom_mae: 0.000168
val_e/atom_rmse: 0.000194
2025-04-09 01:59:16.842 INFO: val_e/atom_rmse: 0.000194
val_f_mae: 0.005458
2025-04-09 01:59:16.842 INFO: val_f_mae: 0.005458
val_f_rmse: 0.007779
2025-04-09 01:59:16.842 INFO: val_f_rmse: 0.007779
##### Step: 103 Learning rate: 0.0003125 #####
2025-04-09 02:00:41.284 INFO: ##### Step: 103 Learning rate: 0.0003125 #####
Epoch 64, Train Loss: 0.0596, Val Loss: 0.0633
2025-04-09 02:00:41.284 INFO: Epoch 64, Train Loss: 0.0596, Val Loss: 0.0633
train_e/atom_mae: 0.000171
2025-04-09 02:00:41.285 INFO: train_e/atom_mae: 0.000171
train_e/atom_rmse: 0.000215
2025-04-09 02:00:41.285 INFO: train_e/atom_rmse: 0.000215
train_f_mae: 0.005422
2025-04-09 02:00:41.288 INFO: train_f_mae: 0.005422
train_f_rmse: 0.007685
2025-04-09 02:00:41.288 INFO: train_f_rmse: 0.007685
val_e/atom_mae: 0.000124
2025-04-09 02:00:41.290 INFO: val_e/atom_mae: 0.000124
val_e/atom_rmse: 0.000160
2025-04-09 02:00:41.291 INFO: val_e/atom_rmse: 0.000160
val_f_mae: 0.005577
2025-04-09 02:00:41.291 INFO: val_f_mae: 0.005577
val_f_rmse: 0.007936
2025-04-09 02:00:41.291 INFO: val_f_rmse: 0.007936
##### Step: 104 Learning rate: 0.0003125 #####
2025-04-09 02:02:05.790 INFO: ##### Step: 104 Learning rate: 0.0003125 #####
Epoch 65, Train Loss: 0.0616, Val Loss: 0.0619
2025-04-09 02:02:05.790 INFO: Epoch 65, Train Loss: 0.0616, Val Loss: 0.0619
train_e/atom_mae: 0.000176
2025-04-09 02:02:05.791 INFO: train_e/atom_mae: 0.000176
train_e/atom_rmse: 0.000225
2025-04-09 02:02:05.791 INFO: train_e/atom_rmse: 0.000225
train_f_mae: 0.005488
2025-04-09 02:02:05.794 INFO: train_f_mae: 0.005488
train_f_rmse: 0.007810
2025-04-09 02:02:05.794 INFO: train_f_rmse: 0.007810
val_e/atom_mae: 0.000205
2025-04-09 02:02:05.796 INFO: val_e/atom_mae: 0.000205
val_e/atom_rmse: 0.000245
2025-04-09 02:02:05.797 INFO: val_e/atom_rmse: 0.000245
val_f_mae: 0.005501
2025-04-09 02:02:05.797 INFO: val_f_mae: 0.005501
val_f_rmse: 0.007820
2025-04-09 02:02:05.797 INFO: val_f_rmse: 0.007820
##### Step: 105 Learning rate: 0.0003125 #####
2025-04-09 02:03:30.286 INFO: ##### Step: 105 Learning rate: 0.0003125 #####
Epoch 66, Train Loss: 0.0586, Val Loss: 0.0653
2025-04-09 02:03:30.287 INFO: Epoch 66, Train Loss: 0.0586, Val Loss: 0.0653
train_e/atom_mae: 0.000155
2025-04-09 02:03:30.288 INFO: train_e/atom_mae: 0.000155
train_e/atom_rmse: 0.000197
2025-04-09 02:03:30.288 INFO: train_e/atom_rmse: 0.000197
train_f_mae: 0.005387
2025-04-09 02:03:30.291 INFO: train_f_mae: 0.005387
train_f_rmse: 0.007625
2025-04-09 02:03:30.291 INFO: train_f_rmse: 0.007625
val_e/atom_mae: 0.000253
2025-04-09 02:03:30.293 INFO: val_e/atom_mae: 0.000253
val_e/atom_rmse: 0.000286
2025-04-09 02:03:30.293 INFO: val_e/atom_rmse: 0.000286
val_f_mae: 0.005625
2025-04-09 02:03:30.294 INFO: val_f_mae: 0.005625
val_f_rmse: 0.008017
2025-04-09 02:03:30.294 INFO: val_f_rmse: 0.008017
##### Step: 106 Learning rate: 0.0003125 #####
2025-04-09 02:04:54.800 INFO: ##### Step: 106 Learning rate: 0.0003125 #####
Epoch 67, Train Loss: 0.0594, Val Loss: 0.0614
2025-04-09 02:04:54.801 INFO: Epoch 67, Train Loss: 0.0594, Val Loss: 0.0614
train_e/atom_mae: 0.000224
2025-04-09 02:04:54.801 INFO: train_e/atom_mae: 0.000224
train_e/atom_rmse: 0.000281
2025-04-09 02:04:54.802 INFO: train_e/atom_rmse: 0.000281
train_f_mae: 0.005398
2025-04-09 02:04:54.804 INFO: train_f_mae: 0.005398
train_f_rmse: 0.007645
2025-04-09 02:04:54.804 INFO: train_f_rmse: 0.007645
val_e/atom_mae: 0.000151
2025-04-09 02:04:54.807 INFO: val_e/atom_mae: 0.000151
val_e/atom_rmse: 0.000195
2025-04-09 02:04:54.807 INFO: val_e/atom_rmse: 0.000195
val_f_mae: 0.005513
2025-04-09 02:04:54.808 INFO: val_f_mae: 0.005513
val_f_rmse: 0.007806
2025-04-09 02:04:54.808 INFO: val_f_rmse: 0.007806
##### Step: 107 Learning rate: 0.0003125 #####
2025-04-09 02:06:19.293 INFO: ##### Step: 107 Learning rate: 0.0003125 #####
Epoch 68, Train Loss: 0.0604, Val Loss: 0.0648
2025-04-09 02:06:19.294 INFO: Epoch 68, Train Loss: 0.0604, Val Loss: 0.0648
train_e/atom_mae: 0.000177
2025-04-09 02:06:19.294 INFO: train_e/atom_mae: 0.000177
train_e/atom_rmse: 0.000223
2025-04-09 02:06:19.294 INFO: train_e/atom_rmse: 0.000223
train_f_mae: 0.005446
2025-04-09 02:06:19.297 INFO: train_f_mae: 0.005446
train_f_rmse: 0.007732
2025-04-09 02:06:19.297 INFO: train_f_rmse: 0.007732
val_e/atom_mae: 0.000239
2025-04-09 02:06:19.299 INFO: val_e/atom_mae: 0.000239
val_e/atom_rmse: 0.000297
2025-04-09 02:06:19.300 INFO: val_e/atom_rmse: 0.000297
val_f_mae: 0.005634
2025-04-09 02:06:19.300 INFO: val_f_mae: 0.005634
val_f_rmse: 0.007984
2025-04-09 02:06:19.300 INFO: val_f_rmse: 0.007984
##### Step: 108 Learning rate: 0.0003125 #####
2025-04-09 02:07:43.794 INFO: ##### Step: 108 Learning rate: 0.0003125 #####
Epoch 69, Train Loss: 0.0609, Val Loss: 0.0628
2025-04-09 02:07:43.795 INFO: Epoch 69, Train Loss: 0.0609, Val Loss: 0.0628
train_e/atom_mae: 0.000212
2025-04-09 02:07:43.796 INFO: train_e/atom_mae: 0.000212
train_e/atom_rmse: 0.000264
2025-04-09 02:07:43.796 INFO: train_e/atom_rmse: 0.000264
train_f_mae: 0.005447
2025-04-09 02:07:43.799 INFO: train_f_mae: 0.005447
train_f_rmse: 0.007747
2025-04-09 02:07:43.799 INFO: train_f_rmse: 0.007747
val_e/atom_mae: 0.000117
2025-04-09 02:07:43.801 INFO: val_e/atom_mae: 0.000117
val_e/atom_rmse: 0.000160
2025-04-09 02:07:43.801 INFO: val_e/atom_rmse: 0.000160
val_f_mae: 0.005584
2025-04-09 02:07:43.802 INFO: val_f_mae: 0.005584
val_f_rmse: 0.007903
2025-04-09 02:07:43.802 INFO: val_f_rmse: 0.007903
##### Step: 109 Learning rate: 0.0003125 #####
2025-04-09 02:09:08.295 INFO: ##### Step: 109 Learning rate: 0.0003125 #####
Epoch 70, Train Loss: 0.0596, Val Loss: 0.0625
2025-04-09 02:09:08.295 INFO: Epoch 70, Train Loss: 0.0596, Val Loss: 0.0625
train_e/atom_mae: 0.000202
2025-04-09 02:09:08.296 INFO: train_e/atom_mae: 0.000202
train_e/atom_rmse: 0.000252
2025-04-09 02:09:08.296 INFO: train_e/atom_rmse: 0.000252
train_f_mae: 0.005410
2025-04-09 02:09:08.299 INFO: train_f_mae: 0.005410
train_f_rmse: 0.007671
2025-04-09 02:09:08.299 INFO: train_f_rmse: 0.007671
val_e/atom_mae: 0.000214
2025-04-09 02:09:08.301 INFO: val_e/atom_mae: 0.000214
val_e/atom_rmse: 0.000253
2025-04-09 02:09:08.302 INFO: val_e/atom_rmse: 0.000253
val_f_mae: 0.005520
2025-04-09 02:09:08.302 INFO: val_f_mae: 0.005520
val_f_rmse: 0.007855
2025-04-09 02:09:08.302 INFO: val_f_rmse: 0.007855
##### Step: 110 Learning rate: 0.0003125 #####
2025-04-09 02:10:32.749 INFO: ##### Step: 110 Learning rate: 0.0003125 #####
Epoch 71, Train Loss: 0.0587, Val Loss: 0.0642
2025-04-09 02:10:32.750 INFO: Epoch 71, Train Loss: 0.0587, Val Loss: 0.0642
train_e/atom_mae: 0.000215
2025-04-09 02:10:32.751 INFO: train_e/atom_mae: 0.000215
train_e/atom_rmse: 0.000266
2025-04-09 02:10:32.751 INFO: train_e/atom_rmse: 0.000266
train_f_mae: 0.005369
2025-04-09 02:10:32.754 INFO: train_f_mae: 0.005369
train_f_rmse: 0.007605
2025-04-09 02:10:32.754 INFO: train_f_rmse: 0.007605
val_e/atom_mae: 0.000150
2025-04-09 02:10:32.756 INFO: val_e/atom_mae: 0.000150
val_e/atom_rmse: 0.000174
2025-04-09 02:10:32.756 INFO: val_e/atom_rmse: 0.000174
val_f_mae: 0.005586
2025-04-09 02:10:32.757 INFO: val_f_mae: 0.005586
val_f_rmse: 0.007991
2025-04-09 02:10:32.757 INFO: val_f_rmse: 0.007991
##### Step: 111 Learning rate: 0.0003125 #####
2025-04-09 02:11:57.182 INFO: ##### Step: 111 Learning rate: 0.0003125 #####
Epoch 72, Train Loss: 0.0597, Val Loss: 0.0655
2025-04-09 02:11:57.183 INFO: Epoch 72, Train Loss: 0.0597, Val Loss: 0.0655
train_e/atom_mae: 0.000200
2025-04-09 02:11:57.184 INFO: train_e/atom_mae: 0.000200
train_e/atom_rmse: 0.000252
2025-04-09 02:11:57.184 INFO: train_e/atom_rmse: 0.000252
train_f_mae: 0.005412
2025-04-09 02:11:57.187 INFO: train_f_mae: 0.005412
train_f_rmse: 0.007679
2025-04-09 02:11:57.187 INFO: train_f_rmse: 0.007679
val_e/atom_mae: 0.000436
2025-04-09 02:11:57.189 INFO: val_e/atom_mae: 0.000436
val_e/atom_rmse: 0.000463
2025-04-09 02:11:57.189 INFO: val_e/atom_rmse: 0.000463
val_f_mae: 0.005558
2025-04-09 02:11:57.190 INFO: val_f_mae: 0.005558
val_f_rmse: 0.007929
2025-04-09 02:11:57.190 INFO: val_f_rmse: 0.007929
##### Step: 112 Learning rate: 0.0003125 #####
2025-04-09 02:13:21.637 INFO: ##### Step: 112 Learning rate: 0.0003125 #####
Epoch 73, Train Loss: 0.0595, Val Loss: 0.0600
2025-04-09 02:13:21.637 INFO: Epoch 73, Train Loss: 0.0595, Val Loss: 0.0600
train_e/atom_mae: 0.000214
2025-04-09 02:13:21.638 INFO: train_e/atom_mae: 0.000214
train_e/atom_rmse: 0.000268
2025-04-09 02:13:21.638 INFO: train_e/atom_rmse: 0.000268
train_f_mae: 0.005395
2025-04-09 02:13:21.641 INFO: train_f_mae: 0.005395
train_f_rmse: 0.007660
2025-04-09 02:13:21.641 INFO: train_f_rmse: 0.007660
val_e/atom_mae: 0.000197
2025-04-09 02:13:21.643 INFO: val_e/atom_mae: 0.000197
val_e/atom_rmse: 0.000232
2025-04-09 02:13:21.644 INFO: val_e/atom_rmse: 0.000232
val_f_mae: 0.005445
2025-04-09 02:13:21.644 INFO: val_f_mae: 0.005445
val_f_rmse: 0.007706
2025-04-09 02:13:21.644 INFO: val_f_rmse: 0.007706
##### Step: 113 Learning rate: 0.0003125 #####
2025-04-09 02:14:46.077 INFO: ##### Step: 113 Learning rate: 0.0003125 #####
Epoch 74, Train Loss: 0.0592, Val Loss: 0.0610
2025-04-09 02:14:46.078 INFO: Epoch 74, Train Loss: 0.0592, Val Loss: 0.0610
train_e/atom_mae: 0.000178
2025-04-09 02:14:46.079 INFO: train_e/atom_mae: 0.000178
train_e/atom_rmse: 0.000225
2025-04-09 02:14:46.079 INFO: train_e/atom_rmse: 0.000225
train_f_mae: 0.005400
2025-04-09 02:14:46.081 INFO: train_f_mae: 0.005400
train_f_rmse: 0.007654
2025-04-09 02:14:46.082 INFO: train_f_rmse: 0.007654
val_e/atom_mae: 0.000132
2025-04-09 02:14:46.084 INFO: val_e/atom_mae: 0.000132
val_e/atom_rmse: 0.000159
2025-04-09 02:14:46.084 INFO: val_e/atom_rmse: 0.000159
val_f_mae: 0.005464
2025-04-09 02:14:46.085 INFO: val_f_mae: 0.005464
val_f_rmse: 0.007789
2025-04-09 02:14:46.085 INFO: val_f_rmse: 0.007789
##### Step: 114 Learning rate: 0.0003125 #####
2025-04-09 02:16:10.532 INFO: ##### Step: 114 Learning rate: 0.0003125 #####
Epoch 75, Train Loss: 0.0580, Val Loss: 0.0636
2025-04-09 02:16:10.533 INFO: Epoch 75, Train Loss: 0.0580, Val Loss: 0.0636
train_e/atom_mae: 0.000169
2025-04-09 02:16:10.533 INFO: train_e/atom_mae: 0.000169
train_e/atom_rmse: 0.000213
2025-04-09 02:16:10.534 INFO: train_e/atom_rmse: 0.000213
train_f_mae: 0.005349
2025-04-09 02:16:10.536 INFO: train_f_mae: 0.005349
train_f_rmse: 0.007580
2025-04-09 02:16:10.536 INFO: train_f_rmse: 0.007580
val_e/atom_mae: 0.000343
2025-04-09 02:16:10.539 INFO: val_e/atom_mae: 0.000343
val_e/atom_rmse: 0.000378
2025-04-09 02:16:10.539 INFO: val_e/atom_rmse: 0.000378
val_f_mae: 0.005530
2025-04-09 02:16:10.540 INFO: val_f_mae: 0.005530
val_f_rmse: 0.007864
2025-04-09 02:16:10.540 INFO: val_f_rmse: 0.007864
##### Step: 115 Learning rate: 0.0003125 #####
2025-04-09 02:17:35.022 INFO: ##### Step: 115 Learning rate: 0.0003125 #####
Epoch 76, Train Loss: 0.0596, Val Loss: 0.0636
2025-04-09 02:17:35.023 INFO: Epoch 76, Train Loss: 0.0596, Val Loss: 0.0636
train_e/atom_mae: 0.000227
2025-04-09 02:17:35.024 INFO: train_e/atom_mae: 0.000227
train_e/atom_rmse: 0.000281
2025-04-09 02:17:35.024 INFO: train_e/atom_rmse: 0.000281
train_f_mae: 0.005400
2025-04-09 02:17:35.026 INFO: train_f_mae: 0.005400
train_f_rmse: 0.007656
2025-04-09 02:17:35.027 INFO: train_f_rmse: 0.007656
val_e/atom_mae: 0.000467
2025-04-09 02:17:35.029 INFO: val_e/atom_mae: 0.000467
val_e/atom_rmse: 0.000485
2025-04-09 02:17:35.029 INFO: val_e/atom_rmse: 0.000485
val_f_mae: 0.005518
2025-04-09 02:17:35.029 INFO: val_f_mae: 0.005518
val_f_rmse: 0.007794
2025-04-09 02:17:35.030 INFO: val_f_rmse: 0.007794
##### Step: 116 Learning rate: 0.0003125 #####
2025-04-09 02:18:59.487 INFO: ##### Step: 116 Learning rate: 0.0003125 #####
Epoch 77, Train Loss: 0.0585, Val Loss: 0.0617
2025-04-09 02:18:59.487 INFO: Epoch 77, Train Loss: 0.0585, Val Loss: 0.0617
train_e/atom_mae: 0.000186
2025-04-09 02:18:59.488 INFO: train_e/atom_mae: 0.000186
train_e/atom_rmse: 0.000235
2025-04-09 02:18:59.488 INFO: train_e/atom_rmse: 0.000235
train_f_mae: 0.005366
2025-04-09 02:18:59.491 INFO: train_f_mae: 0.005366
train_f_rmse: 0.007604
2025-04-09 02:18:59.491 INFO: train_f_rmse: 0.007604
val_e/atom_mae: 0.000126
2025-04-09 02:18:59.493 INFO: val_e/atom_mae: 0.000126
val_e/atom_rmse: 0.000152
2025-04-09 02:18:59.494 INFO: val_e/atom_rmse: 0.000152
val_f_mae: 0.005491
2025-04-09 02:18:59.494 INFO: val_f_mae: 0.005491
val_f_rmse: 0.007836
2025-04-09 02:18:59.494 INFO: val_f_rmse: 0.007836
##### Step: 117 Learning rate: 0.0003125 #####
2025-04-09 02:20:23.959 INFO: ##### Step: 117 Learning rate: 0.0003125 #####
Epoch 78, Train Loss: 0.0600, Val Loss: 0.0623
2025-04-09 02:20:23.959 INFO: Epoch 78, Train Loss: 0.0600, Val Loss: 0.0623
train_e/atom_mae: 0.000193
2025-04-09 02:20:23.960 INFO: train_e/atom_mae: 0.000193
train_e/atom_rmse: 0.000246
2025-04-09 02:20:23.960 INFO: train_e/atom_rmse: 0.000246
train_f_mae: 0.005421
2025-04-09 02:20:23.963 INFO: train_f_mae: 0.005421
train_f_rmse: 0.007700
2025-04-09 02:20:23.963 INFO: train_f_rmse: 0.007700
val_e/atom_mae: 0.000241
2025-04-09 02:20:23.965 INFO: val_e/atom_mae: 0.000241
val_e/atom_rmse: 0.000282
2025-04-09 02:20:23.965 INFO: val_e/atom_rmse: 0.000282
val_f_mae: 0.005470
2025-04-09 02:20:23.966 INFO: val_f_mae: 0.005470
val_f_rmse: 0.007833
2025-04-09 02:20:23.966 INFO: val_f_rmse: 0.007833
##### Step: 118 Learning rate: 0.0003125 #####
2025-04-09 02:21:48.417 INFO: ##### Step: 118 Learning rate: 0.0003125 #####
Epoch 79, Train Loss: 0.0580, Val Loss: 0.0594
2025-04-09 02:21:48.418 INFO: Epoch 79, Train Loss: 0.0580, Val Loss: 0.0594
train_e/atom_mae: 0.000178
2025-04-09 02:21:48.419 INFO: train_e/atom_mae: 0.000178
train_e/atom_rmse: 0.000224
2025-04-09 02:21:48.419 INFO: train_e/atom_rmse: 0.000224
train_f_mae: 0.005355
2025-04-09 02:21:48.422 INFO: train_f_mae: 0.005355
train_f_rmse: 0.007576
2025-04-09 02:21:48.422 INFO: train_f_rmse: 0.007576
val_e/atom_mae: 0.000200
2025-04-09 02:21:48.424 INFO: val_e/atom_mae: 0.000200
val_e/atom_rmse: 0.000240
2025-04-09 02:21:48.425 INFO: val_e/atom_rmse: 0.000240
val_f_mae: 0.005408
2025-04-09 02:21:48.425 INFO: val_f_mae: 0.005408
val_f_rmse: 0.007659
2025-04-09 02:21:48.425 INFO: val_f_rmse: 0.007659
##### Step: 119 Learning rate: 0.0003125 #####
2025-04-09 02:23:12.876 INFO: ##### Step: 119 Learning rate: 0.0003125 #####
Epoch 80, Train Loss: 0.0586, Val Loss: 0.0594
2025-04-09 02:23:12.877 INFO: Epoch 80, Train Loss: 0.0586, Val Loss: 0.0594
train_e/atom_mae: 0.000162
2025-04-09 02:23:12.878 INFO: train_e/atom_mae: 0.000162
train_e/atom_rmse: 0.000207
2025-04-09 02:23:12.878 INFO: train_e/atom_rmse: 0.000207
train_f_mae: 0.005379
2025-04-09 02:23:12.880 INFO: train_f_mae: 0.005379
train_f_rmse: 0.007623
2025-04-09 02:23:12.881 INFO: train_f_rmse: 0.007623
val_e/atom_mae: 0.000140
2025-04-09 02:23:12.883 INFO: val_e/atom_mae: 0.000140
val_e/atom_rmse: 0.000177
2025-04-09 02:23:12.883 INFO: val_e/atom_rmse: 0.000177
val_f_mae: 0.005419
2025-04-09 02:23:12.884 INFO: val_f_mae: 0.005419
val_f_rmse: 0.007683
2025-04-09 02:23:12.884 INFO: val_f_rmse: 0.007683
##### Step: 120 Learning rate: 0.00015625 #####
2025-04-09 02:24:37.340 INFO: ##### Step: 120 Learning rate: 0.00015625 #####
Epoch 81, Train Loss: 0.0561, Val Loss: 0.0600
2025-04-09 02:24:37.340 INFO: Epoch 81, Train Loss: 0.0561, Val Loss: 0.0600
train_e/atom_mae: 0.000164
2025-04-09 02:24:37.341 INFO: train_e/atom_mae: 0.000164
train_e/atom_rmse: 0.000206
2025-04-09 02:24:37.341 INFO: train_e/atom_rmse: 0.000206
train_f_mae: 0.005280
2025-04-09 02:24:37.344 INFO: train_f_mae: 0.005280
train_f_rmse: 0.007459
2025-04-09 02:24:37.344 INFO: train_f_rmse: 0.007459
val_e/atom_mae: 0.000227
2025-04-09 02:24:37.346 INFO: val_e/atom_mae: 0.000227
val_e/atom_rmse: 0.000264
2025-04-09 02:24:37.347 INFO: val_e/atom_rmse: 0.000264
val_f_mae: 0.005429
2025-04-09 02:24:37.347 INFO: val_f_mae: 0.005429
val_f_rmse: 0.007690
2025-04-09 02:24:37.347 INFO: val_f_rmse: 0.007690
##### Step: 121 Learning rate: 0.00015625 #####
2025-04-09 02:26:01.763 INFO: ##### Step: 121 Learning rate: 0.00015625 #####
Epoch 82, Train Loss: 0.0568, Val Loss: 0.0599
2025-04-09 02:26:01.763 INFO: Epoch 82, Train Loss: 0.0568, Val Loss: 0.0599
train_e/atom_mae: 0.000147
2025-04-09 02:26:01.764 INFO: train_e/atom_mae: 0.000147
train_e/atom_rmse: 0.000187
2025-04-09 02:26:01.764 INFO: train_e/atom_rmse: 0.000187
train_f_mae: 0.005308
2025-04-09 02:26:01.767 INFO: train_f_mae: 0.005308
train_f_rmse: 0.007508
2025-04-09 02:26:01.767 INFO: train_f_rmse: 0.007508
val_e/atom_mae: 0.000108
2025-04-09 02:26:01.769 INFO: val_e/atom_mae: 0.000108
val_e/atom_rmse: 0.000147
2025-04-09 02:26:01.770 INFO: val_e/atom_rmse: 0.000147
val_f_mae: 0.005436
2025-04-09 02:26:01.770 INFO: val_f_mae: 0.005436
val_f_rmse: 0.007724
2025-04-09 02:26:01.770 INFO: val_f_rmse: 0.007724
##### Step: 122 Learning rate: 0.00015625 #####
2025-04-09 02:27:26.215 INFO: ##### Step: 122 Learning rate: 0.00015625 #####
Epoch 83, Train Loss: 0.0559, Val Loss: 0.0586
2025-04-09 02:27:26.215 INFO: Epoch 83, Train Loss: 0.0559, Val Loss: 0.0586
train_e/atom_mae: 0.000137
2025-04-09 02:27:26.216 INFO: train_e/atom_mae: 0.000137
train_e/atom_rmse: 0.000174
2025-04-09 02:27:26.216 INFO: train_e/atom_rmse: 0.000174
train_f_mae: 0.005275
2025-04-09 02:27:26.219 INFO: train_f_mae: 0.005275
train_f_rmse: 0.007449
2025-04-09 02:27:26.219 INFO: train_f_rmse: 0.007449
val_e/atom_mae: 0.000106
2025-04-09 02:27:26.221 INFO: val_e/atom_mae: 0.000106
val_e/atom_rmse: 0.000135
2025-04-09 02:27:26.221 INFO: val_e/atom_rmse: 0.000135
val_f_mae: 0.005393
2025-04-09 02:27:26.222 INFO: val_f_mae: 0.005393
val_f_rmse: 0.007640
2025-04-09 02:27:26.222 INFO: val_f_rmse: 0.007640
##### Step: 123 Learning rate: 0.00015625 #####
2025-04-09 02:28:50.718 INFO: ##### Step: 123 Learning rate: 0.00015625 #####
Epoch 84, Train Loss: 0.0563, Val Loss: 0.0588
2025-04-09 02:28:50.718 INFO: Epoch 84, Train Loss: 0.0563, Val Loss: 0.0588
train_e/atom_mae: 0.000139
2025-04-09 02:28:50.719 INFO: train_e/atom_mae: 0.000139
train_e/atom_rmse: 0.000178
2025-04-09 02:28:50.719 INFO: train_e/atom_rmse: 0.000178
train_f_mae: 0.005290
2025-04-09 02:28:50.722 INFO: train_f_mae: 0.005290
train_f_rmse: 0.007481
2025-04-09 02:28:50.722 INFO: train_f_rmse: 0.007481
val_e/atom_mae: 0.000180
2025-04-09 02:28:50.724 INFO: val_e/atom_mae: 0.000180
val_e/atom_rmse: 0.000216
2025-04-09 02:28:50.725 INFO: val_e/atom_rmse: 0.000216
val_f_mae: 0.005394
2025-04-09 02:28:50.725 INFO: val_f_mae: 0.005394
val_f_rmse: 0.007633
2025-04-09 02:28:50.725 INFO: val_f_rmse: 0.007633
##### Step: 124 Learning rate: 0.00015625 #####
2025-04-09 02:30:15.181 INFO: ##### Step: 124 Learning rate: 0.00015625 #####
Epoch 85, Train Loss: 0.0560, Val Loss: 0.0592
2025-04-09 02:30:15.181 INFO: Epoch 85, Train Loss: 0.0560, Val Loss: 0.0592
train_e/atom_mae: 0.000135
2025-04-09 02:30:15.182 INFO: train_e/atom_mae: 0.000135
train_e/atom_rmse: 0.000173
2025-04-09 02:30:15.182 INFO: train_e/atom_rmse: 0.000173
train_f_mae: 0.005276
2025-04-09 02:30:15.185 INFO: train_f_mae: 0.005276
train_f_rmse: 0.007462
2025-04-09 02:30:15.185 INFO: train_f_rmse: 0.007462
val_e/atom_mae: 0.000127
2025-04-09 02:30:15.187 INFO: val_e/atom_mae: 0.000127
val_e/atom_rmse: 0.000163
2025-04-09 02:30:15.187 INFO: val_e/atom_rmse: 0.000163
val_f_mae: 0.005416
2025-04-09 02:30:15.188 INFO: val_f_mae: 0.005416
val_f_rmse: 0.007676
2025-04-09 02:30:15.188 INFO: val_f_rmse: 0.007676
##### Step: 125 Learning rate: 0.00015625 #####
2025-04-09 02:31:39.632 INFO: ##### Step: 125 Learning rate: 0.00015625 #####
Epoch 86, Train Loss: 0.0563, Val Loss: 0.0585
2025-04-09 02:31:39.633 INFO: Epoch 86, Train Loss: 0.0563, Val Loss: 0.0585
train_e/atom_mae: 0.000148
2025-04-09 02:31:39.634 INFO: train_e/atom_mae: 0.000148
train_e/atom_rmse: 0.000188
2025-04-09 02:31:39.634 INFO: train_e/atom_rmse: 0.000188
train_f_mae: 0.005287
2025-04-09 02:31:39.636 INFO: train_f_mae: 0.005287
train_f_rmse: 0.007472
2025-04-09 02:31:39.637 INFO: train_f_rmse: 0.007472
val_e/atom_mae: 0.000139
2025-04-09 02:31:39.639 INFO: val_e/atom_mae: 0.000139
val_e/atom_rmse: 0.000180
2025-04-09 02:31:39.639 INFO: val_e/atom_rmse: 0.000180
val_f_mae: 0.005388
2025-04-09 02:31:39.640 INFO: val_f_mae: 0.005388
val_f_rmse: 0.007622
2025-04-09 02:31:39.640 INFO: val_f_rmse: 0.007622
##### Step: 126 Learning rate: 0.00015625 #####
2025-04-09 02:33:04.107 INFO: ##### Step: 126 Learning rate: 0.00015625 #####
Epoch 87, Train Loss: 0.0555, Val Loss: 0.0580
2025-04-09 02:33:04.107 INFO: Epoch 87, Train Loss: 0.0555, Val Loss: 0.0580
train_e/atom_mae: 0.000134
2025-04-09 02:33:04.108 INFO: train_e/atom_mae: 0.000134
train_e/atom_rmse: 0.000169
2025-04-09 02:33:04.108 INFO: train_e/atom_rmse: 0.000169
train_f_mae: 0.005260
2025-04-09 02:33:04.111 INFO: train_f_mae: 0.005260
train_f_rmse: 0.007426
2025-04-09 02:33:04.111 INFO: train_f_rmse: 0.007426
val_e/atom_mae: 0.000121
2025-04-09 02:33:04.113 INFO: val_e/atom_mae: 0.000121
val_e/atom_rmse: 0.000150
2025-04-09 02:33:04.113 INFO: val_e/atom_rmse: 0.000150
val_f_mae: 0.005365
2025-04-09 02:33:04.114 INFO: val_f_mae: 0.005365
val_f_rmse: 0.007595
2025-04-09 02:33:04.114 INFO: val_f_rmse: 0.007595
##### Step: 127 Learning rate: 0.00015625 #####
2025-04-09 02:34:28.581 INFO: ##### Step: 127 Learning rate: 0.00015625 #####
Epoch 88, Train Loss: 0.0556, Val Loss: 0.0585
2025-04-09 02:34:28.581 INFO: Epoch 88, Train Loss: 0.0556, Val Loss: 0.0585
train_e/atom_mae: 0.000135
2025-04-09 02:34:28.582 INFO: train_e/atom_mae: 0.000135
train_e/atom_rmse: 0.000174
2025-04-09 02:34:28.582 INFO: train_e/atom_rmse: 0.000174
train_f_mae: 0.005262
2025-04-09 02:34:28.585 INFO: train_f_mae: 0.005262
train_f_rmse: 0.007431
2025-04-09 02:34:28.585 INFO: train_f_rmse: 0.007431
val_e/atom_mae: 0.000154
2025-04-09 02:34:28.587 INFO: val_e/atom_mae: 0.000154
val_e/atom_rmse: 0.000189
2025-04-09 02:34:28.588 INFO: val_e/atom_rmse: 0.000189
val_f_mae: 0.005381
2025-04-09 02:34:28.588 INFO: val_f_mae: 0.005381
val_f_rmse: 0.007620
2025-04-09 02:34:28.588 INFO: val_f_rmse: 0.007620
##### Step: 128 Learning rate: 0.00015625 #####
2025-04-09 02:35:53.079 INFO: ##### Step: 128 Learning rate: 0.00015625 #####
Epoch 89, Train Loss: 0.0567, Val Loss: 0.0628
2025-04-09 02:35:53.079 INFO: Epoch 89, Train Loss: 0.0567, Val Loss: 0.0628
train_e/atom_mae: 0.000150
2025-04-09 02:35:53.080 INFO: train_e/atom_mae: 0.000150
train_e/atom_rmse: 0.000190
2025-04-09 02:35:53.080 INFO: train_e/atom_rmse: 0.000190
train_f_mae: 0.005302
2025-04-09 02:35:53.083 INFO: train_f_mae: 0.005302
train_f_rmse: 0.007500
2025-04-09 02:35:53.083 INFO: train_f_rmse: 0.007500
val_e/atom_mae: 0.000144
2025-04-09 02:35:53.086 INFO: val_e/atom_mae: 0.000144
val_e/atom_rmse: 0.000188
2025-04-09 02:35:53.086 INFO: val_e/atom_rmse: 0.000188
val_f_mae: 0.005519
2025-04-09 02:35:53.087 INFO: val_f_mae: 0.005519
val_f_rmse: 0.007895
2025-04-09 02:35:53.087 INFO: val_f_rmse: 0.007895
##### Step: 129 Learning rate: 0.00015625 #####
2025-04-09 02:37:17.574 INFO: ##### Step: 129 Learning rate: 0.00015625 #####
Epoch 90, Train Loss: 0.0561, Val Loss: 0.0586
2025-04-09 02:37:17.575 INFO: Epoch 90, Train Loss: 0.0561, Val Loss: 0.0586
train_e/atom_mae: 0.000151
2025-04-09 02:37:17.576 INFO: train_e/atom_mae: 0.000151
train_e/atom_rmse: 0.000192
2025-04-09 02:37:17.576 INFO: train_e/atom_rmse: 0.000192
train_f_mae: 0.005279
2025-04-09 02:37:17.578 INFO: train_f_mae: 0.005279
train_f_rmse: 0.007463
2025-04-09 02:37:17.578 INFO: train_f_rmse: 0.007463
val_e/atom_mae: 0.000156
2025-04-09 02:37:17.581 INFO: val_e/atom_mae: 0.000156
val_e/atom_rmse: 0.000196
2025-04-09 02:37:17.581 INFO: val_e/atom_rmse: 0.000196
val_f_mae: 0.005376
2025-04-09 02:37:17.581 INFO: val_f_mae: 0.005376
val_f_rmse: 0.007623
2025-04-09 02:37:17.582 INFO: val_f_rmse: 0.007623
##### Step: 130 Learning rate: 0.00015625 #####
2025-04-09 02:38:42.099 INFO: ##### Step: 130 Learning rate: 0.00015625 #####
Epoch 91, Train Loss: 0.0561, Val Loss: 0.0600
2025-04-09 02:38:42.100 INFO: Epoch 91, Train Loss: 0.0561, Val Loss: 0.0600
train_e/atom_mae: 0.000143
2025-04-09 02:38:42.100 INFO: train_e/atom_mae: 0.000143
train_e/atom_rmse: 0.000181
2025-04-09 02:38:42.100 INFO: train_e/atom_rmse: 0.000181
train_f_mae: 0.005280
2025-04-09 02:38:42.103 INFO: train_f_mae: 0.005280
train_f_rmse: 0.007464
2025-04-09 02:38:42.103 INFO: train_f_rmse: 0.007464
val_e/atom_mae: 0.000230
2025-04-09 02:38:42.105 INFO: val_e/atom_mae: 0.000230
val_e/atom_rmse: 0.000265
2025-04-09 02:38:42.106 INFO: val_e/atom_rmse: 0.000265
val_f_mae: 0.005426
2025-04-09 02:38:42.106 INFO: val_f_mae: 0.005426
val_f_rmse: 0.007690
2025-04-09 02:38:42.106 INFO: val_f_rmse: 0.007690
##### Step: 131 Learning rate: 0.00015625 #####
2025-04-09 02:40:06.599 INFO: ##### Step: 131 Learning rate: 0.00015625 #####
Epoch 92, Train Loss: 0.0562, Val Loss: 0.0589
2025-04-09 02:40:06.600 INFO: Epoch 92, Train Loss: 0.0562, Val Loss: 0.0589
train_e/atom_mae: 0.000168
2025-04-09 02:40:06.601 INFO: train_e/atom_mae: 0.000168
train_e/atom_rmse: 0.000215
2025-04-09 02:40:06.601 INFO: train_e/atom_rmse: 0.000215
train_f_mae: 0.005273
2025-04-09 02:40:06.603 INFO: train_f_mae: 0.005273
train_f_rmse: 0.007457
2025-04-09 02:40:06.603 INFO: train_f_rmse: 0.007457
val_e/atom_mae: 0.000118
2025-04-09 02:40:06.606 INFO: val_e/atom_mae: 0.000118
val_e/atom_rmse: 0.000150
2025-04-09 02:40:06.606 INFO: val_e/atom_rmse: 0.000150
val_f_mae: 0.005398
2025-04-09 02:40:06.606 INFO: val_f_mae: 0.005398
val_f_rmse: 0.007654
2025-04-09 02:40:06.607 INFO: val_f_rmse: 0.007654
##### Step: 132 Learning rate: 0.00015625 #####
2025-04-09 02:41:31.065 INFO: ##### Step: 132 Learning rate: 0.00015625 #####
Epoch 93, Train Loss: 0.0555, Val Loss: 0.0581
2025-04-09 02:41:31.066 INFO: Epoch 93, Train Loss: 0.0555, Val Loss: 0.0581
train_e/atom_mae: 0.000138
2025-04-09 02:41:31.067 INFO: train_e/atom_mae: 0.000138
train_e/atom_rmse: 0.000176
2025-04-09 02:41:31.067 INFO: train_e/atom_rmse: 0.000176
train_f_mae: 0.005254
2025-04-09 02:41:31.069 INFO: train_f_mae: 0.005254
train_f_rmse: 0.007423
2025-04-09 02:41:31.069 INFO: train_f_rmse: 0.007423
val_e/atom_mae: 0.000188
2025-04-09 02:41:31.072 INFO: val_e/atom_mae: 0.000188
val_e/atom_rmse: 0.000223
2025-04-09 02:41:31.072 INFO: val_e/atom_rmse: 0.000223
val_f_mae: 0.005358
2025-04-09 02:41:31.072 INFO: val_f_mae: 0.005358
val_f_rmse: 0.007586
2025-04-09 02:41:31.073 INFO: val_f_rmse: 0.007586
##### Step: 133 Learning rate: 0.00015625 #####
2025-04-09 02:42:55.532 INFO: ##### Step: 133 Learning rate: 0.00015625 #####
Epoch 94, Train Loss: 0.0560, Val Loss: 0.0579
2025-04-09 02:42:55.533 INFO: Epoch 94, Train Loss: 0.0560, Val Loss: 0.0579
train_e/atom_mae: 0.000137
2025-04-09 02:42:55.534 INFO: train_e/atom_mae: 0.000137
train_e/atom_rmse: 0.000174
2025-04-09 02:42:55.534 INFO: train_e/atom_rmse: 0.000174
train_f_mae: 0.005276
2025-04-09 02:42:55.537 INFO: train_f_mae: 0.005276
train_f_rmse: 0.007460
2025-04-09 02:42:55.537 INFO: train_f_rmse: 0.007460
val_e/atom_mae: 0.000118
2025-04-09 02:42:55.539 INFO: val_e/atom_mae: 0.000118
val_e/atom_rmse: 0.000154
2025-04-09 02:42:55.540 INFO: val_e/atom_rmse: 0.000154
val_f_mae: 0.005360
2025-04-09 02:42:55.540 INFO: val_f_mae: 0.005360
val_f_rmse: 0.007590
2025-04-09 02:42:55.540 INFO: val_f_rmse: 0.007590
##### Step: 134 Learning rate: 0.00015625 #####
2025-04-09 02:44:20.014 INFO: ##### Step: 134 Learning rate: 0.00015625 #####
Epoch 95, Train Loss: 0.0555, Val Loss: 0.0592
2025-04-09 02:44:20.015 INFO: Epoch 95, Train Loss: 0.0555, Val Loss: 0.0592
train_e/atom_mae: 0.000138
2025-04-09 02:44:20.015 INFO: train_e/atom_mae: 0.000138
train_e/atom_rmse: 0.000174
2025-04-09 02:44:20.015 INFO: train_e/atom_rmse: 0.000174
train_f_mae: 0.005260
2025-04-09 02:44:20.018 INFO: train_f_mae: 0.005260
train_f_rmse: 0.007425
2025-04-09 02:44:20.018 INFO: train_f_rmse: 0.007425
val_e/atom_mae: 0.000133
2025-04-09 02:44:20.020 INFO: val_e/atom_mae: 0.000133
val_e/atom_rmse: 0.000185
2025-04-09 02:44:20.021 INFO: val_e/atom_rmse: 0.000185
val_f_mae: 0.005409
2025-04-09 02:44:20.021 INFO: val_f_mae: 0.005409
val_f_rmse: 0.007665
2025-04-09 02:44:20.021 INFO: val_f_rmse: 0.007665
##### Step: 135 Learning rate: 0.00015625 #####
2025-04-09 02:45:44.514 INFO: ##### Step: 135 Learning rate: 0.00015625 #####
Epoch 96, Train Loss: 0.0556, Val Loss: 0.0588
2025-04-09 02:45:44.515 INFO: Epoch 96, Train Loss: 0.0556, Val Loss: 0.0588
train_e/atom_mae: 0.000154
2025-04-09 02:45:44.516 INFO: train_e/atom_mae: 0.000154
train_e/atom_rmse: 0.000193
2025-04-09 02:45:44.516 INFO: train_e/atom_rmse: 0.000193
train_f_mae: 0.005262
2025-04-09 02:45:44.519 INFO: train_f_mae: 0.005262
train_f_rmse: 0.007427
2025-04-09 02:45:44.519 INFO: train_f_rmse: 0.007427
val_e/atom_mae: 0.000141
2025-04-09 02:45:44.521 INFO: val_e/atom_mae: 0.000141
val_e/atom_rmse: 0.000180
2025-04-09 02:45:44.521 INFO: val_e/atom_rmse: 0.000180
val_f_mae: 0.005377
2025-04-09 02:45:44.522 INFO: val_f_mae: 0.005377
val_f_rmse: 0.007642
2025-04-09 02:45:44.522 INFO: val_f_rmse: 0.007642
##### Step: 136 Learning rate: 0.00015625 #####
2025-04-09 02:47:08.981 INFO: ##### Step: 136 Learning rate: 0.00015625 #####
Epoch 97, Train Loss: 0.0557, Val Loss: 0.0615
2025-04-09 02:47:08.981 INFO: Epoch 97, Train Loss: 0.0557, Val Loss: 0.0615
train_e/atom_mae: 0.000145
2025-04-09 02:47:08.982 INFO: train_e/atom_mae: 0.000145
train_e/atom_rmse: 0.000182
2025-04-09 02:47:08.982 INFO: train_e/atom_rmse: 0.000182
train_f_mae: 0.005261
2025-04-09 02:47:08.985 INFO: train_f_mae: 0.005261
train_f_rmse: 0.007434
2025-04-09 02:47:08.985 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000197
2025-04-09 02:47:08.987 INFO: val_e/atom_mae: 0.000197
val_e/atom_rmse: 0.000226
2025-04-09 02:47:08.988 INFO: val_e/atom_rmse: 0.000226
val_f_mae: 0.005499
2025-04-09 02:47:08.988 INFO: val_f_mae: 0.005499
val_f_rmse: 0.007804
2025-04-09 02:47:08.988 INFO: val_f_rmse: 0.007804
##### Step: 137 Learning rate: 0.00015625 #####
2025-04-09 02:48:33.458 INFO: ##### Step: 137 Learning rate: 0.00015625 #####
Epoch 98, Train Loss: 0.0565, Val Loss: 0.0587
2025-04-09 02:48:33.459 INFO: Epoch 98, Train Loss: 0.0565, Val Loss: 0.0587
train_e/atom_mae: 0.000155
2025-04-09 02:48:33.460 INFO: train_e/atom_mae: 0.000155
train_e/atom_rmse: 0.000197
2025-04-09 02:48:33.460 INFO: train_e/atom_rmse: 0.000197
train_f_mae: 0.005289
2025-04-09 02:48:33.462 INFO: train_f_mae: 0.005289
train_f_rmse: 0.007485
2025-04-09 02:48:33.462 INFO: train_f_rmse: 0.007485
val_e/atom_mae: 0.000138
2025-04-09 02:48:33.465 INFO: val_e/atom_mae: 0.000138
val_e/atom_rmse: 0.000176
2025-04-09 02:48:33.465 INFO: val_e/atom_rmse: 0.000176
val_f_mae: 0.005379
2025-04-09 02:48:33.465 INFO: val_f_mae: 0.005379
val_f_rmse: 0.007638
2025-04-09 02:48:33.466 INFO: val_f_rmse: 0.007638
##### Step: 138 Learning rate: 0.00015625 #####
2025-04-09 02:49:57.958 INFO: ##### Step: 138 Learning rate: 0.00015625 #####
Epoch 99, Train Loss: 0.0561, Val Loss: 0.0593
2025-04-09 02:49:57.959 INFO: Epoch 99, Train Loss: 0.0561, Val Loss: 0.0593
train_e/atom_mae: 0.000182
2025-04-09 02:49:57.960 INFO: train_e/atom_mae: 0.000182
train_e/atom_rmse: 0.000226
2025-04-09 02:49:57.960 INFO: train_e/atom_rmse: 0.000226
train_f_mae: 0.005271
2025-04-09 02:49:57.963 INFO: train_f_mae: 0.005271
train_f_rmse: 0.007448
2025-04-09 02:49:57.963 INFO: train_f_rmse: 0.007448
val_e/atom_mae: 0.000158
2025-04-09 02:49:57.965 INFO: val_e/atom_mae: 0.000158
val_e/atom_rmse: 0.000189
2025-04-09 02:49:57.965 INFO: val_e/atom_rmse: 0.000189
val_f_mae: 0.005411
2025-04-09 02:49:57.966 INFO: val_f_mae: 0.005411
val_f_rmse: 0.007670
2025-04-09 02:49:57.966 INFO: val_f_rmse: 0.007670
##### Step: 139 Learning rate: 0.00015625 #####
2025-04-09 02:51:22.452 INFO: ##### Step: 139 Learning rate: 0.00015625 #####
Epoch 100, Train Loss: 0.0558, Val Loss: 0.0589
2025-04-09 02:51:22.452 INFO: Epoch 100, Train Loss: 0.0558, Val Loss: 0.0589
train_e/atom_mae: 0.000151
2025-04-09 02:51:22.453 INFO: train_e/atom_mae: 0.000151
train_e/atom_rmse: 0.000190
2025-04-09 02:51:22.453 INFO: train_e/atom_rmse: 0.000190
train_f_mae: 0.005265
2025-04-09 02:51:22.456 INFO: train_f_mae: 0.005265
train_f_rmse: 0.007443
2025-04-09 02:51:22.456 INFO: train_f_rmse: 0.007443
val_e/atom_mae: 0.000140
2025-04-09 02:51:22.458 INFO: val_e/atom_mae: 0.000140
val_e/atom_rmse: 0.000177
2025-04-09 02:51:22.459 INFO: val_e/atom_rmse: 0.000177
val_f_mae: 0.005389
2025-04-09 02:51:22.459 INFO: val_f_mae: 0.005389
val_f_rmse: 0.007651
2025-04-09 02:51:22.459 INFO: val_f_rmse: 0.007651
2025-04-09 02:51:22.492 INFO: Third train loop:
##### Step: 140 Learning rate: 7.8125e-05 #####
2025-04-09 02:52:46.974 INFO: ##### Step: 140 Learning rate: 7.8125e-05 #####
Epoch 1, Train Loss: 0.0565, Val Loss: 0.0587
2025-04-09 02:52:46.974 INFO: Epoch 1, Train Loss: 0.0565, Val Loss: 0.0587
train_e/atom_mae: 0.000104
2025-04-09 02:52:46.975 INFO: train_e/atom_mae: 0.000104
train_e/atom_rmse: 0.000136
2025-04-09 02:52:46.975 INFO: train_e/atom_rmse: 0.000136
train_f_mae: 0.005221
2025-04-09 02:52:46.978 INFO: train_f_mae: 0.005221
train_f_rmse: 0.007368
2025-04-09 02:52:46.978 INFO: train_f_rmse: 0.007368
val_e/atom_mae: 0.000083
2025-04-09 02:52:46.980 INFO: val_e/atom_mae: 0.000083
val_e/atom_rmse: 0.000114
2025-04-09 02:52:46.980 INFO: val_e/atom_rmse: 0.000114
val_f_mae: 0.005331
2025-04-09 02:52:46.981 INFO: val_f_mae: 0.005331
val_f_rmse: 0.007559
2025-04-09 02:52:46.981 INFO: val_f_rmse: 0.007559
##### Step: 141 Learning rate: 7.8125e-05 #####
2025-04-09 02:54:11.426 INFO: ##### Step: 141 Learning rate: 7.8125e-05 #####
Epoch 2, Train Loss: 0.0563, Val Loss: 0.0597
2025-04-09 02:54:11.427 INFO: Epoch 2, Train Loss: 0.0563, Val Loss: 0.0597
train_e/atom_mae: 0.000105
2025-04-09 02:54:11.428 INFO: train_e/atom_mae: 0.000105
train_e/atom_rmse: 0.000134
2025-04-09 02:54:11.428 INFO: train_e/atom_rmse: 0.000134
train_f_mae: 0.005218
2025-04-09 02:54:11.431 INFO: train_f_mae: 0.005218
train_f_rmse: 0.007358
2025-04-09 02:54:11.431 INFO: train_f_rmse: 0.007358
val_e/atom_mae: 0.000121
2025-04-09 02:54:11.433 INFO: val_e/atom_mae: 0.000121
val_e/atom_rmse: 0.000143
2025-04-09 02:54:11.433 INFO: val_e/atom_rmse: 0.000143
val_f_mae: 0.005343
2025-04-09 02:54:11.434 INFO: val_f_mae: 0.005343
val_f_rmse: 0.007562
2025-04-09 02:54:11.434 INFO: val_f_rmse: 0.007562
##### Step: 142 Learning rate: 7.8125e-05 #####
2025-04-09 02:55:35.873 INFO: ##### Step: 142 Learning rate: 7.8125e-05 #####
Epoch 3, Train Loss: 0.0569, Val Loss: 0.0597
2025-04-09 02:55:35.874 INFO: Epoch 3, Train Loss: 0.0569, Val Loss: 0.0597
train_e/atom_mae: 0.000116
2025-04-09 02:55:35.874 INFO: train_e/atom_mae: 0.000116
train_e/atom_rmse: 0.000152
2025-04-09 02:55:35.874 INFO: train_e/atom_rmse: 0.000152
train_f_mae: 0.005213
2025-04-09 02:55:35.877 INFO: train_f_mae: 0.005213
train_f_rmse: 0.007354
2025-04-09 02:55:35.877 INFO: train_f_rmse: 0.007354
val_e/atom_mae: 0.000127
2025-04-09 02:55:35.879 INFO: val_e/atom_mae: 0.000127
val_e/atom_rmse: 0.000155
2025-04-09 02:55:35.880 INFO: val_e/atom_rmse: 0.000155
val_f_mae: 0.005330
2025-04-09 02:55:35.880 INFO: val_f_mae: 0.005330
val_f_rmse: 0.007534
2025-04-09 02:55:35.880 INFO: val_f_rmse: 0.007534
##### Step: 143 Learning rate: 7.8125e-05 #####
2025-04-09 02:57:00.362 INFO: ##### Step: 143 Learning rate: 7.8125e-05 #####
Epoch 4, Train Loss: 0.0565, Val Loss: 0.0585
2025-04-09 02:57:00.362 INFO: Epoch 4, Train Loss: 0.0565, Val Loss: 0.0585
train_e/atom_mae: 0.000106
2025-04-09 02:57:00.363 INFO: train_e/atom_mae: 0.000106
train_e/atom_rmse: 0.000139
2025-04-09 02:57:00.363 INFO: train_e/atom_rmse: 0.000139
train_f_mae: 0.005216
2025-04-09 02:57:00.366 INFO: train_f_mae: 0.005216
train_f_rmse: 0.007357
2025-04-09 02:57:00.366 INFO: train_f_rmse: 0.007357
val_e/atom_mae: 0.000084
2025-04-09 02:57:00.368 INFO: val_e/atom_mae: 0.000084
val_e/atom_rmse: 0.000115
2025-04-09 02:57:00.369 INFO: val_e/atom_rmse: 0.000115
val_f_mae: 0.005324
2025-04-09 02:57:00.369 INFO: val_f_mae: 0.005324
val_f_rmse: 0.007540
2025-04-09 02:57:00.369 INFO: val_f_rmse: 0.007540
##### Step: 144 Learning rate: 7.8125e-05 #####
2025-04-09 02:58:24.864 INFO: ##### Step: 144 Learning rate: 7.8125e-05 #####
Epoch 5, Train Loss: 0.0563, Val Loss: 0.0602
2025-04-09 02:58:24.865 INFO: Epoch 5, Train Loss: 0.0563, Val Loss: 0.0602
train_e/atom_mae: 0.000103
2025-04-09 02:58:24.866 INFO: train_e/atom_mae: 0.000103
train_e/atom_rmse: 0.000135
2025-04-09 02:58:24.866 INFO: train_e/atom_rmse: 0.000135
train_f_mae: 0.005214
2025-04-09 02:58:24.868 INFO: train_f_mae: 0.005214
train_f_rmse: 0.007353
2025-04-09 02:58:24.869 INFO: train_f_rmse: 0.007353
val_e/atom_mae: 0.000134
2025-04-09 02:58:24.871 INFO: val_e/atom_mae: 0.000134
val_e/atom_rmse: 0.000170
2025-04-09 02:58:24.871 INFO: val_e/atom_rmse: 0.000170
val_f_mae: 0.005324
2025-04-09 02:58:24.871 INFO: val_f_mae: 0.005324
val_f_rmse: 0.007529
2025-04-09 02:58:24.872 INFO: val_f_rmse: 0.007529
##### Step: 145 Learning rate: 7.8125e-05 #####
2025-04-09 02:59:49.354 INFO: ##### Step: 145 Learning rate: 7.8125e-05 #####
Epoch 6, Train Loss: 0.0569, Val Loss: 0.0611
2025-04-09 02:59:49.355 INFO: Epoch 6, Train Loss: 0.0569, Val Loss: 0.0611
train_e/atom_mae: 0.000113
2025-04-09 02:59:49.355 INFO: train_e/atom_mae: 0.000113
train_e/atom_rmse: 0.000147
2025-04-09 02:59:49.355 INFO: train_e/atom_rmse: 0.000147
train_f_mae: 0.005221
2025-04-09 02:59:49.358 INFO: train_f_mae: 0.005221
train_f_rmse: 0.007370
2025-04-09 02:59:49.358 INFO: train_f_rmse: 0.007370
val_e/atom_mae: 0.000137
2025-04-09 02:59:49.360 INFO: val_e/atom_mae: 0.000137
val_e/atom_rmse: 0.000177
2025-04-09 02:59:49.361 INFO: val_e/atom_rmse: 0.000177
val_f_mae: 0.005340
2025-04-09 02:59:49.361 INFO: val_f_mae: 0.005340
val_f_rmse: 0.007571
2025-04-09 02:59:49.361 INFO: val_f_rmse: 0.007571
##### Step: 146 Learning rate: 7.8125e-05 #####
2025-04-09 03:01:13.870 INFO: ##### Step: 146 Learning rate: 7.8125e-05 #####
Epoch 7, Train Loss: 0.0566, Val Loss: 0.0583
2025-04-09 03:01:13.871 INFO: Epoch 7, Train Loss: 0.0566, Val Loss: 0.0583
train_e/atom_mae: 0.000106
2025-04-09 03:01:13.871 INFO: train_e/atom_mae: 0.000106
train_e/atom_rmse: 0.000137
2025-04-09 03:01:13.871 INFO: train_e/atom_rmse: 0.000137
train_f_mae: 0.005225
2025-04-09 03:01:13.875 INFO: train_f_mae: 0.005225
train_f_rmse: 0.007373
2025-04-09 03:01:13.875 INFO: train_f_rmse: 0.007373
val_e/atom_mae: 0.000064
2025-04-09 03:01:13.877 INFO: val_e/atom_mae: 0.000064
val_e/atom_rmse: 0.000093
2025-04-09 03:01:13.878 INFO: val_e/atom_rmse: 0.000093
val_f_mae: 0.005352
2025-04-09 03:01:13.878 INFO: val_f_mae: 0.005352
val_f_rmse: 0.007569
2025-04-09 03:01:13.878 INFO: val_f_rmse: 0.007569
##### Step: 147 Learning rate: 7.8125e-05 #####
2025-04-09 03:02:38.383 INFO: ##### Step: 147 Learning rate: 7.8125e-05 #####
Epoch 8, Train Loss: 0.0562, Val Loss: 0.0593
2025-04-09 03:02:38.384 INFO: Epoch 8, Train Loss: 0.0562, Val Loss: 0.0593
train_e/atom_mae: 0.000100
2025-04-09 03:02:38.385 INFO: train_e/atom_mae: 0.000100
train_e/atom_rmse: 0.000130
2025-04-09 03:02:38.385 INFO: train_e/atom_rmse: 0.000130
train_f_mae: 0.005214
2025-04-09 03:02:38.388 INFO: train_f_mae: 0.005214
train_f_rmse: 0.007356
2025-04-09 03:02:38.388 INFO: train_f_rmse: 0.007356
val_e/atom_mae: 0.000107
2025-04-09 03:02:38.390 INFO: val_e/atom_mae: 0.000107
val_e/atom_rmse: 0.000141
2025-04-09 03:02:38.390 INFO: val_e/atom_rmse: 0.000141
val_f_mae: 0.005329
2025-04-09 03:02:38.391 INFO: val_f_mae: 0.005329
val_f_rmse: 0.007544
2025-04-09 03:02:38.391 INFO: val_f_rmse: 0.007544
##### Step: 148 Learning rate: 7.8125e-05 #####
2025-04-09 03:04:03.025 INFO: ##### Step: 148 Learning rate: 7.8125e-05 #####
Epoch 9, Train Loss: 0.0562, Val Loss: 0.0585
2025-04-09 03:04:03.026 INFO: Epoch 9, Train Loss: 0.0562, Val Loss: 0.0585
train_e/atom_mae: 0.000101
2025-04-09 03:04:03.027 INFO: train_e/atom_mae: 0.000101
train_e/atom_rmse: 0.000130
2025-04-09 03:04:03.027 INFO: train_e/atom_rmse: 0.000130
train_f_mae: 0.005218
2025-04-09 03:04:03.030 INFO: train_f_mae: 0.005218
train_f_rmse: 0.007362
2025-04-09 03:04:03.030 INFO: train_f_rmse: 0.007362
val_e/atom_mae: 0.000089
2025-04-09 03:04:03.032 INFO: val_e/atom_mae: 0.000089
val_e/atom_rmse: 0.000117
2025-04-09 03:04:03.032 INFO: val_e/atom_rmse: 0.000117
val_f_mae: 0.005326
2025-04-09 03:04:03.033 INFO: val_f_mae: 0.005326
val_f_rmse: 0.007542
2025-04-09 03:04:03.033 INFO: val_f_rmse: 0.007542
##### Step: 149 Learning rate: 7.8125e-05 #####
2025-04-09 03:05:27.512 INFO: ##### Step: 149 Learning rate: 7.8125e-05 #####
Epoch 10, Train Loss: 0.0565, Val Loss: 0.0591
2025-04-09 03:05:27.514 INFO: Epoch 10, Train Loss: 0.0565, Val Loss: 0.0591
train_e/atom_mae: 0.000110
2025-04-09 03:05:27.515 INFO: train_e/atom_mae: 0.000110
train_e/atom_rmse: 0.000142
2025-04-09 03:05:27.515 INFO: train_e/atom_rmse: 0.000142
train_f_mae: 0.005214
2025-04-09 03:05:27.518 INFO: train_f_mae: 0.005214
train_f_rmse: 0.007356
2025-04-09 03:05:27.518 INFO: train_f_rmse: 0.007356
val_e/atom_mae: 0.000115
2025-04-09 03:05:27.520 INFO: val_e/atom_mae: 0.000115
val_e/atom_rmse: 0.000138
2025-04-09 03:05:27.521 INFO: val_e/atom_rmse: 0.000138
val_f_mae: 0.005334
2025-04-09 03:05:27.521 INFO: val_f_mae: 0.005334
val_f_rmse: 0.007540
2025-04-09 03:05:27.521 INFO: val_f_rmse: 0.007540
##### Step: 150 Learning rate: 7.8125e-05 #####
2025-04-09 03:06:51.988 INFO: ##### Step: 150 Learning rate: 7.8125e-05 #####
Epoch 11, Train Loss: 0.0567, Val Loss: 0.0616
2025-04-09 03:06:51.988 INFO: Epoch 11, Train Loss: 0.0567, Val Loss: 0.0616
train_e/atom_mae: 0.000111
2025-04-09 03:06:51.989 INFO: train_e/atom_mae: 0.000111
train_e/atom_rmse: 0.000146
2025-04-09 03:06:51.989 INFO: train_e/atom_rmse: 0.000146
train_f_mae: 0.005216
2025-04-09 03:06:51.992 INFO: train_f_mae: 0.005216
train_f_rmse: 0.007357
2025-04-09 03:06:51.992 INFO: train_f_rmse: 0.007357
val_e/atom_mae: 0.000145
2025-04-09 03:06:51.994 INFO: val_e/atom_mae: 0.000145
val_e/atom_rmse: 0.000174
2025-04-09 03:06:51.995 INFO: val_e/atom_rmse: 0.000174
val_f_mae: 0.005361
2025-04-09 03:06:51.996 INFO: val_f_mae: 0.005361
val_f_rmse: 0.007614
2025-04-09 03:06:51.996 INFO: val_f_rmse: 0.007614
##### Step: 151 Learning rate: 7.8125e-05 #####
2025-04-09 03:08:16.492 INFO: ##### Step: 151 Learning rate: 7.8125e-05 #####
Epoch 12, Train Loss: 0.0566, Val Loss: 0.0584
2025-04-09 03:08:16.493 INFO: Epoch 12, Train Loss: 0.0566, Val Loss: 0.0584
train_e/atom_mae: 0.000103
2025-04-09 03:08:16.494 INFO: train_e/atom_mae: 0.000103
train_e/atom_rmse: 0.000139
2025-04-09 03:08:16.494 INFO: train_e/atom_rmse: 0.000139
train_f_mae: 0.005218
2025-04-09 03:08:16.496 INFO: train_f_mae: 0.005218
train_f_rmse: 0.007367
2025-04-09 03:08:16.497 INFO: train_f_rmse: 0.007367
val_e/atom_mae: 0.000097
2025-04-09 03:08:16.499 INFO: val_e/atom_mae: 0.000097
val_e/atom_rmse: 0.000120
2025-04-09 03:08:16.499 INFO: val_e/atom_rmse: 0.000120
val_f_mae: 0.005328
2025-04-09 03:08:16.500 INFO: val_f_mae: 0.005328
val_f_rmse: 0.007527
2025-04-09 03:08:16.500 INFO: val_f_rmse: 0.007527
##### Step: 152 Learning rate: 7.8125e-05 #####
2025-04-09 03:09:41.004 INFO: ##### Step: 152 Learning rate: 7.8125e-05 #####
Epoch 13, Train Loss: 0.0562, Val Loss: 0.0592
2025-04-09 03:09:41.005 INFO: Epoch 13, Train Loss: 0.0562, Val Loss: 0.0592
train_e/atom_mae: 0.000103
2025-04-09 03:09:41.005 INFO: train_e/atom_mae: 0.000103
train_e/atom_rmse: 0.000136
2025-04-09 03:09:41.005 INFO: train_e/atom_rmse: 0.000136
train_f_mae: 0.005208
2025-04-09 03:09:41.008 INFO: train_f_mae: 0.005208
train_f_rmse: 0.007348
2025-04-09 03:09:41.008 INFO: train_f_rmse: 0.007348
val_e/atom_mae: 0.000109
2025-04-09 03:09:41.011 INFO: val_e/atom_mae: 0.000109
val_e/atom_rmse: 0.000136
2025-04-09 03:09:41.011 INFO: val_e/atom_rmse: 0.000136
val_f_mae: 0.005331
2025-04-09 03:09:41.011 INFO: val_f_mae: 0.005331
val_f_rmse: 0.007544
2025-04-09 03:09:41.011 INFO: val_f_rmse: 0.007544
##### Step: 153 Learning rate: 7.8125e-05 #####
2025-04-09 03:11:05.479 INFO: ##### Step: 153 Learning rate: 7.8125e-05 #####
Epoch 14, Train Loss: 0.0571, Val Loss: 0.0579
2025-04-09 03:11:05.480 INFO: Epoch 14, Train Loss: 0.0571, Val Loss: 0.0579
train_e/atom_mae: 0.000129
2025-04-09 03:11:05.481 INFO: train_e/atom_mae: 0.000129
train_e/atom_rmse: 0.000164
2025-04-09 03:11:05.481 INFO: train_e/atom_rmse: 0.000164
train_f_mae: 0.005203
2025-04-09 03:11:05.484 INFO: train_f_mae: 0.005203
train_f_rmse: 0.007337
2025-04-09 03:11:05.484 INFO: train_f_rmse: 0.007337
val_e/atom_mae: 0.000072
2025-04-09 03:11:05.486 INFO: val_e/atom_mae: 0.000072
val_e/atom_rmse: 0.000101
2025-04-09 03:11:05.486 INFO: val_e/atom_rmse: 0.000101
val_f_mae: 0.005328
2025-04-09 03:11:05.487 INFO: val_f_mae: 0.005328
val_f_rmse: 0.007530
2025-04-09 03:11:05.487 INFO: val_f_rmse: 0.007530
##### Step: 154 Learning rate: 7.8125e-05 #####
2025-04-09 03:12:29.971 INFO: ##### Step: 154 Learning rate: 7.8125e-05 #####
Epoch 15, Train Loss: 0.0559, Val Loss: 0.0602
2025-04-09 03:12:29.971 INFO: Epoch 15, Train Loss: 0.0559, Val Loss: 0.0602
train_e/atom_mae: 0.000091
2025-04-09 03:12:29.972 INFO: train_e/atom_mae: 0.000091
train_e/atom_rmse: 0.000120
2025-04-09 03:12:29.972 INFO: train_e/atom_rmse: 0.000120
train_f_mae: 0.005215
2025-04-09 03:12:29.975 INFO: train_f_mae: 0.005215
train_f_rmse: 0.007356
2025-04-09 03:12:29.975 INFO: train_f_rmse: 0.007356
val_e/atom_mae: 0.000088
2025-04-09 03:12:29.977 INFO: val_e/atom_mae: 0.000088
val_e/atom_rmse: 0.000125
2025-04-09 03:12:29.978 INFO: val_e/atom_rmse: 0.000125
val_f_mae: 0.005384
2025-04-09 03:12:29.978 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007635
2025-04-09 03:12:29.978 INFO: val_f_rmse: 0.007635
##### Step: 155 Learning rate: 7.8125e-05 #####
2025-04-09 03:13:54.426 INFO: ##### Step: 155 Learning rate: 7.8125e-05 #####
Epoch 16, Train Loss: 0.0569, Val Loss: 0.0579
2025-04-09 03:13:54.427 INFO: Epoch 16, Train Loss: 0.0569, Val Loss: 0.0579
train_e/atom_mae: 0.000104
2025-04-09 03:13:54.428 INFO: train_e/atom_mae: 0.000104
train_e/atom_rmse: 0.000140
2025-04-09 03:13:54.428 INFO: train_e/atom_rmse: 0.000140
train_f_mae: 0.005232
2025-04-09 03:13:54.431 INFO: train_f_mae: 0.005232
train_f_rmse: 0.007387
2025-04-09 03:13:54.431 INFO: train_f_rmse: 0.007387
val_e/atom_mae: 0.000068
2025-04-09 03:13:54.433 INFO: val_e/atom_mae: 0.000068
val_e/atom_rmse: 0.000098
2025-04-09 03:13:54.434 INFO: val_e/atom_rmse: 0.000098
val_f_mae: 0.005321
2025-04-09 03:13:54.434 INFO: val_f_mae: 0.005321
val_f_rmse: 0.007530
2025-04-09 03:13:54.434 INFO: val_f_rmse: 0.007530
##### Step: 156 Learning rate: 7.8125e-05 #####
2025-04-09 03:15:18.931 INFO: ##### Step: 156 Learning rate: 7.8125e-05 #####
Epoch 17, Train Loss: 0.0567, Val Loss: 0.0639
2025-04-09 03:15:18.932 INFO: Epoch 17, Train Loss: 0.0567, Val Loss: 0.0639
train_e/atom_mae: 0.000110
2025-04-09 03:15:18.932 INFO: train_e/atom_mae: 0.000110
train_e/atom_rmse: 0.000144
2025-04-09 03:15:18.932 INFO: train_e/atom_rmse: 0.000144
train_f_mae: 0.005213
2025-04-09 03:15:18.935 INFO: train_f_mae: 0.005213
train_f_rmse: 0.007358
2025-04-09 03:15:18.935 INFO: train_f_rmse: 0.007358
val_e/atom_mae: 0.000197
2025-04-09 03:15:18.937 INFO: val_e/atom_mae: 0.000197
val_e/atom_rmse: 0.000239
2025-04-09 03:15:18.938 INFO: val_e/atom_rmse: 0.000239
val_f_mae: 0.005332
2025-04-09 03:15:18.938 INFO: val_f_mae: 0.005332
val_f_rmse: 0.007550
2025-04-09 03:15:18.938 INFO: val_f_rmse: 0.007550
##### Step: 157 Learning rate: 7.8125e-05 #####
2025-04-09 03:16:43.406 INFO: ##### Step: 157 Learning rate: 7.8125e-05 #####
Epoch 18, Train Loss: 0.0561, Val Loss: 0.0584
2025-04-09 03:16:43.407 INFO: Epoch 18, Train Loss: 0.0561, Val Loss: 0.0584
train_e/atom_mae: 0.000106
2025-04-09 03:16:43.407 INFO: train_e/atom_mae: 0.000106
train_e/atom_rmse: 0.000138
2025-04-09 03:16:43.408 INFO: train_e/atom_rmse: 0.000138
train_f_mae: 0.005201
2025-04-09 03:16:43.410 INFO: train_f_mae: 0.005201
train_f_rmse: 0.007333
2025-04-09 03:16:43.410 INFO: train_f_rmse: 0.007333
val_e/atom_mae: 0.000100
2025-04-09 03:16:43.413 INFO: val_e/atom_mae: 0.000100
val_e/atom_rmse: 0.000126
2025-04-09 03:16:43.413 INFO: val_e/atom_rmse: 0.000126
val_f_mae: 0.005311
2025-04-09 03:16:43.413 INFO: val_f_mae: 0.005311
val_f_rmse: 0.007513
2025-04-09 03:16:43.413 INFO: val_f_rmse: 0.007513
##### Step: 158 Learning rate: 7.8125e-05 #####
2025-04-09 03:18:07.882 INFO: ##### Step: 158 Learning rate: 7.8125e-05 #####
Epoch 19, Train Loss: 0.0561, Val Loss: 0.0595
2025-04-09 03:18:07.883 INFO: Epoch 19, Train Loss: 0.0561, Val Loss: 0.0595
train_e/atom_mae: 0.000104
2025-04-09 03:18:07.884 INFO: train_e/atom_mae: 0.000104
train_e/atom_rmse: 0.000135
2025-04-09 03:18:07.884 INFO: train_e/atom_rmse: 0.000135
train_f_mae: 0.005209
2025-04-09 03:18:07.887 INFO: train_f_mae: 0.005209
train_f_rmse: 0.007344
2025-04-09 03:18:07.887 INFO: train_f_rmse: 0.007344
val_e/atom_mae: 0.000115
2025-04-09 03:18:07.889 INFO: val_e/atom_mae: 0.000115
val_e/atom_rmse: 0.000148
2025-04-09 03:18:07.890 INFO: val_e/atom_rmse: 0.000148
val_f_mae: 0.005334
2025-04-09 03:18:07.890 INFO: val_f_mae: 0.005334
val_f_rmse: 0.007537
2025-04-09 03:18:07.890 INFO: val_f_rmse: 0.007537
##### Step: 159 Learning rate: 7.8125e-05 #####
2025-04-09 03:19:32.387 INFO: ##### Step: 159 Learning rate: 7.8125e-05 #####
Epoch 20, Train Loss: 0.0560, Val Loss: 0.0587
2025-04-09 03:19:32.388 INFO: Epoch 20, Train Loss: 0.0560, Val Loss: 0.0587
train_e/atom_mae: 0.000103
2025-04-09 03:19:32.388 INFO: train_e/atom_mae: 0.000103
train_e/atom_rmse: 0.000135
2025-04-09 03:19:32.388 INFO: train_e/atom_rmse: 0.000135
train_f_mae: 0.005200
2025-04-09 03:19:32.391 INFO: train_f_mae: 0.005200
train_f_rmse: 0.007331
2025-04-09 03:19:32.391 INFO: train_f_rmse: 0.007331
val_e/atom_mae: 0.000071
2025-04-09 03:19:32.393 INFO: val_e/atom_mae: 0.000071
val_e/atom_rmse: 0.000099
2025-04-09 03:19:32.394 INFO: val_e/atom_rmse: 0.000099
val_f_mae: 0.005358
2025-04-09 03:19:32.394 INFO: val_f_mae: 0.005358
val_f_rmse: 0.007587
2025-04-09 03:19:32.394 INFO: val_f_rmse: 0.007587
##### Step: 160 Learning rate: 3.90625e-05 #####
2025-04-09 03:20:56.919 INFO: ##### Step: 160 Learning rate: 3.90625e-05 #####
Epoch 21, Train Loss: 0.0553, Val Loss: 0.0591
2025-04-09 03:20:56.919 INFO: Epoch 21, Train Loss: 0.0553, Val Loss: 0.0591
train_e/atom_mae: 0.000087
2025-04-09 03:20:56.920 INFO: train_e/atom_mae: 0.000087
train_e/atom_rmse: 0.000116
2025-04-09 03:20:56.920 INFO: train_e/atom_rmse: 0.000116
train_f_mae: 0.005199
2025-04-09 03:20:56.923 INFO: train_f_mae: 0.005199
train_f_rmse: 0.007329
2025-04-09 03:20:56.923 INFO: train_f_rmse: 0.007329
val_e/atom_mae: 0.000114
2025-04-09 03:20:56.925 INFO: val_e/atom_mae: 0.000114
val_e/atom_rmse: 0.000138
2025-04-09 03:20:56.926 INFO: val_e/atom_rmse: 0.000138
val_f_mae: 0.005334
2025-04-09 03:20:56.926 INFO: val_f_mae: 0.005334
val_f_rmse: 0.007538
2025-04-09 03:20:56.926 INFO: val_f_rmse: 0.007538
##### Step: 161 Learning rate: 3.90625e-05 #####
2025-04-09 03:22:21.405 INFO: ##### Step: 161 Learning rate: 3.90625e-05 #####
Epoch 22, Train Loss: 0.0551, Val Loss: 0.0579
2025-04-09 03:22:21.405 INFO: Epoch 22, Train Loss: 0.0551, Val Loss: 0.0579
train_e/atom_mae: 0.000086
2025-04-09 03:22:21.406 INFO: train_e/atom_mae: 0.000086
train_e/atom_rmse: 0.000116
2025-04-09 03:22:21.406 INFO: train_e/atom_rmse: 0.000116
train_f_mae: 0.005187
2025-04-09 03:22:21.409 INFO: train_f_mae: 0.005187
train_f_rmse: 0.007309
2025-04-09 03:22:21.409 INFO: train_f_rmse: 0.007309
val_e/atom_mae: 0.000067
2025-04-09 03:22:21.411 INFO: val_e/atom_mae: 0.000067
val_e/atom_rmse: 0.000097
2025-04-09 03:22:21.412 INFO: val_e/atom_rmse: 0.000097
val_f_mae: 0.005326
2025-04-09 03:22:21.412 INFO: val_f_mae: 0.005326
val_f_rmse: 0.007532
2025-04-09 03:22:21.412 INFO: val_f_rmse: 0.007532
##### Step: 162 Learning rate: 3.90625e-05 #####
2025-04-09 03:23:45.898 INFO: ##### Step: 162 Learning rate: 3.90625e-05 #####
Epoch 23, Train Loss: 0.0547, Val Loss: 0.0579
2025-04-09 03:23:45.899 INFO: Epoch 23, Train Loss: 0.0547, Val Loss: 0.0579
train_e/atom_mae: 0.000073
2025-04-09 03:23:45.899 INFO: train_e/atom_mae: 0.000073
train_e/atom_rmse: 0.000101
2025-04-09 03:23:45.900 INFO: train_e/atom_rmse: 0.000101
train_f_mae: 0.005189
2025-04-09 03:23:45.902 INFO: train_f_mae: 0.005189
train_f_rmse: 0.007312
2025-04-09 03:23:45.902 INFO: train_f_rmse: 0.007312
val_e/atom_mae: 0.000071
2025-04-09 03:23:45.905 INFO: val_e/atom_mae: 0.000071
val_e/atom_rmse: 0.000099
2025-04-09 03:23:45.905 INFO: val_e/atom_rmse: 0.000099
val_f_mae: 0.005323
2025-04-09 03:23:45.905 INFO: val_f_mae: 0.005323
val_f_rmse: 0.007533
2025-04-09 03:23:45.905 INFO: val_f_rmse: 0.007533
##### Step: 163 Learning rate: 3.90625e-05 #####
2025-04-09 03:25:10.324 INFO: ##### Step: 163 Learning rate: 3.90625e-05 #####
Epoch 24, Train Loss: 0.0550, Val Loss: 0.0576
2025-04-09 03:25:10.325 INFO: Epoch 24, Train Loss: 0.0550, Val Loss: 0.0576
train_e/atom_mae: 0.000086
2025-04-09 03:25:10.325 INFO: train_e/atom_mae: 0.000086
train_e/atom_rmse: 0.000115
2025-04-09 03:25:10.326 INFO: train_e/atom_rmse: 0.000115
train_f_mae: 0.005183
2025-04-09 03:25:10.329 INFO: train_f_mae: 0.005183
train_f_rmse: 0.007304
2025-04-09 03:25:10.329 INFO: train_f_rmse: 0.007304
val_e/atom_mae: 0.000069
2025-04-09 03:25:10.331 INFO: val_e/atom_mae: 0.000069
val_e/atom_rmse: 0.000098
2025-04-09 03:25:10.331 INFO: val_e/atom_rmse: 0.000098
val_f_mae: 0.005314
2025-04-09 03:25:10.332 INFO: val_f_mae: 0.005314
val_f_rmse: 0.007510
2025-04-09 03:25:10.332 INFO: val_f_rmse: 0.007510
##### Step: 164 Learning rate: 3.90625e-05 #####
2025-04-09 03:26:34.790 INFO: ##### Step: 164 Learning rate: 3.90625e-05 #####
Epoch 25, Train Loss: 0.0551, Val Loss: 0.0587
2025-04-09 03:26:34.790 INFO: Epoch 25, Train Loss: 0.0551, Val Loss: 0.0587
train_e/atom_mae: 0.000086
2025-04-09 03:26:34.791 INFO: train_e/atom_mae: 0.000086
train_e/atom_rmse: 0.000114
2025-04-09 03:26:34.791 INFO: train_e/atom_rmse: 0.000114
train_f_mae: 0.005191
2025-04-09 03:26:34.794 INFO: train_f_mae: 0.005191
train_f_rmse: 0.007315
2025-04-09 03:26:34.794 INFO: train_f_rmse: 0.007315
val_e/atom_mae: 0.000079
2025-04-09 03:26:34.796 INFO: val_e/atom_mae: 0.000079
val_e/atom_rmse: 0.000111
2025-04-09 03:26:34.797 INFO: val_e/atom_rmse: 0.000111
val_f_mae: 0.005346
2025-04-09 03:26:34.797 INFO: val_f_mae: 0.005346
val_f_rmse: 0.007564
2025-04-09 03:26:34.797 INFO: val_f_rmse: 0.007564
##### Step: 165 Learning rate: 3.90625e-05 #####
2025-04-09 03:27:59.299 INFO: ##### Step: 165 Learning rate: 3.90625e-05 #####
Epoch 26, Train Loss: 0.0554, Val Loss: 0.0574
2025-04-09 03:27:59.300 INFO: Epoch 26, Train Loss: 0.0554, Val Loss: 0.0574
train_e/atom_mae: 0.000097
2025-04-09 03:27:59.300 INFO: train_e/atom_mae: 0.000097
train_e/atom_rmse: 0.000126
2025-04-09 03:27:59.300 INFO: train_e/atom_rmse: 0.000126
train_f_mae: 0.005190
2025-04-09 03:27:59.303 INFO: train_f_mae: 0.005190
train_f_rmse: 0.007316
2025-04-09 03:27:59.303 INFO: train_f_rmse: 0.007316
val_e/atom_mae: 0.000063
2025-04-09 03:27:59.305 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000092
2025-04-09 03:27:59.306 INFO: val_e/atom_rmse: 0.000092
val_f_mae: 0.005301
2025-04-09 03:27:59.306 INFO: val_f_mae: 0.005301
val_f_rmse: 0.007509
2025-04-09 03:27:59.306 INFO: val_f_rmse: 0.007509
##### Step: 166 Learning rate: 3.90625e-05 #####
2025-04-09 03:29:23.812 INFO: ##### Step: 166 Learning rate: 3.90625e-05 #####
Epoch 27, Train Loss: 0.0548, Val Loss: 0.0590
2025-04-09 03:29:23.812 INFO: Epoch 27, Train Loss: 0.0548, Val Loss: 0.0590
train_e/atom_mae: 0.000074
2025-04-09 03:29:23.813 INFO: train_e/atom_mae: 0.000074
train_e/atom_rmse: 0.000101
2025-04-09 03:29:23.813 INFO: train_e/atom_rmse: 0.000101
train_f_mae: 0.005190
2025-04-09 03:29:23.816 INFO: train_f_mae: 0.005190
train_f_rmse: 0.007317
2025-04-09 03:29:23.816 INFO: train_f_rmse: 0.007317
val_e/atom_mae: 0.000099
2025-04-09 03:29:23.818 INFO: val_e/atom_mae: 0.000099
val_e/atom_rmse: 0.000132
2025-04-09 03:29:23.819 INFO: val_e/atom_rmse: 0.000132
val_f_mae: 0.005332
2025-04-09 03:29:23.819 INFO: val_f_mae: 0.005332
val_f_rmse: 0.007544
2025-04-09 03:29:23.819 INFO: val_f_rmse: 0.007544
##### Step: 167 Learning rate: 3.90625e-05 #####
2025-04-09 03:30:48.287 INFO: ##### Step: 167 Learning rate: 3.90625e-05 #####
Epoch 28, Train Loss: 0.0547, Val Loss: 0.0591
2025-04-09 03:30:48.288 INFO: Epoch 28, Train Loss: 0.0547, Val Loss: 0.0591
train_e/atom_mae: 0.000079
2025-04-09 03:30:48.289 INFO: train_e/atom_mae: 0.000079
train_e/atom_rmse: 0.000107
2025-04-09 03:30:48.289 INFO: train_e/atom_rmse: 0.000107
train_f_mae: 0.005183
2025-04-09 03:30:48.291 INFO: train_f_mae: 0.005183
train_f_rmse: 0.007303
2025-04-09 03:30:48.291 INFO: train_f_rmse: 0.007303
val_e/atom_mae: 0.000102
2025-04-09 03:30:48.294 INFO: val_e/atom_mae: 0.000102
val_e/atom_rmse: 0.000127
2025-04-09 03:30:48.294 INFO: val_e/atom_rmse: 0.000127
val_f_mae: 0.005329
2025-04-09 03:30:48.294 INFO: val_f_mae: 0.005329
val_f_rmse: 0.007558
2025-04-09 03:30:48.294 INFO: val_f_rmse: 0.007558
##### Step: 168 Learning rate: 3.90625e-05 #####
2025-04-09 03:32:12.781 INFO: ##### Step: 168 Learning rate: 3.90625e-05 #####
Epoch 29, Train Loss: 0.0549, Val Loss: 0.0579
2025-04-09 03:32:12.782 INFO: Epoch 29, Train Loss: 0.0549, Val Loss: 0.0579
train_e/atom_mae: 0.000081
2025-04-09 03:32:12.783 INFO: train_e/atom_mae: 0.000081
train_e/atom_rmse: 0.000110
2025-04-09 03:32:12.783 INFO: train_e/atom_rmse: 0.000110
train_f_mae: 0.005184
2025-04-09 03:32:12.786 INFO: train_f_mae: 0.005184
train_f_rmse: 0.007308
2025-04-09 03:32:12.786 INFO: train_f_rmse: 0.007308
val_e/atom_mae: 0.000062
2025-04-09 03:32:12.789 INFO: val_e/atom_mae: 0.000062
val_e/atom_rmse: 0.000093
2025-04-09 03:32:12.789 INFO: val_e/atom_rmse: 0.000093
val_f_mae: 0.005324
2025-04-09 03:32:12.789 INFO: val_f_mae: 0.005324
val_f_rmse: 0.007538
2025-04-09 03:32:12.790 INFO: val_f_rmse: 0.007538
##### Step: 169 Learning rate: 3.90625e-05 #####
2025-04-09 03:33:37.261 INFO: ##### Step: 169 Learning rate: 3.90625e-05 #####
Epoch 30, Train Loss: 0.0549, Val Loss: 0.0575
2025-04-09 03:33:37.262 INFO: Epoch 30, Train Loss: 0.0549, Val Loss: 0.0575
train_e/atom_mae: 0.000084
2025-04-09 03:33:37.263 INFO: train_e/atom_mae: 0.000084
train_e/atom_rmse: 0.000112
2025-04-09 03:33:37.263 INFO: train_e/atom_rmse: 0.000112
train_f_mae: 0.005183
2025-04-09 03:33:37.266 INFO: train_f_mae: 0.005183
train_f_rmse: 0.007304
2025-04-09 03:33:37.266 INFO: train_f_rmse: 0.007304
val_e/atom_mae: 0.000066
2025-04-09 03:33:37.268 INFO: val_e/atom_mae: 0.000066
val_e/atom_rmse: 0.000093
2025-04-09 03:33:37.268 INFO: val_e/atom_rmse: 0.000093
val_f_mae: 0.005307
2025-04-09 03:33:37.269 INFO: val_f_mae: 0.005307
val_f_rmse: 0.007511
2025-04-09 03:33:37.269 INFO: val_f_rmse: 0.007511
##### Step: 170 Learning rate: 3.90625e-05 #####
2025-04-09 03:35:01.759 INFO: ##### Step: 170 Learning rate: 3.90625e-05 #####
Epoch 31, Train Loss: 0.0548, Val Loss: 0.0576
2025-04-09 03:35:01.759 INFO: Epoch 31, Train Loss: 0.0548, Val Loss: 0.0576
train_e/atom_mae: 0.000082
2025-04-09 03:35:01.760 INFO: train_e/atom_mae: 0.000082
train_e/atom_rmse: 0.000110
2025-04-09 03:35:01.760 INFO: train_e/atom_rmse: 0.000110
train_f_mae: 0.005183
2025-04-09 03:35:01.763 INFO: train_f_mae: 0.005183
train_f_rmse: 0.007306
2025-04-09 03:35:01.763 INFO: train_f_rmse: 0.007306
val_e/atom_mae: 0.000078
2025-04-09 03:35:01.765 INFO: val_e/atom_mae: 0.000078
val_e/atom_rmse: 0.000108
2025-04-09 03:35:01.765 INFO: val_e/atom_rmse: 0.000108
val_f_mae: 0.005300
2025-04-09 03:35:01.766 INFO: val_f_mae: 0.005300
val_f_rmse: 0.007498
2025-04-09 03:35:01.766 INFO: val_f_rmse: 0.007498
##### Step: 171 Learning rate: 3.90625e-05 #####
2025-04-09 03:36:26.240 INFO: ##### Step: 171 Learning rate: 3.90625e-05 #####
Epoch 32, Train Loss: 0.0550, Val Loss: 0.0579
2025-04-09 03:36:26.241 INFO: Epoch 32, Train Loss: 0.0550, Val Loss: 0.0579
train_e/atom_mae: 0.000086
2025-04-09 03:36:26.242 INFO: train_e/atom_mae: 0.000086
train_e/atom_rmse: 0.000116
2025-04-09 03:36:26.242 INFO: train_e/atom_rmse: 0.000116
train_f_mae: 0.005184
2025-04-09 03:36:26.245 INFO: train_f_mae: 0.005184
train_f_rmse: 0.007307
2025-04-09 03:36:26.245 INFO: train_f_rmse: 0.007307
val_e/atom_mae: 0.000067
2025-04-09 03:36:26.247 INFO: val_e/atom_mae: 0.000067
val_e/atom_rmse: 0.000097
2025-04-09 03:36:26.247 INFO: val_e/atom_rmse: 0.000097
val_f_mae: 0.005321
2025-04-09 03:36:26.248 INFO: val_f_mae: 0.005321
val_f_rmse: 0.007531
2025-04-09 03:36:26.248 INFO: val_f_rmse: 0.007531
##### Step: 172 Learning rate: 3.90625e-05 #####
2025-04-09 03:37:50.754 INFO: ##### Step: 172 Learning rate: 3.90625e-05 #####
Epoch 33, Train Loss: 0.0547, Val Loss: 0.0575
2025-04-09 03:37:50.755 INFO: Epoch 33, Train Loss: 0.0547, Val Loss: 0.0575
train_e/atom_mae: 0.000081
2025-04-09 03:37:50.756 INFO: train_e/atom_mae: 0.000081
train_e/atom_rmse: 0.000108
2025-04-09 03:37:50.756 INFO: train_e/atom_rmse: 0.000108
train_f_mae: 0.005183
2025-04-09 03:37:50.759 INFO: train_f_mae: 0.005183
train_f_rmse: 0.007303
2025-04-09 03:37:50.759 INFO: train_f_rmse: 0.007303
val_e/atom_mae: 0.000066
2025-04-09 03:37:50.761 INFO: val_e/atom_mae: 0.000066
val_e/atom_rmse: 0.000093
2025-04-09 03:37:50.761 INFO: val_e/atom_rmse: 0.000093
val_f_mae: 0.005306
2025-04-09 03:37:50.762 INFO: val_f_mae: 0.005306
val_f_rmse: 0.007514
2025-04-09 03:37:50.762 INFO: val_f_rmse: 0.007514
##### Step: 173 Learning rate: 3.90625e-05 #####
2025-04-09 03:39:15.197 INFO: ##### Step: 173 Learning rate: 3.90625e-05 #####
Epoch 34, Train Loss: 0.0549, Val Loss: 0.0574
2025-04-09 03:39:15.197 INFO: Epoch 34, Train Loss: 0.0549, Val Loss: 0.0574
train_e/atom_mae: 0.000083
2025-04-09 03:39:15.198 INFO: train_e/atom_mae: 0.000083
train_e/atom_rmse: 0.000111
2025-04-09 03:39:15.198 INFO: train_e/atom_rmse: 0.000111
train_f_mae: 0.005185
2025-04-09 03:39:15.201 INFO: train_f_mae: 0.005185
train_f_rmse: 0.007307
2025-04-09 03:39:15.202 INFO: train_f_rmse: 0.007307
val_e/atom_mae: 0.000068
2025-04-09 03:39:15.204 INFO: val_e/atom_mae: 0.000068
val_e/atom_rmse: 0.000099
2025-04-09 03:39:15.204 INFO: val_e/atom_rmse: 0.000099
val_f_mae: 0.005309
2025-04-09 03:39:15.205 INFO: val_f_mae: 0.005309
val_f_rmse: 0.007499
2025-04-09 03:39:15.205 INFO: val_f_rmse: 0.007499
##### Step: 174 Learning rate: 3.90625e-05 #####
2025-04-09 03:40:39.671 INFO: ##### Step: 174 Learning rate: 3.90625e-05 #####
Epoch 35, Train Loss: 0.0550, Val Loss: 0.0579
2025-04-09 03:40:39.672 INFO: Epoch 35, Train Loss: 0.0550, Val Loss: 0.0579
train_e/atom_mae: 0.000086
2025-04-09 03:40:39.673 INFO: train_e/atom_mae: 0.000086
train_e/atom_rmse: 0.000114
2025-04-09 03:40:39.673 INFO: train_e/atom_rmse: 0.000114
train_f_mae: 0.005184
2025-04-09 03:40:39.676 INFO: train_f_mae: 0.005184
train_f_rmse: 0.007309
2025-04-09 03:40:39.676 INFO: train_f_rmse: 0.007309
val_e/atom_mae: 0.000079
2025-04-09 03:40:39.678 INFO: val_e/atom_mae: 0.000079
val_e/atom_rmse: 0.000107
2025-04-09 03:40:39.678 INFO: val_e/atom_rmse: 0.000107
val_f_mae: 0.005316
2025-04-09 03:40:39.679 INFO: val_f_mae: 0.005316
val_f_rmse: 0.007514
2025-04-09 03:40:39.679 INFO: val_f_rmse: 0.007514
##### Step: 175 Learning rate: 3.90625e-05 #####
2025-04-09 03:42:04.180 INFO: ##### Step: 175 Learning rate: 3.90625e-05 #####
Epoch 36, Train Loss: 0.0546, Val Loss: 0.0574
2025-04-09 03:42:04.181 INFO: Epoch 36, Train Loss: 0.0546, Val Loss: 0.0574
train_e/atom_mae: 0.000080
2025-04-09 03:42:04.182 INFO: train_e/atom_mae: 0.000080
train_e/atom_rmse: 0.000108
2025-04-09 03:42:04.182 INFO: train_e/atom_rmse: 0.000108
train_f_mae: 0.005177
2025-04-09 03:42:04.185 INFO: train_f_mae: 0.005177
train_f_rmse: 0.007294
2025-04-09 03:42:04.185 INFO: train_f_rmse: 0.007294
val_e/atom_mae: 0.000063
2025-04-09 03:42:04.187 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000091
2025-04-09 03:42:04.187 INFO: val_e/atom_rmse: 0.000091
val_f_mae: 0.005313
2025-04-09 03:42:04.188 INFO: val_f_mae: 0.005313
val_f_rmse: 0.007507
2025-04-09 03:42:04.188 INFO: val_f_rmse: 0.007507
##### Step: 176 Learning rate: 3.90625e-05 #####
2025-04-09 03:43:28.670 INFO: ##### Step: 176 Learning rate: 3.90625e-05 #####
Epoch 37, Train Loss: 0.0549, Val Loss: 0.0575
2025-04-09 03:43:28.670 INFO: Epoch 37, Train Loss: 0.0549, Val Loss: 0.0575
train_e/atom_mae: 0.000088
2025-04-09 03:43:28.671 INFO: train_e/atom_mae: 0.000088
train_e/atom_rmse: 0.000117
2025-04-09 03:43:28.671 INFO: train_e/atom_rmse: 0.000117
train_f_mae: 0.005178
2025-04-09 03:43:28.674 INFO: train_f_mae: 0.005178
train_f_rmse: 0.007295
2025-04-09 03:43:28.674 INFO: train_f_rmse: 0.007295
val_e/atom_mae: 0.000076
2025-04-09 03:43:28.676 INFO: val_e/atom_mae: 0.000076
val_e/atom_rmse: 0.000106
2025-04-09 03:43:28.677 INFO: val_e/atom_rmse: 0.000106
val_f_mae: 0.005302
2025-04-09 03:43:28.677 INFO: val_f_mae: 0.005302
val_f_rmse: 0.007492
2025-04-09 03:43:28.677 INFO: val_f_rmse: 0.007492
##### Step: 177 Learning rate: 3.90625e-05 #####
2025-04-09 03:44:53.166 INFO: ##### Step: 177 Learning rate: 3.90625e-05 #####
Epoch 38, Train Loss: 0.0552, Val Loss: 0.0608
2025-04-09 03:44:53.167 INFO: Epoch 38, Train Loss: 0.0552, Val Loss: 0.0608
train_e/atom_mae: 0.000093
2025-04-09 03:44:53.168 INFO: train_e/atom_mae: 0.000093
train_e/atom_rmse: 0.000121
2025-04-09 03:44:53.168 INFO: train_e/atom_rmse: 0.000121
train_f_mae: 0.005183
2025-04-09 03:44:53.170 INFO: train_f_mae: 0.005183
train_f_rmse: 0.007307
2025-04-09 03:44:53.170 INFO: train_f_rmse: 0.007307
val_e/atom_mae: 0.000158
2025-04-09 03:44:53.173 INFO: val_e/atom_mae: 0.000158
val_e/atom_rmse: 0.000184
2025-04-09 03:44:53.173 INFO: val_e/atom_rmse: 0.000184
val_f_mae: 0.005329
2025-04-09 03:44:53.173 INFO: val_f_mae: 0.005329
val_f_rmse: 0.007532
2025-04-09 03:44:53.174 INFO: val_f_rmse: 0.007532
##### Step: 178 Learning rate: 3.90625e-05 #####
2025-04-09 03:46:17.656 INFO: ##### Step: 178 Learning rate: 3.90625e-05 #####
Epoch 39, Train Loss: 0.0548, Val Loss: 0.0579
2025-04-09 03:46:17.656 INFO: Epoch 39, Train Loss: 0.0548, Val Loss: 0.0579
train_e/atom_mae: 0.000081
2025-04-09 03:46:17.657 INFO: train_e/atom_mae: 0.000081
train_e/atom_rmse: 0.000109
2025-04-09 03:46:17.657 INFO: train_e/atom_rmse: 0.000109
train_f_mae: 0.005184
2025-04-09 03:46:17.660 INFO: train_f_mae: 0.005184
train_f_rmse: 0.007306
2025-04-09 03:46:17.660 INFO: train_f_rmse: 0.007306
val_e/atom_mae: 0.000063
2025-04-09 03:46:17.662 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000091
2025-04-09 03:46:17.663 INFO: val_e/atom_rmse: 0.000091
val_f_mae: 0.005329
2025-04-09 03:46:17.663 INFO: val_f_mae: 0.005329
val_f_rmse: 0.007545
2025-04-09 03:46:17.663 INFO: val_f_rmse: 0.007545
##### Step: 179 Learning rate: 3.90625e-05 #####
2025-04-09 03:47:42.164 INFO: ##### Step: 179 Learning rate: 3.90625e-05 #####
Epoch 40, Train Loss: 0.0550, Val Loss: 0.0574
2025-04-09 03:47:42.165 INFO: Epoch 40, Train Loss: 0.0550, Val Loss: 0.0574
train_e/atom_mae: 0.000087
2025-04-09 03:47:42.166 INFO: train_e/atom_mae: 0.000087
train_e/atom_rmse: 0.000117
2025-04-09 03:47:42.166 INFO: train_e/atom_rmse: 0.000117
train_f_mae: 0.005182
2025-04-09 03:47:42.169 INFO: train_f_mae: 0.005182
train_f_rmse: 0.007301
2025-04-09 03:47:42.169 INFO: train_f_rmse: 0.007301
val_e/atom_mae: 0.000075
2025-04-09 03:47:42.171 INFO: val_e/atom_mae: 0.000075
val_e/atom_rmse: 0.000103
2025-04-09 03:47:42.171 INFO: val_e/atom_rmse: 0.000103
val_f_mae: 0.005303
2025-04-09 03:47:42.172 INFO: val_f_mae: 0.005303
val_f_rmse: 0.007493
2025-04-09 03:47:42.172 INFO: val_f_rmse: 0.007493
##### Step: 180 Learning rate: 1.953125e-05 #####
2025-04-09 03:49:06.612 INFO: ##### Step: 180 Learning rate: 1.953125e-05 #####
Epoch 41, Train Loss: 0.0542, Val Loss: 0.0576
2025-04-09 03:49:06.613 INFO: Epoch 41, Train Loss: 0.0542, Val Loss: 0.0576
train_e/atom_mae: 0.000072
2025-04-09 03:49:06.614 INFO: train_e/atom_mae: 0.000072
train_e/atom_rmse: 0.000098
2025-04-09 03:49:06.614 INFO: train_e/atom_rmse: 0.000098
train_f_mae: 0.005170
2025-04-09 03:49:06.616 INFO: train_f_mae: 0.005170
train_f_rmse: 0.007282
2025-04-09 03:49:06.616 INFO: train_f_rmse: 0.007282
val_e/atom_mae: 0.000085
2025-04-09 03:49:06.619 INFO: val_e/atom_mae: 0.000085
val_e/atom_rmse: 0.000115
2025-04-09 03:49:06.619 INFO: val_e/atom_rmse: 0.000115
val_f_mae: 0.005301
2025-04-09 03:49:06.619 INFO: val_f_mae: 0.005301
val_f_rmse: 0.007486
2025-04-09 03:49:06.620 INFO: val_f_rmse: 0.007486
##### Step: 181 Learning rate: 1.953125e-05 #####
2025-04-09 03:50:31.044 INFO: ##### Step: 181 Learning rate: 1.953125e-05 #####
Epoch 42, Train Loss: 0.0542, Val Loss: 0.0573
2025-04-09 03:50:31.045 INFO: Epoch 42, Train Loss: 0.0542, Val Loss: 0.0573
train_e/atom_mae: 0.000072
2025-04-09 03:50:31.046 INFO: train_e/atom_mae: 0.000072
train_e/atom_rmse: 0.000100
2025-04-09 03:50:31.046 INFO: train_e/atom_rmse: 0.000100
train_f_mae: 0.005172
2025-04-09 03:50:31.048 INFO: train_f_mae: 0.005172
train_f_rmse: 0.007283
2025-04-09 03:50:31.049 INFO: train_f_rmse: 0.007283
val_e/atom_mae: 0.000075
2025-04-09 03:50:31.051 INFO: val_e/atom_mae: 0.000075
val_e/atom_rmse: 0.000105
2025-04-09 03:50:31.051 INFO: val_e/atom_rmse: 0.000105
val_f_mae: 0.005290
2025-04-09 03:50:31.052 INFO: val_f_mae: 0.005290
val_f_rmse: 0.007481
2025-04-09 03:50:31.052 INFO: val_f_rmse: 0.007481
##### Step: 182 Learning rate: 1.953125e-05 #####
2025-04-09 03:51:55.525 INFO: ##### Step: 182 Learning rate: 1.953125e-05 #####
Epoch 43, Train Loss: 0.0543, Val Loss: 0.0580
2025-04-09 03:51:55.525 INFO: Epoch 43, Train Loss: 0.0543, Val Loss: 0.0580
train_e/atom_mae: 0.000076
2025-04-09 03:51:55.526 INFO: train_e/atom_mae: 0.000076
train_e/atom_rmse: 0.000103
2025-04-09 03:51:55.526 INFO: train_e/atom_rmse: 0.000103
train_f_mae: 0.005170
2025-04-09 03:51:55.529 INFO: train_f_mae: 0.005170
train_f_rmse: 0.007284
2025-04-09 03:51:55.529 INFO: train_f_rmse: 0.007284
val_e/atom_mae: 0.000095
2025-04-09 03:51:55.531 INFO: val_e/atom_mae: 0.000095
val_e/atom_rmse: 0.000122
2025-04-09 03:51:55.532 INFO: val_e/atom_rmse: 0.000122
val_f_mae: 0.005302
2025-04-09 03:51:55.532 INFO: val_f_mae: 0.005302
val_f_rmse: 0.007494
2025-04-09 03:51:55.532 INFO: val_f_rmse: 0.007494
##### Step: 183 Learning rate: 1.953125e-05 #####
2025-04-09 03:53:20.001 INFO: ##### Step: 183 Learning rate: 1.953125e-05 #####
Epoch 44, Train Loss: 0.0542, Val Loss: 0.0588
2025-04-09 03:53:20.002 INFO: Epoch 44, Train Loss: 0.0542, Val Loss: 0.0588
train_e/atom_mae: 0.000073
2025-04-09 03:53:20.003 INFO: train_e/atom_mae: 0.000073
train_e/atom_rmse: 0.000102
2025-04-09 03:53:20.003 INFO: train_e/atom_rmse: 0.000102
train_f_mae: 0.005167
2025-04-09 03:53:20.006 INFO: train_f_mae: 0.005167
train_f_rmse: 0.007278
2025-04-09 03:53:20.006 INFO: train_f_rmse: 0.007278
val_e/atom_mae: 0.000105
2025-04-09 03:53:20.009 INFO: val_e/atom_mae: 0.000105
val_e/atom_rmse: 0.000134
2025-04-09 03:53:20.009 INFO: val_e/atom_rmse: 0.000134
val_f_mae: 0.005322
2025-04-09 03:53:20.009 INFO: val_f_mae: 0.005322
val_f_rmse: 0.007525
2025-04-09 03:53:20.009 INFO: val_f_rmse: 0.007525
##### Step: 184 Learning rate: 1.953125e-05 #####
2025-04-09 03:54:44.438 INFO: ##### Step: 184 Learning rate: 1.953125e-05 #####
Epoch 45, Train Loss: 0.0542, Val Loss: 0.0578
2025-04-09 03:54:44.439 INFO: Epoch 45, Train Loss: 0.0542, Val Loss: 0.0578
train_e/atom_mae: 0.000076
2025-04-09 03:54:44.440 INFO: train_e/atom_mae: 0.000076
train_e/atom_rmse: 0.000103
2025-04-09 03:54:44.440 INFO: train_e/atom_rmse: 0.000103
train_f_mae: 0.005168
2025-04-09 03:54:44.443 INFO: train_f_mae: 0.005168
train_f_rmse: 0.007277
2025-04-09 03:54:44.443 INFO: train_f_rmse: 0.007277
val_e/atom_mae: 0.000097
2025-04-09 03:54:44.445 INFO: val_e/atom_mae: 0.000097
val_e/atom_rmse: 0.000121
2025-04-09 03:54:44.445 INFO: val_e/atom_rmse: 0.000121
val_f_mae: 0.005295
2025-04-09 03:54:44.446 INFO: val_f_mae: 0.005295
val_f_rmse: 0.007486
2025-04-09 03:54:44.446 INFO: val_f_rmse: 0.007486
##### Step: 185 Learning rate: 1.953125e-05 #####
2025-04-09 03:56:08.901 INFO: ##### Step: 185 Learning rate: 1.953125e-05 #####
Epoch 46, Train Loss: 0.0543, Val Loss: 0.0580
2025-04-09 03:56:08.901 INFO: Epoch 46, Train Loss: 0.0543, Val Loss: 0.0580
train_e/atom_mae: 0.000077
2025-04-09 03:56:08.902 INFO: train_e/atom_mae: 0.000077
train_e/atom_rmse: 0.000104
2025-04-09 03:56:08.902 INFO: train_e/atom_rmse: 0.000104
train_f_mae: 0.005168
2025-04-09 03:56:08.905 INFO: train_f_mae: 0.005168
train_f_rmse: 0.007280
2025-04-09 03:56:08.905 INFO: train_f_rmse: 0.007280
val_e/atom_mae: 0.000094
2025-04-09 03:56:08.907 INFO: val_e/atom_mae: 0.000094
val_e/atom_rmse: 0.000124
2025-04-09 03:56:08.908 INFO: val_e/atom_rmse: 0.000124
val_f_mae: 0.005306
2025-04-09 03:56:08.908 INFO: val_f_mae: 0.005306
val_f_rmse: 0.007496
2025-04-09 03:56:08.908 INFO: val_f_rmse: 0.007496
##### Step: 186 Learning rate: 1.953125e-05 #####
2025-04-09 03:57:33.388 INFO: ##### Step: 186 Learning rate: 1.953125e-05 #####
Epoch 47, Train Loss: 0.0544, Val Loss: 0.0572
2025-04-09 03:57:33.389 INFO: Epoch 47, Train Loss: 0.0544, Val Loss: 0.0572
train_e/atom_mae: 0.000079
2025-04-09 03:57:33.390 INFO: train_e/atom_mae: 0.000079
train_e/atom_rmse: 0.000107
2025-04-09 03:57:33.390 INFO: train_e/atom_rmse: 0.000107
train_f_mae: 0.005169
2025-04-09 03:57:33.393 INFO: train_f_mae: 0.005169
train_f_rmse: 0.007283
2025-04-09 03:57:33.393 INFO: train_f_rmse: 0.007283
val_e/atom_mae: 0.000073
2025-04-09 03:57:33.395 INFO: val_e/atom_mae: 0.000073
val_e/atom_rmse: 0.000103
2025-04-09 03:57:33.395 INFO: val_e/atom_rmse: 0.000103
val_f_mae: 0.005292
2025-04-09 03:57:33.396 INFO: val_f_mae: 0.005292
val_f_rmse: 0.007480
2025-04-09 03:57:33.396 INFO: val_f_rmse: 0.007480
##### Step: 187 Learning rate: 1.953125e-05 #####
2025-04-09 03:58:57.878 INFO: ##### Step: 187 Learning rate: 1.953125e-05 #####
Epoch 48, Train Loss: 0.0543, Val Loss: 0.0581
2025-04-09 03:58:57.879 INFO: Epoch 48, Train Loss: 0.0543, Val Loss: 0.0581
train_e/atom_mae: 0.000074
2025-04-09 03:58:57.879 INFO: train_e/atom_mae: 0.000074
train_e/atom_rmse: 0.000102
2025-04-09 03:58:57.880 INFO: train_e/atom_rmse: 0.000102
train_f_mae: 0.005170
2025-04-09 03:58:57.882 INFO: train_f_mae: 0.005170
train_f_rmse: 0.007281
2025-04-09 03:58:57.882 INFO: train_f_rmse: 0.007281
val_e/atom_mae: 0.000092
2025-04-09 03:58:57.885 INFO: val_e/atom_mae: 0.000092
val_e/atom_rmse: 0.000117
2025-04-09 03:58:57.885 INFO: val_e/atom_rmse: 0.000117
val_f_mae: 0.005318
2025-04-09 03:58:57.885 INFO: val_f_mae: 0.005318
val_f_rmse: 0.007511
2025-04-09 03:58:57.885 INFO: val_f_rmse: 0.007511
##### Step: 188 Learning rate: 1.953125e-05 #####
2025-04-09 04:00:22.338 INFO: ##### Step: 188 Learning rate: 1.953125e-05 #####
Epoch 49, Train Loss: 0.0543, Val Loss: 0.0575
2025-04-09 04:00:22.339 INFO: Epoch 49, Train Loss: 0.0543, Val Loss: 0.0575
train_e/atom_mae: 0.000076
2025-04-09 04:00:22.340 INFO: train_e/atom_mae: 0.000076
train_e/atom_rmse: 0.000104
2025-04-09 04:00:22.340 INFO: train_e/atom_rmse: 0.000104
train_f_mae: 0.005168
2025-04-09 04:00:22.343 INFO: train_f_mae: 0.005168
train_f_rmse: 0.007278
2025-04-09 04:00:22.343 INFO: train_f_rmse: 0.007278
val_e/atom_mae: 0.000085
2025-04-09 04:00:22.345 INFO: val_e/atom_mae: 0.000085
val_e/atom_rmse: 0.000113
2025-04-09 04:00:22.346 INFO: val_e/atom_rmse: 0.000113
val_f_mae: 0.005294
2025-04-09 04:00:22.346 INFO: val_f_mae: 0.005294
val_f_rmse: 0.007478
2025-04-09 04:00:22.346 INFO: val_f_rmse: 0.007478
##### Step: 189 Learning rate: 1.953125e-05 #####
2025-04-09 04:01:46.831 INFO: ##### Step: 189 Learning rate: 1.953125e-05 #####
Epoch 50, Train Loss: 0.0542, Val Loss: 0.0575
2025-04-09 04:01:46.832 INFO: Epoch 50, Train Loss: 0.0542, Val Loss: 0.0575
train_e/atom_mae: 0.000076
2025-04-09 04:01:46.833 INFO: train_e/atom_mae: 0.000076
train_e/atom_rmse: 0.000103
2025-04-09 04:01:46.833 INFO: train_e/atom_rmse: 0.000103
train_f_mae: 0.005166
2025-04-09 04:01:46.836 INFO: train_f_mae: 0.005166
train_f_rmse: 0.007276
2025-04-09 04:01:46.836 INFO: train_f_rmse: 0.007276
val_e/atom_mae: 0.000070
2025-04-09 04:01:46.838 INFO: val_e/atom_mae: 0.000070
val_e/atom_rmse: 0.000100
2025-04-09 04:01:46.838 INFO: val_e/atom_rmse: 0.000100
val_f_mae: 0.005306
2025-04-09 04:01:46.839 INFO: val_f_mae: 0.005306
val_f_rmse: 0.007504
2025-04-09 04:01:46.839 INFO: val_f_rmse: 0.007504
##### Step: 190 Learning rate: 1.953125e-05 #####
2025-04-09 04:03:11.327 INFO: ##### Step: 190 Learning rate: 1.953125e-05 #####
Epoch 51, Train Loss: 0.0541, Val Loss: 0.0570
2025-04-09 04:03:11.328 INFO: Epoch 51, Train Loss: 0.0541, Val Loss: 0.0570
train_e/atom_mae: 0.000070
2025-04-09 04:03:11.329 INFO: train_e/atom_mae: 0.000070
train_e/atom_rmse: 0.000098
2025-04-09 04:03:11.329 INFO: train_e/atom_rmse: 0.000098
train_f_mae: 0.005165
2025-04-09 04:03:11.332 INFO: train_f_mae: 0.005165
train_f_rmse: 0.007275
2025-04-09 04:03:11.332 INFO: train_f_rmse: 0.007275
val_e/atom_mae: 0.000062
2025-04-09 04:03:11.334 INFO: val_e/atom_mae: 0.000062
val_e/atom_rmse: 0.000090
2025-04-09 04:03:11.334 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.005294
2025-04-09 04:03:11.335 INFO: val_f_mae: 0.005294
val_f_rmse: 0.007481
2025-04-09 04:03:11.335 INFO: val_f_rmse: 0.007481
##### Step: 191 Learning rate: 1.953125e-05 #####
2025-04-09 04:04:35.835 INFO: ##### Step: 191 Learning rate: 1.953125e-05 #####
Epoch 52, Train Loss: 0.0541, Val Loss: 0.0572
2025-04-09 04:04:35.836 INFO: Epoch 52, Train Loss: 0.0541, Val Loss: 0.0572
train_e/atom_mae: 0.000070
2025-04-09 04:04:35.837 INFO: train_e/atom_mae: 0.000070
train_e/atom_rmse: 0.000098
2025-04-09 04:04:35.837 INFO: train_e/atom_rmse: 0.000098
train_f_mae: 0.005167
2025-04-09 04:04:35.839 INFO: train_f_mae: 0.005167
train_f_rmse: 0.007277
2025-04-09 04:04:35.839 INFO: train_f_rmse: 0.007277
val_e/atom_mae: 0.000067
2025-04-09 04:04:35.842 INFO: val_e/atom_mae: 0.000067
val_e/atom_rmse: 0.000098
2025-04-09 04:04:35.842 INFO: val_e/atom_rmse: 0.000098
val_f_mae: 0.005297
2025-04-09 04:04:35.842 INFO: val_f_mae: 0.005297
val_f_rmse: 0.007487
2025-04-09 04:04:35.843 INFO: val_f_rmse: 0.007487
##### Step: 192 Learning rate: 1.953125e-05 #####
2025-04-09 04:06:00.265 INFO: ##### Step: 192 Learning rate: 1.953125e-05 #####
Epoch 53, Train Loss: 0.0541, Val Loss: 0.0576
2025-04-09 04:06:00.265 INFO: Epoch 53, Train Loss: 0.0541, Val Loss: 0.0576
train_e/atom_mae: 0.000071
2025-04-09 04:06:00.266 INFO: train_e/atom_mae: 0.000071
train_e/atom_rmse: 0.000099
2025-04-09 04:06:00.266 INFO: train_e/atom_rmse: 0.000099
train_f_mae: 0.005166
2025-04-09 04:06:00.269 INFO: train_f_mae: 0.005166
train_f_rmse: 0.007278
2025-04-09 04:06:00.269 INFO: train_f_rmse: 0.007278
val_e/atom_mae: 0.000089
2025-04-09 04:06:00.271 INFO: val_e/atom_mae: 0.000089
val_e/atom_rmse: 0.000113
2025-04-09 04:06:00.272 INFO: val_e/atom_rmse: 0.000113
val_f_mae: 0.005295
2025-04-09 04:06:00.272 INFO: val_f_mae: 0.005295
val_f_rmse: 0.007488
2025-04-09 04:06:00.272 INFO: val_f_rmse: 0.007488
##### Step: 193 Learning rate: 1.953125e-05 #####
2025-04-09 04:07:24.746 INFO: ##### Step: 193 Learning rate: 1.953125e-05 #####
Epoch 54, Train Loss: 0.0541, Val Loss: 0.0570
2025-04-09 04:07:24.747 INFO: Epoch 54, Train Loss: 0.0541, Val Loss: 0.0570
train_e/atom_mae: 0.000073
2025-04-09 04:07:24.747 INFO: train_e/atom_mae: 0.000073
train_e/atom_rmse: 0.000100
2025-04-09 04:07:24.748 INFO: train_e/atom_rmse: 0.000100
train_f_mae: 0.005166
2025-04-09 04:07:24.750 INFO: train_f_mae: 0.005166
train_f_rmse: 0.007275
2025-04-09 04:07:24.750 INFO: train_f_rmse: 0.007275
val_e/atom_mae: 0.000061
2025-04-09 04:07:24.753 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000089
2025-04-09 04:07:24.753 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.005298
2025-04-09 04:07:24.753 INFO: val_f_mae: 0.005298
val_f_rmse: 0.007488
2025-04-09 04:07:24.753 INFO: val_f_rmse: 0.007488
##### Step: 194 Learning rate: 1.953125e-05 #####
2025-04-09 04:08:49.225 INFO: ##### Step: 194 Learning rate: 1.953125e-05 #####
Epoch 55, Train Loss: 0.0545, Val Loss: 0.0572
2025-04-09 04:08:49.225 INFO: Epoch 55, Train Loss: 0.0545, Val Loss: 0.0572
train_e/atom_mae: 0.000083
2025-04-09 04:08:49.226 INFO: train_e/atom_mae: 0.000083
train_e/atom_rmse: 0.000111
2025-04-09 04:08:49.226 INFO: train_e/atom_rmse: 0.000111
train_f_mae: 0.005166
2025-04-09 04:08:49.229 INFO: train_f_mae: 0.005166
train_f_rmse: 0.007278
2025-04-09 04:08:49.229 INFO: train_f_rmse: 0.007278
val_e/atom_mae: 0.000074
2025-04-09 04:08:49.232 INFO: val_e/atom_mae: 0.000074
val_e/atom_rmse: 0.000103
2025-04-09 04:08:49.232 INFO: val_e/atom_rmse: 0.000103
val_f_mae: 0.005295
2025-04-09 04:08:49.233 INFO: val_f_mae: 0.005295
val_f_rmse: 0.007476
2025-04-09 04:08:49.233 INFO: val_f_rmse: 0.007476
##### Step: 195 Learning rate: 1.953125e-05 #####
2025-04-09 04:10:13.701 INFO: ##### Step: 195 Learning rate: 1.953125e-05 #####
Epoch 56, Train Loss: 0.0542, Val Loss: 0.0575
2025-04-09 04:10:13.701 INFO: Epoch 56, Train Loss: 0.0542, Val Loss: 0.0575
train_e/atom_mae: 0.000076
2025-04-09 04:10:13.702 INFO: train_e/atom_mae: 0.000076
train_e/atom_rmse: 0.000104
2025-04-09 04:10:13.702 INFO: train_e/atom_rmse: 0.000104
train_f_mae: 0.005164
2025-04-09 04:10:13.705 INFO: train_f_mae: 0.005164
train_f_rmse: 0.007273
2025-04-09 04:10:13.705 INFO: train_f_rmse: 0.007273
val_e/atom_mae: 0.000087
2025-04-09 04:10:13.707 INFO: val_e/atom_mae: 0.000087
val_e/atom_rmse: 0.000111
2025-04-09 04:10:13.708 INFO: val_e/atom_rmse: 0.000111
val_f_mae: 0.005298
2025-04-09 04:10:13.708 INFO: val_f_mae: 0.005298
val_f_rmse: 0.007483
2025-04-09 04:10:13.708 INFO: val_f_rmse: 0.007483
##### Step: 196 Learning rate: 1.953125e-05 #####
2025-04-09 04:11:38.188 INFO: ##### Step: 196 Learning rate: 1.953125e-05 #####
Epoch 57, Train Loss: 0.0542, Val Loss: 0.0572
2025-04-09 04:11:38.189 INFO: Epoch 57, Train Loss: 0.0542, Val Loss: 0.0572
train_e/atom_mae: 0.000071
2025-04-09 04:11:38.189 INFO: train_e/atom_mae: 0.000071
train_e/atom_rmse: 0.000098
2025-04-09 04:11:38.189 INFO: train_e/atom_rmse: 0.000098
train_f_mae: 0.005168
2025-04-09 04:11:38.192 INFO: train_f_mae: 0.005168
train_f_rmse: 0.007280
2025-04-09 04:11:38.192 INFO: train_f_rmse: 0.007280
val_e/atom_mae: 0.000063
2025-04-09 04:11:38.194 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000090
2025-04-09 04:11:38.195 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.005302
2025-04-09 04:11:38.195 INFO: val_f_mae: 0.005302
val_f_rmse: 0.007495
2025-04-09 04:11:38.195 INFO: val_f_rmse: 0.007495
##### Step: 197 Learning rate: 1.953125e-05 #####
2025-04-09 04:13:02.671 INFO: ##### Step: 197 Learning rate: 1.953125e-05 #####
Epoch 58, Train Loss: 0.0543, Val Loss: 0.0568
2025-04-09 04:13:02.672 INFO: Epoch 58, Train Loss: 0.0543, Val Loss: 0.0568
train_e/atom_mae: 0.000077
2025-04-09 04:13:02.673 INFO: train_e/atom_mae: 0.000077
train_e/atom_rmse: 0.000105
2025-04-09 04:13:02.673 INFO: train_e/atom_rmse: 0.000105
train_f_mae: 0.005167
2025-04-09 04:13:02.675 INFO: train_f_mae: 0.005167
train_f_rmse: 0.007278
2025-04-09 04:13:02.675 INFO: train_f_rmse: 0.007278
val_e/atom_mae: 0.000060
2025-04-09 04:13:02.678 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000088
2025-04-09 04:13:02.678 INFO: val_e/atom_rmse: 0.000088
val_f_mae: 0.005288
2025-04-09 04:13:02.678 INFO: val_f_mae: 0.005288
val_f_rmse: 0.007471
2025-04-09 04:13:02.679 INFO: val_f_rmse: 0.007471
##### Step: 198 Learning rate: 1.953125e-05 #####
2025-04-09 04:14:27.176 INFO: ##### Step: 198 Learning rate: 1.953125e-05 #####
Epoch 59, Train Loss: 0.0540, Val Loss: 0.0575
2025-04-09 04:14:27.176 INFO: Epoch 59, Train Loss: 0.0540, Val Loss: 0.0575
train_e/atom_mae: 0.000071
2025-04-09 04:14:27.177 INFO: train_e/atom_mae: 0.000071
train_e/atom_rmse: 0.000098
2025-04-09 04:14:27.177 INFO: train_e/atom_rmse: 0.000098
train_f_mae: 0.005162
2025-04-09 04:14:27.180 INFO: train_f_mae: 0.005162
train_f_rmse: 0.007271
2025-04-09 04:14:27.180 INFO: train_f_rmse: 0.007271
val_e/atom_mae: 0.000082
2025-04-09 04:14:27.182 INFO: val_e/atom_mae: 0.000082
val_e/atom_rmse: 0.000112
2025-04-09 04:14:27.183 INFO: val_e/atom_rmse: 0.000112
val_f_mae: 0.005295
2025-04-09 04:14:27.183 INFO: val_f_mae: 0.005295
val_f_rmse: 0.007480
2025-04-09 04:14:27.183 INFO: val_f_rmse: 0.007480
##### Step: 199 Learning rate: 1.953125e-05 #####
2025-04-09 04:15:51.659 INFO: ##### Step: 199 Learning rate: 1.953125e-05 #####
Epoch 60, Train Loss: 0.0543, Val Loss: 0.0571
2025-04-09 04:15:51.660 INFO: Epoch 60, Train Loss: 0.0543, Val Loss: 0.0571
train_e/atom_mae: 0.000080
2025-04-09 04:15:51.660 INFO: train_e/atom_mae: 0.000080
train_e/atom_rmse: 0.000108
2025-04-09 04:15:51.661 INFO: train_e/atom_rmse: 0.000108
train_f_mae: 0.005165
2025-04-09 04:15:51.663 INFO: train_f_mae: 0.005165
train_f_rmse: 0.007275
2025-04-09 04:15:51.663 INFO: train_f_rmse: 0.007275
val_e/atom_mae: 0.000063
2025-04-09 04:15:51.666 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000092
2025-04-09 04:15:51.666 INFO: val_e/atom_rmse: 0.000092
val_f_mae: 0.005303
2025-04-09 04:15:51.666 INFO: val_f_mae: 0.005303
val_f_rmse: 0.007491
2025-04-09 04:15:51.666 INFO: val_f_rmse: 0.007491
##### Step: 200 Learning rate: 9.765625e-06 #####
2025-04-09 04:17:16.325 INFO: ##### Step: 200 Learning rate: 9.765625e-06 #####
Epoch 61, Train Loss: 0.0539, Val Loss: 0.0573
2025-04-09 04:17:16.326 INFO: Epoch 61, Train Loss: 0.0539, Val Loss: 0.0573
train_e/atom_mae: 0.000067
2025-04-09 04:17:16.327 INFO: train_e/atom_mae: 0.000067
train_e/atom_rmse: 0.000094
2025-04-09 04:17:16.327 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.005161
2025-04-09 04:17:16.329 INFO: train_f_mae: 0.005161
train_f_rmse: 0.007269
2025-04-09 04:17:16.330 INFO: train_f_rmse: 0.007269
val_e/atom_mae: 0.000071
2025-04-09 04:17:16.332 INFO: val_e/atom_mae: 0.000071
val_e/atom_rmse: 0.000103
2025-04-09 04:17:16.332 INFO: val_e/atom_rmse: 0.000103
val_f_mae: 0.005292
2025-04-09 04:17:16.333 INFO: val_f_mae: 0.005292
val_f_rmse: 0.007484
2025-04-09 04:17:16.333 INFO: val_f_rmse: 0.007484
##### Step: 201 Learning rate: 9.765625e-06 #####
2025-04-09 04:18:40.799 INFO: ##### Step: 201 Learning rate: 9.765625e-06 #####
Epoch 62, Train Loss: 0.0539, Val Loss: 0.0570
2025-04-09 04:18:40.799 INFO: Epoch 62, Train Loss: 0.0539, Val Loss: 0.0570
train_e/atom_mae: 0.000065
2025-04-09 04:18:40.800 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000092
2025-04-09 04:18:40.800 INFO: train_e/atom_rmse: 0.000092
train_f_mae: 0.005161
2025-04-09 04:18:40.803 INFO: train_f_mae: 0.005161
train_f_rmse: 0.007270
2025-04-09 04:18:40.803 INFO: train_f_rmse: 0.007270
val_e/atom_mae: 0.000062
2025-04-09 04:18:40.805 INFO: val_e/atom_mae: 0.000062
val_e/atom_rmse: 0.000089
2025-04-09 04:18:40.805 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.005295
2025-04-09 04:18:40.806 INFO: val_f_mae: 0.005295
val_f_rmse: 0.007485
2025-04-09 04:18:40.806 INFO: val_f_rmse: 0.007485
##### Step: 202 Learning rate: 9.765625e-06 #####
2025-04-09 04:20:05.295 INFO: ##### Step: 202 Learning rate: 9.765625e-06 #####
Epoch 63, Train Loss: 0.0539, Val Loss: 0.0569
2025-04-09 04:20:05.296 INFO: Epoch 63, Train Loss: 0.0539, Val Loss: 0.0569
train_e/atom_mae: 0.000069
2025-04-09 04:20:05.297 INFO: train_e/atom_mae: 0.000069
train_e/atom_rmse: 0.000096
2025-04-09 04:20:05.297 INFO: train_e/atom_rmse: 0.000096
train_f_mae: 0.005161
2025-04-09 04:20:05.300 INFO: train_f_mae: 0.005161
train_f_rmse: 0.007266
2025-04-09 04:20:05.300 INFO: train_f_rmse: 0.007266
val_e/atom_mae: 0.000062
2025-04-09 04:20:05.302 INFO: val_e/atom_mae: 0.000062
val_e/atom_rmse: 0.000091
2025-04-09 04:20:05.302 INFO: val_e/atom_rmse: 0.000091
val_f_mae: 0.005291
2025-04-09 04:20:05.303 INFO: val_f_mae: 0.005291
val_f_rmse: 0.007474
2025-04-09 04:20:05.303 INFO: val_f_rmse: 0.007474
##### Step: 203 Learning rate: 9.765625e-06 #####
2025-04-09 04:21:29.776 INFO: ##### Step: 203 Learning rate: 9.765625e-06 #####
Epoch 64, Train Loss: 0.0538, Val Loss: 0.0572
2025-04-09 04:21:29.777 INFO: Epoch 64, Train Loss: 0.0538, Val Loss: 0.0572
train_e/atom_mae: 0.000067
2025-04-09 04:21:29.777 INFO: train_e/atom_mae: 0.000067
train_e/atom_rmse: 0.000094
2025-04-09 04:21:29.777 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.005159
2025-04-09 04:21:29.780 INFO: train_f_mae: 0.005159
train_f_rmse: 0.007264
2025-04-09 04:21:29.780 INFO: train_f_rmse: 0.007264
val_e/atom_mae: 0.000070
2025-04-09 04:21:29.782 INFO: val_e/atom_mae: 0.000070
val_e/atom_rmse: 0.000098
2025-04-09 04:21:29.783 INFO: val_e/atom_rmse: 0.000098
val_f_mae: 0.005292
2025-04-09 04:21:29.783 INFO: val_f_mae: 0.005292
val_f_rmse: 0.007486
2025-04-09 04:21:29.783 INFO: val_f_rmse: 0.007486
##### Step: 204 Learning rate: 9.765625e-06 #####
2025-04-09 04:22:54.271 INFO: ##### Step: 204 Learning rate: 9.765625e-06 #####
Epoch 65, Train Loss: 0.0538, Val Loss: 0.0573
2025-04-09 04:22:54.272 INFO: Epoch 65, Train Loss: 0.0538, Val Loss: 0.0573
train_e/atom_mae: 0.000065
2025-04-09 04:22:54.273 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000093
2025-04-09 04:22:54.273 INFO: train_e/atom_rmse: 0.000093
train_f_mae: 0.005160
2025-04-09 04:22:54.275 INFO: train_f_mae: 0.005160
train_f_rmse: 0.007266
2025-04-09 04:22:54.275 INFO: train_f_rmse: 0.007266
val_e/atom_mae: 0.000076
2025-04-09 04:22:54.278 INFO: val_e/atom_mae: 0.000076
val_e/atom_rmse: 0.000105
2025-04-09 04:22:54.278 INFO: val_e/atom_rmse: 0.000105
val_f_mae: 0.005293
2025-04-09 04:22:54.278 INFO: val_f_mae: 0.005293
val_f_rmse: 0.007483
2025-04-09 04:22:54.279 INFO: val_f_rmse: 0.007483
##### Step: 205 Learning rate: 9.765625e-06 #####
2025-04-09 04:24:18.782 INFO: ##### Step: 205 Learning rate: 9.765625e-06 #####
Epoch 66, Train Loss: 0.0538, Val Loss: 0.0568
2025-04-09 04:24:18.782 INFO: Epoch 66, Train Loss: 0.0538, Val Loss: 0.0568
train_e/atom_mae: 0.000065
2025-04-09 04:24:18.783 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000092
2025-04-09 04:24:18.783 INFO: train_e/atom_rmse: 0.000092
train_f_mae: 0.005161
2025-04-09 04:24:18.786 INFO: train_f_mae: 0.005161
train_f_rmse: 0.007267
2025-04-09 04:24:18.786 INFO: train_f_rmse: 0.007267
val_e/atom_mae: 0.000060
2025-04-09 04:24:18.788 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000090
2025-04-09 04:24:18.789 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.005285
2025-04-09 04:24:18.789 INFO: val_f_mae: 0.005285
val_f_rmse: 0.007474
2025-04-09 04:24:18.789 INFO: val_f_rmse: 0.007474
##### Step: 206 Learning rate: 9.765625e-06 #####
2025-04-09 04:25:43.299 INFO: ##### Step: 206 Learning rate: 9.765625e-06 #####
Epoch 67, Train Loss: 0.0540, Val Loss: 0.0571
2025-04-09 04:25:43.300 INFO: Epoch 67, Train Loss: 0.0540, Val Loss: 0.0571
train_e/atom_mae: 0.000068
2025-04-09 04:25:43.301 INFO: train_e/atom_mae: 0.000068
train_e/atom_rmse: 0.000096
2025-04-09 04:25:43.301 INFO: train_e/atom_rmse: 0.000096
train_f_mae: 0.005161
2025-04-09 04:25:43.304 INFO: train_f_mae: 0.005161
train_f_rmse: 0.007269
2025-04-09 04:25:43.304 INFO: train_f_rmse: 0.007269
val_e/atom_mae: 0.000060
2025-04-09 04:25:43.306 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000089
2025-04-09 04:25:43.307 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.005298
2025-04-09 04:25:43.307 INFO: val_f_mae: 0.005298
val_f_rmse: 0.007496
2025-04-09 04:25:43.307 INFO: val_f_rmse: 0.007496
##### Step: 207 Learning rate: 9.765625e-06 #####
2025-04-09 04:27:07.790 INFO: ##### Step: 207 Learning rate: 9.765625e-06 #####
Epoch 68, Train Loss: 0.0539, Val Loss: 0.0573
2025-04-09 04:27:07.791 INFO: Epoch 68, Train Loss: 0.0539, Val Loss: 0.0573
train_e/atom_mae: 0.000070
2025-04-09 04:27:07.792 INFO: train_e/atom_mae: 0.000070
train_e/atom_rmse: 0.000096
2025-04-09 04:27:07.792 INFO: train_e/atom_rmse: 0.000096
train_f_mae: 0.005161
2025-04-09 04:27:07.795 INFO: train_f_mae: 0.005161
train_f_rmse: 0.007268
2025-04-09 04:27:07.795 INFO: train_f_rmse: 0.007268
val_e/atom_mae: 0.000065
2025-04-09 04:27:07.797 INFO: val_e/atom_mae: 0.000065
val_e/atom_rmse: 0.000094
2025-04-09 04:27:07.797 INFO: val_e/atom_rmse: 0.000094
val_f_mae: 0.005303
2025-04-09 04:27:07.798 INFO: val_f_mae: 0.005303
val_f_rmse: 0.007501
2025-04-09 04:27:07.798 INFO: val_f_rmse: 0.007501
##### Step: 208 Learning rate: 9.765625e-06 #####
2025-04-09 04:28:32.274 INFO: ##### Step: 208 Learning rate: 9.765625e-06 #####
Epoch 69, Train Loss: 0.0539, Val Loss: 0.0570
2025-04-09 04:28:32.275 INFO: Epoch 69, Train Loss: 0.0539, Val Loss: 0.0570
train_e/atom_mae: 0.000066
2025-04-09 04:28:32.275 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000094
2025-04-09 04:28:32.276 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.005161
2025-04-09 04:28:32.278 INFO: train_f_mae: 0.005161
train_f_rmse: 0.007268
2025-04-09 04:28:32.278 INFO: train_f_rmse: 0.007268
val_e/atom_mae: 0.000070
2025-04-09 04:28:32.281 INFO: val_e/atom_mae: 0.000070
val_e/atom_rmse: 0.000099
2025-04-09 04:28:32.281 INFO: val_e/atom_rmse: 0.000099
val_f_mae: 0.005291
2025-04-09 04:28:32.281 INFO: val_f_mae: 0.005291
val_f_rmse: 0.007473
2025-04-09 04:28:32.281 INFO: val_f_rmse: 0.007473
##### Step: 209 Learning rate: 9.765625e-06 #####
2025-04-09 04:29:56.768 INFO: ##### Step: 209 Learning rate: 9.765625e-06 #####
Epoch 70, Train Loss: 0.0538, Val Loss: 0.0571
2025-04-09 04:29:56.768 INFO: Epoch 70, Train Loss: 0.0538, Val Loss: 0.0571
train_e/atom_mae: 0.000065
2025-04-09 04:29:56.769 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000093
2025-04-09 04:29:56.769 INFO: train_e/atom_rmse: 0.000093
train_f_mae: 0.005160
2025-04-09 04:29:56.772 INFO: train_f_mae: 0.005160
train_f_rmse: 0.007266
2025-04-09 04:29:56.772 INFO: train_f_rmse: 0.007266
val_e/atom_mae: 0.000065
2025-04-09 04:29:56.774 INFO: val_e/atom_mae: 0.000065
val_e/atom_rmse: 0.000095
2025-04-09 04:29:56.775 INFO: val_e/atom_rmse: 0.000095
val_f_mae: 0.005292
2025-04-09 04:29:56.775 INFO: val_f_mae: 0.005292
val_f_rmse: 0.007486
2025-04-09 04:29:56.775 INFO: val_f_rmse: 0.007486
##### Step: 210 Learning rate: 9.765625e-06 #####
2025-04-09 04:31:21.281 INFO: ##### Step: 210 Learning rate: 9.765625e-06 #####
Epoch 71, Train Loss: 0.0538, Val Loss: 0.0570
2025-04-09 04:31:21.285 INFO: Epoch 71, Train Loss: 0.0538, Val Loss: 0.0570
train_e/atom_mae: 0.000065
2025-04-09 04:31:21.286 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000093
2025-04-09 04:31:21.286 INFO: train_e/atom_rmse: 0.000093
train_f_mae: 0.005159
2025-04-09 04:31:21.289 INFO: train_f_mae: 0.005159
train_f_rmse: 0.007266
2025-04-09 04:31:21.289 INFO: train_f_rmse: 0.007266
val_e/atom_mae: 0.000069
2025-04-09 04:31:21.291 INFO: val_e/atom_mae: 0.000069
val_e/atom_rmse: 0.000098
2025-04-09 04:31:21.292 INFO: val_e/atom_rmse: 0.000098
val_f_mae: 0.005291
2025-04-09 04:31:21.292 INFO: val_f_mae: 0.005291
val_f_rmse: 0.007474
2025-04-09 04:31:21.292 INFO: val_f_rmse: 0.007474
##### Step: 211 Learning rate: 9.765625e-06 #####
2025-04-09 04:32:45.699 INFO: ##### Step: 211 Learning rate: 9.765625e-06 #####
Epoch 72, Train Loss: 0.0539, Val Loss: 0.0570
2025-04-09 04:32:45.700 INFO: Epoch 72, Train Loss: 0.0539, Val Loss: 0.0570
train_e/atom_mae: 0.000066
2025-04-09 04:32:45.701 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000093
2025-04-09 04:32:45.701 INFO: train_e/atom_rmse: 0.000093
train_f_mae: 0.005161
2025-04-09 04:32:45.704 INFO: train_f_mae: 0.005161
train_f_rmse: 0.007267
2025-04-09 04:32:45.704 INFO: train_f_rmse: 0.007267
val_e/atom_mae: 0.000072
2025-04-09 04:32:45.706 INFO: val_e/atom_mae: 0.000072
val_e/atom_rmse: 0.000098
2025-04-09 04:32:45.706 INFO: val_e/atom_rmse: 0.000098
val_f_mae: 0.005291
2025-04-09 04:32:45.707 INFO: val_f_mae: 0.005291
val_f_rmse: 0.007474
2025-04-09 04:32:45.707 INFO: val_f_rmse: 0.007474
##### Step: 212 Learning rate: 9.765625e-06 #####
2025-04-09 04:34:10.198 INFO: ##### Step: 212 Learning rate: 9.765625e-06 #####
Epoch 73, Train Loss: 0.0539, Val Loss: 0.0568
2025-04-09 04:34:10.199 INFO: Epoch 73, Train Loss: 0.0539, Val Loss: 0.0568
train_e/atom_mae: 0.000067
2025-04-09 04:34:10.199 INFO: train_e/atom_mae: 0.000067
train_e/atom_rmse: 0.000094
2025-04-09 04:34:10.199 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.005159
2025-04-09 04:34:10.202 INFO: train_f_mae: 0.005159
train_f_rmse: 0.007266
2025-04-09 04:34:10.202 INFO: train_f_rmse: 0.007266
val_e/atom_mae: 0.000060
2025-04-09 04:34:10.204 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000089
2025-04-09 04:34:10.205 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.005289
2025-04-09 04:34:10.205 INFO: val_f_mae: 0.005289
val_f_rmse: 0.007473
2025-04-09 04:34:10.205 INFO: val_f_rmse: 0.007473
##### Step: 213 Learning rate: 9.765625e-06 #####
2025-04-09 04:35:34.694 INFO: ##### Step: 213 Learning rate: 9.765625e-06 #####
Epoch 74, Train Loss: 0.0539, Val Loss: 0.0572
2025-04-09 04:35:34.695 INFO: Epoch 74, Train Loss: 0.0539, Val Loss: 0.0572
train_e/atom_mae: 0.000069
2025-04-09 04:35:34.696 INFO: train_e/atom_mae: 0.000069
train_e/atom_rmse: 0.000097
2025-04-09 04:35:34.696 INFO: train_e/atom_rmse: 0.000097
train_f_mae: 0.005158
2025-04-09 04:35:34.698 INFO: train_f_mae: 0.005158
train_f_rmse: 0.007264
2025-04-09 04:35:34.699 INFO: train_f_rmse: 0.007264
val_e/atom_mae: 0.000068
2025-04-09 04:35:34.701 INFO: val_e/atom_mae: 0.000068
val_e/atom_rmse: 0.000098
2025-04-09 04:35:34.701 INFO: val_e/atom_rmse: 0.000098
val_f_mae: 0.005299
2025-04-09 04:35:34.702 INFO: val_f_mae: 0.005299
val_f_rmse: 0.007488
2025-04-09 04:35:34.702 INFO: val_f_rmse: 0.007488
##### Step: 214 Learning rate: 9.765625e-06 #####
2025-04-09 04:36:59.170 INFO: ##### Step: 214 Learning rate: 9.765625e-06 #####
Epoch 75, Train Loss: 0.0538, Val Loss: 0.0569
2025-04-09 04:36:59.171 INFO: Epoch 75, Train Loss: 0.0538, Val Loss: 0.0569
train_e/atom_mae: 0.000067
2025-04-09 04:36:59.171 INFO: train_e/atom_mae: 0.000067
train_e/atom_rmse: 0.000094
2025-04-09 04:36:59.171 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.005158
2025-04-09 04:36:59.174 INFO: train_f_mae: 0.005158
train_f_rmse: 0.007264
2025-04-09 04:36:59.174 INFO: train_f_rmse: 0.007264
val_e/atom_mae: 0.000063
2025-04-09 04:36:59.176 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000092
2025-04-09 04:36:59.177 INFO: val_e/atom_rmse: 0.000092
val_f_mae: 0.005290
2025-04-09 04:36:59.177 INFO: val_f_mae: 0.005290
val_f_rmse: 0.007475
2025-04-09 04:36:59.177 INFO: val_f_rmse: 0.007475
##### Step: 215 Learning rate: 9.765625e-06 #####
2025-04-09 04:38:23.676 INFO: ##### Step: 215 Learning rate: 9.765625e-06 #####
Epoch 76, Train Loss: 0.0538, Val Loss: 0.0573
2025-04-09 04:38:23.676 INFO: Epoch 76, Train Loss: 0.0538, Val Loss: 0.0573
train_e/atom_mae: 0.000067
2025-04-09 04:38:23.677 INFO: train_e/atom_mae: 0.000067
train_e/atom_rmse: 0.000094
2025-04-09 04:38:23.677 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.005158
2025-04-09 04:38:23.680 INFO: train_f_mae: 0.005158
train_f_rmse: 0.007264
2025-04-09 04:38:23.680 INFO: train_f_rmse: 0.007264
val_e/atom_mae: 0.000073
2025-04-09 04:38:23.682 INFO: val_e/atom_mae: 0.000073
val_e/atom_rmse: 0.000104
2025-04-09 04:38:23.683 INFO: val_e/atom_rmse: 0.000104
val_f_mae: 0.005291
2025-04-09 04:38:23.683 INFO: val_f_mae: 0.005291
val_f_rmse: 0.007481
2025-04-09 04:38:23.683 INFO: val_f_rmse: 0.007481
##### Step: 216 Learning rate: 9.765625e-06 #####
2025-04-09 04:39:48.156 INFO: ##### Step: 216 Learning rate: 9.765625e-06 #####
Epoch 77, Train Loss: 0.0538, Val Loss: 0.0569
2025-04-09 04:39:48.157 INFO: Epoch 77, Train Loss: 0.0538, Val Loss: 0.0569
train_e/atom_mae: 0.000066
2025-04-09 04:39:48.158 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000094
2025-04-09 04:39:48.158 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.005158
2025-04-09 04:39:48.160 INFO: train_f_mae: 0.005158
train_f_rmse: 0.007264
2025-04-09 04:39:48.161 INFO: train_f_rmse: 0.007264
val_e/atom_mae: 0.000063
2025-04-09 04:39:48.163 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000091
2025-04-09 04:39:48.163 INFO: val_e/atom_rmse: 0.000091
val_f_mae: 0.005289
2025-04-09 04:39:48.164 INFO: val_f_mae: 0.005289
val_f_rmse: 0.007474
2025-04-09 04:39:48.164 INFO: val_f_rmse: 0.007474
##### Step: 217 Learning rate: 9.765625e-06 #####
2025-04-09 04:41:12.672 INFO: ##### Step: 217 Learning rate: 9.765625e-06 #####
Epoch 78, Train Loss: 0.0538, Val Loss: 0.0569
2025-04-09 04:41:12.673 INFO: Epoch 78, Train Loss: 0.0538, Val Loss: 0.0569
train_e/atom_mae: 0.000067
2025-04-09 04:41:12.674 INFO: train_e/atom_mae: 0.000067
train_e/atom_rmse: 0.000094
2025-04-09 04:41:12.674 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.005157
2025-04-09 04:41:12.676 INFO: train_f_mae: 0.005157
train_f_rmse: 0.007262
2025-04-09 04:41:12.677 INFO: train_f_rmse: 0.007262
val_e/atom_mae: 0.000063
2025-04-09 04:41:12.679 INFO: val_e/atom_mae: 0.000063
val_e/atom_rmse: 0.000094
2025-04-09 04:41:12.679 INFO: val_e/atom_rmse: 0.000094
val_f_mae: 0.005289
2025-04-09 04:41:12.680 INFO: val_f_mae: 0.005289
val_f_rmse: 0.007475
2025-04-09 04:41:12.680 INFO: val_f_rmse: 0.007475
##### Step: 218 Learning rate: 9.765625e-06 #####
2025-04-09 04:42:37.176 INFO: ##### Step: 218 Learning rate: 9.765625e-06 #####
Epoch 79, Train Loss: 0.0539, Val Loss: 0.0576
2025-04-09 04:42:37.177 INFO: Epoch 79, Train Loss: 0.0539, Val Loss: 0.0576
train_e/atom_mae: 0.000069
2025-04-09 04:42:37.178 INFO: train_e/atom_mae: 0.000069
train_e/atom_rmse: 0.000096
2025-04-09 04:42:37.178 INFO: train_e/atom_rmse: 0.000096
train_f_mae: 0.005159
2025-04-09 04:42:37.180 INFO: train_f_mae: 0.005159
train_f_rmse: 0.007265
2025-04-09 04:42:37.180 INFO: train_f_rmse: 0.007265
val_e/atom_mae: 0.000083
2025-04-09 04:42:37.183 INFO: val_e/atom_mae: 0.000083
val_e/atom_rmse: 0.000110
2025-04-09 04:42:37.183 INFO: val_e/atom_rmse: 0.000110
val_f_mae: 0.005294
2025-04-09 04:42:37.183 INFO: val_f_mae: 0.005294
val_f_rmse: 0.007489
2025-04-09 04:42:37.184 INFO: val_f_rmse: 0.007489
##### Step: 219 Learning rate: 9.765625e-06 #####
2025-04-09 04:44:01.631 INFO: ##### Step: 219 Learning rate: 9.765625e-06 #####
Epoch 80, Train Loss: 0.0538, Val Loss: 0.0569
2025-04-09 04:44:01.632 INFO: Epoch 80, Train Loss: 0.0538, Val Loss: 0.0569
train_e/atom_mae: 0.000066
2025-04-09 04:44:01.633 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000092
2025-04-09 04:44:01.633 INFO: train_e/atom_rmse: 0.000092
train_f_mae: 0.005160
2025-04-09 04:44:01.636 INFO: train_f_mae: 0.005160
train_f_rmse: 0.007267
2025-04-09 04:44:01.636 INFO: train_f_rmse: 0.007267
val_e/atom_mae: 0.000060
2025-04-09 04:44:01.638 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000089
2025-04-09 04:44:01.639 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.005289
2025-04-09 04:44:01.639 INFO: val_f_mae: 0.005289
val_f_rmse: 0.007476
2025-04-09 04:44:01.639 INFO: val_f_rmse: 0.007476
##### Step: 220 Learning rate: 4.8828125e-06 #####
2025-04-09 04:45:26.112 INFO: ##### Step: 220 Learning rate: 4.8828125e-06 #####
Epoch 81, Train Loss: 0.0537, Val Loss: 0.0568
2025-04-09 04:45:26.114 INFO: Epoch 81, Train Loss: 0.0537, Val Loss: 0.0568
train_e/atom_mae: 0.000063
2025-04-09 04:45:26.114 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000090
2025-04-09 04:45:26.114 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.005155
2025-04-09 04:45:26.117 INFO: train_f_mae: 0.005155
train_f_rmse: 0.007260
2025-04-09 04:45:26.117 INFO: train_f_rmse: 0.007260
val_e/atom_mae: 0.000061
2025-04-09 04:45:26.119 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000090
2025-04-09 04:45:26.120 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.005289
2025-04-09 04:45:26.120 INFO: val_f_mae: 0.005289
val_f_rmse: 0.007473
2025-04-09 04:45:26.120 INFO: val_f_rmse: 0.007473
##### Step: 221 Learning rate: 4.8828125e-06 #####
2025-04-09 04:46:50.620 INFO: ##### Step: 221 Learning rate: 4.8828125e-06 #####
Epoch 82, Train Loss: 0.0537, Val Loss: 0.0569
2025-04-09 04:46:50.621 INFO: Epoch 82, Train Loss: 0.0537, Val Loss: 0.0569
train_e/atom_mae: 0.000066
2025-04-09 04:46:50.622 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000093
2025-04-09 04:46:50.622 INFO: train_e/atom_rmse: 0.000093
train_f_mae: 0.005155
2025-04-09 04:46:50.625 INFO: train_f_mae: 0.005155
train_f_rmse: 0.007259
2025-04-09 04:46:50.625 INFO: train_f_rmse: 0.007259
val_e/atom_mae: 0.000061
2025-04-09 04:46:50.627 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000092
2025-04-09 04:46:50.627 INFO: val_e/atom_rmse: 0.000092
val_f_mae: 0.005288
2025-04-09 04:46:50.628 INFO: val_f_mae: 0.005288
val_f_rmse: 0.007473
2025-04-09 04:46:50.628 INFO: val_f_rmse: 0.007473
##### Step: 222 Learning rate: 4.8828125e-06 #####
2025-04-09 04:48:15.087 INFO: ##### Step: 222 Learning rate: 4.8828125e-06 #####
Epoch 83, Train Loss: 0.0537, Val Loss: 0.0568
2025-04-09 04:48:15.087 INFO: Epoch 83, Train Loss: 0.0537, Val Loss: 0.0568
train_e/atom_mae: 0.000063
2025-04-09 04:48:15.088 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000090
2025-04-09 04:48:15.088 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.005156
2025-04-09 04:48:15.091 INFO: train_f_mae: 0.005156
train_f_rmse: 0.007260
2025-04-09 04:48:15.091 INFO: train_f_rmse: 0.007260
val_e/atom_mae: 0.000060
2025-04-09 04:48:15.093 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000090
2025-04-09 04:48:15.094 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.005288
2025-04-09 04:48:15.094 INFO: val_f_mae: 0.005288
val_f_rmse: 0.007474
2025-04-09 04:48:15.094 INFO: val_f_rmse: 0.007474
##### Step: 223 Learning rate: 4.8828125e-06 #####
2025-04-09 04:49:39.568 INFO: ##### Step: 223 Learning rate: 4.8828125e-06 #####
Epoch 84, Train Loss: 0.0537, Val Loss: 0.0572
2025-04-09 04:49:39.569 INFO: Epoch 84, Train Loss: 0.0537, Val Loss: 0.0572
train_e/atom_mae: 0.000062
2025-04-09 04:49:39.569 INFO: train_e/atom_mae: 0.000062
train_e/atom_rmse: 0.000090
2025-04-09 04:49:39.569 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.005157
2025-04-09 04:49:39.572 INFO: train_f_mae: 0.005157
train_f_rmse: 0.007261
2025-04-09 04:49:39.572 INFO: train_f_rmse: 0.007261
val_e/atom_mae: 0.000077
2025-04-09 04:49:39.574 INFO: val_e/atom_mae: 0.000077
val_e/atom_rmse: 0.000106
2025-04-09 04:49:39.575 INFO: val_e/atom_rmse: 0.000106
val_f_mae: 0.005285
2025-04-09 04:49:39.576 INFO: val_f_mae: 0.005285
val_f_rmse: 0.007469
2025-04-09 04:49:39.576 INFO: val_f_rmse: 0.007469
##### Step: 224 Learning rate: 4.8828125e-06 #####
2025-04-09 04:51:04.052 INFO: ##### Step: 224 Learning rate: 4.8828125e-06 #####
Epoch 85, Train Loss: 0.0537, Val Loss: 0.0568
2025-04-09 04:51:04.052 INFO: Epoch 85, Train Loss: 0.0537, Val Loss: 0.0568
train_e/atom_mae: 0.000066
2025-04-09 04:51:04.053 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000093
2025-04-09 04:51:04.053 INFO: train_e/atom_rmse: 0.000093
train_f_mae: 0.005156
2025-04-09 04:51:04.056 INFO: train_f_mae: 0.005156
train_f_rmse: 0.007260
2025-04-09 04:51:04.056 INFO: train_f_rmse: 0.007260
val_e/atom_mae: 0.000065
2025-04-09 04:51:04.058 INFO: val_e/atom_mae: 0.000065
val_e/atom_rmse: 0.000094
2025-04-09 04:51:04.059 INFO: val_e/atom_rmse: 0.000094
val_f_mae: 0.005285
2025-04-09 04:51:04.059 INFO: val_f_mae: 0.005285
val_f_rmse: 0.007467
2025-04-09 04:51:04.060 INFO: val_f_rmse: 0.007467
##### Step: 225 Learning rate: 4.8828125e-06 #####
2025-04-09 04:52:28.553 INFO: ##### Step: 225 Learning rate: 4.8828125e-06 #####
Epoch 86, Train Loss: 0.0537, Val Loss: 0.0568
2025-04-09 04:52:28.554 INFO: Epoch 86, Train Loss: 0.0537, Val Loss: 0.0568
train_e/atom_mae: 0.000064
2025-04-09 04:52:28.554 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000091
2025-04-09 04:52:28.554 INFO: train_e/atom_rmse: 0.000091
train_f_mae: 0.005156
2025-04-09 04:52:28.557 INFO: train_f_mae: 0.005156
train_f_rmse: 0.007260
2025-04-09 04:52:28.557 INFO: train_f_rmse: 0.007260
val_e/atom_mae: 0.000060
2025-04-09 04:52:28.559 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000089
2025-04-09 04:52:28.560 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.005286
2025-04-09 04:52:28.560 INFO: val_f_mae: 0.005286
val_f_rmse: 0.007473
2025-04-09 04:52:28.560 INFO: val_f_rmse: 0.007473
##### Step: 226 Learning rate: 4.8828125e-06 #####
2025-04-09 04:53:53.032 INFO: ##### Step: 226 Learning rate: 4.8828125e-06 #####
Epoch 87, Train Loss: 0.0537, Val Loss: 0.0567
2025-04-09 04:53:53.033 INFO: Epoch 87, Train Loss: 0.0537, Val Loss: 0.0567
train_e/atom_mae: 0.000066
2025-04-09 04:53:53.034 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000093
2025-04-09 04:53:53.034 INFO: train_e/atom_rmse: 0.000093
train_f_mae: 0.005154
2025-04-09 04:53:53.037 INFO: train_f_mae: 0.005154
train_f_rmse: 0.007258
2025-04-09 04:53:53.037 INFO: train_f_rmse: 0.007258
val_e/atom_mae: 0.000059
2025-04-09 04:53:53.039 INFO: val_e/atom_mae: 0.000059
val_e/atom_rmse: 0.000089
2025-04-09 04:53:53.040 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.005284
2025-04-09 04:53:53.040 INFO: val_f_mae: 0.005284
val_f_rmse: 0.007469
2025-04-09 04:53:53.040 INFO: val_f_rmse: 0.007469
##### Step: 227 Learning rate: 4.8828125e-06 #####
2025-04-09 04:55:17.472 INFO: ##### Step: 227 Learning rate: 4.8828125e-06 #####
Epoch 88, Train Loss: 0.0537, Val Loss: 0.0568
2025-04-09 04:55:17.473 INFO: Epoch 88, Train Loss: 0.0537, Val Loss: 0.0568
train_e/atom_mae: 0.000063
2025-04-09 04:55:17.474 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000090
2025-04-09 04:55:17.474 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.005155
2025-04-09 04:55:17.476 INFO: train_f_mae: 0.005155
train_f_rmse: 0.007260
2025-04-09 04:55:17.477 INFO: train_f_rmse: 0.007260
val_e/atom_mae: 0.000061
2025-04-09 04:55:17.479 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000090
2025-04-09 04:55:17.479 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.005288
2025-04-09 04:55:17.480 INFO: val_f_mae: 0.005288
val_f_rmse: 0.007472
2025-04-09 04:55:17.480 INFO: val_f_rmse: 0.007472
##### Step: 228 Learning rate: 4.8828125e-06 #####
2025-04-09 04:56:41.949 INFO: ##### Step: 228 Learning rate: 4.8828125e-06 #####
Epoch 89, Train Loss: 0.0537, Val Loss: 0.0568
2025-04-09 04:56:41.950 INFO: Epoch 89, Train Loss: 0.0537, Val Loss: 0.0568
train_e/atom_mae: 0.000063
2025-04-09 04:56:41.950 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000090
2025-04-09 04:56:41.950 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.005154
2025-04-09 04:56:41.953 INFO: train_f_mae: 0.005154
train_f_rmse: 0.007258
2025-04-09 04:56:41.953 INFO: train_f_rmse: 0.007258
val_e/atom_mae: 0.000061
2025-04-09 04:56:41.955 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000090
2025-04-09 04:56:41.956 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.005285
2025-04-09 04:56:41.956 INFO: val_f_mae: 0.005285
val_f_rmse: 0.007468
2025-04-09 04:56:41.956 INFO: val_f_rmse: 0.007468
##### Step: 229 Learning rate: 4.8828125e-06 #####
2025-04-09 04:58:06.420 INFO: ##### Step: 229 Learning rate: 4.8828125e-06 #####
Epoch 90, Train Loss: 0.0537, Val Loss: 0.0568
2025-04-09 04:58:06.421 INFO: Epoch 90, Train Loss: 0.0537, Val Loss: 0.0568
train_e/atom_mae: 0.000064
2025-04-09 04:58:06.421 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000090
2025-04-09 04:58:06.421 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.005156
2025-04-09 04:58:06.424 INFO: train_f_mae: 0.005156
train_f_rmse: 0.007262
2025-04-09 04:58:06.424 INFO: train_f_rmse: 0.007262
val_e/atom_mae: 0.000060
2025-04-09 04:58:06.426 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000090
2025-04-09 04:58:06.427 INFO: val_e/atom_rmse: 0.000090
val_f_mae: 0.005287
2025-04-09 04:58:06.427 INFO: val_f_mae: 0.005287
val_f_rmse: 0.007473
2025-04-09 04:58:06.427 INFO: val_f_rmse: 0.007473
##### Step: 230 Learning rate: 4.8828125e-06 #####
2025-04-09 04:59:30.936 INFO: ##### Step: 230 Learning rate: 4.8828125e-06 #####
Epoch 91, Train Loss: 0.0537, Val Loss: 0.0569
2025-04-09 04:59:30.936 INFO: Epoch 91, Train Loss: 0.0537, Val Loss: 0.0569
train_e/atom_mae: 0.000064
2025-04-09 04:59:30.937 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000090
2025-04-09 04:59:30.937 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.005157
2025-04-09 04:59:30.940 INFO: train_f_mae: 0.005157
train_f_rmse: 0.007261
2025-04-09 04:59:30.940 INFO: train_f_rmse: 0.007261
val_e/atom_mae: 0.000065
2025-04-09 04:59:30.942 INFO: val_e/atom_mae: 0.000065
val_e/atom_rmse: 0.000093
2025-04-09 04:59:30.943 INFO: val_e/atom_rmse: 0.000093
val_f_mae: 0.005285
2025-04-09 04:59:30.943 INFO: val_f_mae: 0.005285
val_f_rmse: 0.007470
2025-04-09 04:59:30.943 INFO: val_f_rmse: 0.007470
##### Step: 231 Learning rate: 4.8828125e-06 #####
2025-04-09 05:00:55.438 INFO: ##### Step: 231 Learning rate: 4.8828125e-06 #####
Epoch 92, Train Loss: 0.0537, Val Loss: 0.0569
2025-04-09 05:00:55.438 INFO: Epoch 92, Train Loss: 0.0537, Val Loss: 0.0569
train_e/atom_mae: 0.000064
2025-04-09 05:00:55.439 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000091
2025-04-09 05:00:55.439 INFO: train_e/atom_rmse: 0.000091
train_f_mae: 0.005155
2025-04-09 05:00:55.442 INFO: train_f_mae: 0.005155
train_f_rmse: 0.007258
2025-04-09 05:00:55.442 INFO: train_f_rmse: 0.007258
val_e/atom_mae: 0.000060
2025-04-09 05:00:55.444 INFO: val_e/atom_mae: 0.000060
val_e/atom_rmse: 0.000089
2025-04-09 05:00:55.445 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.005293
2025-04-09 05:00:55.445 INFO: val_f_mae: 0.005293
val_f_rmse: 0.007480
2025-04-09 05:00:55.445 INFO: val_f_rmse: 0.007480
##### Step: 232 Learning rate: 4.8828125e-06 #####
2025-04-09 05:02:19.951 INFO: ##### Step: 232 Learning rate: 4.8828125e-06 #####
Epoch 93, Train Loss: 0.0537, Val Loss: 0.0569
2025-04-09 05:02:19.951 INFO: Epoch 93, Train Loss: 0.0537, Val Loss: 0.0569
train_e/atom_mae: 0.000063
2025-04-09 05:02:19.952 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000090
2025-04-09 05:02:19.952 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.005155
2025-04-09 05:02:19.955 INFO: train_f_mae: 0.005155
train_f_rmse: 0.007259
2025-04-09 05:02:19.955 INFO: train_f_rmse: 0.007259
val_e/atom_mae: 0.000066
2025-04-09 05:02:19.957 INFO: val_e/atom_mae: 0.000066
val_e/atom_rmse: 0.000095
2025-04-09 05:02:19.958 INFO: val_e/atom_rmse: 0.000095
val_f_mae: 0.005286
2025-04-09 05:02:19.958 INFO: val_f_mae: 0.005286
val_f_rmse: 0.007468
2025-04-09 05:02:19.958 INFO: val_f_rmse: 0.007468
##### Step: 233 Learning rate: 4.8828125e-06 #####
2025-04-09 05:03:44.456 INFO: ##### Step: 233 Learning rate: 4.8828125e-06 #####
Epoch 94, Train Loss: 0.0537, Val Loss: 0.0568
2025-04-09 05:03:44.457 INFO: Epoch 94, Train Loss: 0.0537, Val Loss: 0.0568
train_e/atom_mae: 0.000063
2025-04-09 05:03:44.458 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000090
2025-04-09 05:03:44.458 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.005156
2025-04-09 05:03:44.460 INFO: train_f_mae: 0.005156
train_f_rmse: 0.007260
2025-04-09 05:03:44.461 INFO: train_f_rmse: 0.007260
val_e/atom_mae: 0.000061
2025-04-09 05:03:44.463 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000091
2025-04-09 05:03:44.463 INFO: val_e/atom_rmse: 0.000091
val_f_mae: 0.005284
2025-04-09 05:03:44.464 INFO: val_f_mae: 0.005284
val_f_rmse: 0.007470
2025-04-09 05:03:44.464 INFO: val_f_rmse: 0.007470
##### Step: 234 Learning rate: 4.8828125e-06 #####
2025-04-09 05:05:08.952 INFO: ##### Step: 234 Learning rate: 4.8828125e-06 #####
Epoch 95, Train Loss: 0.0537, Val Loss: 0.0569
2025-04-09 05:05:08.953 INFO: Epoch 95, Train Loss: 0.0537, Val Loss: 0.0569
train_e/atom_mae: 0.000064
2025-04-09 05:05:08.954 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000092
2025-04-09 05:05:08.954 INFO: train_e/atom_rmse: 0.000092
train_f_mae: 0.005155
2025-04-09 05:05:08.956 INFO: train_f_mae: 0.005155
train_f_rmse: 0.007260
2025-04-09 05:05:08.957 INFO: train_f_rmse: 0.007260
val_e/atom_mae: 0.000064
2025-04-09 05:05:08.959 INFO: val_e/atom_mae: 0.000064
val_e/atom_rmse: 0.000093
2025-04-09 05:05:08.959 INFO: val_e/atom_rmse: 0.000093
val_f_mae: 0.005288
2025-04-09 05:05:08.959 INFO: val_f_mae: 0.005288
val_f_rmse: 0.007474
2025-04-09 05:05:08.960 INFO: val_f_rmse: 0.007474
##### Step: 235 Learning rate: 4.8828125e-06 #####
2025-04-09 05:06:33.425 INFO: ##### Step: 235 Learning rate: 4.8828125e-06 #####
Epoch 96, Train Loss: 0.0538, Val Loss: 0.0569
2025-04-09 05:06:33.425 INFO: Epoch 96, Train Loss: 0.0538, Val Loss: 0.0569
train_e/atom_mae: 0.000066
2025-04-09 05:06:33.426 INFO: train_e/atom_mae: 0.000066
train_e/atom_rmse: 0.000094
2025-04-09 05:06:33.426 INFO: train_e/atom_rmse: 0.000094
train_f_mae: 0.005155
2025-04-09 05:06:33.429 INFO: train_f_mae: 0.005155
train_f_rmse: 0.007259
2025-04-09 05:06:33.429 INFO: train_f_rmse: 0.007259
val_e/atom_mae: 0.000064
2025-04-09 05:06:33.431 INFO: val_e/atom_mae: 0.000064
val_e/atom_rmse: 0.000094
2025-04-09 05:06:33.432 INFO: val_e/atom_rmse: 0.000094
val_f_mae: 0.005285
2025-04-09 05:06:33.432 INFO: val_f_mae: 0.005285
val_f_rmse: 0.007472
2025-04-09 05:06:33.432 INFO: val_f_rmse: 0.007472
##### Step: 236 Learning rate: 4.8828125e-06 #####
2025-04-09 05:07:57.928 INFO: ##### Step: 236 Learning rate: 4.8828125e-06 #####
Epoch 97, Train Loss: 0.0537, Val Loss: 0.0568
2025-04-09 05:07:57.929 INFO: Epoch 97, Train Loss: 0.0537, Val Loss: 0.0568
train_e/atom_mae: 0.000064
2025-04-09 05:07:57.930 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000091
2025-04-09 05:07:57.930 INFO: train_e/atom_rmse: 0.000091
train_f_mae: 0.005154
2025-04-09 05:07:57.932 INFO: train_f_mae: 0.005154
train_f_rmse: 0.007258
2025-04-09 05:07:57.933 INFO: train_f_rmse: 0.007258
val_e/atom_mae: 0.000059
2025-04-09 05:07:57.935 INFO: val_e/atom_mae: 0.000059
val_e/atom_rmse: 0.000089
2025-04-09 05:07:57.935 INFO: val_e/atom_rmse: 0.000089
val_f_mae: 0.005287
2025-04-09 05:07:57.936 INFO: val_f_mae: 0.005287
val_f_rmse: 0.007475
2025-04-09 05:07:57.936 INFO: val_f_rmse: 0.007475
##### Step: 237 Learning rate: 4.8828125e-06 #####
2025-04-09 05:09:22.396 INFO: ##### Step: 237 Learning rate: 4.8828125e-06 #####
Epoch 98, Train Loss: 0.0537, Val Loss: 0.0569
2025-04-09 05:09:22.397 INFO: Epoch 98, Train Loss: 0.0537, Val Loss: 0.0569
train_e/atom_mae: 0.000064
2025-04-09 05:09:22.398 INFO: train_e/atom_mae: 0.000064
train_e/atom_rmse: 0.000091
2025-04-09 05:09:22.398 INFO: train_e/atom_rmse: 0.000091
train_f_mae: 0.005155
2025-04-09 05:09:22.400 INFO: train_f_mae: 0.005155
train_f_rmse: 0.007257
2025-04-09 05:09:22.401 INFO: train_f_rmse: 0.007257
val_e/atom_mae: 0.000061
2025-04-09 05:09:22.403 INFO: val_e/atom_mae: 0.000061
val_e/atom_rmse: 0.000091
2025-04-09 05:09:22.403 INFO: val_e/atom_rmse: 0.000091
val_f_mae: 0.005287
2025-04-09 05:09:22.403 INFO: val_f_mae: 0.005287
val_f_rmse: 0.007474
2025-04-09 05:09:22.404 INFO: val_f_rmse: 0.007474
##### Step: 238 Learning rate: 4.8828125e-06 #####
2025-04-09 05:10:46.884 INFO: ##### Step: 238 Learning rate: 4.8828125e-06 #####
Epoch 99, Train Loss: 0.0537, Val Loss: 0.0568
2025-04-09 05:10:46.884 INFO: Epoch 99, Train Loss: 0.0537, Val Loss: 0.0568
train_e/atom_mae: 0.000065
2025-04-09 05:10:46.885 INFO: train_e/atom_mae: 0.000065
train_e/atom_rmse: 0.000092
2025-04-09 05:10:46.885 INFO: train_e/atom_rmse: 0.000092
train_f_mae: 0.005155
2025-04-09 05:10:46.888 INFO: train_f_mae: 0.005155
train_f_rmse: 0.007258
2025-04-09 05:10:46.888 INFO: train_f_rmse: 0.007258
val_e/atom_mae: 0.000062
2025-04-09 05:10:46.890 INFO: val_e/atom_mae: 0.000062
val_e/atom_rmse: 0.000092
2025-04-09 05:10:46.891 INFO: val_e/atom_rmse: 0.000092
val_f_mae: 0.005286
2025-04-09 05:10:46.891 INFO: val_f_mae: 0.005286
val_f_rmse: 0.007469
2025-04-09 05:10:46.891 INFO: val_f_rmse: 0.007469
##### Step: 239 Learning rate: 4.8828125e-06 #####
2025-04-09 05:12:11.396 INFO: ##### Step: 239 Learning rate: 4.8828125e-06 #####
Epoch 100, Train Loss: 0.0537, Val Loss: 0.0567
2025-04-09 05:12:11.397 INFO: Epoch 100, Train Loss: 0.0537, Val Loss: 0.0567
train_e/atom_mae: 0.000063
2025-04-09 05:12:11.398 INFO: train_e/atom_mae: 0.000063
train_e/atom_rmse: 0.000090
2025-04-09 05:12:11.398 INFO: train_e/atom_rmse: 0.000090
train_f_mae: 0.005154
2025-04-09 05:12:11.401 INFO: train_f_mae: 0.005154
train_f_rmse: 0.007258
2025-04-09 05:12:11.401 INFO: train_f_rmse: 0.007258
val_e/atom_mae: 0.000062
2025-04-09 05:12:11.403 INFO: val_e/atom_mae: 0.000062
val_e/atom_rmse: 0.000091
2025-04-09 05:12:11.403 INFO: val_e/atom_rmse: 0.000091
val_f_mae: 0.005284
2025-04-09 05:12:11.404 INFO: val_f_mae: 0.005284
val_f_rmse: 0.007466
2025-04-09 05:12:11.404 INFO: val_f_rmse: 0.007466
2025-04-09 05:12:11.438 INFO: Fourth train loop:
##### Step: 240 Learning rate: 2.44140625e-06 #####
2025-04-09 05:13:35.920 INFO: ##### Step: 240 Learning rate: 2.44140625e-06 #####
Epoch 1, Train Loss: 0.1370, Val Loss: 0.1321
2025-04-09 05:13:35.921 INFO: Epoch 1, Train Loss: 0.1370, Val Loss: 0.1321
train_e/atom_mae: 0.000055
2025-04-09 05:13:35.922 INFO: train_e/atom_mae: 0.000055
train_e/atom_rmse: 0.000083
2025-04-09 05:13:35.922 INFO: train_e/atom_rmse: 0.000083
train_f_mae: 0.005163
2025-04-09 05:13:35.925 INFO: train_f_mae: 0.005163
train_f_rmse: 0.007282
2025-04-09 05:13:35.925 INFO: train_f_rmse: 0.007282
val_e/atom_mae: 0.000048
2025-04-09 05:13:35.927 INFO: val_e/atom_mae: 0.000048
val_e/atom_rmse: 0.000079
2025-04-09 05:13:35.927 INFO: val_e/atom_rmse: 0.000079
val_f_mae: 0.005309
2025-04-09 05:13:35.928 INFO: val_f_mae: 0.005309
val_f_rmse: 0.007547
2025-04-09 05:13:35.928 INFO: val_f_rmse: 0.007547
##### Step: 241 Learning rate: 2.44140625e-06 #####
2025-04-09 05:15:00.417 INFO: ##### Step: 241 Learning rate: 2.44140625e-06 #####
Epoch 2, Train Loss: 0.1208, Val Loss: 0.1279
2025-04-09 05:15:00.417 INFO: Epoch 2, Train Loss: 0.1208, Val Loss: 0.1279
train_e/atom_mae: 0.000044
2025-04-09 05:15:00.418 INFO: train_e/atom_mae: 0.000044
train_e/atom_rmse: 0.000074
2025-04-09 05:15:00.418 INFO: train_e/atom_rmse: 0.000074
train_f_mae: 0.005193
2025-04-09 05:15:00.421 INFO: train_f_mae: 0.005193
train_f_rmse: 0.007355
2025-04-09 05:15:00.421 INFO: train_f_rmse: 0.007355
val_e/atom_mae: 0.000040
2025-04-09 05:15:00.423 INFO: val_e/atom_mae: 0.000040
val_e/atom_rmse: 0.000076
2025-04-09 05:15:00.424 INFO: val_e/atom_rmse: 0.000076
val_f_mae: 0.005339
2025-04-09 05:15:00.424 INFO: val_f_mae: 0.005339
val_f_rmse: 0.007618
2025-04-09 05:15:00.424 INFO: val_f_rmse: 0.007618
##### Step: 242 Learning rate: 2.44140625e-06 #####
2025-04-09 05:16:24.931 INFO: ##### Step: 242 Learning rate: 2.44140625e-06 #####
Epoch 3, Train Loss: 0.1174, Val Loss: 0.1302
2025-04-09 05:16:24.932 INFO: Epoch 3, Train Loss: 0.1174, Val Loss: 0.1302
train_e/atom_mae: 0.000040
2025-04-09 05:16:24.933 INFO: train_e/atom_mae: 0.000040
train_e/atom_rmse: 0.000072
2025-04-09 05:16:24.933 INFO: train_e/atom_rmse: 0.000072
train_f_mae: 0.005214
2025-04-09 05:16:24.935 INFO: train_f_mae: 0.005214
train_f_rmse: 0.007397
2025-04-09 05:16:24.935 INFO: train_f_rmse: 0.007397
val_e/atom_mae: 0.000038
2025-04-09 05:16:24.938 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000077
2025-04-09 05:16:24.938 INFO: val_e/atom_rmse: 0.000077
val_f_mae: 0.005354
2025-04-09 05:16:24.939 INFO: val_f_mae: 0.005354
val_f_rmse: 0.007637
2025-04-09 05:16:24.939 INFO: val_f_rmse: 0.007637
##### Step: 243 Learning rate: 2.44140625e-06 #####
2025-04-09 05:17:49.416 INFO: ##### Step: 243 Learning rate: 2.44140625e-06 #####
Epoch 4, Train Loss: 0.1160, Val Loss: 0.1287
2025-04-09 05:17:49.417 INFO: Epoch 4, Train Loss: 0.1160, Val Loss: 0.1287
train_e/atom_mae: 0.000038
2025-04-09 05:17:49.418 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000071
2025-04-09 05:17:49.418 INFO: train_e/atom_rmse: 0.000071
train_f_mae: 0.005224
2025-04-09 05:17:49.421 INFO: train_f_mae: 0.005224
train_f_rmse: 0.007408
2025-04-09 05:17:49.421 INFO: train_f_rmse: 0.007408
val_e/atom_mae: 0.000037
2025-04-09 05:17:49.423 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000076
2025-04-09 05:17:49.423 INFO: val_e/atom_rmse: 0.000076
val_f_mae: 0.005366
2025-04-09 05:17:49.424 INFO: val_f_mae: 0.005366
val_f_rmse: 0.007648
2025-04-09 05:17:49.424 INFO: val_f_rmse: 0.007648
##### Step: 244 Learning rate: 2.44140625e-06 #####
2025-04-09 05:19:13.879 INFO: ##### Step: 244 Learning rate: 2.44140625e-06 #####
Epoch 5, Train Loss: 0.1151, Val Loss: 0.1273
2025-04-09 05:19:13.879 INFO: Epoch 5, Train Loss: 0.1151, Val Loss: 0.1273
train_e/atom_mae: 0.000037
2025-04-09 05:19:13.880 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000070
2025-04-09 05:19:13.880 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.005232
2025-04-09 05:19:13.883 INFO: train_f_mae: 0.005232
train_f_rmse: 0.007419
2025-04-09 05:19:13.883 INFO: train_f_rmse: 0.007419
val_e/atom_mae: 0.000036
2025-04-09 05:19:13.885 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000075
2025-04-09 05:19:13.886 INFO: val_e/atom_rmse: 0.000075
val_f_mae: 0.005370
2025-04-09 05:19:13.886 INFO: val_f_mae: 0.005370
val_f_rmse: 0.007647
2025-04-09 05:19:13.886 INFO: val_f_rmse: 0.007647
##### Step: 245 Learning rate: 2.44140625e-06 #####
2025-04-09 05:20:38.372 INFO: ##### Step: 245 Learning rate: 2.44140625e-06 #####
Epoch 6, Train Loss: 0.1148, Val Loss: 0.1294
2025-04-09 05:20:38.373 INFO: Epoch 6, Train Loss: 0.1148, Val Loss: 0.1294
train_e/atom_mae: 0.000038
2025-04-09 05:20:38.373 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000070
2025-04-09 05:20:38.373 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.005236
2025-04-09 05:20:38.376 INFO: train_f_mae: 0.005236
train_f_rmse: 0.007423
2025-04-09 05:20:38.376 INFO: train_f_rmse: 0.007423
val_e/atom_mae: 0.000042
2025-04-09 05:20:38.378 INFO: val_e/atom_mae: 0.000042
val_e/atom_rmse: 0.000076
2025-04-09 05:20:38.379 INFO: val_e/atom_rmse: 0.000076
val_f_mae: 0.005382
2025-04-09 05:20:38.379 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007673
2025-04-09 05:20:38.379 INFO: val_f_rmse: 0.007673
##### Step: 246 Learning rate: 2.44140625e-06 #####
2025-04-09 05:22:02.868 INFO: ##### Step: 246 Learning rate: 2.44140625e-06 #####
Epoch 7, Train Loss: 0.1146, Val Loss: 0.1253
2025-04-09 05:22:02.869 INFO: Epoch 7, Train Loss: 0.1146, Val Loss: 0.1253
train_e/atom_mae: 0.000037
2025-04-09 05:22:02.870 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000070
2025-04-09 05:22:02.870 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.005242
2025-04-09 05:22:02.873 INFO: train_f_mae: 0.005242
train_f_rmse: 0.007435
2025-04-09 05:22:02.873 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000036
2025-04-09 05:22:02.875 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000074
2025-04-09 05:22:02.875 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.005382
2025-04-09 05:22:02.876 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007668
2025-04-09 05:22:02.876 INFO: val_f_rmse: 0.007668
##### Step: 247 Learning rate: 2.44140625e-06 #####
2025-04-09 05:23:27.360 INFO: ##### Step: 247 Learning rate: 2.44140625e-06 #####
Epoch 8, Train Loss: 0.1111, Val Loss: 0.1244
2025-04-09 05:23:27.361 INFO: Epoch 8, Train Loss: 0.1111, Val Loss: 0.1244
train_e/atom_mae: 0.000035
2025-04-09 05:23:27.361 INFO: train_e/atom_mae: 0.000035
train_e/atom_rmse: 0.000068
2025-04-09 05:23:27.361 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.005242
2025-04-09 05:23:27.364 INFO: train_f_mae: 0.005242
train_f_rmse: 0.007430
2025-04-09 05:23:27.364 INFO: train_f_rmse: 0.007430
val_e/atom_mae: 0.000037
2025-04-09 05:23:27.367 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000074
2025-04-09 05:23:27.367 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.005382
2025-04-09 05:23:27.367 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007663
2025-04-09 05:23:27.367 INFO: val_f_rmse: 0.007663
##### Step: 248 Learning rate: 2.44140625e-06 #####
2025-04-09 05:24:51.859 INFO: ##### Step: 248 Learning rate: 2.44140625e-06 #####
Epoch 9, Train Loss: 0.1106, Val Loss: 0.1223
2025-04-09 05:24:51.860 INFO: Epoch 9, Train Loss: 0.1106, Val Loss: 0.1223
train_e/atom_mae: 0.000035
2025-04-09 05:24:51.861 INFO: train_e/atom_mae: 0.000035
train_e/atom_rmse: 0.000068
2025-04-09 05:24:51.861 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.005245
2025-04-09 05:24:51.864 INFO: train_f_mae: 0.005245
train_f_rmse: 0.007434
2025-04-09 05:24:51.864 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000037
2025-04-09 05:24:51.866 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-04-09 05:24:51.866 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005386
2025-04-09 05:24:51.867 INFO: val_f_mae: 0.005386
val_f_rmse: 0.007669
2025-04-09 05:24:51.867 INFO: val_f_rmse: 0.007669
##### Step: 249 Learning rate: 2.44140625e-06 #####
2025-04-09 05:26:16.347 INFO: ##### Step: 249 Learning rate: 2.44140625e-06 #####
Epoch 10, Train Loss: 0.1140, Val Loss: 0.1381
2025-04-09 05:26:16.348 INFO: Epoch 10, Train Loss: 0.1140, Val Loss: 0.1381
train_e/atom_mae: 0.000038
2025-04-09 05:26:16.348 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000070
2025-04-09 05:26:16.349 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.005246
2025-04-09 05:26:16.351 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007434
2025-04-09 05:26:16.351 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000042
2025-04-09 05:26:16.354 INFO: val_e/atom_mae: 0.000042
val_e/atom_rmse: 0.000081
2025-04-09 05:26:16.354 INFO: val_e/atom_rmse: 0.000081
val_f_mae: 0.005382
2025-04-09 05:26:16.354 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007662
2025-04-09 05:26:16.354 INFO: val_f_rmse: 0.007662
##### Step: 250 Learning rate: 2.44140625e-06 #####
2025-04-09 05:27:40.873 INFO: ##### Step: 250 Learning rate: 2.44140625e-06 #####
Epoch 11, Train Loss: 0.1114, Val Loss: 0.1237
2025-04-09 05:27:40.874 INFO: Epoch 11, Train Loss: 0.1114, Val Loss: 0.1237
train_e/atom_mae: 0.000035
2025-04-09 05:27:40.875 INFO: train_e/atom_mae: 0.000035
train_e/atom_rmse: 0.000068
2025-04-09 05:27:40.875 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.005245
2025-04-09 05:27:40.878 INFO: train_f_mae: 0.005245
train_f_rmse: 0.007433
2025-04-09 05:27:40.878 INFO: train_f_rmse: 0.007433
val_e/atom_mae: 0.000036
2025-04-09 05:27:40.880 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000073
2025-04-09 05:27:40.880 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005381
2025-04-09 05:27:40.881 INFO: val_f_mae: 0.005381
val_f_rmse: 0.007657
2025-04-09 05:27:40.881 INFO: val_f_rmse: 0.007657
##### Step: 251 Learning rate: 2.44140625e-06 #####
2025-04-09 05:29:05.337 INFO: ##### Step: 251 Learning rate: 2.44140625e-06 #####
Epoch 12, Train Loss: 0.1120, Val Loss: 0.1289
2025-04-09 05:29:05.337 INFO: Epoch 12, Train Loss: 0.1120, Val Loss: 0.1289
train_e/atom_mae: 0.000036
2025-04-09 05:29:05.338 INFO: train_e/atom_mae: 0.000036
train_e/atom_rmse: 0.000069
2025-04-09 05:29:05.338 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.005243
2025-04-09 05:29:05.341 INFO: train_f_mae: 0.005243
train_f_rmse: 0.007430
2025-04-09 05:29:05.341 INFO: train_f_rmse: 0.007430
val_e/atom_mae: 0.000038
2025-04-09 05:29:05.343 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000076
2025-04-09 05:29:05.344 INFO: val_e/atom_rmse: 0.000076
val_f_mae: 0.005383
2025-04-09 05:29:05.344 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007660
2025-04-09 05:29:05.344 INFO: val_f_rmse: 0.007660
##### Step: 252 Learning rate: 2.44140625e-06 #####
2025-04-09 05:30:29.821 INFO: ##### Step: 252 Learning rate: 2.44140625e-06 #####
Epoch 13, Train Loss: 0.1134, Val Loss: 0.1207
2025-04-09 05:30:29.822 INFO: Epoch 13, Train Loss: 0.1134, Val Loss: 0.1207
train_e/atom_mae: 0.000037
2025-04-09 05:30:29.823 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000069
2025-04-09 05:30:29.823 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.005245
2025-04-09 05:30:29.825 INFO: train_f_mae: 0.005245
train_f_rmse: 0.007434
2025-04-09 05:30:29.825 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 05:30:29.828 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 05:30:29.828 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005385
2025-04-09 05:30:29.828 INFO: val_f_mae: 0.005385
val_f_rmse: 0.007667
2025-04-09 05:30:29.829 INFO: val_f_rmse: 0.007667
##### Step: 253 Learning rate: 2.44140625e-06 #####
2025-04-09 05:31:54.294 INFO: ##### Step: 253 Learning rate: 2.44140625e-06 #####
Epoch 14, Train Loss: 0.1101, Val Loss: 0.1234
2025-04-09 05:31:54.295 INFO: Epoch 14, Train Loss: 0.1101, Val Loss: 0.1234
train_e/atom_mae: 0.000034
2025-04-09 05:31:54.295 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:31:54.295 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005246
2025-04-09 05:31:54.298 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007436
2025-04-09 05:31:54.298 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000036
2025-04-09 05:31:54.301 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000073
2025-04-09 05:31:54.301 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 05:31:54.302 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007659
2025-04-09 05:31:54.302 INFO: val_f_rmse: 0.007659
##### Step: 254 Learning rate: 2.44140625e-06 #####
2025-04-09 05:33:18.787 INFO: ##### Step: 254 Learning rate: 2.44140625e-06 #####
Epoch 15, Train Loss: 0.1130, Val Loss: 0.1222
2025-04-09 05:33:18.787 INFO: Epoch 15, Train Loss: 0.1130, Val Loss: 0.1222
train_e/atom_mae: 0.000037
2025-04-09 05:33:18.788 INFO: train_e/atom_mae: 0.000037
train_e/atom_rmse: 0.000069
2025-04-09 05:33:18.788 INFO: train_e/atom_rmse: 0.000069
train_f_mae: 0.005246
2025-04-09 05:33:18.791 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007436
2025-04-09 05:33:18.791 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 05:33:18.793 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 05:33:18.794 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005385
2025-04-09 05:33:18.794 INFO: val_f_mae: 0.005385
val_f_rmse: 0.007665
2025-04-09 05:33:18.794 INFO: val_f_rmse: 0.007665
##### Step: 255 Learning rate: 2.44140625e-06 #####
2025-04-09 05:34:43.280 INFO: ##### Step: 255 Learning rate: 2.44140625e-06 #####
Epoch 16, Train Loss: 0.1102, Val Loss: 0.1245
2025-04-09 05:34:43.281 INFO: Epoch 16, Train Loss: 0.1102, Val Loss: 0.1245
train_e/atom_mae: 0.000034
2025-04-09 05:34:43.282 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:34:43.282 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 05:34:43.284 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 05:34:43.284 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000036
2025-04-09 05:34:43.287 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000074
2025-04-09 05:34:43.287 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.005384
2025-04-09 05:34:43.287 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007660
2025-04-09 05:34:43.287 INFO: val_f_rmse: 0.007660
##### Step: 256 Learning rate: 2.44140625e-06 #####
2025-04-09 05:36:07.942 INFO: ##### Step: 256 Learning rate: 2.44140625e-06 #####
Epoch 17, Train Loss: 0.1100, Val Loss: 0.1314
2025-04-09 05:36:07.942 INFO: Epoch 17, Train Loss: 0.1100, Val Loss: 0.1314
train_e/atom_mae: 0.000034
2025-04-09 05:36:07.943 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:36:07.943 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 05:36:07.946 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007434
2025-04-09 05:36:07.946 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000039
2025-04-09 05:36:07.949 INFO: val_e/atom_mae: 0.000039
val_e/atom_rmse: 0.000077
2025-04-09 05:36:07.949 INFO: val_e/atom_rmse: 0.000077
val_f_mae: 0.005388
2025-04-09 05:36:07.950 INFO: val_f_mae: 0.005388
val_f_rmse: 0.007670
2025-04-09 05:36:07.950 INFO: val_f_rmse: 0.007670
##### Step: 257 Learning rate: 2.44140625e-06 #####
2025-04-09 05:37:32.411 INFO: ##### Step: 257 Learning rate: 2.44140625e-06 #####
Epoch 18, Train Loss: 0.1144, Val Loss: 0.1218
2025-04-09 05:37:32.411 INFO: Epoch 18, Train Loss: 0.1144, Val Loss: 0.1218
train_e/atom_mae: 0.000038
2025-04-09 05:37:32.412 INFO: train_e/atom_mae: 0.000038
train_e/atom_rmse: 0.000070
2025-04-09 05:37:32.412 INFO: train_e/atom_rmse: 0.000070
train_f_mae: 0.005248
2025-04-09 05:37:32.415 INFO: train_f_mae: 0.005248
train_f_rmse: 0.007440
2025-04-09 05:37:32.415 INFO: train_f_rmse: 0.007440
val_e/atom_mae: 0.000035
2025-04-09 05:37:32.417 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 05:37:32.418 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 05:37:32.418 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007660
2025-04-09 05:37:32.418 INFO: val_f_rmse: 0.007660
##### Step: 258 Learning rate: 2.44140625e-06 #####
2025-04-09 05:38:56.891 INFO: ##### Step: 258 Learning rate: 2.44140625e-06 #####
Epoch 19, Train Loss: 0.1109, Val Loss: 0.1293
2025-04-09 05:38:56.891 INFO: Epoch 19, Train Loss: 0.1109, Val Loss: 0.1293
train_e/atom_mae: 0.000035
2025-04-09 05:38:56.892 INFO: train_e/atom_mae: 0.000035
train_e/atom_rmse: 0.000068
2025-04-09 05:38:56.892 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.005245
2025-04-09 05:38:56.895 INFO: train_f_mae: 0.005245
train_f_rmse: 0.007432
2025-04-09 05:38:56.895 INFO: train_f_rmse: 0.007432
val_e/atom_mae: 0.000038
2025-04-09 05:38:56.897 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000076
2025-04-09 05:38:56.898 INFO: val_e/atom_rmse: 0.000076
val_f_mae: 0.005382
2025-04-09 05:38:56.898 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007658
2025-04-09 05:38:56.898 INFO: val_f_rmse: 0.007658
##### Step: 259 Learning rate: 2.44140625e-06 #####
2025-04-09 05:40:21.348 INFO: ##### Step: 259 Learning rate: 2.44140625e-06 #####
Epoch 20, Train Loss: 0.1107, Val Loss: 0.1246
2025-04-09 05:40:21.349 INFO: Epoch 20, Train Loss: 0.1107, Val Loss: 0.1246
train_e/atom_mae: 0.000035
2025-04-09 05:40:21.350 INFO: train_e/atom_mae: 0.000035
train_e/atom_rmse: 0.000068
2025-04-09 05:40:21.350 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.005245
2025-04-09 05:40:21.352 INFO: train_f_mae: 0.005245
train_f_rmse: 0.007434
2025-04-09 05:40:21.353 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 05:40:21.355 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000074
2025-04-09 05:40:21.355 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.005382
2025-04-09 05:40:21.356 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 05:40:21.356 INFO: val_f_rmse: 0.007657
##### Step: 260 Learning rate: 1.220703125e-06 #####
2025-04-09 05:41:45.851 INFO: ##### Step: 260 Learning rate: 1.220703125e-06 #####
Epoch 21, Train Loss: 0.1100, Val Loss: 0.1235
2025-04-09 05:41:45.852 INFO: Epoch 21, Train Loss: 0.1100, Val Loss: 0.1235
train_e/atom_mae: 0.000034
2025-04-09 05:41:45.853 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:41:45.853 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 05:41:45.855 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007437
2025-04-09 05:41:45.856 INFO: train_f_rmse: 0.007437
val_e/atom_mae: 0.000038
2025-04-09 05:41:45.858 INFO: val_e/atom_mae: 0.000038
val_e/atom_rmse: 0.000073
2025-04-09 05:41:45.858 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005384
2025-04-09 05:41:45.858 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007661
2025-04-09 05:41:45.859 INFO: val_f_rmse: 0.007661
##### Step: 261 Learning rate: 1.220703125e-06 #####
2025-04-09 05:43:10.381 INFO: ##### Step: 261 Learning rate: 1.220703125e-06 #####
Epoch 22, Train Loss: 0.1103, Val Loss: 0.1231
2025-04-09 05:43:10.382 INFO: Epoch 22, Train Loss: 0.1103, Val Loss: 0.1231
train_e/atom_mae: 0.000035
2025-04-09 05:43:10.383 INFO: train_e/atom_mae: 0.000035
train_e/atom_rmse: 0.000067
2025-04-09 05:43:10.383 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005246
2025-04-09 05:43:10.385 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007435
2025-04-09 05:43:10.386 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 05:43:10.388 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 05:43:10.388 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005381
2025-04-09 05:43:10.389 INFO: val_f_mae: 0.005381
val_f_rmse: 0.007655
2025-04-09 05:43:10.389 INFO: val_f_rmse: 0.007655
##### Step: 262 Learning rate: 1.220703125e-06 #####
2025-04-09 05:44:34.871 INFO: ##### Step: 262 Learning rate: 1.220703125e-06 #####
Epoch 23, Train Loss: 0.1095, Val Loss: 0.1245
2025-04-09 05:44:34.871 INFO: Epoch 23, Train Loss: 0.1095, Val Loss: 0.1245
train_e/atom_mae: 0.000034
2025-04-09 05:44:34.872 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:44:34.872 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005246
2025-04-09 05:44:34.875 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007433
2025-04-09 05:44:34.875 INFO: train_f_rmse: 0.007433
val_e/atom_mae: 0.000036
2025-04-09 05:44:34.877 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000074
2025-04-09 05:44:34.878 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.005383
2025-04-09 05:44:34.878 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007659
2025-04-09 05:44:34.878 INFO: val_f_rmse: 0.007659
##### Step: 263 Learning rate: 1.220703125e-06 #####
2025-04-09 05:45:59.345 INFO: ##### Step: 263 Learning rate: 1.220703125e-06 #####
Epoch 24, Train Loss: 0.1091, Val Loss: 0.1266
2025-04-09 05:45:59.346 INFO: Epoch 24, Train Loss: 0.1091, Val Loss: 0.1266
train_e/atom_mae: 0.000034
2025-04-09 05:45:59.347 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:45:59.347 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 05:45:59.350 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 05:45:59.350 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000037
2025-04-09 05:45:59.352 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000075
2025-04-09 05:45:59.352 INFO: val_e/atom_rmse: 0.000075
val_f_mae: 0.005386
2025-04-09 05:45:59.353 INFO: val_f_mae: 0.005386
val_f_rmse: 0.007664
2025-04-09 05:45:59.353 INFO: val_f_rmse: 0.007664
##### Step: 264 Learning rate: 1.220703125e-06 #####
2025-04-09 05:47:23.821 INFO: ##### Step: 264 Learning rate: 1.220703125e-06 #####
Epoch 25, Train Loss: 0.1095, Val Loss: 0.1219
2025-04-09 05:47:23.822 INFO: Epoch 25, Train Loss: 0.1095, Val Loss: 0.1219
train_e/atom_mae: 0.000034
2025-04-09 05:47:23.822 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:47:23.822 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005248
2025-04-09 05:47:23.825 INFO: train_f_mae: 0.005248
train_f_rmse: 0.007438
2025-04-09 05:47:23.825 INFO: train_f_rmse: 0.007438
val_e/atom_mae: 0.000037
2025-04-09 05:47:23.827 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000072
2025-04-09 05:47:23.828 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 05:47:23.828 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 05:47:23.828 INFO: val_f_rmse: 0.007657
##### Step: 265 Learning rate: 1.220703125e-06 #####
2025-04-09 05:48:48.317 INFO: ##### Step: 265 Learning rate: 1.220703125e-06 #####
Epoch 26, Train Loss: 0.1084, Val Loss: 0.1236
2025-04-09 05:48:48.318 INFO: Epoch 26, Train Loss: 0.1084, Val Loss: 0.1236
train_e/atom_mae: 0.000034
2025-04-09 05:48:48.318 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000066
2025-04-09 05:48:48.319 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005244
2025-04-09 05:48:48.321 INFO: train_f_mae: 0.005244
train_f_rmse: 0.007431
2025-04-09 05:48:48.321 INFO: train_f_rmse: 0.007431
val_e/atom_mae: 0.000035
2025-04-09 05:48:48.324 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 05:48:48.324 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005380
2025-04-09 05:48:48.324 INFO: val_f_mae: 0.005380
val_f_rmse: 0.007654
2025-04-09 05:48:48.324 INFO: val_f_rmse: 0.007654
##### Step: 266 Learning rate: 1.220703125e-06 #####
2025-04-09 05:50:12.811 INFO: ##### Step: 266 Learning rate: 1.220703125e-06 #####
Epoch 27, Train Loss: 0.1104, Val Loss: 0.1202
2025-04-09 05:50:12.812 INFO: Epoch 27, Train Loss: 0.1104, Val Loss: 0.1202
train_e/atom_mae: 0.000035
2025-04-09 05:50:12.813 INFO: train_e/atom_mae: 0.000035
train_e/atom_rmse: 0.000068
2025-04-09 05:50:12.813 INFO: train_e/atom_rmse: 0.000068
train_f_mae: 0.005245
2025-04-09 05:50:12.815 INFO: train_f_mae: 0.005245
train_f_rmse: 0.007432
2025-04-09 05:50:12.816 INFO: train_f_rmse: 0.007432
val_e/atom_mae: 0.000036
2025-04-09 05:50:12.818 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000071
2025-04-09 05:50:12.818 INFO: val_e/atom_rmse: 0.000071
val_f_mae: 0.005383
2025-04-09 05:50:12.819 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 05:50:12.819 INFO: val_f_rmse: 0.007658
##### Step: 267 Learning rate: 1.220703125e-06 #####
2025-04-09 05:51:37.299 INFO: ##### Step: 267 Learning rate: 1.220703125e-06 #####
Epoch 28, Train Loss: 0.1096, Val Loss: 0.1222
2025-04-09 05:51:37.300 INFO: Epoch 28, Train Loss: 0.1096, Val Loss: 0.1222
train_e/atom_mae: 0.000034
2025-04-09 05:51:37.300 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:51:37.301 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005246
2025-04-09 05:51:37.303 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007434
2025-04-09 05:51:37.303 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000036
2025-04-09 05:51:37.306 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000072
2025-04-09 05:51:37.306 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005385
2025-04-09 05:51:37.306 INFO: val_f_mae: 0.005385
val_f_rmse: 0.007663
2025-04-09 05:51:37.306 INFO: val_f_rmse: 0.007663
##### Step: 268 Learning rate: 1.220703125e-06 #####
2025-04-09 05:53:01.773 INFO: ##### Step: 268 Learning rate: 1.220703125e-06 #####
Epoch 29, Train Loss: 0.1098, Val Loss: 0.1213
2025-04-09 05:53:01.774 INFO: Epoch 29, Train Loss: 0.1098, Val Loss: 0.1213
train_e/atom_mae: 0.000034
2025-04-09 05:53:01.775 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:53:01.775 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 05:53:01.777 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 05:53:01.778 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 05:53:01.780 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 05:53:01.780 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005384
2025-04-09 05:53:01.781 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007660
2025-04-09 05:53:01.781 INFO: val_f_rmse: 0.007660
##### Step: 269 Learning rate: 1.220703125e-06 #####
2025-04-09 05:54:26.260 INFO: ##### Step: 269 Learning rate: 1.220703125e-06 #####
Epoch 30, Train Loss: 0.1093, Val Loss: 0.1217
2025-04-09 05:54:26.261 INFO: Epoch 30, Train Loss: 0.1093, Val Loss: 0.1217
train_e/atom_mae: 0.000034
2025-04-09 05:54:26.262 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:54:26.262 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005248
2025-04-09 05:54:26.264 INFO: train_f_mae: 0.005248
train_f_rmse: 0.007437
2025-04-09 05:54:26.264 INFO: train_f_rmse: 0.007437
val_e/atom_mae: 0.000035
2025-04-09 05:54:26.267 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 05:54:26.267 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005385
2025-04-09 05:54:26.267 INFO: val_f_mae: 0.005385
val_f_rmse: 0.007663
2025-04-09 05:54:26.268 INFO: val_f_rmse: 0.007663
##### Step: 270 Learning rate: 1.220703125e-06 #####
2025-04-09 05:55:50.748 INFO: ##### Step: 270 Learning rate: 1.220703125e-06 #####
Epoch 31, Train Loss: 0.1089, Val Loss: 0.1235
2025-04-09 05:55:50.749 INFO: Epoch 31, Train Loss: 0.1089, Val Loss: 0.1235
train_e/atom_mae: 0.000034
2025-04-09 05:55:50.750 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:55:50.750 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 05:55:50.752 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 05:55:50.752 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 05:55:50.755 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 05:55:50.755 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 05:55:50.755 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007657
2025-04-09 05:55:50.755 INFO: val_f_rmse: 0.007657
##### Step: 271 Learning rate: 1.220703125e-06 #####
2025-04-09 05:57:15.260 INFO: ##### Step: 271 Learning rate: 1.220703125e-06 #####
Epoch 32, Train Loss: 0.1095, Val Loss: 0.1256
2025-04-09 05:57:15.261 INFO: Epoch 32, Train Loss: 0.1095, Val Loss: 0.1256
train_e/atom_mae: 0.000034
2025-04-09 05:57:15.261 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:57:15.261 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 05:57:15.264 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 05:57:15.264 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000036
2025-04-09 05:57:15.266 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000074
2025-04-09 05:57:15.267 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.005385
2025-04-09 05:57:15.267 INFO: val_f_mae: 0.005385
val_f_rmse: 0.007664
2025-04-09 05:57:15.267 INFO: val_f_rmse: 0.007664
##### Step: 272 Learning rate: 1.220703125e-06 #####
2025-04-09 05:58:39.747 INFO: ##### Step: 272 Learning rate: 1.220703125e-06 #####
Epoch 33, Train Loss: 0.1092, Val Loss: 0.1228
2025-04-09 05:58:39.747 INFO: Epoch 33, Train Loss: 0.1092, Val Loss: 0.1228
train_e/atom_mae: 0.000034
2025-04-09 05:58:39.748 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 05:58:39.748 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 05:58:39.751 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 05:58:39.751 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 05:58:39.753 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 05:58:39.754 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005384
2025-04-09 05:58:39.754 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007662
2025-04-09 05:58:39.754 INFO: val_f_rmse: 0.007662
##### Step: 273 Learning rate: 1.220703125e-06 #####
2025-04-09 06:00:04.249 INFO: ##### Step: 273 Learning rate: 1.220703125e-06 #####
Epoch 34, Train Loss: 0.1099, Val Loss: 0.1220
2025-04-09 06:00:04.249 INFO: Epoch 34, Train Loss: 0.1099, Val Loss: 0.1220
train_e/atom_mae: 0.000034
2025-04-09 06:00:04.250 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 06:00:04.250 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 06:00:04.253 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 06:00:04.253 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 06:00:04.255 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:00:04.255 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005385
2025-04-09 06:00:04.256 INFO: val_f_mae: 0.005385
val_f_rmse: 0.007662
2025-04-09 06:00:04.256 INFO: val_f_rmse: 0.007662
##### Step: 274 Learning rate: 1.220703125e-06 #####
2025-04-09 06:01:28.736 INFO: ##### Step: 274 Learning rate: 1.220703125e-06 #####
Epoch 35, Train Loss: 0.1088, Val Loss: 0.1222
2025-04-09 06:01:28.737 INFO: Epoch 35, Train Loss: 0.1088, Val Loss: 0.1222
train_e/atom_mae: 0.000033
2025-04-09 06:01:28.738 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000067
2025-04-09 06:01:28.738 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005246
2025-04-09 06:01:28.740 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007433
2025-04-09 06:01:28.740 INFO: train_f_rmse: 0.007433
val_e/atom_mae: 0.000035
2025-04-09 06:01:28.743 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:01:28.743 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005380
2025-04-09 06:01:28.743 INFO: val_f_mae: 0.005380
val_f_rmse: 0.007652
2025-04-09 06:01:28.744 INFO: val_f_rmse: 0.007652
##### Step: 275 Learning rate: 1.220703125e-06 #####
2025-04-09 06:02:53.204 INFO: ##### Step: 275 Learning rate: 1.220703125e-06 #####
Epoch 36, Train Loss: 0.1087, Val Loss: 0.1219
2025-04-09 06:02:53.204 INFO: Epoch 36, Train Loss: 0.1087, Val Loss: 0.1219
train_e/atom_mae: 0.000033
2025-04-09 06:02:53.205 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:02:53.205 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005245
2025-04-09 06:02:53.208 INFO: train_f_mae: 0.005245
train_f_rmse: 0.007432
2025-04-09 06:02:53.208 INFO: train_f_rmse: 0.007432
val_e/atom_mae: 0.000036
2025-04-09 06:02:53.210 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000072
2025-04-09 06:02:53.210 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 06:02:53.211 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 06:02:53.211 INFO: val_f_rmse: 0.007658
##### Step: 276 Learning rate: 1.220703125e-06 #####
2025-04-09 06:04:17.668 INFO: ##### Step: 276 Learning rate: 1.220703125e-06 #####
Epoch 37, Train Loss: 0.1087, Val Loss: 0.1206
2025-04-09 06:04:17.669 INFO: Epoch 37, Train Loss: 0.1087, Val Loss: 0.1206
train_e/atom_mae: 0.000034
2025-04-09 06:04:17.670 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000066
2025-04-09 06:04:17.670 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:04:17.672 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:04:17.673 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:04:17.675 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:04:17.675 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005384
2025-04-09 06:04:17.676 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007661
2025-04-09 06:04:17.676 INFO: val_f_rmse: 0.007661
##### Step: 277 Learning rate: 1.220703125e-06 #####
2025-04-09 06:05:42.119 INFO: ##### Step: 277 Learning rate: 1.220703125e-06 #####
Epoch 38, Train Loss: 0.1090, Val Loss: 0.1228
2025-04-09 06:05:42.120 INFO: Epoch 38, Train Loss: 0.1090, Val Loss: 0.1228
train_e/atom_mae: 0.000033
2025-04-09 06:05:42.121 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000067
2025-04-09 06:05:42.121 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 06:05:42.124 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:05:42.124 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:05:42.126 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:05:42.126 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005384
2025-04-09 06:05:42.127 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007660
2025-04-09 06:05:42.127 INFO: val_f_rmse: 0.007660
##### Step: 278 Learning rate: 1.220703125e-06 #####
2025-04-09 06:07:06.600 INFO: ##### Step: 278 Learning rate: 1.220703125e-06 #####
Epoch 39, Train Loss: 0.1093, Val Loss: 0.1217
2025-04-09 06:07:06.601 INFO: Epoch 39, Train Loss: 0.1093, Val Loss: 0.1217
train_e/atom_mae: 0.000034
2025-04-09 06:07:06.602 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 06:07:06.602 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 06:07:06.605 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:07:06.605 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:07:06.607 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:07:06.607 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005384
2025-04-09 06:07:06.608 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007659
2025-04-09 06:07:06.608 INFO: val_f_rmse: 0.007659
##### Step: 279 Learning rate: 1.220703125e-06 #####
2025-04-09 06:08:31.112 INFO: ##### Step: 279 Learning rate: 1.220703125e-06 #####
Epoch 40, Train Loss: 0.1091, Val Loss: 0.1219
2025-04-09 06:08:31.113 INFO: Epoch 40, Train Loss: 0.1091, Val Loss: 0.1219
train_e/atom_mae: 0.000034
2025-04-09 06:08:31.113 INFO: train_e/atom_mae: 0.000034
train_e/atom_rmse: 0.000067
2025-04-09 06:08:31.113 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005246
2025-04-09 06:08:31.116 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007434
2025-04-09 06:08:31.116 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 06:08:31.118 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:08:31.119 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 06:08:31.119 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 06:08:31.119 INFO: val_f_rmse: 0.007657
##### Step: 280 Learning rate: 6.103515625e-07 #####
2025-04-09 06:09:55.624 INFO: ##### Step: 280 Learning rate: 6.103515625e-07 #####
Epoch 41, Train Loss: 0.1086, Val Loss: 0.1228
2025-04-09 06:09:55.625 INFO: Epoch 41, Train Loss: 0.1086, Val Loss: 0.1228
train_e/atom_mae: 0.000033
2025-04-09 06:09:55.626 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:09:55.626 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 06:09:55.629 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007433
2025-04-09 06:09:55.629 INFO: train_f_rmse: 0.007433
val_e/atom_mae: 0.000035
2025-04-09 06:09:55.631 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:09:55.632 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 06:09:55.632 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 06:09:55.632 INFO: val_f_rmse: 0.007656
##### Step: 281 Learning rate: 6.103515625e-07 #####
2025-04-09 06:11:20.099 INFO: ##### Step: 281 Learning rate: 6.103515625e-07 #####
Epoch 42, Train Loss: 0.1087, Val Loss: 0.1261
2025-04-09 06:11:20.100 INFO: Epoch 42, Train Loss: 0.1087, Val Loss: 0.1261
train_e/atom_mae: 0.000033
2025-04-09 06:11:20.101 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:11:20.101 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 06:11:20.104 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007433
2025-04-09 06:11:20.104 INFO: train_f_rmse: 0.007433
val_e/atom_mae: 0.000036
2025-04-09 06:11:20.106 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000075
2025-04-09 06:11:20.106 INFO: val_e/atom_rmse: 0.000075
val_f_mae: 0.005382
2025-04-09 06:11:20.107 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 06:11:20.107 INFO: val_f_rmse: 0.007657
##### Step: 282 Learning rate: 6.103515625e-07 #####
2025-04-09 06:12:44.594 INFO: ##### Step: 282 Learning rate: 6.103515625e-07 #####
Epoch 43, Train Loss: 0.1088, Val Loss: 0.1217
2025-04-09 06:12:44.595 INFO: Epoch 43, Train Loss: 0.1088, Val Loss: 0.1217
train_e/atom_mae: 0.000033
2025-04-09 06:12:44.595 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:12:44.595 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:12:44.598 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:12:44.598 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:12:44.601 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:12:44.601 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 06:12:44.601 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007661
2025-04-09 06:12:44.601 INFO: val_f_rmse: 0.007661
##### Step: 283 Learning rate: 6.103515625e-07 #####
2025-04-09 06:14:09.081 INFO: ##### Step: 283 Learning rate: 6.103515625e-07 #####
Epoch 44, Train Loss: 0.1088, Val Loss: 0.1226
2025-04-09 06:14:09.082 INFO: Epoch 44, Train Loss: 0.1088, Val Loss: 0.1226
train_e/atom_mae: 0.000033
2025-04-09 06:14:09.082 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000067
2025-04-09 06:14:09.082 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005246
2025-04-09 06:14:09.085 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007433
2025-04-09 06:14:09.085 INFO: train_f_rmse: 0.007433
val_e/atom_mae: 0.000036
2025-04-09 06:14:09.087 INFO: val_e/atom_mae: 0.000036
val_e/atom_rmse: 0.000073
2025-04-09 06:14:09.088 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 06:14:09.088 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 06:14:09.088 INFO: val_f_rmse: 0.007658
##### Step: 284 Learning rate: 6.103515625e-07 #####
2025-04-09 06:15:33.529 INFO: ##### Step: 284 Learning rate: 6.103515625e-07 #####
Epoch 45, Train Loss: 0.1085, Val Loss: 0.1230
2025-04-09 06:15:33.530 INFO: Epoch 45, Train Loss: 0.1085, Val Loss: 0.1230
train_e/atom_mae: 0.000033
2025-04-09 06:15:33.530 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:15:33.530 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 06:15:33.533 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007432
2025-04-09 06:15:33.533 INFO: train_f_rmse: 0.007432
val_e/atom_mae: 0.000035
2025-04-09 06:15:33.535 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:15:33.536 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005381
2025-04-09 06:15:33.536 INFO: val_f_mae: 0.005381
val_f_rmse: 0.007653
2025-04-09 06:15:33.536 INFO: val_f_rmse: 0.007653
##### Step: 285 Learning rate: 6.103515625e-07 #####
2025-04-09 06:16:58.004 INFO: ##### Step: 285 Learning rate: 6.103515625e-07 #####
Epoch 46, Train Loss: 0.1084, Val Loss: 0.1217
2025-04-09 06:16:58.005 INFO: Epoch 46, Train Loss: 0.1084, Val Loss: 0.1217
train_e/atom_mae: 0.000033
2025-04-09 06:16:58.006 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:16:58.006 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005245
2025-04-09 06:16:58.008 INFO: train_f_mae: 0.005245
train_f_rmse: 0.007431
2025-04-09 06:16:58.009 INFO: train_f_rmse: 0.007431
val_e/atom_mae: 0.000035
2025-04-09 06:16:58.011 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:16:58.011 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005380
2025-04-09 06:16:58.012 INFO: val_f_mae: 0.005380
val_f_rmse: 0.007652
2025-04-09 06:16:58.012 INFO: val_f_rmse: 0.007652
##### Step: 286 Learning rate: 6.103515625e-07 #####
2025-04-09 06:18:22.479 INFO: ##### Step: 286 Learning rate: 6.103515625e-07 #####
Epoch 47, Train Loss: 0.1085, Val Loss: 0.1225
2025-04-09 06:18:22.480 INFO: Epoch 47, Train Loss: 0.1085, Val Loss: 0.1225
train_e/atom_mae: 0.000033
2025-04-09 06:18:22.481 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:18:22.481 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 06:18:22.484 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007433
2025-04-09 06:18:22.484 INFO: train_f_rmse: 0.007433
val_e/atom_mae: 0.000035
2025-04-09 06:18:22.486 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:18:22.486 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005384
2025-04-09 06:18:22.487 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007659
2025-04-09 06:18:22.487 INFO: val_f_rmse: 0.007659
##### Step: 287 Learning rate: 6.103515625e-07 #####
2025-04-09 06:19:46.961 INFO: ##### Step: 287 Learning rate: 6.103515625e-07 #####
Epoch 48, Train Loss: 0.1083, Val Loss: 0.1233
2025-04-09 06:19:46.962 INFO: Epoch 48, Train Loss: 0.1083, Val Loss: 0.1233
train_e/atom_mae: 0.000033
2025-04-09 06:19:46.963 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:19:46.963 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:19:46.966 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:19:46.966 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:19:46.968 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:19:46.969 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 06:19:46.969 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 06:19:46.969 INFO: val_f_rmse: 0.007656
##### Step: 288 Learning rate: 6.103515625e-07 #####
2025-04-09 06:21:11.460 INFO: ##### Step: 288 Learning rate: 6.103515625e-07 #####
Epoch 49, Train Loss: 0.1085, Val Loss: 0.1210
2025-04-09 06:21:11.460 INFO: Epoch 49, Train Loss: 0.1085, Val Loss: 0.1210
train_e/atom_mae: 0.000033
2025-04-09 06:21:11.461 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:21:11.461 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:21:11.464 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:21:11.464 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000034
2025-04-09 06:21:11.466 INFO: val_e/atom_mae: 0.000034
val_e/atom_rmse: 0.000072
2025-04-09 06:21:11.467 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 06:21:11.467 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 06:21:11.467 INFO: val_f_rmse: 0.007658
##### Step: 289 Learning rate: 6.103515625e-07 #####
2025-04-09 06:22:35.958 INFO: ##### Step: 289 Learning rate: 6.103515625e-07 #####
Epoch 50, Train Loss: 0.1082, Val Loss: 0.1223
2025-04-09 06:22:35.958 INFO: Epoch 50, Train Loss: 0.1082, Val Loss: 0.1223
train_e/atom_mae: 0.000033
2025-04-09 06:22:35.959 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:22:35.959 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:22:35.962 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:22:35.962 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:22:35.964 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:22:35.965 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005384
2025-04-09 06:22:35.965 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007660
2025-04-09 06:22:35.965 INFO: val_f_rmse: 0.007660
##### Step: 290 Learning rate: 6.103515625e-07 #####
2025-04-09 06:24:00.426 INFO: ##### Step: 290 Learning rate: 6.103515625e-07 #####
Epoch 51, Train Loss: 0.1086, Val Loss: 0.1230
2025-04-09 06:24:00.427 INFO: Epoch 51, Train Loss: 0.1086, Val Loss: 0.1230
train_e/atom_mae: 0.000033
2025-04-09 06:24:00.428 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:24:00.428 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 06:24:00.431 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007434
2025-04-09 06:24:00.431 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 06:24:00.433 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:24:00.433 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 06:24:00.434 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 06:24:00.434 INFO: val_f_rmse: 0.007658
##### Step: 291 Learning rate: 6.103515625e-07 #####
2025-04-09 06:25:24.926 INFO: ##### Step: 291 Learning rate: 6.103515625e-07 #####
Epoch 52, Train Loss: 0.1086, Val Loss: 0.1219
2025-04-09 06:25:24.927 INFO: Epoch 52, Train Loss: 0.1086, Val Loss: 0.1219
train_e/atom_mae: 0.000033
2025-04-09 06:25:24.928 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:25:24.928 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:25:24.931 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:25:24.931 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:25:24.933 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:25:24.933 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 06:25:24.934 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007655
2025-04-09 06:25:24.934 INFO: val_f_rmse: 0.007655
##### Step: 292 Learning rate: 6.103515625e-07 #####
2025-04-09 06:26:49.437 INFO: ##### Step: 292 Learning rate: 6.103515625e-07 #####
Epoch 53, Train Loss: 0.1088, Val Loss: 0.1217
2025-04-09 06:26:49.438 INFO: Epoch 53, Train Loss: 0.1088, Val Loss: 0.1217
train_e/atom_mae: 0.000033
2025-04-09 06:26:49.439 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000067
2025-04-09 06:26:49.439 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 06:26:49.442 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:26:49.442 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:26:49.444 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:26:49.445 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005385
2025-04-09 06:26:49.445 INFO: val_f_mae: 0.005385
val_f_rmse: 0.007661
2025-04-09 06:26:49.445 INFO: val_f_rmse: 0.007661
##### Step: 293 Learning rate: 6.103515625e-07 #####
2025-04-09 06:28:13.913 INFO: ##### Step: 293 Learning rate: 6.103515625e-07 #####
Epoch 54, Train Loss: 0.1090, Val Loss: 0.1222
2025-04-09 06:28:13.913 INFO: Epoch 54, Train Loss: 0.1090, Val Loss: 0.1222
train_e/atom_mae: 0.000033
2025-04-09 06:28:13.914 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000067
2025-04-09 06:28:13.914 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005249
2025-04-09 06:28:13.917 INFO: train_f_mae: 0.005249
train_f_rmse: 0.007438
2025-04-09 06:28:13.917 INFO: train_f_rmse: 0.007438
val_e/atom_mae: 0.000035
2025-04-09 06:28:13.919 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:28:13.919 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005385
2025-04-09 06:28:13.920 INFO: val_f_mae: 0.005385
val_f_rmse: 0.007661
2025-04-09 06:28:13.920 INFO: val_f_rmse: 0.007661
##### Step: 294 Learning rate: 6.103515625e-07 #####
2025-04-09 06:29:38.427 INFO: ##### Step: 294 Learning rate: 6.103515625e-07 #####
Epoch 55, Train Loss: 0.1086, Val Loss: 0.1274
2025-04-09 06:29:38.427 INFO: Epoch 55, Train Loss: 0.1086, Val Loss: 0.1274
train_e/atom_mae: 0.000033
2025-04-09 06:29:38.428 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:29:38.428 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:29:38.431 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 06:29:38.431 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000037
2025-04-09 06:29:38.433 INFO: val_e/atom_mae: 0.000037
val_e/atom_rmse: 0.000075
2025-04-09 06:29:38.434 INFO: val_e/atom_rmse: 0.000075
val_f_mae: 0.005383
2025-04-09 06:29:38.434 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007657
2025-04-09 06:29:38.434 INFO: val_f_rmse: 0.007657
##### Step: 295 Learning rate: 6.103515625e-07 #####
2025-04-09 06:31:02.948 INFO: ##### Step: 295 Learning rate: 6.103515625e-07 #####
Epoch 56, Train Loss: 0.1087, Val Loss: 0.1223
2025-04-09 06:31:02.949 INFO: Epoch 56, Train Loss: 0.1087, Val Loss: 0.1223
train_e/atom_mae: 0.000033
2025-04-09 06:31:02.950 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:31:02.950 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:31:02.953 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:31:02.953 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:31:02.955 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:31:02.955 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005384
2025-04-09 06:31:02.956 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007659
2025-04-09 06:31:02.956 INFO: val_f_rmse: 0.007659
##### Step: 296 Learning rate: 6.103515625e-07 #####
2025-04-09 06:32:27.452 INFO: ##### Step: 296 Learning rate: 6.103515625e-07 #####
Epoch 57, Train Loss: 0.1088, Val Loss: 0.1220
2025-04-09 06:32:27.453 INFO: Epoch 57, Train Loss: 0.1088, Val Loss: 0.1220
train_e/atom_mae: 0.000033
2025-04-09 06:32:27.454 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000067
2025-04-09 06:32:27.454 INFO: train_e/atom_rmse: 0.000067
train_f_mae: 0.005247
2025-04-09 06:32:27.457 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:32:27.457 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:32:27.459 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:32:27.459 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005384
2025-04-09 06:32:27.460 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007660
2025-04-09 06:32:27.460 INFO: val_f_rmse: 0.007660
##### Step: 297 Learning rate: 6.103515625e-07 #####
2025-04-09 06:33:51.910 INFO: ##### Step: 297 Learning rate: 6.103515625e-07 #####
Epoch 58, Train Loss: 0.1084, Val Loss: 0.1210
2025-04-09 06:33:51.911 INFO: Epoch 58, Train Loss: 0.1084, Val Loss: 0.1210
train_e/atom_mae: 0.000033
2025-04-09 06:33:51.912 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:33:51.912 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005249
2025-04-09 06:33:51.914 INFO: train_f_mae: 0.005249
train_f_rmse: 0.007439
2025-04-09 06:33:51.915 INFO: train_f_rmse: 0.007439
val_e/atom_mae: 0.000035
2025-04-09 06:33:51.917 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:33:51.917 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005384
2025-04-09 06:33:51.917 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007661
2025-04-09 06:33:51.918 INFO: val_f_rmse: 0.007661
##### Step: 298 Learning rate: 6.103515625e-07 #####
2025-04-09 06:35:16.402 INFO: ##### Step: 298 Learning rate: 6.103515625e-07 #####
Epoch 59, Train Loss: 0.1080, Val Loss: 0.1221
2025-04-09 06:35:16.403 INFO: Epoch 59, Train Loss: 0.1080, Val Loss: 0.1221
train_e/atom_mae: 0.000033
2025-04-09 06:35:16.404 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:35:16.404 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005248
2025-04-09 06:35:16.406 INFO: train_f_mae: 0.005248
train_f_rmse: 0.007438
2025-04-09 06:35:16.406 INFO: train_f_rmse: 0.007438
val_e/atom_mae: 0.000035
2025-04-09 06:35:16.409 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:35:16.409 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 06:35:16.409 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 06:35:16.410 INFO: val_f_rmse: 0.007657
##### Step: 299 Learning rate: 6.103515625e-07 #####
2025-04-09 06:36:40.907 INFO: ##### Step: 299 Learning rate: 6.103515625e-07 #####
Epoch 60, Train Loss: 0.1087, Val Loss: 0.1222
2025-04-09 06:36:40.907 INFO: Epoch 60, Train Loss: 0.1087, Val Loss: 0.1222
train_e/atom_mae: 0.000033
2025-04-09 06:36:40.908 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:36:40.908 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:36:40.911 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 06:36:40.911 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 06:36:40.913 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:36:40.914 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005384
2025-04-09 06:36:40.914 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007661
2025-04-09 06:36:40.914 INFO: val_f_rmse: 0.007661
##### Step: 300 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:38:05.403 INFO: ##### Step: 300 Learning rate: 3.0517578125e-07 #####
Epoch 61, Train Loss: 0.1082, Val Loss: 0.1230
2025-04-09 06:38:05.404 INFO: Epoch 61, Train Loss: 0.1082, Val Loss: 0.1230
train_e/atom_mae: 0.000033
2025-04-09 06:38:05.405 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:38:05.405 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:38:05.408 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 06:38:05.408 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 06:38:05.410 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:38:05.410 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005384
2025-04-09 06:38:05.411 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007661
2025-04-09 06:38:05.411 INFO: val_f_rmse: 0.007661
##### Step: 301 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:39:29.868 INFO: ##### Step: 301 Learning rate: 3.0517578125e-07 #####
Epoch 62, Train Loss: 0.1085, Val Loss: 0.1229
2025-04-09 06:39:29.869 INFO: Epoch 62, Train Loss: 0.1085, Val Loss: 0.1229
train_e/atom_mae: 0.000033
2025-04-09 06:39:29.870 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:39:29.870 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005248
2025-04-09 06:39:29.872 INFO: train_f_mae: 0.005248
train_f_rmse: 0.007437
2025-04-09 06:39:29.872 INFO: train_f_rmse: 0.007437
val_e/atom_mae: 0.000035
2025-04-09 06:39:29.875 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:39:29.875 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005384
2025-04-09 06:39:29.875 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007660
2025-04-09 06:39:29.875 INFO: val_f_rmse: 0.007660
##### Step: 302 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:40:54.374 INFO: ##### Step: 302 Learning rate: 3.0517578125e-07 #####
Epoch 63, Train Loss: 0.1083, Val Loss: 0.1218
2025-04-09 06:40:54.375 INFO: Epoch 63, Train Loss: 0.1083, Val Loss: 0.1218
train_e/atom_mae: 0.000033
2025-04-09 06:40:54.376 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:40:54.376 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:40:54.379 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 06:40:54.379 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 06:40:54.381 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:40:54.381 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005384
2025-04-09 06:40:54.382 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007660
2025-04-09 06:40:54.382 INFO: val_f_rmse: 0.007660
##### Step: 303 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:42:18.836 INFO: ##### Step: 303 Learning rate: 3.0517578125e-07 #####
Epoch 64, Train Loss: 0.1081, Val Loss: 0.1228
2025-04-09 06:42:18.837 INFO: Epoch 64, Train Loss: 0.1081, Val Loss: 0.1228
train_e/atom_mae: 0.000033
2025-04-09 06:42:18.837 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:42:18.838 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:42:18.840 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 06:42:18.840 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 06:42:18.843 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:42:18.843 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 06:42:18.843 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007659
2025-04-09 06:42:18.843 INFO: val_f_rmse: 0.007659
##### Step: 304 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:43:43.325 INFO: ##### Step: 304 Learning rate: 3.0517578125e-07 #####
Epoch 65, Train Loss: 0.1083, Val Loss: 0.1218
2025-04-09 06:43:43.325 INFO: Epoch 65, Train Loss: 0.1083, Val Loss: 0.1218
train_e/atom_mae: 0.000033
2025-04-09 06:43:43.326 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:43:43.326 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:43:43.329 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 06:43:43.329 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 06:43:43.331 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:43:43.332 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 06:43:43.332 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 06:43:43.332 INFO: val_f_rmse: 0.007658
##### Step: 305 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:45:07.804 INFO: ##### Step: 305 Learning rate: 3.0517578125e-07 #####
Epoch 66, Train Loss: 0.1082, Val Loss: 0.1229
2025-04-09 06:45:07.804 INFO: Epoch 66, Train Loss: 0.1082, Val Loss: 0.1229
train_e/atom_mae: 0.000033
2025-04-09 06:45:07.805 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:45:07.805 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:45:07.808 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:45:07.808 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:45:07.810 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:45:07.811 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 06:45:07.811 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 06:45:07.811 INFO: val_f_rmse: 0.007657
##### Step: 306 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:46:32.291 INFO: ##### Step: 306 Learning rate: 3.0517578125e-07 #####
Epoch 67, Train Loss: 0.1083, Val Loss: 0.1217
2025-04-09 06:46:32.291 INFO: Epoch 67, Train Loss: 0.1083, Val Loss: 0.1217
train_e/atom_mae: 0.000033
2025-04-09 06:46:32.292 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:46:32.292 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:46:32.295 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:46:32.295 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:46:32.297 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:46:32.298 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 06:46:32.298 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 06:46:32.298 INFO: val_f_rmse: 0.007658
##### Step: 307 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:47:56.935 INFO: ##### Step: 307 Learning rate: 3.0517578125e-07 #####
Epoch 68, Train Loss: 0.1083, Val Loss: 0.1224
2025-04-09 06:47:56.936 INFO: Epoch 68, Train Loss: 0.1083, Val Loss: 0.1224
train_e/atom_mae: 0.000033
2025-04-09 06:47:56.937 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:47:56.937 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:47:56.940 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 06:47:56.940 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 06:47:56.942 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:47:56.942 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 06:47:56.943 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007660
2025-04-09 06:47:56.943 INFO: val_f_rmse: 0.007660
##### Step: 308 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:49:21.405 INFO: ##### Step: 308 Learning rate: 3.0517578125e-07 #####
Epoch 69, Train Loss: 0.1083, Val Loss: 0.1238
2025-04-09 06:49:21.405 INFO: Epoch 69, Train Loss: 0.1083, Val Loss: 0.1238
train_e/atom_mae: 0.000033
2025-04-09 06:49:21.406 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:49:21.406 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:49:21.409 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 06:49:21.409 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 06:49:21.411 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:49:21.412 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005384
2025-04-09 06:49:21.412 INFO: val_f_mae: 0.005384
val_f_rmse: 0.007660
2025-04-09 06:49:21.412 INFO: val_f_rmse: 0.007660
##### Step: 309 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:50:45.892 INFO: ##### Step: 309 Learning rate: 3.0517578125e-07 #####
Epoch 70, Train Loss: 0.1085, Val Loss: 0.1223
2025-04-09 06:50:45.893 INFO: Epoch 70, Train Loss: 0.1085, Val Loss: 0.1223
train_e/atom_mae: 0.000033
2025-04-09 06:50:45.894 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:50:45.894 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:50:45.896 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 06:50:45.896 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 06:50:45.899 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:50:45.899 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 06:50:45.899 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007657
2025-04-09 06:50:45.900 INFO: val_f_rmse: 0.007657
##### Step: 310 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:52:10.369 INFO: ##### Step: 310 Learning rate: 3.0517578125e-07 #####
Epoch 71, Train Loss: 0.1086, Val Loss: 0.1226
2025-04-09 06:52:10.370 INFO: Epoch 71, Train Loss: 0.1086, Val Loss: 0.1226
train_e/atom_mae: 0.000033
2025-04-09 06:52:10.371 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:52:10.371 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:52:10.373 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:52:10.373 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:52:10.376 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:52:10.376 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 06:52:10.376 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007655
2025-04-09 06:52:10.377 INFO: val_f_rmse: 0.007655
##### Step: 311 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:53:34.865 INFO: ##### Step: 311 Learning rate: 3.0517578125e-07 #####
Epoch 72, Train Loss: 0.1084, Val Loss: 0.1223
2025-04-09 06:53:34.866 INFO: Epoch 72, Train Loss: 0.1084, Val Loss: 0.1223
train_e/atom_mae: 0.000033
2025-04-09 06:53:34.867 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:53:34.867 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 06:53:34.870 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007434
2025-04-09 06:53:34.870 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 06:53:34.872 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:53:34.872 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 06:53:34.873 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 06:53:34.873 INFO: val_f_rmse: 0.007656
##### Step: 312 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:54:59.349 INFO: ##### Step: 312 Learning rate: 3.0517578125e-07 #####
Epoch 73, Train Loss: 0.1085, Val Loss: 0.1231
2025-04-09 06:54:59.350 INFO: Epoch 73, Train Loss: 0.1085, Val Loss: 0.1231
train_e/atom_mae: 0.000033
2025-04-09 06:54:59.350 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:54:59.351 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:54:59.353 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:54:59.353 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:54:59.356 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:54:59.356 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 06:54:59.356 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 06:54:59.357 INFO: val_f_rmse: 0.007658
##### Step: 313 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:56:23.848 INFO: ##### Step: 313 Learning rate: 3.0517578125e-07 #####
Epoch 74, Train Loss: 0.1084, Val Loss: 0.1216
2025-04-09 06:56:23.849 INFO: Epoch 74, Train Loss: 0.1084, Val Loss: 0.1216
train_e/atom_mae: 0.000033
2025-04-09 06:56:23.849 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:56:23.850 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 06:56:23.852 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 06:56:23.852 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 06:56:23.855 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 06:56:23.855 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 06:56:23.855 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 06:56:23.855 INFO: val_f_rmse: 0.007658
##### Step: 314 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:57:48.339 INFO: ##### Step: 314 Learning rate: 3.0517578125e-07 #####
Epoch 75, Train Loss: 0.1083, Val Loss: 0.1227
2025-04-09 06:57:48.340 INFO: Epoch 75, Train Loss: 0.1083, Val Loss: 0.1227
train_e/atom_mae: 0.000033
2025-04-09 06:57:48.341 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:57:48.341 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 06:57:48.344 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007434
2025-04-09 06:57:48.344 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 06:57:48.346 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:57:48.347 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 06:57:48.347 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007655
2025-04-09 06:57:48.347 INFO: val_f_rmse: 0.007655
##### Step: 315 Learning rate: 3.0517578125e-07 #####
2025-04-09 06:59:12.850 INFO: ##### Step: 315 Learning rate: 3.0517578125e-07 #####
Epoch 76, Train Loss: 0.1086, Val Loss: 0.1228
2025-04-09 06:59:12.851 INFO: Epoch 76, Train Loss: 0.1086, Val Loss: 0.1228
train_e/atom_mae: 0.000033
2025-04-09 06:59:12.852 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 06:59:12.852 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 06:59:12.855 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007433
2025-04-09 06:59:12.855 INFO: train_f_rmse: 0.007433
val_e/atom_mae: 0.000035
2025-04-09 06:59:12.857 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 06:59:12.857 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 06:59:12.858 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 06:59:12.858 INFO: val_f_rmse: 0.007656
##### Step: 316 Learning rate: 3.0517578125e-07 #####
2025-04-09 07:00:37.328 INFO: ##### Step: 316 Learning rate: 3.0517578125e-07 #####
Epoch 77, Train Loss: 0.1083, Val Loss: 0.1247
2025-04-09 07:00:37.328 INFO: Epoch 77, Train Loss: 0.1083, Val Loss: 0.1247
train_e/atom_mae: 0.000033
2025-04-09 07:00:37.329 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:00:37.329 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:00:37.332 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:00:37.332 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:00:37.334 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000074
2025-04-09 07:00:37.335 INFO: val_e/atom_rmse: 0.000074
val_f_mae: 0.005382
2025-04-09 07:00:37.335 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 07:00:37.335 INFO: val_f_rmse: 0.007657
##### Step: 317 Learning rate: 3.0517578125e-07 #####
2025-04-09 07:02:01.792 INFO: ##### Step: 317 Learning rate: 3.0517578125e-07 #####
Epoch 78, Train Loss: 0.1086, Val Loss: 0.1221
2025-04-09 07:02:01.793 INFO: Epoch 78, Train Loss: 0.1086, Val Loss: 0.1221
train_e/atom_mae: 0.000033
2025-04-09 07:02:01.794 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:02:01.794 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:02:01.797 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:02:01.797 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:02:01.799 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 07:02:01.799 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 07:02:01.800 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 07:02:01.800 INFO: val_f_rmse: 0.007658
##### Step: 318 Learning rate: 3.0517578125e-07 #####
2025-04-09 07:03:26.288 INFO: ##### Step: 318 Learning rate: 3.0517578125e-07 #####
Epoch 79, Train Loss: 0.1083, Val Loss: 0.1213
2025-04-09 07:03:26.288 INFO: Epoch 79, Train Loss: 0.1083, Val Loss: 0.1213
train_e/atom_mae: 0.000033
2025-04-09 07:03:26.289 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:03:26.289 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:03:26.292 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007436
2025-04-09 07:03:26.292 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000035
2025-04-09 07:03:26.294 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 07:03:26.295 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 07:03:26.295 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007659
2025-04-09 07:03:26.295 INFO: val_f_rmse: 0.007659
##### Step: 319 Learning rate: 3.0517578125e-07 #####
2025-04-09 07:04:50.811 INFO: ##### Step: 319 Learning rate: 3.0517578125e-07 #####
Epoch 80, Train Loss: 0.1080, Val Loss: 0.1223
2025-04-09 07:04:50.812 INFO: Epoch 80, Train Loss: 0.1080, Val Loss: 0.1223
train_e/atom_mae: 0.000033
2025-04-09 07:04:50.813 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:04:50.813 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:04:50.816 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:04:50.816 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:04:50.818 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 07:04:50.819 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 07:04:50.819 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007655
2025-04-09 07:04:50.819 INFO: val_f_rmse: 0.007655
##### Step: 320 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:06:15.313 INFO: ##### Step: 320 Learning rate: 1.52587890625e-07 #####
Epoch 81, Train Loss: 0.1082, Val Loss: 0.1218
2025-04-09 07:06:15.314 INFO: Epoch 81, Train Loss: 0.1082, Val Loss: 0.1218
train_e/atom_mae: 0.000033
2025-04-09 07:06:15.314 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:06:15.315 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 07:06:15.317 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007434
2025-04-09 07:06:15.317 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 07:06:15.320 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 07:06:15.320 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 07:06:15.320 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 07:06:15.320 INFO: val_f_rmse: 0.007656
##### Step: 321 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:07:39.798 INFO: ##### Step: 321 Learning rate: 1.52587890625e-07 #####
Epoch 82, Train Loss: 0.1081, Val Loss: 0.1226
2025-04-09 07:07:39.798 INFO: Epoch 82, Train Loss: 0.1081, Val Loss: 0.1226
train_e/atom_mae: 0.000033
2025-04-09 07:07:39.799 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:07:39.799 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:07:39.802 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007434
2025-04-09 07:07:39.802 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 07:07:39.804 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 07:07:39.805 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 07:07:39.805 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 07:07:39.806 INFO: val_f_rmse: 0.007657
##### Step: 322 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:09:04.295 INFO: ##### Step: 322 Learning rate: 1.52587890625e-07 #####
Epoch 83, Train Loss: 0.1080, Val Loss: 0.1215
2025-04-09 07:09:04.296 INFO: Epoch 83, Train Loss: 0.1080, Val Loss: 0.1215
train_e/atom_mae: 0.000033
2025-04-09 07:09:04.296 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:09:04.297 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 07:09:04.299 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007434
2025-04-09 07:09:04.299 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000034
2025-04-09 07:09:04.302 INFO: val_e/atom_mae: 0.000034
val_e/atom_rmse: 0.000072
2025-04-09 07:09:04.302 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 07:09:04.302 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 07:09:04.302 INFO: val_f_rmse: 0.007656
##### Step: 323 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:10:28.735 INFO: ##### Step: 323 Learning rate: 1.52587890625e-07 #####
Epoch 84, Train Loss: 0.1080, Val Loss: 0.1219
2025-04-09 07:10:28.736 INFO: Epoch 84, Train Loss: 0.1080, Val Loss: 0.1219
train_e/atom_mae: 0.000033
2025-04-09 07:10:28.736 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:10:28.737 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 07:10:28.739 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007434
2025-04-09 07:10:28.739 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000034
2025-04-09 07:10:28.742 INFO: val_e/atom_mae: 0.000034
val_e/atom_rmse: 0.000072
2025-04-09 07:10:28.742 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 07:10:28.742 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 07:10:28.742 INFO: val_f_rmse: 0.007656
##### Step: 324 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:11:53.211 INFO: ##### Step: 324 Learning rate: 1.52587890625e-07 #####
Epoch 85, Train Loss: 0.1080, Val Loss: 0.1230
2025-04-09 07:11:53.212 INFO: Epoch 85, Train Loss: 0.1080, Val Loss: 0.1230
train_e/atom_mae: 0.000033
2025-04-09 07:11:53.213 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:11:53.213 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:11:53.216 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007434
2025-04-09 07:11:53.216 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 07:11:53.218 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 07:11:53.219 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 07:11:53.219 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 07:11:53.219 INFO: val_f_rmse: 0.007656
##### Step: 325 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:13:17.624 INFO: ##### Step: 325 Learning rate: 1.52587890625e-07 #####
Epoch 86, Train Loss: 0.1082, Val Loss: 0.1228
2025-04-09 07:13:17.624 INFO: Epoch 86, Train Loss: 0.1082, Val Loss: 0.1228
train_e/atom_mae: 0.000033
2025-04-09 07:13:17.625 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:13:17.625 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:13:17.628 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:13:17.628 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:13:17.630 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 07:13:17.631 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 07:13:17.631 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 07:13:17.631 INFO: val_f_rmse: 0.007658
##### Step: 326 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:14:42.100 INFO: ##### Step: 326 Learning rate: 1.52587890625e-07 #####
Epoch 87, Train Loss: 0.1082, Val Loss: 0.1219
2025-04-09 07:14:42.101 INFO: Epoch 87, Train Loss: 0.1082, Val Loss: 0.1219
train_e/atom_mae: 0.000033
2025-04-09 07:14:42.102 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:14:42.102 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:14:42.105 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:14:42.105 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000034
2025-04-09 07:14:42.107 INFO: val_e/atom_mae: 0.000034
val_e/atom_rmse: 0.000072
2025-04-09 07:14:42.107 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 07:14:42.108 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007657
2025-04-09 07:14:42.108 INFO: val_f_rmse: 0.007657
##### Step: 327 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:16:06.570 INFO: ##### Step: 327 Learning rate: 1.52587890625e-07 #####
Epoch 88, Train Loss: 0.1082, Val Loss: 0.1228
2025-04-09 07:16:06.571 INFO: Epoch 88, Train Loss: 0.1082, Val Loss: 0.1228
train_e/atom_mae: 0.000033
2025-04-09 07:16:06.571 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:16:06.571 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:16:06.574 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:16:06.574 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:16:06.577 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 07:16:06.577 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 07:16:06.577 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 07:16:06.577 INFO: val_f_rmse: 0.007657
##### Step: 328 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:17:31.038 INFO: ##### Step: 328 Learning rate: 1.52587890625e-07 #####
Epoch 89, Train Loss: 0.1083, Val Loss: 0.1220
2025-04-09 07:17:31.039 INFO: Epoch 89, Train Loss: 0.1083, Val Loss: 0.1220
train_e/atom_mae: 0.000033
2025-04-09 07:17:31.040 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:17:31.040 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:17:31.042 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:17:31.043 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000034
2025-04-09 07:17:31.045 INFO: val_e/atom_mae: 0.000034
val_e/atom_rmse: 0.000072
2025-04-09 07:17:31.045 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 07:17:31.045 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007657
2025-04-09 07:17:31.046 INFO: val_f_rmse: 0.007657
##### Step: 329 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:18:55.514 INFO: ##### Step: 329 Learning rate: 1.52587890625e-07 #####
Epoch 90, Train Loss: 0.1082, Val Loss: 0.1217
2025-04-09 07:18:55.514 INFO: Epoch 90, Train Loss: 0.1082, Val Loss: 0.1217
train_e/atom_mae: 0.000033
2025-04-09 07:18:55.515 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:18:55.515 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:18:55.518 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:18:55.518 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:18:55.520 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 07:18:55.521 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 07:18:55.521 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 07:18:55.521 INFO: val_f_rmse: 0.007658
##### Step: 330 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:20:20.006 INFO: ##### Step: 330 Learning rate: 1.52587890625e-07 #####
Epoch 91, Train Loss: 0.1080, Val Loss: 0.1219
2025-04-09 07:20:20.007 INFO: Epoch 91, Train Loss: 0.1080, Val Loss: 0.1219
train_e/atom_mae: 0.000033
2025-04-09 07:20:20.008 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:20:20.008 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005248
2025-04-09 07:20:20.011 INFO: train_f_mae: 0.005248
train_f_rmse: 0.007436
2025-04-09 07:20:20.011 INFO: train_f_rmse: 0.007436
val_e/atom_mae: 0.000034
2025-04-09 07:20:20.013 INFO: val_e/atom_mae: 0.000034
val_e/atom_rmse: 0.000072
2025-04-09 07:20:20.013 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005383
2025-04-09 07:20:20.014 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007657
2025-04-09 07:20:20.014 INFO: val_f_rmse: 0.007657
##### Step: 331 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:21:44.496 INFO: ##### Step: 331 Learning rate: 1.52587890625e-07 #####
Epoch 92, Train Loss: 0.1082, Val Loss: 0.1218
2025-04-09 07:21:44.497 INFO: Epoch 92, Train Loss: 0.1082, Val Loss: 0.1218
train_e/atom_mae: 0.000033
2025-04-09 07:21:44.498 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:21:44.498 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:21:44.500 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:21:44.501 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:21:44.503 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 07:21:44.503 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 07:21:44.504 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 07:21:44.504 INFO: val_f_rmse: 0.007656
##### Step: 332 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:23:09.000 INFO: ##### Step: 332 Learning rate: 1.52587890625e-07 #####
Epoch 93, Train Loss: 0.1081, Val Loss: 0.1221
2025-04-09 07:23:09.000 INFO: Epoch 93, Train Loss: 0.1081, Val Loss: 0.1221
train_e/atom_mae: 0.000033
2025-04-09 07:23:09.001 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:23:09.001 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:23:09.004 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:23:09.004 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:23:09.006 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 07:23:09.007 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 07:23:09.007 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 07:23:09.007 INFO: val_f_rmse: 0.007656
##### Step: 333 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:24:33.466 INFO: ##### Step: 333 Learning rate: 1.52587890625e-07 #####
Epoch 94, Train Loss: 0.1082, Val Loss: 0.1229
2025-04-09 07:24:33.466 INFO: Epoch 94, Train Loss: 0.1082, Val Loss: 0.1229
train_e/atom_mae: 0.000033
2025-04-09 07:24:33.467 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:24:33.467 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:24:33.470 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007434
2025-04-09 07:24:33.470 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 07:24:33.472 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 07:24:33.473 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005382
2025-04-09 07:24:33.473 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 07:24:33.473 INFO: val_f_rmse: 0.007656
##### Step: 334 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:25:57.912 INFO: ##### Step: 334 Learning rate: 1.52587890625e-07 #####
Epoch 95, Train Loss: 0.1081, Val Loss: 0.1226
2025-04-09 07:25:57.912 INFO: Epoch 95, Train Loss: 0.1081, Val Loss: 0.1226
train_e/atom_mae: 0.000033
2025-04-09 07:25:57.913 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:25:57.913 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:25:57.916 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007434
2025-04-09 07:25:57.916 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000035
2025-04-09 07:25:57.918 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 07:25:57.919 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 07:25:57.919 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007657
2025-04-09 07:25:57.919 INFO: val_f_rmse: 0.007657
##### Step: 335 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:27:22.324 INFO: ##### Step: 335 Learning rate: 1.52587890625e-07 #####
Epoch 96, Train Loss: 0.1081, Val Loss: 0.1216
2025-04-09 07:27:22.325 INFO: Epoch 96, Train Loss: 0.1081, Val Loss: 0.1216
train_e/atom_mae: 0.000033
2025-04-09 07:27:22.326 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:27:22.326 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:27:22.328 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:27:22.329 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:27:22.331 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 07:27:22.331 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 07:27:22.332 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 07:27:22.332 INFO: val_f_rmse: 0.007657
##### Step: 336 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:28:46.850 INFO: ##### Step: 336 Learning rate: 1.52587890625e-07 #####
Epoch 97, Train Loss: 0.1081, Val Loss: 0.1222
2025-04-09 07:28:46.850 INFO: Epoch 97, Train Loss: 0.1081, Val Loss: 0.1222
train_e/atom_mae: 0.000033
2025-04-09 07:28:46.851 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:28:46.851 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:28:46.854 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:28:46.854 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:28:46.856 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000072
2025-04-09 07:28:46.857 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 07:28:46.857 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007657
2025-04-09 07:28:46.857 INFO: val_f_rmse: 0.007657
##### Step: 337 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:30:11.350 INFO: ##### Step: 337 Learning rate: 1.52587890625e-07 #####
Epoch 98, Train Loss: 0.1082, Val Loss: 0.1218
2025-04-09 07:30:11.350 INFO: Epoch 98, Train Loss: 0.1082, Val Loss: 0.1218
train_e/atom_mae: 0.000033
2025-04-09 07:30:11.351 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:30:11.351 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005246
2025-04-09 07:30:11.354 INFO: train_f_mae: 0.005246
train_f_rmse: 0.007434
2025-04-09 07:30:11.354 INFO: train_f_rmse: 0.007434
val_e/atom_mae: 0.000034
2025-04-09 07:30:11.356 INFO: val_e/atom_mae: 0.000034
val_e/atom_rmse: 0.000072
2025-04-09 07:30:11.357 INFO: val_e/atom_rmse: 0.000072
val_f_mae: 0.005382
2025-04-09 07:30:11.357 INFO: val_f_mae: 0.005382
val_f_rmse: 0.007656
2025-04-09 07:30:11.357 INFO: val_f_rmse: 0.007656
##### Step: 338 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:31:35.845 INFO: ##### Step: 338 Learning rate: 1.52587890625e-07 #####
Epoch 99, Train Loss: 0.1083, Val Loss: 0.1227
2025-04-09 07:31:35.845 INFO: Epoch 99, Train Loss: 0.1083, Val Loss: 0.1227
train_e/atom_mae: 0.000033
2025-04-09 07:31:35.846 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:31:35.846 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:31:35.849 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:31:35.849 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:31:35.851 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 07:31:35.852 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 07:31:35.852 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 07:31:35.852 INFO: val_f_rmse: 0.007658
##### Step: 339 Learning rate: 1.52587890625e-07 #####
2025-04-09 07:33:00.325 INFO: ##### Step: 339 Learning rate: 1.52587890625e-07 #####
Epoch 100, Train Loss: 0.1081, Val Loss: 0.1227
2025-04-09 07:33:00.326 INFO: Epoch 100, Train Loss: 0.1081, Val Loss: 0.1227
train_e/atom_mae: 0.000033
2025-04-09 07:33:00.327 INFO: train_e/atom_mae: 0.000033
train_e/atom_rmse: 0.000066
2025-04-09 07:33:00.327 INFO: train_e/atom_rmse: 0.000066
train_f_mae: 0.005247
2025-04-09 07:33:00.330 INFO: train_f_mae: 0.005247
train_f_rmse: 0.007435
2025-04-09 07:33:00.330 INFO: train_f_rmse: 0.007435
val_e/atom_mae: 0.000035
2025-04-09 07:33:00.332 INFO: val_e/atom_mae: 0.000035
val_e/atom_rmse: 0.000073
2025-04-09 07:33:00.332 INFO: val_e/atom_rmse: 0.000073
val_f_mae: 0.005383
2025-04-09 07:33:00.333 INFO: val_f_mae: 0.005383
val_f_rmse: 0.007658
2025-04-09 07:33:00.333 INFO: val_f_rmse: 0.007658
2025-04-09 07:33:00.370 INFO: Finished
2025-04-09 07:33:00.370 INFO: Number of trainable parameters: 81834
