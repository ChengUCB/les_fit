2025-06-25 04:13:18.787 INFO: reading data
2025-06-25 04:13:21.254 INFO: Loaded 5000 training configurations from '../Au-MgO-Al.xyz'
2025-06-25 04:13:21.255 INFO: Using random 10.0% of training set for validation
2025-06-25 04:13:39.138 INFO: CUDA version: 12.1, CUDA device: 0
2025-06-25 04:13:39.138 INFO: device: cuda
2025-06-25 04:13:39.138 INFO: building CACE representation
2025-06-25 04:13:39.282 INFO: Representation: Cace(
  (node_onehot): NodeEncoder(num_classes=4)
  (node_embedding_sender): NodeEmbedding(num_classes=4, embedding_dim=4)
  (node_embedding_receiver): NodeEmbedding(num_classes=4, embedding_dim=4)
  (edge_coding): EdgeEncoder(directed=True)
  (radial_basis): BesselRBF(cutoff=5.5, n_rbf=6, trainable=True)
  (cutoff_fn): PolynomialCutoff(p=6.0, cutoff=5.5)
  (angular_basis): AngularComponent(l_max=2)
  (radial_transform): SharedRadialLinearTransform(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 6x12x16 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 6x12x16 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 6x12x16 (cuda:0)]
    )
  )
  (symmetrizer): Symmetrizer()
  (message_passing_list): ModuleList()
)
2025-06-25 04:13:39.282 INFO: building CACE NNP
2025-06-25 04:13:39.283 INFO: First train loop:
2025-06-25 04:13:39.283 INFO: creating training task
2025-06-25 04:13:39.284 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-25 04:14:11.315 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 59.9084, Val Loss: 29.2216
2025-06-25 04:14:11.315 INFO: Epoch 1, Train Loss: 59.9084, Val Loss: 29.2216
train_e/atom_mae: 0.007923
2025-06-25 04:14:11.318 INFO: train_e/atom_mae: 0.007923
train_e/atom_rmse: 0.010218
2025-06-25 04:14:11.328 INFO: train_e/atom_rmse: 0.010218
train_f_mae: 0.132514
2025-06-25 04:14:11.332 INFO: train_f_mae: 0.132514
train_f_rmse: 0.244504
2025-06-25 04:14:11.332 INFO: train_f_rmse: 0.244504
val_e/atom_mae: 0.006693
2025-06-25 04:14:11.335 INFO: val_e/atom_mae: 0.006693
val_e/atom_rmse: 0.007251
2025-06-25 04:14:11.336 INFO: val_e/atom_rmse: 0.007251
val_f_mae: 0.100727
2025-06-25 04:14:11.336 INFO: val_f_mae: 0.100727
val_f_rmse: 0.170757
2025-06-25 04:14:11.336 INFO: val_f_rmse: 0.170757
##### Step: 1 Learning rate: 0.004 #####
2025-06-25 04:14:41.836 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 22.0421, Val Loss: 19.4266
2025-06-25 04:14:41.836 INFO: Epoch 2, Train Loss: 22.0421, Val Loss: 19.4266
train_e/atom_mae: 0.008141
2025-06-25 04:14:41.837 INFO: train_e/atom_mae: 0.008141
train_e/atom_rmse: 0.010084
2025-06-25 04:14:41.838 INFO: train_e/atom_rmse: 0.010084
train_f_mae: 0.091379
2025-06-25 04:14:41.841 INFO: train_f_mae: 0.091379
train_f_rmse: 0.148051
2025-06-25 04:14:41.841 INFO: train_f_rmse: 0.148051
val_e/atom_mae: 0.010637
2025-06-25 04:14:41.844 INFO: val_e/atom_mae: 0.010637
val_e/atom_rmse: 0.012710
2025-06-25 04:14:41.844 INFO: val_e/atom_rmse: 0.012710
val_f_mae: 0.086000
2025-06-25 04:14:41.845 INFO: val_f_mae: 0.086000
val_f_rmse: 0.138677
2025-06-25 04:14:41.845 INFO: val_f_rmse: 0.138677
##### Step: 2 Learning rate: 0.006 #####
2025-06-25 04:15:12.333 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 17.0063, Val Loss: 17.4284
2025-06-25 04:15:12.334 INFO: Epoch 3, Train Loss: 17.0063, Val Loss: 17.4284
train_e/atom_mae: 0.010433
2025-06-25 04:15:12.335 INFO: train_e/atom_mae: 0.010433
train_e/atom_rmse: 0.013789
2025-06-25 04:15:12.335 INFO: train_e/atom_rmse: 0.013789
train_f_mae: 0.082011
2025-06-25 04:15:12.338 INFO: train_f_mae: 0.082011
train_f_rmse: 0.129523
2025-06-25 04:15:12.339 INFO: train_f_rmse: 0.129523
val_e/atom_mae: 0.006791
2025-06-25 04:15:12.341 INFO: val_e/atom_mae: 0.006791
val_e/atom_rmse: 0.007354
2025-06-25 04:15:12.342 INFO: val_e/atom_rmse: 0.007354
val_f_mae: 0.082817
2025-06-25 04:15:12.342 INFO: val_f_mae: 0.082817
val_f_rmse: 0.131769
2025-06-25 04:15:12.342 INFO: val_f_rmse: 0.131769
##### Step: 3 Learning rate: 0.008 #####
2025-06-25 04:15:42.823 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 13.2138, Val Loss: 10.5219
2025-06-25 04:15:42.824 INFO: Epoch 4, Train Loss: 13.2138, Val Loss: 10.5219
train_e/atom_mae: 0.007380
2025-06-25 04:15:42.825 INFO: train_e/atom_mae: 0.007380
train_e/atom_rmse: 0.009127
2025-06-25 04:15:42.825 INFO: train_e/atom_rmse: 0.009127
train_f_mae: 0.070202
2025-06-25 04:15:42.828 INFO: train_f_mae: 0.070202
train_f_rmse: 0.114512
2025-06-25 04:15:42.828 INFO: train_f_rmse: 0.114512
val_e/atom_mae: 0.002615
2025-06-25 04:15:42.831 INFO: val_e/atom_mae: 0.002615
val_e/atom_rmse: 0.004114
2025-06-25 04:15:42.831 INFO: val_e/atom_rmse: 0.004114
val_f_mae: 0.063143
2025-06-25 04:15:42.832 INFO: val_f_mae: 0.063143
val_f_rmse: 0.102477
2025-06-25 04:15:42.832 INFO: val_f_rmse: 0.102477
##### Step: 4 Learning rate: 0.01 #####
2025-06-25 04:16:13.241 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 10.8015, Val Loss: 10.1127
2025-06-25 04:16:13.241 INFO: Epoch 5, Train Loss: 10.8015, Val Loss: 10.1127
train_e/atom_mae: 0.009281
2025-06-25 04:16:13.242 INFO: train_e/atom_mae: 0.009281
train_e/atom_rmse: 0.012016
2025-06-25 04:16:13.242 INFO: train_e/atom_rmse: 0.012016
train_f_mae: 0.061594
2025-06-25 04:16:13.246 INFO: train_f_mae: 0.061594
train_f_rmse: 0.103086
2025-06-25 04:16:13.246 INFO: train_f_rmse: 0.103086
val_e/atom_mae: 0.013285
2025-06-25 04:16:13.249 INFO: val_e/atom_mae: 0.013285
val_e/atom_rmse: 0.014074
2025-06-25 04:16:13.249 INFO: val_e/atom_rmse: 0.014074
val_f_mae: 0.059135
2025-06-25 04:16:13.250 INFO: val_f_mae: 0.059135
val_f_rmse: 0.099363
2025-06-25 04:16:13.250 INFO: val_f_rmse: 0.099363
##### Step: 5 Learning rate: 0.01 #####
2025-06-25 04:16:43.550 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 9.6826, Val Loss: 9.9686
2025-06-25 04:16:43.551 INFO: Epoch 6, Train Loss: 9.6826, Val Loss: 9.9686
train_e/atom_mae: 0.011110
2025-06-25 04:16:43.552 INFO: train_e/atom_mae: 0.011110
train_e/atom_rmse: 0.013639
2025-06-25 04:16:43.552 INFO: train_e/atom_rmse: 0.013639
train_f_mae: 0.058651
2025-06-25 04:16:43.555 INFO: train_f_mae: 0.058651
train_f_rmse: 0.097250
2025-06-25 04:16:43.555 INFO: train_f_rmse: 0.097250
val_e/atom_mae: 0.004692
2025-06-25 04:16:43.558 INFO: val_e/atom_mae: 0.004692
val_e/atom_rmse: 0.005058
2025-06-25 04:16:43.559 INFO: val_e/atom_rmse: 0.005058
val_f_mae: 0.060570
2025-06-25 04:16:43.559 INFO: val_f_mae: 0.060570
val_f_rmse: 0.099688
2025-06-25 04:16:43.559 INFO: val_f_rmse: 0.099688
##### Step: 6 Learning rate: 0.01 #####
2025-06-25 04:17:13.917 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 9.1955, Val Loss: 9.1444
2025-06-25 04:17:13.918 INFO: Epoch 7, Train Loss: 9.1955, Val Loss: 9.1444
train_e/atom_mae: 0.011440
2025-06-25 04:17:13.919 INFO: train_e/atom_mae: 0.011440
train_e/atom_rmse: 0.014289
2025-06-25 04:17:13.919 INFO: train_e/atom_rmse: 0.014289
train_f_mae: 0.057758
2025-06-25 04:17:13.922 INFO: train_f_mae: 0.057758
train_f_rmse: 0.094596
2025-06-25 04:17:13.923 INFO: train_f_rmse: 0.094596
val_e/atom_mae: 0.025169
2025-06-25 04:17:13.925 INFO: val_e/atom_mae: 0.025169
val_e/atom_rmse: 0.025349
2025-06-25 04:17:13.926 INFO: val_e/atom_rmse: 0.025349
val_f_mae: 0.057949
2025-06-25 04:17:13.926 INFO: val_f_mae: 0.057949
val_f_rmse: 0.091471
2025-06-25 04:17:13.926 INFO: val_f_rmse: 0.091471
##### Step: 7 Learning rate: 0.01 #####
2025-06-25 04:17:44.149 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 8.6956, Val Loss: 10.6195
2025-06-25 04:17:44.149 INFO: Epoch 8, Train Loss: 8.6956, Val Loss: 10.6195
train_e/atom_mae: 0.010129
2025-06-25 04:17:44.150 INFO: train_e/atom_mae: 0.010129
train_e/atom_rmse: 0.012892
2025-06-25 04:17:44.150 INFO: train_e/atom_rmse: 0.012892
train_f_mae: 0.057446
2025-06-25 04:17:44.154 INFO: train_f_mae: 0.057446
train_f_rmse: 0.092165
2025-06-25 04:17:44.154 INFO: train_f_rmse: 0.092165
val_e/atom_mae: 0.014424
2025-06-25 04:17:44.157 INFO: val_e/atom_mae: 0.014424
val_e/atom_rmse: 0.014886
2025-06-25 04:17:44.157 INFO: val_e/atom_rmse: 0.014886
val_f_mae: 0.067082
2025-06-25 04:17:44.158 INFO: val_f_mae: 0.067082
val_f_rmse: 0.101742
2025-06-25 04:17:44.158 INFO: val_f_rmse: 0.101742
##### Step: 8 Learning rate: 0.01 #####
2025-06-25 04:18:14.499 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 8.1320, Val Loss: 7.4793
2025-06-25 04:18:14.500 INFO: Epoch 9, Train Loss: 8.1320, Val Loss: 7.4793
train_e/atom_mae: 0.007915
2025-06-25 04:18:14.500 INFO: train_e/atom_mae: 0.007915
train_e/atom_rmse: 0.009872
2025-06-25 04:18:14.501 INFO: train_e/atom_rmse: 0.009872
train_f_mae: 0.056332
2025-06-25 04:18:14.504 INFO: train_f_mae: 0.056332
train_f_rmse: 0.089521
2025-06-25 04:18:14.504 INFO: train_f_rmse: 0.089521
val_e/atom_mae: 0.011384
2025-06-25 04:18:14.507 INFO: val_e/atom_mae: 0.011384
val_e/atom_rmse: 0.011640
2025-06-25 04:18:14.507 INFO: val_e/atom_rmse: 0.011640
val_f_mae: 0.053924
2025-06-25 04:18:14.508 INFO: val_f_mae: 0.053924
val_f_rmse: 0.085530
2025-06-25 04:18:14.508 INFO: val_f_rmse: 0.085530
##### Step: 9 Learning rate: 0.01 #####
2025-06-25 04:18:44.709 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 7.7009, Val Loss: 8.9805
2025-06-25 04:18:44.709 INFO: Epoch 10, Train Loss: 7.7009, Val Loss: 8.9805
train_e/atom_mae: 0.008645
2025-06-25 04:18:44.710 INFO: train_e/atom_mae: 0.008645
train_e/atom_rmse: 0.010859
2025-06-25 04:18:44.710 INFO: train_e/atom_rmse: 0.010859
train_f_mae: 0.054849
2025-06-25 04:18:44.714 INFO: train_f_mae: 0.054849
train_f_rmse: 0.086938
2025-06-25 04:18:44.714 INFO: train_f_rmse: 0.086938
val_e/atom_mae: 0.010346
2025-06-25 04:18:44.717 INFO: val_e/atom_mae: 0.010346
val_e/atom_rmse: 0.010691
2025-06-25 04:18:44.717 INFO: val_e/atom_rmse: 0.010691
val_f_mae: 0.059140
2025-06-25 04:18:44.718 INFO: val_f_mae: 0.059140
val_f_rmse: 0.094033
2025-06-25 04:18:44.718 INFO: val_f_rmse: 0.094033
##### Step: 10 Learning rate: 0.01 #####
2025-06-25 04:19:14.974 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 7.4581, Val Loss: 8.1558
2025-06-25 04:19:14.974 INFO: Epoch 11, Train Loss: 7.4581, Val Loss: 8.1558
train_e/atom_mae: 0.009539
2025-06-25 04:19:14.975 INFO: train_e/atom_mae: 0.009539
train_e/atom_rmse: 0.012414
2025-06-25 04:19:14.975 INFO: train_e/atom_rmse: 0.012414
train_f_mae: 0.054118
2025-06-25 04:19:14.979 INFO: train_f_mae: 0.054118
train_f_rmse: 0.085274
2025-06-25 04:19:14.979 INFO: train_f_rmse: 0.085274
val_e/atom_mae: 0.004605
2025-06-25 04:19:14.982 INFO: val_e/atom_mae: 0.004605
val_e/atom_rmse: 0.005243
2025-06-25 04:19:14.982 INFO: val_e/atom_rmse: 0.005243
val_f_mae: 0.059402
2025-06-25 04:19:14.983 INFO: val_f_mae: 0.059402
val_f_rmse: 0.090125
2025-06-25 04:19:14.983 INFO: val_f_rmse: 0.090125
##### Step: 11 Learning rate: 0.01 #####
2025-06-25 04:19:45.322 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 7.2536, Val Loss: 6.9508
2025-06-25 04:19:45.322 INFO: Epoch 12, Train Loss: 7.2536, Val Loss: 6.9508
train_e/atom_mae: 0.010310
2025-06-25 04:19:45.323 INFO: train_e/atom_mae: 0.010310
train_e/atom_rmse: 0.013163
2025-06-25 04:19:45.324 INFO: train_e/atom_rmse: 0.013163
train_f_mae: 0.053008
2025-06-25 04:19:45.327 INFO: train_f_mae: 0.053008
train_f_rmse: 0.083928
2025-06-25 04:19:45.327 INFO: train_f_rmse: 0.083928
val_e/atom_mae: 0.002289
2025-06-25 04:19:45.330 INFO: val_e/atom_mae: 0.002289
val_e/atom_rmse: 0.002894
2025-06-25 04:19:45.330 INFO: val_e/atom_rmse: 0.002894
val_f_mae: 0.054132
2025-06-25 04:19:45.331 INFO: val_f_mae: 0.054132
val_f_rmse: 0.083311
2025-06-25 04:19:45.331 INFO: val_f_rmse: 0.083311
##### Step: 12 Learning rate: 0.01 #####
2025-06-25 04:20:15.566 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 7.0590, Val Loss: 7.0875
2025-06-25 04:20:15.566 INFO: Epoch 13, Train Loss: 7.0590, Val Loss: 7.0875
train_e/atom_mae: 0.008019
2025-06-25 04:20:15.567 INFO: train_e/atom_mae: 0.008019
train_e/atom_rmse: 0.010114
2025-06-25 04:20:15.567 INFO: train_e/atom_rmse: 0.010114
train_f_mae: 0.052564
2025-06-25 04:20:15.571 INFO: train_f_mae: 0.052564
train_f_rmse: 0.083278
2025-06-25 04:20:15.571 INFO: train_f_rmse: 0.083278
val_e/atom_mae: 0.008010
2025-06-25 04:20:15.573 INFO: val_e/atom_mae: 0.008010
val_e/atom_rmse: 0.008342
2025-06-25 04:20:15.574 INFO: val_e/atom_rmse: 0.008342
val_f_mae: 0.054664
2025-06-25 04:20:15.574 INFO: val_f_mae: 0.054664
val_f_rmse: 0.083686
2025-06-25 04:20:15.574 INFO: val_f_rmse: 0.083686
##### Step: 13 Learning rate: 0.01 #####
2025-06-25 04:20:45.947 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 6.9535, Val Loss: 7.1554
2025-06-25 04:20:45.947 INFO: Epoch 14, Train Loss: 6.9535, Val Loss: 7.1554
train_e/atom_mae: 0.008510
2025-06-25 04:20:45.948 INFO: train_e/atom_mae: 0.008510
train_e/atom_rmse: 0.010750
2025-06-25 04:20:45.948 INFO: train_e/atom_rmse: 0.010750
train_f_mae: 0.051914
2025-06-25 04:20:45.952 INFO: train_f_mae: 0.051914
train_f_rmse: 0.082545
2025-06-25 04:20:45.952 INFO: train_f_rmse: 0.082545
val_e/atom_mae: 0.006485
2025-06-25 04:20:45.955 INFO: val_e/atom_mae: 0.006485
val_e/atom_rmse: 0.007825
2025-06-25 04:20:45.955 INFO: val_e/atom_rmse: 0.007825
val_f_mae: 0.054235
2025-06-25 04:20:45.955 INFO: val_f_mae: 0.054235
val_f_rmse: 0.084150
2025-06-25 04:20:45.955 INFO: val_f_rmse: 0.084150
##### Step: 14 Learning rate: 0.01 #####
2025-06-25 04:21:16.182 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 6.5309, Val Loss: 6.8807
2025-06-25 04:21:16.182 INFO: Epoch 15, Train Loss: 6.5309, Val Loss: 6.8807
train_e/atom_mae: 0.008190
2025-06-25 04:21:16.183 INFO: train_e/atom_mae: 0.008190
train_e/atom_rmse: 0.010141
2025-06-25 04:21:16.183 INFO: train_e/atom_rmse: 0.010141
train_f_mae: 0.050338
2025-06-25 04:21:16.186 INFO: train_f_mae: 0.050338
train_f_rmse: 0.080041
2025-06-25 04:21:16.187 INFO: train_f_rmse: 0.080041
val_e/atom_mae: 0.005819
2025-06-25 04:21:16.189 INFO: val_e/atom_mae: 0.005819
val_e/atom_rmse: 0.007339
2025-06-25 04:21:16.190 INFO: val_e/atom_rmse: 0.007339
val_f_mae: 0.054358
2025-06-25 04:21:16.190 INFO: val_f_mae: 0.054358
val_f_rmse: 0.082557
2025-06-25 04:21:16.190 INFO: val_f_rmse: 0.082557
##### Step: 15 Learning rate: 0.01 #####
2025-06-25 04:21:46.514 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 6.6544, Val Loss: 6.0969
2025-06-25 04:21:46.514 INFO: Epoch 16, Train Loss: 6.6544, Val Loss: 6.0969
train_e/atom_mae: 0.008982
2025-06-25 04:21:46.515 INFO: train_e/atom_mae: 0.008982
train_e/atom_rmse: 0.011676
2025-06-25 04:21:46.515 INFO: train_e/atom_rmse: 0.011676
train_f_mae: 0.050509
2025-06-25 04:21:46.519 INFO: train_f_mae: 0.050509
train_f_rmse: 0.080557
2025-06-25 04:21:46.519 INFO: train_f_rmse: 0.080557
val_e/atom_mae: 0.009059
2025-06-25 04:21:46.521 INFO: val_e/atom_mae: 0.009059
val_e/atom_rmse: 0.009364
2025-06-25 04:21:46.522 INFO: val_e/atom_rmse: 0.009364
val_f_mae: 0.049252
2025-06-25 04:21:46.522 INFO: val_f_mae: 0.049252
val_f_rmse: 0.077400
2025-06-25 04:21:46.522 INFO: val_f_rmse: 0.077400
##### Step: 16 Learning rate: 0.01 #####
2025-06-25 04:22:16.774 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 6.5798, Val Loss: 6.4472
2025-06-25 04:22:16.774 INFO: Epoch 17, Train Loss: 6.5798, Val Loss: 6.4472
train_e/atom_mae: 0.008097
2025-06-25 04:22:16.775 INFO: train_e/atom_mae: 0.008097
train_e/atom_rmse: 0.010568
2025-06-25 04:22:16.775 INFO: train_e/atom_rmse: 0.010568
train_f_mae: 0.050544
2025-06-25 04:22:16.779 INFO: train_f_mae: 0.050544
train_f_rmse: 0.080279
2025-06-25 04:22:16.779 INFO: train_f_rmse: 0.080279
val_e/atom_mae: 0.003773
2025-06-25 04:22:16.782 INFO: val_e/atom_mae: 0.003773
val_e/atom_rmse: 0.004381
2025-06-25 04:22:16.782 INFO: val_e/atom_rmse: 0.004381
val_f_mae: 0.050852
2025-06-25 04:22:16.782 INFO: val_f_mae: 0.050852
val_f_rmse: 0.080150
2025-06-25 04:22:16.783 INFO: val_f_rmse: 0.080150
##### Step: 17 Learning rate: 0.01 #####
2025-06-25 04:22:46.909 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 6.4100, Val Loss: 5.9219
2025-06-25 04:22:46.909 INFO: Epoch 18, Train Loss: 6.4100, Val Loss: 5.9219
train_e/atom_mae: 0.011257
2025-06-25 04:22:46.910 INFO: train_e/atom_mae: 0.011257
train_e/atom_rmse: 0.013625
2025-06-25 04:22:46.910 INFO: train_e/atom_rmse: 0.013625
train_f_mae: 0.049279
2025-06-25 04:22:46.914 INFO: train_f_mae: 0.049279
train_f_rmse: 0.078647
2025-06-25 04:22:46.914 INFO: train_f_rmse: 0.078647
val_e/atom_mae: 0.005593
2025-06-25 04:22:46.917 INFO: val_e/atom_mae: 0.005593
val_e/atom_rmse: 0.006578
2025-06-25 04:22:46.917 INFO: val_e/atom_rmse: 0.006578
val_f_mae: 0.048651
2025-06-25 04:22:46.918 INFO: val_f_mae: 0.048651
val_f_rmse: 0.076613
2025-06-25 04:22:46.918 INFO: val_f_rmse: 0.076613
##### Step: 18 Learning rate: 0.01 #####
2025-06-25 04:23:17.203 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 6.3279, Val Loss: 6.6897
2025-06-25 04:23:17.204 INFO: Epoch 19, Train Loss: 6.3279, Val Loss: 6.6897
train_e/atom_mae: 0.006342
2025-06-25 04:23:17.205 INFO: train_e/atom_mae: 0.006342
train_e/atom_rmse: 0.008042
2025-06-25 04:23:17.205 INFO: train_e/atom_rmse: 0.008042
train_f_mae: 0.049247
2025-06-25 04:23:17.208 INFO: train_f_mae: 0.049247
train_f_rmse: 0.079054
2025-06-25 04:23:17.208 INFO: train_f_rmse: 0.079054
val_e/atom_mae: 0.009485
2025-06-25 04:23:17.211 INFO: val_e/atom_mae: 0.009485
val_e/atom_rmse: 0.012230
2025-06-25 04:23:17.211 INFO: val_e/atom_rmse: 0.012230
val_f_mae: 0.051103
2025-06-25 04:23:17.212 INFO: val_f_mae: 0.051103
val_f_rmse: 0.080677
2025-06-25 04:23:17.212 INFO: val_f_rmse: 0.080677
##### Step: 19 Learning rate: 0.01 #####
2025-06-25 04:23:47.376 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 6.2835, Val Loss: 6.6703
2025-06-25 04:23:47.376 INFO: Epoch 20, Train Loss: 6.2835, Val Loss: 6.6703
train_e/atom_mae: 0.009436
2025-06-25 04:23:47.377 INFO: train_e/atom_mae: 0.009436
train_e/atom_rmse: 0.011837
2025-06-25 04:23:47.377 INFO: train_e/atom_rmse: 0.011837
train_f_mae: 0.049161
2025-06-25 04:23:47.380 INFO: train_f_mae: 0.049161
train_f_rmse: 0.078192
2025-06-25 04:23:47.381 INFO: train_f_rmse: 0.078192
val_e/atom_mae: 0.005388
2025-06-25 04:23:47.383 INFO: val_e/atom_mae: 0.005388
val_e/atom_rmse: 0.006192
2025-06-25 04:23:47.384 INFO: val_e/atom_rmse: 0.006192
val_f_mae: 0.051664
2025-06-25 04:23:47.384 INFO: val_f_mae: 0.051664
val_f_rmse: 0.081387
2025-06-25 04:23:47.384 INFO: val_f_rmse: 0.081387
##### Step: 20 Learning rate: 0.005 #####
2025-06-25 04:24:17.747 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 5.7697, Val Loss: 6.2660
2025-06-25 04:24:17.747 INFO: Epoch 21, Train Loss: 5.7697, Val Loss: 6.2660
train_e/atom_mae: 0.007588
2025-06-25 04:24:17.748 INFO: train_e/atom_mae: 0.007588
train_e/atom_rmse: 0.009303
2025-06-25 04:24:17.748 INFO: train_e/atom_rmse: 0.009303
train_f_mae: 0.047098
2025-06-25 04:24:17.752 INFO: train_f_mae: 0.047098
train_f_rmse: 0.075266
2025-06-25 04:24:17.752 INFO: train_f_rmse: 0.075266
val_e/atom_mae: 0.005366
2025-06-25 04:24:17.755 INFO: val_e/atom_mae: 0.005366
val_e/atom_rmse: 0.006686
2025-06-25 04:24:17.755 INFO: val_e/atom_rmse: 0.006686
val_f_mae: 0.049503
2025-06-25 04:24:17.755 INFO: val_f_mae: 0.049503
val_f_rmse: 0.078816
2025-06-25 04:24:17.755 INFO: val_f_rmse: 0.078816
##### Step: 21 Learning rate: 0.005 #####
2025-06-25 04:24:47.952 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 5.5934, Val Loss: 5.7018
2025-06-25 04:24:47.952 INFO: Epoch 22, Train Loss: 5.5934, Val Loss: 5.7018
train_e/atom_mae: 0.005314
2025-06-25 04:24:47.953 INFO: train_e/atom_mae: 0.005314
train_e/atom_rmse: 0.006699
2025-06-25 04:24:47.953 INFO: train_e/atom_rmse: 0.006699
train_f_mae: 0.046730
2025-06-25 04:24:47.957 INFO: train_f_mae: 0.046730
train_f_rmse: 0.074425
2025-06-25 04:24:47.957 INFO: train_f_rmse: 0.074425
val_e/atom_mae: 0.009782
2025-06-25 04:24:47.960 INFO: val_e/atom_mae: 0.009782
val_e/atom_rmse: 0.011899
2025-06-25 04:24:47.960 INFO: val_e/atom_rmse: 0.011899
val_f_mae: 0.046747
2025-06-25 04:24:47.961 INFO: val_f_mae: 0.046747
val_f_rmse: 0.074367
2025-06-25 04:24:47.961 INFO: val_f_rmse: 0.074367
##### Step: 22 Learning rate: 0.005 #####
2025-06-25 04:25:18.220 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 5.5131, Val Loss: 5.5839
2025-06-25 04:25:18.221 INFO: Epoch 23, Train Loss: 5.5131, Val Loss: 5.5839
train_e/atom_mae: 0.005919
2025-06-25 04:25:18.221 INFO: train_e/atom_mae: 0.005919
train_e/atom_rmse: 0.007298
2025-06-25 04:25:18.222 INFO: train_e/atom_rmse: 0.007298
train_f_mae: 0.046118
2025-06-25 04:25:18.225 INFO: train_f_mae: 0.046118
train_f_rmse: 0.073815
2025-06-25 04:25:18.225 INFO: train_f_rmse: 0.073815
val_e/atom_mae: 0.009276
2025-06-25 04:25:18.228 INFO: val_e/atom_mae: 0.009276
val_e/atom_rmse: 0.009717
2025-06-25 04:25:18.228 INFO: val_e/atom_rmse: 0.009717
val_f_mae: 0.046407
2025-06-25 04:25:18.229 INFO: val_f_mae: 0.046407
val_f_rmse: 0.073957
2025-06-25 04:25:18.229 INFO: val_f_rmse: 0.073957
##### Step: 23 Learning rate: 0.005 #####
2025-06-25 04:25:48.524 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 5.3974, Val Loss: 5.7656
2025-06-25 04:25:48.524 INFO: Epoch 24, Train Loss: 5.3974, Val Loss: 5.7656
train_e/atom_mae: 0.005982
2025-06-25 04:25:48.525 INFO: train_e/atom_mae: 0.005982
train_e/atom_rmse: 0.007403
2025-06-25 04:25:48.525 INFO: train_e/atom_rmse: 0.007403
train_f_mae: 0.045702
2025-06-25 04:25:48.529 INFO: train_f_mae: 0.045702
train_f_rmse: 0.073014
2025-06-25 04:25:48.529 INFO: train_f_rmse: 0.073014
val_e/atom_mae: 0.009178
2025-06-25 04:25:48.531 INFO: val_e/atom_mae: 0.009178
val_e/atom_rmse: 0.009988
2025-06-25 04:25:48.532 INFO: val_e/atom_rmse: 0.009988
val_f_mae: 0.047625
2025-06-25 04:25:48.532 INFO: val_f_mae: 0.047625
val_f_rmse: 0.075132
2025-06-25 04:25:48.532 INFO: val_f_rmse: 0.075132
##### Step: 24 Learning rate: 0.005 #####
2025-06-25 04:26:18.740 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 5.4772, Val Loss: 6.0446
2025-06-25 04:26:18.741 INFO: Epoch 25, Train Loss: 5.4772, Val Loss: 6.0446
train_e/atom_mae: 0.006377
2025-06-25 04:26:18.742 INFO: train_e/atom_mae: 0.006377
train_e/atom_rmse: 0.007994
2025-06-25 04:26:18.742 INFO: train_e/atom_rmse: 0.007994
train_f_mae: 0.046106
2025-06-25 04:26:18.745 INFO: train_f_mae: 0.046106
train_f_rmse: 0.073484
2025-06-25 04:26:18.746 INFO: train_f_rmse: 0.073484
val_e/atom_mae: 0.009177
2025-06-25 04:26:18.748 INFO: val_e/atom_mae: 0.009177
val_e/atom_rmse: 0.009582
2025-06-25 04:26:18.749 INFO: val_e/atom_rmse: 0.009582
val_f_mae: 0.050546
2025-06-25 04:26:18.749 INFO: val_f_mae: 0.050546
val_f_rmse: 0.077029
2025-06-25 04:26:18.749 INFO: val_f_rmse: 0.077029
##### Step: 25 Learning rate: 0.005 #####
2025-06-25 04:26:49.118 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 5.5137, Val Loss: 5.3725
2025-06-25 04:26:49.118 INFO: Epoch 26, Train Loss: 5.5137, Val Loss: 5.3725
train_e/atom_mae: 0.006088
2025-06-25 04:26:49.119 INFO: train_e/atom_mae: 0.006088
train_e/atom_rmse: 0.007556
2025-06-25 04:26:49.119 INFO: train_e/atom_rmse: 0.007556
train_f_mae: 0.045949
2025-06-25 04:26:49.123 INFO: train_f_mae: 0.045949
train_f_rmse: 0.073788
2025-06-25 04:26:49.123 INFO: train_f_rmse: 0.073788
val_e/atom_mae: 0.007159
2025-06-25 04:26:49.126 INFO: val_e/atom_mae: 0.007159
val_e/atom_rmse: 0.007564
2025-06-25 04:26:49.126 INFO: val_e/atom_rmse: 0.007564
val_f_mae: 0.045880
2025-06-25 04:26:49.127 INFO: val_f_mae: 0.045880
val_f_rmse: 0.072823
2025-06-25 04:26:49.127 INFO: val_f_rmse: 0.072823
##### Step: 26 Learning rate: 0.005 #####
2025-06-25 04:27:19.322 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 5.4595, Val Loss: 5.9335
2025-06-25 04:27:19.322 INFO: Epoch 27, Train Loss: 5.4595, Val Loss: 5.9335
train_e/atom_mae: 0.004939
2025-06-25 04:27:19.323 INFO: train_e/atom_mae: 0.004939
train_e/atom_rmse: 0.006270
2025-06-25 04:27:19.323 INFO: train_e/atom_rmse: 0.006270
train_f_mae: 0.045988
2025-06-25 04:27:19.327 INFO: train_f_mae: 0.045988
train_f_rmse: 0.073566
2025-06-25 04:27:19.327 INFO: train_f_rmse: 0.073566
val_e/atom_mae: 0.002545
2025-06-25 04:27:19.329 INFO: val_e/atom_mae: 0.002545
val_e/atom_rmse: 0.002830
2025-06-25 04:27:19.330 INFO: val_e/atom_rmse: 0.002830
val_f_mae: 0.048960
2025-06-25 04:27:19.330 INFO: val_f_mae: 0.048960
val_f_rmse: 0.076966
2025-06-25 04:27:19.330 INFO: val_f_rmse: 0.076966
##### Step: 27 Learning rate: 0.005 #####
2025-06-25 04:27:49.679 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 5.4176, Val Loss: 5.6866
2025-06-25 04:27:49.679 INFO: Epoch 28, Train Loss: 5.4176, Val Loss: 5.6866
train_e/atom_mae: 0.005055
2025-06-25 04:27:49.680 INFO: train_e/atom_mae: 0.005055
train_e/atom_rmse: 0.006365
2025-06-25 04:27:49.680 INFO: train_e/atom_rmse: 0.006365
train_f_mae: 0.045897
2025-06-25 04:27:49.684 INFO: train_f_mae: 0.045897
train_f_rmse: 0.073271
2025-06-25 04:27:49.684 INFO: train_f_rmse: 0.073271
val_e/atom_mae: 0.002308
2025-06-25 04:27:49.687 INFO: val_e/atom_mae: 0.002308
val_e/atom_rmse: 0.002915
2025-06-25 04:27:49.687 INFO: val_e/atom_rmse: 0.002915
val_f_mae: 0.048015
2025-06-25 04:27:49.688 INFO: val_f_mae: 0.048015
val_f_rmse: 0.075341
2025-06-25 04:27:49.688 INFO: val_f_rmse: 0.075341
##### Step: 28 Learning rate: 0.005 #####
2025-06-25 04:28:19.921 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 5.4257, Val Loss: 5.5300
2025-06-25 04:28:19.922 INFO: Epoch 29, Train Loss: 5.4257, Val Loss: 5.5300
train_e/atom_mae: 0.005389
2025-06-25 04:28:19.923 INFO: train_e/atom_mae: 0.005389
train_e/atom_rmse: 0.007212
2025-06-25 04:28:19.923 INFO: train_e/atom_rmse: 0.007212
train_f_mae: 0.046093
2025-06-25 04:28:19.926 INFO: train_f_mae: 0.046093
train_f_rmse: 0.073231
2025-06-25 04:28:19.927 INFO: train_f_rmse: 0.073231
val_e/atom_mae: 0.004370
2025-06-25 04:28:19.929 INFO: val_e/atom_mae: 0.004370
val_e/atom_rmse: 0.005486
2025-06-25 04:28:19.930 INFO: val_e/atom_rmse: 0.005486
val_f_mae: 0.047003
2025-06-25 04:28:19.930 INFO: val_f_mae: 0.047003
val_f_rmse: 0.074119
2025-06-25 04:28:19.930 INFO: val_f_rmse: 0.074119
##### Step: 29 Learning rate: 0.005 #####
2025-06-25 04:28:50.217 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 5.3522, Val Loss: 5.1494
2025-06-25 04:28:50.217 INFO: Epoch 30, Train Loss: 5.3522, Val Loss: 5.1494
train_e/atom_mae: 0.008781
2025-06-25 04:28:50.218 INFO: train_e/atom_mae: 0.008781
train_e/atom_rmse: 0.011448
2025-06-25 04:28:50.218 INFO: train_e/atom_rmse: 0.011448
train_f_mae: 0.044977
2025-06-25 04:28:50.222 INFO: train_f_mae: 0.044977
train_f_rmse: 0.072067
2025-06-25 04:28:50.222 INFO: train_f_rmse: 0.072067
val_e/atom_mae: 0.002277
2025-06-25 04:28:50.225 INFO: val_e/atom_mae: 0.002277
val_e/atom_rmse: 0.002760
2025-06-25 04:28:50.225 INFO: val_e/atom_rmse: 0.002760
val_f_mae: 0.045622
2025-06-25 04:28:50.225 INFO: val_f_mae: 0.045622
val_f_rmse: 0.071695
2025-06-25 04:28:50.226 INFO: val_f_rmse: 0.071695
##### Step: 30 Learning rate: 0.005 #####
2025-06-25 04:29:20.566 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 5.4818, Val Loss: 5.2769
2025-06-25 04:29:20.566 INFO: Epoch 31, Train Loss: 5.4818, Val Loss: 5.2769
train_e/atom_mae: 0.005884
2025-06-25 04:29:20.567 INFO: train_e/atom_mae: 0.005884
train_e/atom_rmse: 0.007471
2025-06-25 04:29:20.567 INFO: train_e/atom_rmse: 0.007471
train_f_mae: 0.045881
2025-06-25 04:29:20.571 INFO: train_f_mae: 0.045881
train_f_rmse: 0.073582
2025-06-25 04:29:20.571 INFO: train_f_rmse: 0.073582
val_e/atom_mae: 0.004315
2025-06-25 04:29:20.574 INFO: val_e/atom_mae: 0.004315
val_e/atom_rmse: 0.005055
2025-06-25 04:29:20.574 INFO: val_e/atom_rmse: 0.005055
val_f_mae: 0.045607
2025-06-25 04:29:20.575 INFO: val_f_mae: 0.045607
val_f_rmse: 0.072429
2025-06-25 04:29:20.575 INFO: val_f_rmse: 0.072429
##### Step: 31 Learning rate: 0.005 #####
2025-06-25 04:29:50.815 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 5.3619, Val Loss: 5.1985
2025-06-25 04:29:50.815 INFO: Epoch 32, Train Loss: 5.3619, Val Loss: 5.1985
train_e/atom_mae: 0.006578
2025-06-25 04:29:50.816 INFO: train_e/atom_mae: 0.006578
train_e/atom_rmse: 0.008062
2025-06-25 04:29:50.817 INFO: train_e/atom_rmse: 0.008062
train_f_mae: 0.045381
2025-06-25 04:29:50.820 INFO: train_f_mae: 0.045381
train_f_rmse: 0.072686
2025-06-25 04:29:50.820 INFO: train_f_rmse: 0.072686
val_e/atom_mae: 0.003844
2025-06-25 04:29:50.823 INFO: val_e/atom_mae: 0.003844
val_e/atom_rmse: 0.004281
2025-06-25 04:29:50.823 INFO: val_e/atom_rmse: 0.004281
val_f_mae: 0.045495
2025-06-25 04:29:50.824 INFO: val_f_mae: 0.045495
val_f_rmse: 0.071947
2025-06-25 04:29:50.824 INFO: val_f_rmse: 0.071947
##### Step: 32 Learning rate: 0.005 #####
2025-06-25 04:30:21.180 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 5.2204, Val Loss: 5.6411
2025-06-25 04:30:21.180 INFO: Epoch 33, Train Loss: 5.2204, Val Loss: 5.6411
train_e/atom_mae: 0.005380
2025-06-25 04:30:21.181 INFO: train_e/atom_mae: 0.005380
train_e/atom_rmse: 0.006773
2025-06-25 04:30:21.181 INFO: train_e/atom_rmse: 0.006773
train_f_mae: 0.044893
2025-06-25 04:30:21.185 INFO: train_f_mae: 0.044893
train_f_rmse: 0.071867
2025-06-25 04:30:21.185 INFO: train_f_rmse: 0.071867
val_e/atom_mae: 0.002397
2025-06-25 04:30:21.188 INFO: val_e/atom_mae: 0.002397
val_e/atom_rmse: 0.003328
2025-06-25 04:30:21.188 INFO: val_e/atom_rmse: 0.003328
val_f_mae: 0.048621
2025-06-25 04:30:21.188 INFO: val_f_mae: 0.048621
val_f_rmse: 0.075018
2025-06-25 04:30:21.189 INFO: val_f_rmse: 0.075018
##### Step: 33 Learning rate: 0.005 #####
2025-06-25 04:30:51.409 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 5.3872, Val Loss: 6.0886
2025-06-25 04:30:51.410 INFO: Epoch 34, Train Loss: 5.3872, Val Loss: 6.0886
train_e/atom_mae: 0.007178
2025-06-25 04:30:51.411 INFO: train_e/atom_mae: 0.007178
train_e/atom_rmse: 0.008942
2025-06-25 04:30:51.411 INFO: train_e/atom_rmse: 0.008942
train_f_mae: 0.045548
2025-06-25 04:30:51.414 INFO: train_f_mae: 0.045548
train_f_rmse: 0.072736
2025-06-25 04:30:51.414 INFO: train_f_rmse: 0.072736
val_e/atom_mae: 0.009289
2025-06-25 04:30:51.417 INFO: val_e/atom_mae: 0.009289
val_e/atom_rmse: 0.010202
2025-06-25 04:30:51.418 INFO: val_e/atom_rmse: 0.010202
val_f_mae: 0.047934
2025-06-25 04:30:51.418 INFO: val_f_mae: 0.047934
val_f_rmse: 0.077218
2025-06-25 04:30:51.418 INFO: val_f_rmse: 0.077218
##### Step: 34 Learning rate: 0.005 #####
2025-06-25 04:31:21.773 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 5.2629, Val Loss: 5.1013
2025-06-25 04:31:21.774 INFO: Epoch 35, Train Loss: 5.2629, Val Loss: 5.1013
train_e/atom_mae: 0.006642
2025-06-25 04:31:21.774 INFO: train_e/atom_mae: 0.006642
train_e/atom_rmse: 0.008192
2025-06-25 04:31:21.775 INFO: train_e/atom_rmse: 0.008192
train_f_mae: 0.044897
2025-06-25 04:31:21.778 INFO: train_f_mae: 0.044897
train_f_rmse: 0.071984
2025-06-25 04:31:21.778 INFO: train_f_rmse: 0.071984
val_e/atom_mae: 0.003877
2025-06-25 04:31:21.781 INFO: val_e/atom_mae: 0.003877
val_e/atom_rmse: 0.004709
2025-06-25 04:31:21.781 INFO: val_e/atom_rmse: 0.004709
val_f_mae: 0.044953
2025-06-25 04:31:21.782 INFO: val_f_mae: 0.044953
val_f_rmse: 0.071236
2025-06-25 04:31:21.782 INFO: val_f_rmse: 0.071236
##### Step: 35 Learning rate: 0.005 #####
2025-06-25 04:31:52.086 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 5.2261, Val Loss: 5.5711
2025-06-25 04:31:52.086 INFO: Epoch 36, Train Loss: 5.2261, Val Loss: 5.5711
train_e/atom_mae: 0.006483
2025-06-25 04:31:52.087 INFO: train_e/atom_mae: 0.006483
train_e/atom_rmse: 0.008066
2025-06-25 04:31:52.087 INFO: train_e/atom_rmse: 0.008066
train_f_mae: 0.044883
2025-06-25 04:31:52.091 INFO: train_f_mae: 0.044883
train_f_rmse: 0.071746
2025-06-25 04:31:52.091 INFO: train_f_rmse: 0.071746
val_e/atom_mae: 0.004501
2025-06-25 04:31:52.094 INFO: val_e/atom_mae: 0.004501
val_e/atom_rmse: 0.005206
2025-06-25 04:31:52.094 INFO: val_e/atom_rmse: 0.005206
val_f_mae: 0.047092
2025-06-25 04:31:52.094 INFO: val_f_mae: 0.047092
val_f_rmse: 0.074420
2025-06-25 04:31:52.094 INFO: val_f_rmse: 0.074420
##### Step: 36 Learning rate: 0.005 #####
2025-06-25 04:32:22.362 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 5.1177, Val Loss: 5.0490
2025-06-25 04:32:22.362 INFO: Epoch 37, Train Loss: 5.1177, Val Loss: 5.0490
train_e/atom_mae: 0.005302
2025-06-25 04:32:22.363 INFO: train_e/atom_mae: 0.005302
train_e/atom_rmse: 0.006527
2025-06-25 04:32:22.363 INFO: train_e/atom_rmse: 0.006527
train_f_mae: 0.044444
2025-06-25 04:32:22.367 INFO: train_f_mae: 0.044444
train_f_rmse: 0.071177
2025-06-25 04:32:22.367 INFO: train_f_rmse: 0.071177
val_e/atom_mae: 0.007768
2025-06-25 04:32:22.370 INFO: val_e/atom_mae: 0.007768
val_e/atom_rmse: 0.008177
2025-06-25 04:32:22.370 INFO: val_e/atom_rmse: 0.008177
val_f_mae: 0.045170
2025-06-25 04:32:22.371 INFO: val_f_mae: 0.045170
val_f_rmse: 0.070484
2025-06-25 04:32:22.371 INFO: val_f_rmse: 0.070484
##### Step: 37 Learning rate: 0.005 #####
2025-06-25 04:32:52.716 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 5.1197, Val Loss: 5.3761
2025-06-25 04:32:52.716 INFO: Epoch 38, Train Loss: 5.1197, Val Loss: 5.3761
train_e/atom_mae: 0.005869
2025-06-25 04:32:52.717 INFO: train_e/atom_mae: 0.005869
train_e/atom_rmse: 0.007167
2025-06-25 04:32:52.718 INFO: train_e/atom_rmse: 0.007167
train_f_mae: 0.044266
2025-06-25 04:32:52.721 INFO: train_f_mae: 0.044266
train_f_rmse: 0.071117
2025-06-25 04:32:52.721 INFO: train_f_rmse: 0.071117
val_e/atom_mae: 0.007079
2025-06-25 04:32:52.724 INFO: val_e/atom_mae: 0.007079
val_e/atom_rmse: 0.008641
2025-06-25 04:32:52.724 INFO: val_e/atom_rmse: 0.008641
val_f_mae: 0.046280
2025-06-25 04:32:52.725 INFO: val_f_mae: 0.046280
val_f_rmse: 0.072703
2025-06-25 04:32:52.725 INFO: val_f_rmse: 0.072703
##### Step: 38 Learning rate: 0.005 #####
2025-06-25 04:33:22.927 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 5.1327, Val Loss: 5.0300
2025-06-25 04:33:22.927 INFO: Epoch 39, Train Loss: 5.1327, Val Loss: 5.0300
train_e/atom_mae: 0.006289
2025-06-25 04:33:22.928 INFO: train_e/atom_mae: 0.006289
train_e/atom_rmse: 0.007745
2025-06-25 04:33:22.928 INFO: train_e/atom_rmse: 0.007745
train_f_mae: 0.044389
2025-06-25 04:33:22.932 INFO: train_f_mae: 0.044389
train_f_rmse: 0.071135
2025-06-25 04:33:22.932 INFO: train_f_rmse: 0.071135
val_e/atom_mae: 0.003101
2025-06-25 04:33:22.935 INFO: val_e/atom_mae: 0.003101
val_e/atom_rmse: 0.004208
2025-06-25 04:33:22.935 INFO: val_e/atom_rmse: 0.004208
val_f_mae: 0.044758
2025-06-25 04:33:22.935 INFO: val_f_mae: 0.044758
val_f_rmse: 0.070772
2025-06-25 04:33:22.935 INFO: val_f_rmse: 0.070772
##### Step: 39 Learning rate: 0.005 #####
2025-06-25 04:33:53.281 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 5.0877, Val Loss: 5.1217
2025-06-25 04:33:53.281 INFO: Epoch 40, Train Loss: 5.0877, Val Loss: 5.1217
train_e/atom_mae: 0.006073
2025-06-25 04:33:53.282 INFO: train_e/atom_mae: 0.006073
train_e/atom_rmse: 0.007381
2025-06-25 04:33:53.282 INFO: train_e/atom_rmse: 0.007381
train_f_mae: 0.044274
2025-06-25 04:33:53.286 INFO: train_f_mae: 0.044274
train_f_rmse: 0.070865
2025-06-25 04:33:53.286 INFO: train_f_rmse: 0.070865
val_e/atom_mae: 0.007718
2025-06-25 04:33:53.288 INFO: val_e/atom_mae: 0.007718
val_e/atom_rmse: 0.009638
2025-06-25 04:33:53.289 INFO: val_e/atom_rmse: 0.009638
val_f_mae: 0.044429
2025-06-25 04:33:53.289 INFO: val_f_mae: 0.044429
val_f_rmse: 0.070776
2025-06-25 04:33:53.289 INFO: val_f_rmse: 0.070776
2025-06-25 04:33:53.293 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-25 04:34:23.542 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 15.5543, Val Loss: 7.2889
2025-06-25 04:34:23.542 INFO: Epoch 1, Train Loss: 15.5543, Val Loss: 7.2889
train_e/atom_mae: 0.022242
2025-06-25 04:34:23.543 INFO: train_e/atom_mae: 0.022242
train_e/atom_rmse: 0.043082
2025-06-25 04:34:23.543 INFO: train_e/atom_rmse: 0.043082
train_f_mae: 0.063338
2025-06-25 04:34:23.547 INFO: train_f_mae: 0.063338
train_f_rmse: 0.115362
2025-06-25 04:34:23.547 INFO: train_f_rmse: 0.115362
val_e/atom_mae: 0.007286
2025-06-25 04:34:23.550 INFO: val_e/atom_mae: 0.007286
val_e/atom_rmse: 0.008826
2025-06-25 04:34:23.550 INFO: val_e/atom_rmse: 0.008826
val_f_mae: 0.055821
2025-06-25 04:34:23.550 INFO: val_f_mae: 0.055821
val_f_rmse: 0.084821
2025-06-25 04:34:23.551 INFO: val_f_rmse: 0.084821
##### Step: 1 Learning rate: 0.004 #####
2025-06-25 04:34:53.877 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 7.4587, Val Loss: 5.5059
2025-06-25 04:34:53.877 INFO: Epoch 2, Train Loss: 7.4587, Val Loss: 5.5059
train_e/atom_mae: 0.015074
2025-06-25 04:34:53.878 INFO: train_e/atom_mae: 0.015074
train_e/atom_rmse: 0.018257
2025-06-25 04:34:53.878 INFO: train_e/atom_rmse: 0.018257
train_f_mae: 0.051632
2025-06-25 04:34:53.882 INFO: train_f_mae: 0.051632
train_f_rmse: 0.083997
2025-06-25 04:34:53.882 INFO: train_f_rmse: 0.083997
val_e/atom_mae: 0.014795
2025-06-25 04:34:53.884 INFO: val_e/atom_mae: 0.014795
val_e/atom_rmse: 0.015671
2025-06-25 04:34:53.885 INFO: val_e/atom_rmse: 0.015671
val_f_mae: 0.044586
2025-06-25 04:34:53.885 INFO: val_f_mae: 0.044586
val_f_rmse: 0.072172
2025-06-25 04:34:53.885 INFO: val_f_rmse: 0.072172
##### Step: 2 Learning rate: 0.006 #####
2025-06-25 04:35:24.252 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 6.4304, Val Loss: 5.5027
2025-06-25 04:35:24.253 INFO: Epoch 3, Train Loss: 6.4304, Val Loss: 5.5027
train_e/atom_mae: 0.011328
2025-06-25 04:35:24.253 INFO: train_e/atom_mae: 0.011328
train_e/atom_rmse: 0.014572
2025-06-25 04:35:24.254 INFO: train_e/atom_rmse: 0.014572
train_f_mae: 0.048536
2025-06-25 04:35:24.257 INFO: train_f_mae: 0.048536
train_f_rmse: 0.078571
2025-06-25 04:35:24.257 INFO: train_f_rmse: 0.078571
val_e/atom_mae: 0.004238
2025-06-25 04:35:24.260 INFO: val_e/atom_mae: 0.004238
val_e/atom_rmse: 0.004480
2025-06-25 04:35:24.260 INFO: val_e/atom_rmse: 0.004480
val_f_mae: 0.046743
2025-06-25 04:35:24.261 INFO: val_f_mae: 0.046743
val_f_rmse: 0.074016
2025-06-25 04:35:24.261 INFO: val_f_rmse: 0.074016
##### Step: 3 Learning rate: 0.008 #####
2025-06-25 04:35:54.519 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 6.3163, Val Loss: 6.2393
2025-06-25 04:35:54.520 INFO: Epoch 4, Train Loss: 6.3163, Val Loss: 6.2393
train_e/atom_mae: 0.013818
2025-06-25 04:35:54.520 INFO: train_e/atom_mae: 0.013818
train_e/atom_rmse: 0.017551
2025-06-25 04:35:54.521 INFO: train_e/atom_rmse: 0.017551
train_f_mae: 0.047631
2025-06-25 04:35:54.524 INFO: train_f_mae: 0.047631
train_f_rmse: 0.077095
2025-06-25 04:35:54.524 INFO: train_f_rmse: 0.077095
val_e/atom_mae: 0.018889
2025-06-25 04:35:54.527 INFO: val_e/atom_mae: 0.018889
val_e/atom_rmse: 0.020657
2025-06-25 04:35:54.527 INFO: val_e/atom_rmse: 0.020657
val_f_mae: 0.046123
2025-06-25 04:35:54.528 INFO: val_f_mae: 0.046123
val_f_rmse: 0.075651
2025-06-25 04:35:54.528 INFO: val_f_rmse: 0.075651
##### Step: 4 Learning rate: 0.01 #####
2025-06-25 04:36:25.144 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 5.6587, Val Loss: 6.0485
2025-06-25 04:36:25.144 INFO: Epoch 5, Train Loss: 5.6587, Val Loss: 6.0485
train_e/atom_mae: 0.010949
2025-06-25 04:36:25.145 INFO: train_e/atom_mae: 0.010949
train_e/atom_rmse: 0.014022
2025-06-25 04:36:25.145 INFO: train_e/atom_rmse: 0.014022
train_f_mae: 0.045527
2025-06-25 04:36:25.149 INFO: train_f_mae: 0.045527
train_f_rmse: 0.073626
2025-06-25 04:36:25.149 INFO: train_f_rmse: 0.073626
val_e/atom_mae: 0.019380
2025-06-25 04:36:25.152 INFO: val_e/atom_mae: 0.019380
val_e/atom_rmse: 0.019514
2025-06-25 04:36:25.152 INFO: val_e/atom_rmse: 0.019514
val_f_mae: 0.046771
2025-06-25 04:36:25.153 INFO: val_f_mae: 0.046771
val_f_rmse: 0.074751
2025-06-25 04:36:25.153 INFO: val_f_rmse: 0.074751
##### Step: 5 Learning rate: 0.01 #####
2025-06-25 04:36:55.401 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 5.7421, Val Loss: 6.7684
2025-06-25 04:36:55.402 INFO: Epoch 6, Train Loss: 5.7421, Val Loss: 6.7684
train_e/atom_mae: 0.007364
2025-06-25 04:36:55.403 INFO: train_e/atom_mae: 0.007364
train_e/atom_rmse: 0.009284
2025-06-25 04:36:55.403 INFO: train_e/atom_rmse: 0.009284
train_f_mae: 0.046094
2025-06-25 04:36:55.406 INFO: train_f_mae: 0.046094
train_f_rmse: 0.075086
2025-06-25 04:36:55.407 INFO: train_f_rmse: 0.075086
val_e/atom_mae: 0.005253
2025-06-25 04:36:55.409 INFO: val_e/atom_mae: 0.005253
val_e/atom_rmse: 0.005733
2025-06-25 04:36:55.410 INFO: val_e/atom_rmse: 0.005733
val_f_mae: 0.051911
2025-06-25 04:36:55.410 INFO: val_f_mae: 0.051911
val_f_rmse: 0.082028
2025-06-25 04:36:55.410 INFO: val_f_rmse: 0.082028
##### Step: 6 Learning rate: 0.01 #####
2025-06-25 04:37:25.750 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 5.6142, Val Loss: 5.4596
2025-06-25 04:37:25.750 INFO: Epoch 7, Train Loss: 5.6142, Val Loss: 5.4596
train_e/atom_mae: 0.008938
2025-06-25 04:37:25.751 INFO: train_e/atom_mae: 0.008938
train_e/atom_rmse: 0.011677
2025-06-25 04:37:25.751 INFO: train_e/atom_rmse: 0.011677
train_f_mae: 0.045466
2025-06-25 04:37:25.755 INFO: train_f_mae: 0.045466
train_f_rmse: 0.073819
2025-06-25 04:37:25.755 INFO: train_f_rmse: 0.073819
val_e/atom_mae: 0.002312
2025-06-25 04:37:25.757 INFO: val_e/atom_mae: 0.002312
val_e/atom_rmse: 0.002853
2025-06-25 04:37:25.758 INFO: val_e/atom_rmse: 0.002853
val_f_mae: 0.044637
2025-06-25 04:37:25.758 INFO: val_f_mae: 0.044637
val_f_rmse: 0.073822
2025-06-25 04:37:25.758 INFO: val_f_rmse: 0.073822
##### Step: 7 Learning rate: 0.01 #####
2025-06-25 04:37:56.127 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 5.4550, Val Loss: 6.0842
2025-06-25 04:37:56.127 INFO: Epoch 8, Train Loss: 5.4550, Val Loss: 6.0842
train_e/atom_mae: 0.009938
2025-06-25 04:37:56.128 INFO: train_e/atom_mae: 0.009938
train_e/atom_rmse: 0.012683
2025-06-25 04:37:56.128 INFO: train_e/atom_rmse: 0.012683
train_f_mae: 0.044280
2025-06-25 04:37:56.132 INFO: train_f_mae: 0.044280
train_f_rmse: 0.072528
2025-06-25 04:37:56.132 INFO: train_f_rmse: 0.072528
val_e/atom_mae: 0.005273
2025-06-25 04:37:56.134 INFO: val_e/atom_mae: 0.005273
val_e/atom_rmse: 0.006300
2025-06-25 04:37:56.135 INFO: val_e/atom_rmse: 0.006300
val_f_mae: 0.048935
2025-06-25 04:37:56.135 INFO: val_f_mae: 0.048935
val_f_rmse: 0.077693
2025-06-25 04:37:56.135 INFO: val_f_rmse: 0.077693
##### Step: 8 Learning rate: 0.01 #####
2025-06-25 04:38:26.420 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 5.2970, Val Loss: 5.4436
2025-06-25 04:38:26.420 INFO: Epoch 9, Train Loss: 5.2970, Val Loss: 5.4436
train_e/atom_mae: 0.008642
2025-06-25 04:38:26.421 INFO: train_e/atom_mae: 0.008642
train_e/atom_rmse: 0.010890
2025-06-25 04:38:26.421 INFO: train_e/atom_rmse: 0.010890
train_f_mae: 0.043986
2025-06-25 04:38:26.425 INFO: train_f_mae: 0.043986
train_f_rmse: 0.071788
2025-06-25 04:38:26.425 INFO: train_f_rmse: 0.071788
val_e/atom_mae: 0.008411
2025-06-25 04:38:26.428 INFO: val_e/atom_mae: 0.008411
val_e/atom_rmse: 0.009481
2025-06-25 04:38:26.428 INFO: val_e/atom_rmse: 0.009481
val_f_mae: 0.043983
2025-06-25 04:38:26.429 INFO: val_f_mae: 0.043983
val_f_rmse: 0.073040
2025-06-25 04:38:26.429 INFO: val_f_rmse: 0.073040
##### Step: 9 Learning rate: 0.01 #####
2025-06-25 04:38:56.831 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 5.1004, Val Loss: 5.1635
2025-06-25 04:38:56.831 INFO: Epoch 10, Train Loss: 5.1004, Val Loss: 5.1635
train_e/atom_mae: 0.008657
2025-06-25 04:38:56.832 INFO: train_e/atom_mae: 0.008657
train_e/atom_rmse: 0.010791
2025-06-25 04:38:56.832 INFO: train_e/atom_rmse: 0.010791
train_f_mae: 0.043065
2025-06-25 04:38:56.836 INFO: train_f_mae: 0.043065
train_f_rmse: 0.070424
2025-06-25 04:38:56.836 INFO: train_f_rmse: 0.070424
val_e/atom_mae: 0.008891
2025-06-25 04:38:56.839 INFO: val_e/atom_mae: 0.008891
val_e/atom_rmse: 0.010873
2025-06-25 04:38:56.839 INFO: val_e/atom_rmse: 0.010873
val_f_mae: 0.043202
2025-06-25 04:38:56.839 INFO: val_f_mae: 0.043202
val_f_rmse: 0.070855
2025-06-25 04:38:56.840 INFO: val_f_rmse: 0.070855
##### Step: 10 Learning rate: 0.01 #####
2025-06-25 04:39:27.088 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 5.2181, Val Loss: 6.1625
2025-06-25 04:39:27.088 INFO: Epoch 11, Train Loss: 5.2181, Val Loss: 6.1625
train_e/atom_mae: 0.009354
2025-06-25 04:39:27.089 INFO: train_e/atom_mae: 0.009354
train_e/atom_rmse: 0.011549
2025-06-25 04:39:27.089 INFO: train_e/atom_rmse: 0.011549
train_f_mae: 0.043981
2025-06-25 04:39:27.093 INFO: train_f_mae: 0.043981
train_f_rmse: 0.071111
2025-06-25 04:39:27.093 INFO: train_f_rmse: 0.071111
val_e/atom_mae: 0.017812
2025-06-25 04:39:27.095 INFO: val_e/atom_mae: 0.017812
val_e/atom_rmse: 0.017959
2025-06-25 04:39:27.096 INFO: val_e/atom_rmse: 0.017959
val_f_mae: 0.048434
2025-06-25 04:39:27.096 INFO: val_f_mae: 0.048434
val_f_rmse: 0.075975
2025-06-25 04:39:27.096 INFO: val_f_rmse: 0.075975
##### Step: 11 Learning rate: 0.01 #####
2025-06-25 04:39:57.533 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 5.3706, Val Loss: 5.2529
2025-06-25 04:39:57.533 INFO: Epoch 12, Train Loss: 5.3706, Val Loss: 5.2529
train_e/atom_mae: 0.007531
2025-06-25 04:39:57.534 INFO: train_e/atom_mae: 0.007531
train_e/atom_rmse: 0.009387
2025-06-25 04:39:57.534 INFO: train_e/atom_rmse: 0.009387
train_f_mae: 0.044048
2025-06-25 04:39:57.538 INFO: train_f_mae: 0.044048
train_f_rmse: 0.072553
2025-06-25 04:39:57.538 INFO: train_f_rmse: 0.072553
val_e/atom_mae: 0.007312
2025-06-25 04:39:57.540 INFO: val_e/atom_mae: 0.007312
val_e/atom_rmse: 0.008232
2025-06-25 04:39:57.541 INFO: val_e/atom_rmse: 0.008232
val_f_mae: 0.044415
2025-06-25 04:39:57.541 INFO: val_f_mae: 0.044415
val_f_rmse: 0.071909
2025-06-25 04:39:57.541 INFO: val_f_rmse: 0.071909
##### Step: 12 Learning rate: 0.01 #####
2025-06-25 04:40:27.815 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 4.9482, Val Loss: 5.5850
2025-06-25 04:40:27.815 INFO: Epoch 13, Train Loss: 4.9482, Val Loss: 5.5850
train_e/atom_mae: 0.008192
2025-06-25 04:40:27.816 INFO: train_e/atom_mae: 0.008192
train_e/atom_rmse: 0.010227
2025-06-25 04:40:27.816 INFO: train_e/atom_rmse: 0.010227
train_f_mae: 0.042484
2025-06-25 04:40:27.820 INFO: train_f_mae: 0.042484
train_f_rmse: 0.069438
2025-06-25 04:40:27.820 INFO: train_f_rmse: 0.069438
val_e/atom_mae: 0.011313
2025-06-25 04:40:27.823 INFO: val_e/atom_mae: 0.011313
val_e/atom_rmse: 0.011988
2025-06-25 04:40:27.823 INFO: val_e/atom_rmse: 0.011988
val_f_mae: 0.045719
2025-06-25 04:40:27.824 INFO: val_f_mae: 0.045719
val_f_rmse: 0.073561
2025-06-25 04:40:27.824 INFO: val_f_rmse: 0.073561
##### Step: 13 Learning rate: 0.01 #####
2025-06-25 04:40:58.152 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 5.0257, Val Loss: 4.8282
2025-06-25 04:40:58.152 INFO: Epoch 14, Train Loss: 5.0257, Val Loss: 4.8282
train_e/atom_mae: 0.008390
2025-06-25 04:40:58.153 INFO: train_e/atom_mae: 0.008390
train_e/atom_rmse: 0.010784
2025-06-25 04:40:58.153 INFO: train_e/atom_rmse: 0.010784
train_f_mae: 0.043060
2025-06-25 04:40:58.156 INFO: train_f_mae: 0.043060
train_f_rmse: 0.069893
2025-06-25 04:40:58.157 INFO: train_f_rmse: 0.069893
val_e/atom_mae: 0.010518
2025-06-25 04:40:58.159 INFO: val_e/atom_mae: 0.010518
val_e/atom_rmse: 0.011859
2025-06-25 04:40:58.160 INFO: val_e/atom_rmse: 0.011859
val_f_mae: 0.042174
2025-06-25 04:40:58.160 INFO: val_f_mae: 0.042174
val_f_rmse: 0.068249
2025-06-25 04:40:58.160 INFO: val_f_rmse: 0.068249
##### Step: 14 Learning rate: 0.01 #####
2025-06-25 04:41:28.529 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 4.9690, Val Loss: 5.5414
2025-06-25 04:41:28.530 INFO: Epoch 15, Train Loss: 4.9690, Val Loss: 5.5414
train_e/atom_mae: 0.008503
2025-06-25 04:41:28.531 INFO: train_e/atom_mae: 0.008503
train_e/atom_rmse: 0.010480
2025-06-25 04:41:28.531 INFO: train_e/atom_rmse: 0.010480
train_f_mae: 0.042772
2025-06-25 04:41:28.534 INFO: train_f_mae: 0.042772
train_f_rmse: 0.069542
2025-06-25 04:41:28.534 INFO: train_f_rmse: 0.069542
val_e/atom_mae: 0.003748
2025-06-25 04:41:28.537 INFO: val_e/atom_mae: 0.003748
val_e/atom_rmse: 0.004296
2025-06-25 04:41:28.538 INFO: val_e/atom_rmse: 0.004296
val_f_mae: 0.047397
2025-06-25 04:41:28.538 INFO: val_f_mae: 0.047397
val_f_rmse: 0.074290
2025-06-25 04:41:28.538 INFO: val_f_rmse: 0.074290
##### Step: 15 Learning rate: 0.01 #####
2025-06-25 04:41:58.798 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 4.8911, Val Loss: 4.7294
2025-06-25 04:41:58.798 INFO: Epoch 16, Train Loss: 4.8911, Val Loss: 4.7294
train_e/atom_mae: 0.007936
2025-06-25 04:41:58.799 INFO: train_e/atom_mae: 0.007936
train_e/atom_rmse: 0.009918
2025-06-25 04:41:58.799 INFO: train_e/atom_rmse: 0.009918
train_f_mae: 0.042475
2025-06-25 04:41:58.803 INFO: train_f_mae: 0.042475
train_f_rmse: 0.069080
2025-06-25 04:41:58.803 INFO: train_f_rmse: 0.069080
val_e/atom_mae: 0.005262
2025-06-25 04:41:58.805 INFO: val_e/atom_mae: 0.005262
val_e/atom_rmse: 0.006280
2025-06-25 04:41:58.806 INFO: val_e/atom_rmse: 0.006280
val_f_mae: 0.043572
2025-06-25 04:41:58.806 INFO: val_f_mae: 0.043572
val_f_rmse: 0.068423
2025-06-25 04:41:58.806 INFO: val_f_rmse: 0.068423
##### Step: 16 Learning rate: 0.01 #####
2025-06-25 04:42:29.236 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 4.7989, Val Loss: 5.1674
2025-06-25 04:42:29.236 INFO: Epoch 17, Train Loss: 4.7989, Val Loss: 5.1674
train_e/atom_mae: 0.006325
2025-06-25 04:42:29.237 INFO: train_e/atom_mae: 0.006325
train_e/atom_rmse: 0.007851
2025-06-25 04:42:29.237 INFO: train_e/atom_rmse: 0.007851
train_f_mae: 0.041993
2025-06-25 04:42:29.241 INFO: train_f_mae: 0.041993
train_f_rmse: 0.068734
2025-06-25 04:42:29.241 INFO: train_f_rmse: 0.068734
val_e/atom_mae: 0.007960
2025-06-25 04:42:29.244 INFO: val_e/atom_mae: 0.007960
val_e/atom_rmse: 0.008360
2025-06-25 04:42:29.244 INFO: val_e/atom_rmse: 0.008360
val_f_mae: 0.044333
2025-06-25 04:42:29.244 INFO: val_f_mae: 0.044333
val_f_rmse: 0.071294
2025-06-25 04:42:29.244 INFO: val_f_rmse: 0.071294
##### Step: 17 Learning rate: 0.01 #####
2025-06-25 04:42:59.493 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 4.8594, Val Loss: 5.5023
2025-06-25 04:42:59.493 INFO: Epoch 18, Train Loss: 4.8594, Val Loss: 5.5023
train_e/atom_mae: 0.009239
2025-06-25 04:42:59.494 INFO: train_e/atom_mae: 0.009239
train_e/atom_rmse: 0.011463
2025-06-25 04:42:59.494 INFO: train_e/atom_rmse: 0.011463
train_f_mae: 0.042063
2025-06-25 04:42:59.498 INFO: train_f_mae: 0.042063
train_f_rmse: 0.068560
2025-06-25 04:42:59.498 INFO: train_f_rmse: 0.068560
val_e/atom_mae: 0.023218
2025-06-25 04:42:59.501 INFO: val_e/atom_mae: 0.023218
val_e/atom_rmse: 0.023917
2025-06-25 04:42:59.501 INFO: val_e/atom_rmse: 0.023917
val_f_mae: 0.043058
2025-06-25 04:42:59.501 INFO: val_f_mae: 0.043058
val_f_rmse: 0.069355
2025-06-25 04:42:59.502 INFO: val_f_rmse: 0.069355
##### Step: 18 Learning rate: 0.01 #####
2025-06-25 04:43:29.888 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 4.9015, Val Loss: 4.5025
2025-06-25 04:43:29.888 INFO: Epoch 19, Train Loss: 4.9015, Val Loss: 4.5025
train_e/atom_mae: 0.010144
2025-06-25 04:43:29.889 INFO: train_e/atom_mae: 0.010144
train_e/atom_rmse: 0.012368
2025-06-25 04:43:29.889 INFO: train_e/atom_rmse: 0.012368
train_f_mae: 0.042271
2025-06-25 04:43:29.892 INFO: train_f_mae: 0.042271
train_f_rmse: 0.068676
2025-06-25 04:43:29.893 INFO: train_f_rmse: 0.068676
val_e/atom_mae: 0.008563
2025-06-25 04:43:29.895 INFO: val_e/atom_mae: 0.008563
val_e/atom_rmse: 0.009427
2025-06-25 04:43:29.896 INFO: val_e/atom_rmse: 0.009427
val_f_mae: 0.040855
2025-06-25 04:43:29.896 INFO: val_f_mae: 0.040855
val_f_rmse: 0.066294
2025-06-25 04:43:29.896 INFO: val_f_rmse: 0.066294
##### Step: 19 Learning rate: 0.01 #####
2025-06-25 04:44:00.213 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 4.8284, Val Loss: 5.0436
2025-06-25 04:44:00.213 INFO: Epoch 20, Train Loss: 4.8284, Val Loss: 5.0436
train_e/atom_mae: 0.007396
2025-06-25 04:44:00.214 INFO: train_e/atom_mae: 0.007396
train_e/atom_rmse: 0.009213
2025-06-25 04:44:00.214 INFO: train_e/atom_rmse: 0.009213
train_f_mae: 0.041935
2025-06-25 04:44:00.218 INFO: train_f_mae: 0.041935
train_f_rmse: 0.068744
2025-06-25 04:44:00.218 INFO: train_f_rmse: 0.068744
val_e/atom_mae: 0.005792
2025-06-25 04:44:00.220 INFO: val_e/atom_mae: 0.005792
val_e/atom_rmse: 0.006744
2025-06-25 04:44:00.221 INFO: val_e/atom_rmse: 0.006744
val_f_mae: 0.044666
2025-06-25 04:44:00.221 INFO: val_f_mae: 0.044666
val_f_rmse: 0.070630
2025-06-25 04:44:00.221 INFO: val_f_rmse: 0.070630
##### Step: 20 Learning rate: 0.005 #####
2025-06-25 04:44:30.547 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 4.6233, Val Loss: 4.6925
2025-06-25 04:44:30.547 INFO: Epoch 21, Train Loss: 4.6233, Val Loss: 4.6925
train_e/atom_mae: 0.005485
2025-06-25 04:44:30.548 INFO: train_e/atom_mae: 0.005485
train_e/atom_rmse: 0.007028
2025-06-25 04:44:30.549 INFO: train_e/atom_rmse: 0.007028
train_f_mae: 0.040454
2025-06-25 04:44:30.552 INFO: train_f_mae: 0.040454
train_f_rmse: 0.067554
2025-06-25 04:44:30.552 INFO: train_f_rmse: 0.067554
val_e/atom_mae: 0.004455
2025-06-25 04:44:30.555 INFO: val_e/atom_mae: 0.004455
val_e/atom_rmse: 0.005208
2025-06-25 04:44:30.555 INFO: val_e/atom_rmse: 0.005208
val_f_mae: 0.042557
2025-06-25 04:44:30.556 INFO: val_f_mae: 0.042557
val_f_rmse: 0.068262
2025-06-25 04:44:30.556 INFO: val_f_rmse: 0.068262
##### Step: 21 Learning rate: 0.005 #####
2025-06-25 04:45:00.960 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 4.3774, Val Loss: 4.4026
2025-06-25 04:45:00.960 INFO: Epoch 22, Train Loss: 4.3774, Val Loss: 4.4026
train_e/atom_mae: 0.005776
2025-06-25 04:45:00.961 INFO: train_e/atom_mae: 0.005776
train_e/atom_rmse: 0.007232
2025-06-25 04:45:00.961 INFO: train_e/atom_rmse: 0.007232
train_f_mae: 0.040061
2025-06-25 04:45:00.965 INFO: train_f_mae: 0.040061
train_f_rmse: 0.065682
2025-06-25 04:45:00.965 INFO: train_f_rmse: 0.065682
val_e/atom_mae: 0.004445
2025-06-25 04:45:00.968 INFO: val_e/atom_mae: 0.004445
val_e/atom_rmse: 0.005487
2025-06-25 04:45:00.968 INFO: val_e/atom_rmse: 0.005487
val_f_mae: 0.040738
2025-06-25 04:45:00.969 INFO: val_f_mae: 0.040738
val_f_rmse: 0.066077
2025-06-25 04:45:00.969 INFO: val_f_rmse: 0.066077
##### Step: 22 Learning rate: 0.005 #####
2025-06-25 04:45:31.237 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 4.3036, Val Loss: 4.1354
2025-06-25 04:45:31.237 INFO: Epoch 23, Train Loss: 4.3036, Val Loss: 4.1354
train_e/atom_mae: 0.005208
2025-06-25 04:45:31.238 INFO: train_e/atom_mae: 0.005208
train_e/atom_rmse: 0.006769
2025-06-25 04:45:31.238 INFO: train_e/atom_rmse: 0.006769
train_f_mae: 0.039862
2025-06-25 04:45:31.242 INFO: train_f_mae: 0.039862
train_f_rmse: 0.065178
2025-06-25 04:45:31.242 INFO: train_f_rmse: 0.065178
val_e/atom_mae: 0.003830
2025-06-25 04:45:31.244 INFO: val_e/atom_mae: 0.003830
val_e/atom_rmse: 0.004495
2025-06-25 04:45:31.245 INFO: val_e/atom_rmse: 0.004495
val_f_mae: 0.039026
2025-06-25 04:45:31.245 INFO: val_f_mae: 0.039026
val_f_rmse: 0.064116
2025-06-25 04:45:31.245 INFO: val_f_rmse: 0.064116
##### Step: 23 Learning rate: 0.005 #####
2025-06-25 04:46:01.698 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 4.3123, Val Loss: 4.0512
2025-06-25 04:46:01.698 INFO: Epoch 24, Train Loss: 4.3123, Val Loss: 4.0512
train_e/atom_mae: 0.005792
2025-06-25 04:46:01.699 INFO: train_e/atom_mae: 0.005792
train_e/atom_rmse: 0.007283
2025-06-25 04:46:01.699 INFO: train_e/atom_rmse: 0.007283
train_f_mae: 0.039458
2025-06-25 04:46:01.703 INFO: train_f_mae: 0.039458
train_f_rmse: 0.065178
2025-06-25 04:46:01.703 INFO: train_f_rmse: 0.065178
val_e/atom_mae: 0.002493
2025-06-25 04:46:01.706 INFO: val_e/atom_mae: 0.002493
val_e/atom_rmse: 0.002880
2025-06-25 04:46:01.706 INFO: val_e/atom_rmse: 0.002880
val_f_mae: 0.038655
2025-06-25 04:46:01.707 INFO: val_f_mae: 0.038655
val_f_rmse: 0.063570
2025-06-25 04:46:01.707 INFO: val_f_rmse: 0.063570
##### Step: 24 Learning rate: 0.005 #####
2025-06-25 04:46:31.934 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 4.3349, Val Loss: 4.2819
2025-06-25 04:46:31.934 INFO: Epoch 25, Train Loss: 4.3349, Val Loss: 4.2819
train_e/atom_mae: 0.005598
2025-06-25 04:46:31.935 INFO: train_e/atom_mae: 0.005598
train_e/atom_rmse: 0.007118
2025-06-25 04:46:31.935 INFO: train_e/atom_rmse: 0.007118
train_f_mae: 0.039831
2025-06-25 04:46:31.939 INFO: train_f_mae: 0.039831
train_f_rmse: 0.065373
2025-06-25 04:46:31.939 INFO: train_f_rmse: 0.065373
val_e/atom_mae: 0.002222
2025-06-25 04:46:31.941 INFO: val_e/atom_mae: 0.002222
val_e/atom_rmse: 0.002747
2025-06-25 04:46:31.942 INFO: val_e/atom_rmse: 0.002747
val_f_mae: 0.039934
2025-06-25 04:46:31.942 INFO: val_f_mae: 0.039934
val_f_rmse: 0.065366
2025-06-25 04:46:31.942 INFO: val_f_rmse: 0.065366
##### Step: 25 Learning rate: 0.005 #####
2025-06-25 04:47:02.270 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 4.3208, Val Loss: 4.4537
2025-06-25 04:47:02.270 INFO: Epoch 26, Train Loss: 4.3208, Val Loss: 4.4537
train_e/atom_mae: 0.005697
2025-06-25 04:47:02.271 INFO: train_e/atom_mae: 0.005697
train_e/atom_rmse: 0.007126
2025-06-25 04:47:02.271 INFO: train_e/atom_rmse: 0.007126
train_f_mae: 0.039499
2025-06-25 04:47:02.274 INFO: train_f_mae: 0.039499
train_f_rmse: 0.065264
2025-06-25 04:47:02.275 INFO: train_f_rmse: 0.065264
val_e/atom_mae: 0.004722
2025-06-25 04:47:02.277 INFO: val_e/atom_mae: 0.004722
val_e/atom_rmse: 0.005228
2025-06-25 04:47:02.278 INFO: val_e/atom_rmse: 0.005228
val_f_mae: 0.040566
2025-06-25 04:47:02.278 INFO: val_f_mae: 0.040566
val_f_rmse: 0.066488
2025-06-25 04:47:02.278 INFO: val_f_rmse: 0.066488
##### Step: 26 Learning rate: 0.005 #####
2025-06-25 04:47:32.615 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 4.2306, Val Loss: 4.2031
2025-06-25 04:47:32.616 INFO: Epoch 27, Train Loss: 4.2306, Val Loss: 4.2031
train_e/atom_mae: 0.005729
2025-06-25 04:47:32.617 INFO: train_e/atom_mae: 0.005729
train_e/atom_rmse: 0.007254
2025-06-25 04:47:32.617 INFO: train_e/atom_rmse: 0.007254
train_f_mae: 0.039294
2025-06-25 04:47:32.620 INFO: train_f_mae: 0.039294
train_f_rmse: 0.064551
2025-06-25 04:47:32.621 INFO: train_f_rmse: 0.064551
val_e/atom_mae: 0.002788
2025-06-25 04:47:32.623 INFO: val_e/atom_mae: 0.002788
val_e/atom_rmse: 0.003500
2025-06-25 04:47:32.624 INFO: val_e/atom_rmse: 0.003500
val_f_mae: 0.039137
2025-06-25 04:47:32.624 INFO: val_f_mae: 0.039137
val_f_rmse: 0.064717
2025-06-25 04:47:32.624 INFO: val_f_rmse: 0.064717
##### Step: 27 Learning rate: 0.005 #####
2025-06-25 04:48:02.877 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 4.1917, Val Loss: 4.5230
2025-06-25 04:48:02.877 INFO: Epoch 28, Train Loss: 4.1917, Val Loss: 4.5230
train_e/atom_mae: 0.005501
2025-06-25 04:48:02.878 INFO: train_e/atom_mae: 0.005501
train_e/atom_rmse: 0.006800
2025-06-25 04:48:02.878 INFO: train_e/atom_rmse: 0.006800
train_f_mae: 0.038830
2025-06-25 04:48:02.882 INFO: train_f_mae: 0.038830
train_f_rmse: 0.064310
2025-06-25 04:48:02.882 INFO: train_f_rmse: 0.064310
val_e/atom_mae: 0.005869
2025-06-25 04:48:02.885 INFO: val_e/atom_mae: 0.005869
val_e/atom_rmse: 0.006362
2025-06-25 04:48:02.885 INFO: val_e/atom_rmse: 0.006362
val_f_mae: 0.039374
2025-06-25 04:48:02.885 INFO: val_f_mae: 0.039374
val_f_rmse: 0.066888
2025-06-25 04:48:02.886 INFO: val_f_rmse: 0.066888
##### Step: 28 Learning rate: 0.005 #####
2025-06-25 04:48:33.267 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 4.2464, Val Loss: 4.0826
2025-06-25 04:48:33.267 INFO: Epoch 29, Train Loss: 4.2464, Val Loss: 4.0826
train_e/atom_mae: 0.007541
2025-06-25 04:48:33.268 INFO: train_e/atom_mae: 0.007541
train_e/atom_rmse: 0.009529
2025-06-25 04:48:33.268 INFO: train_e/atom_rmse: 0.009529
train_f_mae: 0.038824
2025-06-25 04:48:33.272 INFO: train_f_mae: 0.038824
train_f_rmse: 0.064316
2025-06-25 04:48:33.272 INFO: train_f_rmse: 0.064316
val_e/atom_mae: 0.010744
2025-06-25 04:48:33.275 INFO: val_e/atom_mae: 0.010744
val_e/atom_rmse: 0.010982
2025-06-25 04:48:33.275 INFO: val_e/atom_rmse: 0.010982
val_f_mae: 0.037957
2025-06-25 04:48:33.276 INFO: val_f_mae: 0.037957
val_f_rmse: 0.062743
2025-06-25 04:48:33.276 INFO: val_f_rmse: 0.062743
##### Step: 29 Learning rate: 0.005 #####
2025-06-25 04:49:03.504 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 4.2187, Val Loss: 4.0302
2025-06-25 04:49:03.505 INFO: Epoch 30, Train Loss: 4.2187, Val Loss: 4.0302
train_e/atom_mae: 0.006270
2025-06-25 04:49:03.505 INFO: train_e/atom_mae: 0.006270
train_e/atom_rmse: 0.007860
2025-06-25 04:49:03.506 INFO: train_e/atom_rmse: 0.007860
train_f_mae: 0.039021
2025-06-25 04:49:03.509 INFO: train_f_mae: 0.039021
train_f_rmse: 0.064374
2025-06-25 04:49:03.509 INFO: train_f_rmse: 0.064374
val_e/atom_mae: 0.005477
2025-06-25 04:49:03.512 INFO: val_e/atom_mae: 0.005477
val_e/atom_rmse: 0.006043
2025-06-25 04:49:03.512 INFO: val_e/atom_rmse: 0.006043
val_f_mae: 0.038686
2025-06-25 04:49:03.513 INFO: val_f_mae: 0.038686
val_f_rmse: 0.063135
2025-06-25 04:49:03.513 INFO: val_f_rmse: 0.063135
##### Step: 30 Learning rate: 0.005 #####
2025-06-25 04:49:33.928 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 4.2898, Val Loss: 4.3355
2025-06-25 04:49:33.929 INFO: Epoch 31, Train Loss: 4.2898, Val Loss: 4.3355
train_e/atom_mae: 0.006099
2025-06-25 04:49:33.929 INFO: train_e/atom_mae: 0.006099
train_e/atom_rmse: 0.007543
2025-06-25 04:49:33.930 INFO: train_e/atom_rmse: 0.007543
train_f_mae: 0.039389
2025-06-25 04:49:33.933 INFO: train_f_mae: 0.039389
train_f_rmse: 0.064969
2025-06-25 04:49:33.933 INFO: train_f_rmse: 0.064969
val_e/atom_mae: 0.005553
2025-06-25 04:49:33.936 INFO: val_e/atom_mae: 0.005553
val_e/atom_rmse: 0.006866
2025-06-25 04:49:33.936 INFO: val_e/atom_rmse: 0.006866
val_f_mae: 0.041635
2025-06-25 04:49:33.937 INFO: val_f_mae: 0.041635
val_f_rmse: 0.065410
2025-06-25 04:49:33.937 INFO: val_f_rmse: 0.065410
##### Step: 31 Learning rate: 0.005 #####
2025-06-25 04:50:04.205 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 4.2561, Val Loss: 4.1925
2025-06-25 04:50:04.205 INFO: Epoch 32, Train Loss: 4.2561, Val Loss: 4.1925
train_e/atom_mae: 0.006681
2025-06-25 04:50:04.206 INFO: train_e/atom_mae: 0.006681
train_e/atom_rmse: 0.008277
2025-06-25 04:50:04.206 INFO: train_e/atom_rmse: 0.008277
train_f_mae: 0.039408
2025-06-25 04:50:04.210 INFO: train_f_mae: 0.039408
train_f_rmse: 0.064600
2025-06-25 04:50:04.210 INFO: train_f_rmse: 0.064600
val_e/atom_mae: 0.004592
2025-06-25 04:50:04.213 INFO: val_e/atom_mae: 0.004592
val_e/atom_rmse: 0.005180
2025-06-25 04:50:04.213 INFO: val_e/atom_rmse: 0.005180
val_f_mae: 0.039913
2025-06-25 04:50:04.214 INFO: val_f_mae: 0.039913
val_f_rmse: 0.064498
2025-06-25 04:50:04.214 INFO: val_f_rmse: 0.064498
##### Step: 32 Learning rate: 0.005 #####
2025-06-25 04:50:34.559 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 4.2656, Val Loss: 3.9894
2025-06-25 04:50:34.559 INFO: Epoch 33, Train Loss: 4.2656, Val Loss: 3.9894
train_e/atom_mae: 0.007004
2025-06-25 04:50:34.560 INFO: train_e/atom_mae: 0.007004
train_e/atom_rmse: 0.008606
2025-06-25 04:50:34.560 INFO: train_e/atom_rmse: 0.008606
train_f_mae: 0.039337
2025-06-25 04:50:34.564 INFO: train_f_mae: 0.039337
train_f_rmse: 0.064622
2025-06-25 04:50:34.564 INFO: train_f_rmse: 0.064622
val_e/atom_mae: 0.002968
2025-06-25 04:50:34.566 INFO: val_e/atom_mae: 0.002968
val_e/atom_rmse: 0.003269
2025-06-25 04:50:34.567 INFO: val_e/atom_rmse: 0.003269
val_f_mae: 0.038904
2025-06-25 04:50:34.567 INFO: val_f_mae: 0.038904
val_f_rmse: 0.063060
2025-06-25 04:50:34.567 INFO: val_f_rmse: 0.063060
##### Step: 33 Learning rate: 0.005 #####
2025-06-25 04:51:04.988 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 4.5354, Val Loss: 4.3207
2025-06-25 04:51:04.989 INFO: Epoch 34, Train Loss: 4.5354, Val Loss: 4.3207
train_e/atom_mae: 0.006497
2025-06-25 04:51:04.990 INFO: train_e/atom_mae: 0.006497
train_e/atom_rmse: 0.008158
2025-06-25 04:51:04.990 INFO: train_e/atom_rmse: 0.008158
train_f_mae: 0.038999
2025-06-25 04:51:04.993 INFO: train_f_mae: 0.038999
train_f_rmse: 0.066745
2025-06-25 04:51:04.993 INFO: train_f_rmse: 0.066745
val_e/atom_mae: 0.007984
2025-06-25 04:51:04.996 INFO: val_e/atom_mae: 0.007984
val_e/atom_rmse: 0.008551
2025-06-25 04:51:04.996 INFO: val_e/atom_rmse: 0.008551
val_f_mae: 0.039347
2025-06-25 04:51:04.997 INFO: val_f_mae: 0.039347
val_f_rmse: 0.065056
2025-06-25 04:51:04.997 INFO: val_f_rmse: 0.065056
##### Step: 34 Learning rate: 0.005 #####
2025-06-25 04:51:35.229 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 4.3039, Val Loss: 4.5477
2025-06-25 04:51:35.229 INFO: Epoch 35, Train Loss: 4.3039, Val Loss: 4.5477
train_e/atom_mae: 0.006848
2025-06-25 04:51:35.230 INFO: train_e/atom_mae: 0.006848
train_e/atom_rmse: 0.008457
2025-06-25 04:51:35.231 INFO: train_e/atom_rmse: 0.008457
train_f_mae: 0.039557
2025-06-25 04:51:35.234 INFO: train_f_mae: 0.039557
train_f_rmse: 0.064941
2025-06-25 04:51:35.234 INFO: train_f_rmse: 0.064941
val_e/atom_mae: 0.009968
2025-06-25 04:51:35.237 INFO: val_e/atom_mae: 0.009968
val_e/atom_rmse: 0.012284
2025-06-25 04:51:35.237 INFO: val_e/atom_rmse: 0.012284
val_f_mae: 0.039957
2025-06-25 04:51:35.238 INFO: val_f_mae: 0.039957
val_f_rmse: 0.066069
2025-06-25 04:51:35.238 INFO: val_f_rmse: 0.066069
##### Step: 35 Learning rate: 0.005 #####
2025-06-25 04:52:05.657 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 4.2337, Val Loss: 3.9332
2025-06-25 04:52:05.657 INFO: Epoch 36, Train Loss: 4.2337, Val Loss: 3.9332
train_e/atom_mae: 0.006220
2025-06-25 04:52:05.658 INFO: train_e/atom_mae: 0.006220
train_e/atom_rmse: 0.007663
2025-06-25 04:52:05.658 INFO: train_e/atom_rmse: 0.007663
train_f_mae: 0.038930
2025-06-25 04:52:05.662 INFO: train_f_mae: 0.038930
train_f_rmse: 0.064518
2025-06-25 04:52:05.662 INFO: train_f_rmse: 0.064518
val_e/atom_mae: 0.003000
2025-06-25 04:52:05.665 INFO: val_e/atom_mae: 0.003000
val_e/atom_rmse: 0.003577
2025-06-25 04:52:05.665 INFO: val_e/atom_rmse: 0.003577
val_f_mae: 0.038175
2025-06-25 04:52:05.666 INFO: val_f_mae: 0.038175
val_f_rmse: 0.062592
2025-06-25 04:52:05.666 INFO: val_f_rmse: 0.062592
##### Step: 36 Learning rate: 0.005 #####
2025-06-25 04:52:35.901 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 4.2048, Val Loss: 4.5932
2025-06-25 04:52:35.901 INFO: Epoch 37, Train Loss: 4.2048, Val Loss: 4.5932
train_e/atom_mae: 0.005521
2025-06-25 04:52:35.902 INFO: train_e/atom_mae: 0.005521
train_e/atom_rmse: 0.006957
2025-06-25 04:52:35.902 INFO: train_e/atom_rmse: 0.006957
train_f_mae: 0.039164
2025-06-25 04:52:35.906 INFO: train_f_mae: 0.039164
train_f_rmse: 0.064391
2025-06-25 04:52:35.906 INFO: train_f_rmse: 0.064391
val_e/atom_mae: 0.002865
2025-06-25 04:52:35.909 INFO: val_e/atom_mae: 0.002865
val_e/atom_rmse: 0.003433
2025-06-25 04:52:35.909 INFO: val_e/atom_rmse: 0.003433
val_f_mae: 0.041496
2025-06-25 04:52:35.909 INFO: val_f_mae: 0.041496
val_f_rmse: 0.067668
2025-06-25 04:52:35.910 INFO: val_f_rmse: 0.067668
##### Step: 37 Learning rate: 0.005 #####
2025-06-25 04:53:06.286 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 4.1426, Val Loss: 4.1557
2025-06-25 04:53:06.286 INFO: Epoch 38, Train Loss: 4.1426, Val Loss: 4.1557
train_e/atom_mae: 0.005123
2025-06-25 04:53:06.287 INFO: train_e/atom_mae: 0.005123
train_e/atom_rmse: 0.006463
2025-06-25 04:53:06.287 INFO: train_e/atom_rmse: 0.006463
train_f_mae: 0.038590
2025-06-25 04:53:06.291 INFO: train_f_mae: 0.038590
train_f_rmse: 0.063970
2025-06-25 04:53:06.291 INFO: train_f_rmse: 0.063970
val_e/atom_mae: 0.008276
2025-06-25 04:53:06.293 INFO: val_e/atom_mae: 0.008276
val_e/atom_rmse: 0.010192
2025-06-25 04:53:06.294 INFO: val_e/atom_rmse: 0.010192
val_f_mae: 0.038921
2025-06-25 04:53:06.294 INFO: val_f_mae: 0.038921
val_f_rmse: 0.063482
2025-06-25 04:53:06.294 INFO: val_f_rmse: 0.063482
##### Step: 38 Learning rate: 0.005 #####
2025-06-25 04:53:36.583 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 4.1247, Val Loss: 4.3089
2025-06-25 04:53:36.584 INFO: Epoch 39, Train Loss: 4.1247, Val Loss: 4.3089
train_e/atom_mae: 0.006104
2025-06-25 04:53:36.585 INFO: train_e/atom_mae: 0.006104
train_e/atom_rmse: 0.007750
2025-06-25 04:53:36.585 INFO: train_e/atom_rmse: 0.007750
train_f_mae: 0.038465
2025-06-25 04:53:36.588 INFO: train_f_mae: 0.038465
train_f_rmse: 0.063655
2025-06-25 04:53:36.588 INFO: train_f_rmse: 0.063655
val_e/atom_mae: 0.005642
2025-06-25 04:53:36.591 INFO: val_e/atom_mae: 0.005642
val_e/atom_rmse: 0.006461
2025-06-25 04:53:36.591 INFO: val_e/atom_rmse: 0.006461
val_f_mae: 0.038882
2025-06-25 04:53:36.592 INFO: val_f_mae: 0.038882
val_f_rmse: 0.065257
2025-06-25 04:53:36.592 INFO: val_f_rmse: 0.065257
##### Step: 39 Learning rate: 0.005 #####
2025-06-25 04:54:06.911 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 4.2082, Val Loss: 4.3465
2025-06-25 04:54:06.911 INFO: Epoch 40, Train Loss: 4.2082, Val Loss: 4.3465
train_e/atom_mae: 0.009906
2025-06-25 04:54:06.912 INFO: train_e/atom_mae: 0.009906
train_e/atom_rmse: 0.012259
2025-06-25 04:54:06.912 INFO: train_e/atom_rmse: 0.012259
train_f_mae: 0.038362
2025-06-25 04:54:06.916 INFO: train_f_mae: 0.038362
train_f_rmse: 0.063453
2025-06-25 04:54:06.916 INFO: train_f_rmse: 0.063453
val_e/atom_mae: 0.012764
2025-06-25 04:54:06.919 INFO: val_e/atom_mae: 0.012764
val_e/atom_rmse: 0.013310
2025-06-25 04:54:06.919 INFO: val_e/atom_rmse: 0.013310
val_f_mae: 0.038937
2025-06-25 04:54:06.920 INFO: val_f_mae: 0.038937
val_f_rmse: 0.064281
2025-06-25 04:54:06.920 INFO: val_f_rmse: 0.064281
2025-06-25 04:54:06.925 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-25 04:54:37.268 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 3.9468, Val Loss: 3.9860
2025-06-25 04:54:37.268 INFO: Epoch 1, Train Loss: 3.9468, Val Loss: 3.9860
train_e/atom_mae: 0.004919
2025-06-25 04:54:37.269 INFO: train_e/atom_mae: 0.004919
train_e/atom_rmse: 0.006105
2025-06-25 04:54:37.269 INFO: train_e/atom_rmse: 0.006105
train_f_mae: 0.037398
2025-06-25 04:54:37.273 INFO: train_f_mae: 0.037398
train_f_rmse: 0.062463
2025-06-25 04:54:37.273 INFO: train_f_rmse: 0.062463
val_e/atom_mae: 0.008010
2025-06-25 04:54:37.275 INFO: val_e/atom_mae: 0.008010
val_e/atom_rmse: 0.008782
2025-06-25 04:54:37.276 INFO: val_e/atom_rmse: 0.008782
val_f_mae: 0.037645
2025-06-25 04:54:37.276 INFO: val_f_mae: 0.037645
val_f_rmse: 0.062392
2025-06-25 04:54:37.276 INFO: val_f_rmse: 0.062392
##### Step: 1 Learning rate: 0.004 #####
2025-06-25 04:55:07.549 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 4.1489, Val Loss: 4.2275
2025-06-25 04:55:07.549 INFO: Epoch 2, Train Loss: 4.1489, Val Loss: 4.2275
train_e/atom_mae: 0.006828
2025-06-25 04:55:07.550 INFO: train_e/atom_mae: 0.006828
train_e/atom_rmse: 0.008459
2025-06-25 04:55:07.550 INFO: train_e/atom_rmse: 0.008459
train_f_mae: 0.038422
2025-06-25 04:55:07.554 INFO: train_f_mae: 0.038422
train_f_rmse: 0.063737
2025-06-25 04:55:07.554 INFO: train_f_rmse: 0.063737
val_e/atom_mae: 0.006692
2025-06-25 04:55:07.556 INFO: val_e/atom_mae: 0.006692
val_e/atom_rmse: 0.007811
2025-06-25 04:55:07.557 INFO: val_e/atom_rmse: 0.007811
val_f_mae: 0.039350
2025-06-25 04:55:07.557 INFO: val_f_mae: 0.039350
val_f_rmse: 0.064449
2025-06-25 04:55:07.557 INFO: val_f_rmse: 0.064449
##### Step: 2 Learning rate: 0.006 #####
2025-06-25 04:55:38.012 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 4.3359, Val Loss: 4.3053
2025-06-25 04:55:38.012 INFO: Epoch 3, Train Loss: 4.3359, Val Loss: 4.3053
train_e/atom_mae: 0.006507
2025-06-25 04:55:38.013 INFO: train_e/atom_mae: 0.006507
train_e/atom_rmse: 0.008594
2025-06-25 04:55:38.013 INFO: train_e/atom_rmse: 0.008594
train_f_mae: 0.039691
2025-06-25 04:55:38.017 INFO: train_f_mae: 0.039691
train_f_rmse: 0.065165
2025-06-25 04:55:38.017 INFO: train_f_rmse: 0.065165
val_e/atom_mae: 0.006659
2025-06-25 04:55:38.020 INFO: val_e/atom_mae: 0.006659
val_e/atom_rmse: 0.007707
2025-06-25 04:55:38.020 INFO: val_e/atom_rmse: 0.007707
val_f_mae: 0.039798
2025-06-25 04:55:38.020 INFO: val_f_mae: 0.039798
val_f_rmse: 0.065065
2025-06-25 04:55:38.021 INFO: val_f_rmse: 0.065065
##### Step: 3 Learning rate: 0.008 #####
2025-06-25 04:56:08.236 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 4.3300, Val Loss: 4.3701
2025-06-25 04:56:08.237 INFO: Epoch 4, Train Loss: 4.3300, Val Loss: 4.3701
train_e/atom_mae: 0.006982
2025-06-25 04:56:08.238 INFO: train_e/atom_mae: 0.006982
train_e/atom_rmse: 0.008495
2025-06-25 04:56:08.238 INFO: train_e/atom_rmse: 0.008495
train_f_mae: 0.039206
2025-06-25 04:56:08.241 INFO: train_f_mae: 0.039206
train_f_rmse: 0.065136
2025-06-25 04:56:08.242 INFO: train_f_rmse: 0.065136
val_e/atom_mae: 0.002166
2025-06-25 04:56:08.244 INFO: val_e/atom_mae: 0.002166
val_e/atom_rmse: 0.002318
2025-06-25 04:56:08.245 INFO: val_e/atom_rmse: 0.002318
val_f_mae: 0.039646
2025-06-25 04:56:08.245 INFO: val_f_mae: 0.039646
val_f_rmse: 0.066058
2025-06-25 04:56:08.245 INFO: val_f_rmse: 0.066058
##### Step: 4 Learning rate: 0.01 #####
2025-06-25 04:56:38.567 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 4.7063, Val Loss: 4.5246
2025-06-25 04:56:38.567 INFO: Epoch 5, Train Loss: 4.7063, Val Loss: 4.5246
train_e/atom_mae: 0.009466
2025-06-25 04:56:38.568 INFO: train_e/atom_mae: 0.009466
train_e/atom_rmse: 0.011649
2025-06-25 04:56:38.569 INFO: train_e/atom_rmse: 0.011649
train_f_mae: 0.040627
2025-06-25 04:56:38.572 INFO: train_f_mae: 0.040627
train_f_rmse: 0.067395
2025-06-25 04:56:38.572 INFO: train_f_rmse: 0.067395
val_e/atom_mae: 0.002839
2025-06-25 04:56:38.575 INFO: val_e/atom_mae: 0.002839
val_e/atom_rmse: 0.003528
2025-06-25 04:56:38.575 INFO: val_e/atom_rmse: 0.003528
val_f_mae: 0.042038
2025-06-25 04:56:38.576 INFO: val_f_mae: 0.042038
val_f_rmse: 0.067153
2025-06-25 04:56:38.576 INFO: val_f_rmse: 0.067153
##### Step: 5 Learning rate: 0.01 #####
2025-06-25 04:57:08.915 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 4.6471, Val Loss: 4.7953
2025-06-25 04:57:08.916 INFO: Epoch 6, Train Loss: 4.6471, Val Loss: 4.7953
train_e/atom_mae: 0.007856
2025-06-25 04:57:08.917 INFO: train_e/atom_mae: 0.007856
train_e/atom_rmse: 0.009843
2025-06-25 04:57:08.917 INFO: train_e/atom_rmse: 0.009843
train_f_mae: 0.041026
2025-06-25 04:57:08.920 INFO: train_f_mae: 0.041026
train_f_rmse: 0.067305
2025-06-25 04:57:08.920 INFO: train_f_rmse: 0.067305
val_e/atom_mae: 0.013299
2025-06-25 04:57:08.923 INFO: val_e/atom_mae: 0.013299
val_e/atom_rmse: 0.013504
2025-06-25 04:57:08.924 INFO: val_e/atom_rmse: 0.013504
val_f_mae: 0.039659
2025-06-25 04:57:08.924 INFO: val_f_mae: 0.039659
val_f_rmse: 0.067636
2025-06-25 04:57:08.924 INFO: val_f_rmse: 0.067636
##### Step: 6 Learning rate: 0.01 #####
2025-06-25 04:57:39.235 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 4.6518, Val Loss: 4.6244
2025-06-25 04:57:39.236 INFO: Epoch 7, Train Loss: 4.6518, Val Loss: 4.6244
train_e/atom_mae: 0.006993
2025-06-25 04:57:39.236 INFO: train_e/atom_mae: 0.006993
train_e/atom_rmse: 0.008749
2025-06-25 04:57:39.237 INFO: train_e/atom_rmse: 0.008749
train_f_mae: 0.041237
2025-06-25 04:57:39.240 INFO: train_f_mae: 0.041237
train_f_rmse: 0.067522
2025-06-25 04:57:39.240 INFO: train_f_rmse: 0.067522
val_e/atom_mae: 0.008553
2025-06-25 04:57:39.243 INFO: val_e/atom_mae: 0.008553
val_e/atom_rmse: 0.010307
2025-06-25 04:57:39.243 INFO: val_e/atom_rmse: 0.010307
val_f_mae: 0.040924
2025-06-25 04:57:39.244 INFO: val_f_mae: 0.040924
val_f_rmse: 0.067051
2025-06-25 04:57:39.244 INFO: val_f_rmse: 0.067051
##### Step: 7 Learning rate: 0.01 #####
2025-06-25 04:58:09.665 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 4.8374, Val Loss: 4.7868
2025-06-25 04:58:09.665 INFO: Epoch 8, Train Loss: 4.8374, Val Loss: 4.7868
train_e/atom_mae: 0.009730
2025-06-25 04:58:09.666 INFO: train_e/atom_mae: 0.009730
train_e/atom_rmse: 0.012426
2025-06-25 04:58:09.666 INFO: train_e/atom_rmse: 0.012426
train_f_mae: 0.041544
2025-06-25 04:58:09.670 INFO: train_f_mae: 0.041544
train_f_rmse: 0.068195
2025-06-25 04:58:09.670 INFO: train_f_rmse: 0.068195
val_e/atom_mae: 0.012785
2025-06-25 04:58:09.673 INFO: val_e/atom_mae: 0.012785
val_e/atom_rmse: 0.014497
2025-06-25 04:58:09.673 INFO: val_e/atom_rmse: 0.014497
val_f_mae: 0.041038
2025-06-25 04:58:09.674 INFO: val_f_mae: 0.041038
val_f_rmse: 0.067324
2025-06-25 04:58:09.674 INFO: val_f_rmse: 0.067324
##### Step: 8 Learning rate: 0.01 #####
2025-06-25 04:58:39.924 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 4.5896, Val Loss: 4.3364
2025-06-25 04:58:39.924 INFO: Epoch 9, Train Loss: 4.5896, Val Loss: 4.3364
train_e/atom_mae: 0.006624
2025-06-25 04:58:39.925 INFO: train_e/atom_mae: 0.006624
train_e/atom_rmse: 0.008357
2025-06-25 04:58:39.925 INFO: train_e/atom_rmse: 0.008357
train_f_mae: 0.041092
2025-06-25 04:58:39.929 INFO: train_f_mae: 0.041092
train_f_rmse: 0.067120
2025-06-25 04:58:39.929 INFO: train_f_rmse: 0.067120
val_e/atom_mae: 0.002300
2025-06-25 04:58:39.932 INFO: val_e/atom_mae: 0.002300
val_e/atom_rmse: 0.002766
2025-06-25 04:58:39.932 INFO: val_e/atom_rmse: 0.002766
val_f_mae: 0.040791
2025-06-25 04:58:39.932 INFO: val_f_mae: 0.040791
val_f_rmse: 0.065781
2025-06-25 04:58:39.932 INFO: val_f_rmse: 0.065781
##### Step: 9 Learning rate: 0.01 #####
2025-06-25 04:59:10.327 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 5.0906, Val Loss: 4.5314
2025-06-25 04:59:10.327 INFO: Epoch 10, Train Loss: 5.0906, Val Loss: 4.5314
train_e/atom_mae: 0.008058
2025-06-25 04:59:10.328 INFO: train_e/atom_mae: 0.008058
train_e/atom_rmse: 0.009857
2025-06-25 04:59:10.328 INFO: train_e/atom_rmse: 0.009857
train_f_mae: 0.042016
2025-06-25 04:59:10.332 INFO: train_f_mae: 0.042016
train_f_rmse: 0.070520
2025-06-25 04:59:10.332 INFO: train_f_rmse: 0.070520
val_e/atom_mae: 0.007737
2025-06-25 04:59:10.335 INFO: val_e/atom_mae: 0.007737
val_e/atom_rmse: 0.008254
2025-06-25 04:59:10.335 INFO: val_e/atom_rmse: 0.008254
val_f_mae: 0.041770
2025-06-25 04:59:10.336 INFO: val_f_mae: 0.041770
val_f_rmse: 0.066701
2025-06-25 04:59:10.336 INFO: val_f_rmse: 0.066701
##### Step: 10 Learning rate: 0.01 #####
2025-06-25 04:59:40.585 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 4.6452, Val Loss: 4.5970
2025-06-25 04:59:40.585 INFO: Epoch 11, Train Loss: 4.6452, Val Loss: 4.5970
train_e/atom_mae: 0.008518
2025-06-25 04:59:40.586 INFO: train_e/atom_mae: 0.008518
train_e/atom_rmse: 0.010633
2025-06-25 04:59:40.586 INFO: train_e/atom_rmse: 0.010633
train_f_mae: 0.040834
2025-06-25 04:59:40.590 INFO: train_f_mae: 0.040834
train_f_rmse: 0.067145
2025-06-25 04:59:40.590 INFO: train_f_rmse: 0.067145
val_e/atom_mae: 0.011263
2025-06-25 04:59:40.593 INFO: val_e/atom_mae: 0.011263
val_e/atom_rmse: 0.013069
2025-06-25 04:59:40.593 INFO: val_e/atom_rmse: 0.013069
val_f_mae: 0.040940
2025-06-25 04:59:40.594 INFO: val_f_mae: 0.040940
val_f_rmse: 0.066260
2025-06-25 04:59:40.594 INFO: val_f_rmse: 0.066260
##### Step: 11 Learning rate: 0.01 #####
2025-06-25 05:00:10.926 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 4.5599, Val Loss: 4.8776
2025-06-25 05:00:10.926 INFO: Epoch 12, Train Loss: 4.5599, Val Loss: 4.8776
train_e/atom_mae: 0.007390
2025-06-25 05:00:10.927 INFO: train_e/atom_mae: 0.007390
train_e/atom_rmse: 0.009228
2025-06-25 05:00:10.927 INFO: train_e/atom_rmse: 0.009228
train_f_mae: 0.040781
2025-06-25 05:00:10.930 INFO: train_f_mae: 0.040781
train_f_rmse: 0.066760
2025-06-25 05:00:10.931 INFO: train_f_rmse: 0.066760
val_e/atom_mae: 0.005933
2025-06-25 05:00:10.933 INFO: val_e/atom_mae: 0.005933
val_e/atom_rmse: 0.006841
2025-06-25 05:00:10.934 INFO: val_e/atom_rmse: 0.006841
val_f_mae: 0.041863
2025-06-25 05:00:10.934 INFO: val_f_mae: 0.041863
val_f_rmse: 0.069433
2025-06-25 05:00:10.934 INFO: val_f_rmse: 0.069433
##### Step: 12 Learning rate: 0.01 #####
2025-06-25 05:00:41.308 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 4.6766, Val Loss: 4.3474
2025-06-25 05:00:41.308 INFO: Epoch 13, Train Loss: 4.6766, Val Loss: 4.3474
train_e/atom_mae: 0.009596
2025-06-25 05:00:41.309 INFO: train_e/atom_mae: 0.009596
train_e/atom_rmse: 0.011587
2025-06-25 05:00:41.309 INFO: train_e/atom_rmse: 0.011587
train_f_mae: 0.040629
2025-06-25 05:00:41.313 INFO: train_f_mae: 0.040629
train_f_rmse: 0.067187
2025-06-25 05:00:41.313 INFO: train_f_rmse: 0.067187
val_e/atom_mae: 0.005887
2025-06-25 05:00:41.316 INFO: val_e/atom_mae: 0.005887
val_e/atom_rmse: 0.006714
2025-06-25 05:00:41.316 INFO: val_e/atom_rmse: 0.006714
val_f_mae: 0.039324
2025-06-25 05:00:41.317 INFO: val_f_mae: 0.039324
val_f_rmse: 0.065520
2025-06-25 05:00:41.317 INFO: val_f_rmse: 0.065520
##### Step: 13 Learning rate: 0.01 #####
2025-06-25 05:01:11.580 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 4.7142, Val Loss: 4.2517
2025-06-25 05:01:11.580 INFO: Epoch 14, Train Loss: 4.7142, Val Loss: 4.2517
train_e/atom_mae: 0.007370
2025-06-25 05:01:11.581 INFO: train_e/atom_mae: 0.007370
train_e/atom_rmse: 0.009123
2025-06-25 05:01:11.581 INFO: train_e/atom_rmse: 0.009123
train_f_mae: 0.041585
2025-06-25 05:01:11.584 INFO: train_f_mae: 0.041585
train_f_rmse: 0.067923
2025-06-25 05:01:11.584 INFO: train_f_rmse: 0.067923
val_e/atom_mae: 0.002883
2025-06-25 05:01:11.587 INFO: val_e/atom_mae: 0.002883
val_e/atom_rmse: 0.003151
2025-06-25 05:01:11.587 INFO: val_e/atom_rmse: 0.003151
val_f_mae: 0.039528
2025-06-25 05:01:11.588 INFO: val_f_mae: 0.039528
val_f_rmse: 0.065113
2025-06-25 05:01:11.588 INFO: val_f_rmse: 0.065113
##### Step: 14 Learning rate: 0.01 #####
2025-06-25 05:01:42.010 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 4.6805, Val Loss: 4.7747
2025-06-25 05:01:42.010 INFO: Epoch 15, Train Loss: 4.6805, Val Loss: 4.7747
train_e/atom_mae: 0.008113
2025-06-25 05:01:42.011 INFO: train_e/atom_mae: 0.008113
train_e/atom_rmse: 0.010621
2025-06-25 05:01:42.011 INFO: train_e/atom_rmse: 0.010621
train_f_mae: 0.040978
2025-06-25 05:01:42.015 INFO: train_f_mae: 0.040978
train_f_rmse: 0.067409
2025-06-25 05:01:42.015 INFO: train_f_rmse: 0.067409
val_e/atom_mae: 0.010656
2025-06-25 05:01:42.017 INFO: val_e/atom_mae: 0.010656
val_e/atom_rmse: 0.013203
2025-06-25 05:01:42.018 INFO: val_e/atom_rmse: 0.013203
val_f_mae: 0.041042
2025-06-25 05:01:42.018 INFO: val_f_mae: 0.041042
val_f_rmse: 0.067556
2025-06-25 05:01:42.018 INFO: val_f_rmse: 0.067556
##### Step: 15 Learning rate: 0.01 #####
2025-06-25 05:02:12.305 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 4.5433, Val Loss: 5.5288
2025-06-25 05:02:12.305 INFO: Epoch 16, Train Loss: 4.5433, Val Loss: 5.5288
train_e/atom_mae: 0.007191
2025-06-25 05:02:12.306 INFO: train_e/atom_mae: 0.007191
train_e/atom_rmse: 0.009060
2025-06-25 05:02:12.306 INFO: train_e/atom_rmse: 0.009060
train_f_mae: 0.040674
2025-06-25 05:02:12.310 INFO: train_f_mae: 0.040674
train_f_rmse: 0.066663
2025-06-25 05:02:12.310 INFO: train_f_rmse: 0.066663
val_e/atom_mae: 0.025981
2025-06-25 05:02:12.312 INFO: val_e/atom_mae: 0.025981
val_e/atom_rmse: 0.026157
2025-06-25 05:02:12.313 INFO: val_e/atom_rmse: 0.026157
val_f_mae: 0.042781
2025-06-25 05:02:12.313 INFO: val_f_mae: 0.042781
val_f_rmse: 0.068563
2025-06-25 05:02:12.313 INFO: val_f_rmse: 0.068563
##### Step: 16 Learning rate: 0.01 #####
2025-06-25 05:02:42.730 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 4.4984, Val Loss: 4.4680
2025-06-25 05:02:42.730 INFO: Epoch 17, Train Loss: 4.4984, Val Loss: 4.4680
train_e/atom_mae: 0.008311
2025-06-25 05:02:42.731 INFO: train_e/atom_mae: 0.008311
train_e/atom_rmse: 0.010685
2025-06-25 05:02:42.731 INFO: train_e/atom_rmse: 0.010685
train_f_mae: 0.040036
2025-06-25 05:02:42.735 INFO: train_f_mae: 0.040036
train_f_rmse: 0.066032
2025-06-25 05:02:42.735 INFO: train_f_rmse: 0.066032
val_e/atom_mae: 0.008631
2025-06-25 05:02:42.737 INFO: val_e/atom_mae: 0.008631
val_e/atom_rmse: 0.010109
2025-06-25 05:02:42.738 INFO: val_e/atom_rmse: 0.010109
val_f_mae: 0.041175
2025-06-25 05:02:42.738 INFO: val_f_mae: 0.041175
val_f_rmse: 0.065911
2025-06-25 05:02:42.738 INFO: val_f_rmse: 0.065911
##### Step: 17 Learning rate: 0.01 #####
2025-06-25 05:03:12.984 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 4.8228, Val Loss: 4.6804
2025-06-25 05:03:12.984 INFO: Epoch 18, Train Loss: 4.8228, Val Loss: 4.6804
train_e/atom_mae: 0.008795
2025-06-25 05:03:12.985 INFO: train_e/atom_mae: 0.008795
train_e/atom_rmse: 0.010874
2025-06-25 05:03:12.985 INFO: train_e/atom_rmse: 0.010874
train_f_mae: 0.041178
2025-06-25 05:03:12.989 INFO: train_f_mae: 0.041178
train_f_rmse: 0.068409
2025-06-25 05:03:12.989 INFO: train_f_rmse: 0.068409
val_e/atom_mae: 0.004023
2025-06-25 05:03:12.991 INFO: val_e/atom_mae: 0.004023
val_e/atom_rmse: 0.004717
2025-06-25 05:03:12.992 INFO: val_e/atom_rmse: 0.004717
val_f_mae: 0.042320
2025-06-25 05:03:12.992 INFO: val_f_mae: 0.042320
val_f_rmse: 0.068217
2025-06-25 05:03:12.992 INFO: val_f_rmse: 0.068217
##### Step: 18 Learning rate: 0.01 #####
2025-06-25 05:03:43.282 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 4.4904, Val Loss: 4.6306
2025-06-25 05:03:43.282 INFO: Epoch 19, Train Loss: 4.4904, Val Loss: 4.6306
train_e/atom_mae: 0.007086
2025-06-25 05:03:43.283 INFO: train_e/atom_mae: 0.007086
train_e/atom_rmse: 0.008880
2025-06-25 05:03:43.283 INFO: train_e/atom_rmse: 0.008880
train_f_mae: 0.040025
2025-06-25 05:03:43.287 INFO: train_f_mae: 0.040025
train_f_rmse: 0.066295
2025-06-25 05:03:43.287 INFO: train_f_rmse: 0.066295
val_e/atom_mae: 0.002981
2025-06-25 05:03:43.289 INFO: val_e/atom_mae: 0.002981
val_e/atom_rmse: 0.003498
2025-06-25 05:03:43.290 INFO: val_e/atom_rmse: 0.003498
val_f_mae: 0.041719
2025-06-25 05:03:43.290 INFO: val_f_mae: 0.041719
val_f_rmse: 0.067940
2025-06-25 05:03:43.290 INFO: val_f_rmse: 0.067940
##### Step: 19 Learning rate: 0.01 #####
2025-06-25 05:04:13.679 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 4.7498, Val Loss: 4.7833
2025-06-25 05:04:13.679 INFO: Epoch 20, Train Loss: 4.7498, Val Loss: 4.7833
train_e/atom_mae: 0.008424
2025-06-25 05:04:13.680 INFO: train_e/atom_mae: 0.008424
train_e/atom_rmse: 0.010991
2025-06-25 05:04:13.680 INFO: train_e/atom_rmse: 0.010991
train_f_mae: 0.041110
2025-06-25 05:04:13.684 INFO: train_f_mae: 0.041110
train_f_rmse: 0.067850
2025-06-25 05:04:13.684 INFO: train_f_rmse: 0.067850
val_e/atom_mae: 0.007021
2025-06-25 05:04:13.686 INFO: val_e/atom_mae: 0.007021
val_e/atom_rmse: 0.007657
2025-06-25 05:04:13.687 INFO: val_e/atom_rmse: 0.007657
val_f_mae: 0.042044
2025-06-25 05:04:13.687 INFO: val_f_mae: 0.042044
val_f_rmse: 0.068646
2025-06-25 05:04:13.687 INFO: val_f_rmse: 0.068646
##### Step: 20 Learning rate: 0.005 #####
2025-06-25 05:04:43.943 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 4.1070, Val Loss: 4.2001
2025-06-25 05:04:43.944 INFO: Epoch 21, Train Loss: 4.1070, Val Loss: 4.2001
train_e/atom_mae: 0.005028
2025-06-25 05:04:43.944 INFO: train_e/atom_mae: 0.005028
train_e/atom_rmse: 0.006314
2025-06-25 05:04:43.945 INFO: train_e/atom_rmse: 0.006314
train_f_mae: 0.038436
2025-06-25 05:04:43.948 INFO: train_f_mae: 0.038436
train_f_rmse: 0.063709
2025-06-25 05:04:43.948 INFO: train_f_rmse: 0.063709
val_e/atom_mae: 0.007295
2025-06-25 05:04:43.951 INFO: val_e/atom_mae: 0.007295
val_e/atom_rmse: 0.007644
2025-06-25 05:04:43.951 INFO: val_e/atom_rmse: 0.007644
val_f_mae: 0.040006
2025-06-25 05:04:43.952 INFO: val_f_mae: 0.040006
val_f_rmse: 0.064260
2025-06-25 05:04:43.952 INFO: val_f_rmse: 0.064260
##### Step: 21 Learning rate: 0.005 #####
2025-06-25 05:05:14.403 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 4.1632, Val Loss: 3.9591
2025-06-25 05:05:14.404 INFO: Epoch 22, Train Loss: 4.1632, Val Loss: 3.9591
train_e/atom_mae: 0.006527
2025-06-25 05:05:14.405 INFO: train_e/atom_mae: 0.006527
train_e/atom_rmse: 0.008150
2025-06-25 05:05:14.405 INFO: train_e/atom_rmse: 0.008150
train_f_mae: 0.038324
2025-06-25 05:05:14.408 INFO: train_f_mae: 0.038324
train_f_rmse: 0.063897
2025-06-25 05:05:14.409 INFO: train_f_rmse: 0.063897
val_e/atom_mae: 0.002908
2025-06-25 05:05:14.411 INFO: val_e/atom_mae: 0.002908
val_e/atom_rmse: 0.003579
2025-06-25 05:05:14.412 INFO: val_e/atom_rmse: 0.003579
val_f_mae: 0.038084
2025-06-25 05:05:14.412 INFO: val_f_mae: 0.038084
val_f_rmse: 0.062798
2025-06-25 05:05:14.412 INFO: val_f_rmse: 0.062798
##### Step: 22 Learning rate: 0.005 #####
2025-06-25 05:05:44.700 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 4.1122, Val Loss: 4.0323
2025-06-25 05:05:44.700 INFO: Epoch 23, Train Loss: 4.1122, Val Loss: 4.0323
train_e/atom_mae: 0.005440
2025-06-25 05:05:44.701 INFO: train_e/atom_mae: 0.005440
train_e/atom_rmse: 0.006829
2025-06-25 05:05:44.701 INFO: train_e/atom_rmse: 0.006829
train_f_mae: 0.038378
2025-06-25 05:05:44.705 INFO: train_f_mae: 0.038378
train_f_rmse: 0.063685
2025-06-25 05:05:44.705 INFO: train_f_rmse: 0.063685
val_e/atom_mae: 0.002475
2025-06-25 05:05:44.708 INFO: val_e/atom_mae: 0.002475
val_e/atom_rmse: 0.003071
2025-06-25 05:05:44.708 INFO: val_e/atom_rmse: 0.003071
val_f_mae: 0.038389
2025-06-25 05:05:44.709 INFO: val_f_mae: 0.038389
val_f_rmse: 0.063411
2025-06-25 05:05:44.709 INFO: val_f_rmse: 0.063411
##### Step: 23 Learning rate: 0.005 #####
2025-06-25 05:06:15.308 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 4.0198, Val Loss: 4.1634
2025-06-25 05:06:15.308 INFO: Epoch 24, Train Loss: 4.0198, Val Loss: 4.1634
train_e/atom_mae: 0.004997
2025-06-25 05:06:15.309 INFO: train_e/atom_mae: 0.004997
train_e/atom_rmse: 0.006225
2025-06-25 05:06:15.309 INFO: train_e/atom_rmse: 0.006225
train_f_mae: 0.037888
2025-06-25 05:06:15.313 INFO: train_f_mae: 0.037888
train_f_rmse: 0.063031
2025-06-25 05:06:15.313 INFO: train_f_rmse: 0.063031
val_e/atom_mae: 0.005634
2025-06-25 05:06:15.315 INFO: val_e/atom_mae: 0.005634
val_e/atom_rmse: 0.006330
2025-06-25 05:06:15.316 INFO: val_e/atom_rmse: 0.006330
val_f_mae: 0.038968
2025-06-25 05:06:15.316 INFO: val_f_mae: 0.038968
val_f_rmse: 0.064148
2025-06-25 05:06:15.316 INFO: val_f_rmse: 0.064148
##### Step: 24 Learning rate: 0.005 #####
2025-06-25 05:06:45.680 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 4.0926, Val Loss: 4.0385
2025-06-25 05:06:45.680 INFO: Epoch 25, Train Loss: 4.0926, Val Loss: 4.0385
train_e/atom_mae: 0.005342
2025-06-25 05:06:45.681 INFO: train_e/atom_mae: 0.005342
train_e/atom_rmse: 0.006586
2025-06-25 05:06:45.681 INFO: train_e/atom_rmse: 0.006586
train_f_mae: 0.038290
2025-06-25 05:06:45.685 INFO: train_f_mae: 0.038290
train_f_rmse: 0.063562
2025-06-25 05:06:45.685 INFO: train_f_rmse: 0.063562
val_e/atom_mae: 0.004998
2025-06-25 05:06:45.688 INFO: val_e/atom_mae: 0.004998
val_e/atom_rmse: 0.005791
2025-06-25 05:06:45.688 INFO: val_e/atom_rmse: 0.005791
val_f_mae: 0.037967
2025-06-25 05:06:45.689 INFO: val_f_mae: 0.037967
val_f_rmse: 0.063229
2025-06-25 05:06:45.689 INFO: val_f_rmse: 0.063229
##### Step: 25 Learning rate: 0.005 #####
2025-06-25 05:07:16.017 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 4.1181, Val Loss: 4.1089
2025-06-25 05:07:16.018 INFO: Epoch 26, Train Loss: 4.1181, Val Loss: 4.1089
train_e/atom_mae: 0.005238
2025-06-25 05:07:16.018 INFO: train_e/atom_mae: 0.005238
train_e/atom_rmse: 0.006877
2025-06-25 05:07:16.019 INFO: train_e/atom_rmse: 0.006877
train_f_mae: 0.038489
2025-06-25 05:07:16.022 INFO: train_f_mae: 0.038489
train_f_rmse: 0.063725
2025-06-25 05:07:16.022 INFO: train_f_rmse: 0.063725
val_e/atom_mae: 0.003132
2025-06-25 05:07:16.025 INFO: val_e/atom_mae: 0.003132
val_e/atom_rmse: 0.003911
2025-06-25 05:07:16.025 INFO: val_e/atom_rmse: 0.003911
val_f_mae: 0.038589
2025-06-25 05:07:16.026 INFO: val_f_mae: 0.038589
val_f_rmse: 0.063956
2025-06-25 05:07:16.026 INFO: val_f_rmse: 0.063956
##### Step: 26 Learning rate: 0.005 #####
2025-06-25 05:07:46.497 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 4.1153, Val Loss: 4.1800
2025-06-25 05:07:46.497 INFO: Epoch 27, Train Loss: 4.1153, Val Loss: 4.1800
train_e/atom_mae: 0.005756
2025-06-25 05:07:46.498 INFO: train_e/atom_mae: 0.005756
train_e/atom_rmse: 0.007177
2025-06-25 05:07:46.499 INFO: train_e/atom_rmse: 0.007177
train_f_mae: 0.038269
2025-06-25 05:07:46.502 INFO: train_f_mae: 0.038269
train_f_rmse: 0.063663
2025-06-25 05:07:46.502 INFO: train_f_rmse: 0.063663
val_e/atom_mae: 0.009336
2025-06-25 05:07:46.505 INFO: val_e/atom_mae: 0.009336
val_e/atom_rmse: 0.009878
2025-06-25 05:07:46.505 INFO: val_e/atom_rmse: 0.009878
val_f_mae: 0.037974
2025-06-25 05:07:46.506 INFO: val_f_mae: 0.037974
val_f_rmse: 0.063734
2025-06-25 05:07:46.506 INFO: val_f_rmse: 0.063734
##### Step: 27 Learning rate: 0.005 #####
2025-06-25 05:08:16.775 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 4.1034, Val Loss: 4.4352
2025-06-25 05:08:16.775 INFO: Epoch 28, Train Loss: 4.1034, Val Loss: 4.4352
train_e/atom_mae: 0.005723
2025-06-25 05:08:16.776 INFO: train_e/atom_mae: 0.005723
train_e/atom_rmse: 0.007101
2025-06-25 05:08:16.776 INFO: train_e/atom_rmse: 0.007101
train_f_mae: 0.038461
2025-06-25 05:08:16.780 INFO: train_f_mae: 0.038461
train_f_rmse: 0.063580
2025-06-25 05:08:16.780 INFO: train_f_rmse: 0.063580
val_e/atom_mae: 0.009890
2025-06-25 05:08:16.783 INFO: val_e/atom_mae: 0.009890
val_e/atom_rmse: 0.010201
2025-06-25 05:08:16.783 INFO: val_e/atom_rmse: 0.010201
val_f_mae: 0.038534
2025-06-25 05:08:16.783 INFO: val_f_mae: 0.038534
val_f_rmse: 0.065645
2025-06-25 05:08:16.784 INFO: val_f_rmse: 0.065645
##### Step: 28 Learning rate: 0.005 #####
2025-06-25 05:08:47.227 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 4.1064, Val Loss: 4.5503
2025-06-25 05:08:47.228 INFO: Epoch 29, Train Loss: 4.1064, Val Loss: 4.5503
train_e/atom_mae: 0.004805
2025-06-25 05:08:47.229 INFO: train_e/atom_mae: 0.004805
train_e/atom_rmse: 0.006168
2025-06-25 05:08:47.229 INFO: train_e/atom_rmse: 0.006168
train_f_mae: 0.038073
2025-06-25 05:08:47.232 INFO: train_f_mae: 0.038073
train_f_rmse: 0.063721
2025-06-25 05:08:47.232 INFO: train_f_rmse: 0.063721
val_e/atom_mae: 0.006679
2025-06-25 05:08:47.235 INFO: val_e/atom_mae: 0.006679
val_e/atom_rmse: 0.007245
2025-06-25 05:08:47.235 INFO: val_e/atom_rmse: 0.007245
val_f_mae: 0.040705
2025-06-25 05:08:47.236 INFO: val_f_mae: 0.040705
val_f_rmse: 0.066984
2025-06-25 05:08:47.236 INFO: val_f_rmse: 0.066984
##### Step: 29 Learning rate: 0.005 #####
2025-06-25 05:09:17.553 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 3.9986, Val Loss: 3.9322
2025-06-25 05:09:17.553 INFO: Epoch 30, Train Loss: 3.9986, Val Loss: 3.9322
train_e/atom_mae: 0.005436
2025-06-25 05:09:17.554 INFO: train_e/atom_mae: 0.005436
train_e/atom_rmse: 0.006857
2025-06-25 05:09:17.554 INFO: train_e/atom_rmse: 0.006857
train_f_mae: 0.037902
2025-06-25 05:09:17.558 INFO: train_f_mae: 0.037902
train_f_rmse: 0.062783
2025-06-25 05:09:17.558 INFO: train_f_rmse: 0.062783
val_e/atom_mae: 0.004448
2025-06-25 05:09:17.560 INFO: val_e/atom_mae: 0.004448
val_e/atom_rmse: 0.005228
2025-06-25 05:09:17.561 INFO: val_e/atom_rmse: 0.005228
val_f_mae: 0.037605
2025-06-25 05:09:17.561 INFO: val_f_mae: 0.037605
val_f_rmse: 0.062443
2025-06-25 05:09:17.561 INFO: val_f_rmse: 0.062443
##### Step: 30 Learning rate: 0.005 #####
2025-06-25 05:09:47.934 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 4.0736, Val Loss: 4.3324
2025-06-25 05:09:47.934 INFO: Epoch 31, Train Loss: 4.0736, Val Loss: 4.3324
train_e/atom_mae: 0.004815
2025-06-25 05:09:47.935 INFO: train_e/atom_mae: 0.004815
train_e/atom_rmse: 0.005931
2025-06-25 05:09:47.935 INFO: train_e/atom_rmse: 0.005931
train_f_mae: 0.038188
2025-06-25 05:09:47.939 INFO: train_f_mae: 0.038188
train_f_rmse: 0.063491
2025-06-25 05:09:47.939 INFO: train_f_rmse: 0.063491
val_e/atom_mae: 0.007305
2025-06-25 05:09:47.942 INFO: val_e/atom_mae: 0.007305
val_e/atom_rmse: 0.008361
2025-06-25 05:09:47.942 INFO: val_e/atom_rmse: 0.008361
val_f_mae: 0.039335
2025-06-25 05:09:47.943 INFO: val_f_mae: 0.039335
val_f_rmse: 0.065176
2025-06-25 05:09:47.943 INFO: val_f_rmse: 0.065176
##### Step: 31 Learning rate: 0.005 #####
2025-06-25 05:10:18.357 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 4.0109, Val Loss: 3.9751
2025-06-25 05:10:18.357 INFO: Epoch 32, Train Loss: 4.0109, Val Loss: 3.9751
train_e/atom_mae: 0.004439
2025-06-25 05:10:18.358 INFO: train_e/atom_mae: 0.004439
train_e/atom_rmse: 0.005610
2025-06-25 05:10:18.358 INFO: train_e/atom_rmse: 0.005610
train_f_mae: 0.037850
2025-06-25 05:10:18.362 INFO: train_f_mae: 0.037850
train_f_rmse: 0.063030
2025-06-25 05:10:18.362 INFO: train_f_rmse: 0.063030
val_e/atom_mae: 0.004248
2025-06-25 05:10:18.365 INFO: val_e/atom_mae: 0.004248
val_e/atom_rmse: 0.004874
2025-06-25 05:10:18.365 INFO: val_e/atom_rmse: 0.004874
val_f_mae: 0.038201
2025-06-25 05:10:18.365 INFO: val_f_mae: 0.038201
val_f_rmse: 0.062820
2025-06-25 05:10:18.366 INFO: val_f_rmse: 0.062820
##### Step: 32 Learning rate: 0.005 #####
2025-06-25 05:10:48.654 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 3.9466, Val Loss: 4.1131
2025-06-25 05:10:48.654 INFO: Epoch 33, Train Loss: 3.9466, Val Loss: 4.1131
train_e/atom_mae: 0.004806
2025-06-25 05:10:48.655 INFO: train_e/atom_mae: 0.004806
train_e/atom_rmse: 0.006052
2025-06-25 05:10:48.655 INFO: train_e/atom_rmse: 0.006052
train_f_mae: 0.037510
2025-06-25 05:10:48.659 INFO: train_f_mae: 0.037510
train_f_rmse: 0.062468
2025-06-25 05:10:48.659 INFO: train_f_rmse: 0.062468
val_e/atom_mae: 0.008845
2025-06-25 05:10:48.662 INFO: val_e/atom_mae: 0.008845
val_e/atom_rmse: 0.009880
2025-06-25 05:10:48.662 INFO: val_e/atom_rmse: 0.009880
val_f_mae: 0.038703
2025-06-25 05:10:48.662 INFO: val_f_mae: 0.038703
val_f_rmse: 0.063206
2025-06-25 05:10:48.662 INFO: val_f_rmse: 0.063206
##### Step: 33 Learning rate: 0.005 #####
2025-06-25 05:11:19.033 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 4.1199, Val Loss: 4.1784
2025-06-25 05:11:19.033 INFO: Epoch 34, Train Loss: 4.1199, Val Loss: 4.1784
train_e/atom_mae: 0.005379
2025-06-25 05:11:19.034 INFO: train_e/atom_mae: 0.005379
train_e/atom_rmse: 0.006590
2025-06-25 05:11:19.034 INFO: train_e/atom_rmse: 0.006590
train_f_mae: 0.037610
2025-06-25 05:11:19.038 INFO: train_f_mae: 0.037610
train_f_rmse: 0.063776
2025-06-25 05:11:19.038 INFO: train_f_rmse: 0.063776
val_e/atom_mae: 0.004029
2025-06-25 05:11:19.040 INFO: val_e/atom_mae: 0.004029
val_e/atom_rmse: 0.004777
2025-06-25 05:11:19.041 INFO: val_e/atom_rmse: 0.004777
val_f_mae: 0.039351
2025-06-25 05:11:19.041 INFO: val_f_mae: 0.039351
val_f_rmse: 0.064427
2025-06-25 05:11:19.041 INFO: val_f_rmse: 0.064427
##### Step: 34 Learning rate: 0.005 #####
2025-06-25 05:11:49.254 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 4.0423, Val Loss: 4.1403
2025-06-25 05:11:49.254 INFO: Epoch 35, Train Loss: 4.0423, Val Loss: 4.1403
train_e/atom_mae: 0.004695
2025-06-25 05:11:49.255 INFO: train_e/atom_mae: 0.004695
train_e/atom_rmse: 0.006001
2025-06-25 05:11:49.255 INFO: train_e/atom_rmse: 0.006001
train_f_mae: 0.037802
2025-06-25 05:11:49.259 INFO: train_f_mae: 0.037802
train_f_rmse: 0.063235
2025-06-25 05:11:49.259 INFO: train_f_rmse: 0.063235
val_e/atom_mae: 0.005370
2025-06-25 05:11:49.261 INFO: val_e/atom_mae: 0.005370
val_e/atom_rmse: 0.006533
2025-06-25 05:11:49.262 INFO: val_e/atom_rmse: 0.006533
val_f_mae: 0.039870
2025-06-25 05:11:49.262 INFO: val_f_mae: 0.039870
val_f_rmse: 0.063943
2025-06-25 05:11:49.262 INFO: val_f_rmse: 0.063943
##### Step: 35 Learning rate: 0.005 #####
2025-06-25 05:12:19.643 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 4.0002, Val Loss: 4.1269
2025-06-25 05:12:19.643 INFO: Epoch 36, Train Loss: 4.0002, Val Loss: 4.1269
train_e/atom_mae: 0.005899
2025-06-25 05:12:19.644 INFO: train_e/atom_mae: 0.005899
train_e/atom_rmse: 0.007277
2025-06-25 05:12:19.644 INFO: train_e/atom_rmse: 0.007277
train_f_mae: 0.037639
2025-06-25 05:12:19.648 INFO: train_f_mae: 0.037639
train_f_rmse: 0.062739
2025-06-25 05:12:19.648 INFO: train_f_rmse: 0.062739
val_e/atom_mae: 0.004695
2025-06-25 05:12:19.650 INFO: val_e/atom_mae: 0.004695
val_e/atom_rmse: 0.005928
2025-06-25 05:12:19.651 INFO: val_e/atom_rmse: 0.005928
val_f_mae: 0.039119
2025-06-25 05:12:19.651 INFO: val_f_mae: 0.039119
val_f_rmse: 0.063909
2025-06-25 05:12:19.651 INFO: val_f_rmse: 0.063909
##### Step: 36 Learning rate: 0.005 #####
2025-06-25 05:12:49.953 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 4.0608, Val Loss: 4.0604
2025-06-25 05:12:49.954 INFO: Epoch 37, Train Loss: 4.0608, Val Loss: 4.0604
train_e/atom_mae: 0.005006
2025-06-25 05:12:49.955 INFO: train_e/atom_mae: 0.005006
train_e/atom_rmse: 0.006443
2025-06-25 05:12:49.955 INFO: train_e/atom_rmse: 0.006443
train_f_mae: 0.037910
2025-06-25 05:12:49.958 INFO: train_f_mae: 0.037910
train_f_rmse: 0.063329
2025-06-25 05:12:49.959 INFO: train_f_rmse: 0.063329
val_e/atom_mae: 0.004477
2025-06-25 05:12:49.961 INFO: val_e/atom_mae: 0.004477
val_e/atom_rmse: 0.005025
2025-06-25 05:12:49.962 INFO: val_e/atom_rmse: 0.005025
val_f_mae: 0.037979
2025-06-25 05:12:49.962 INFO: val_f_mae: 0.037979
val_f_rmse: 0.063481
2025-06-25 05:12:49.962 INFO: val_f_rmse: 0.063481
##### Step: 37 Learning rate: 0.005 #####
2025-06-25 05:13:20.280 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 3.9970, Val Loss: 4.2223
2025-06-25 05:13:20.280 INFO: Epoch 38, Train Loss: 3.9970, Val Loss: 4.2223
train_e/atom_mae: 0.005130
2025-06-25 05:13:20.281 INFO: train_e/atom_mae: 0.005130
train_e/atom_rmse: 0.006543
2025-06-25 05:13:20.282 INFO: train_e/atom_rmse: 0.006543
train_f_mae: 0.037862
2025-06-25 05:13:20.285 INFO: train_f_mae: 0.037862
train_f_rmse: 0.062811
2025-06-25 05:13:20.285 INFO: train_f_rmse: 0.062811
val_e/atom_mae: 0.005032
2025-06-25 05:13:20.288 INFO: val_e/atom_mae: 0.005032
val_e/atom_rmse: 0.005524
2025-06-25 05:13:20.288 INFO: val_e/atom_rmse: 0.005524
val_f_mae: 0.038357
2025-06-25 05:13:20.289 INFO: val_f_mae: 0.038357
val_f_rmse: 0.064694
2025-06-25 05:13:20.289 INFO: val_f_rmse: 0.064694
##### Step: 38 Learning rate: 0.005 #####
2025-06-25 05:13:50.702 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 4.0509, Val Loss: 4.2098
2025-06-25 05:13:50.702 INFO: Epoch 39, Train Loss: 4.0509, Val Loss: 4.2098
train_e/atom_mae: 0.006800
2025-06-25 05:13:50.703 INFO: train_e/atom_mae: 0.006800
train_e/atom_rmse: 0.008357
2025-06-25 05:13:50.703 INFO: train_e/atom_rmse: 0.008357
train_f_mae: 0.037877
2025-06-25 05:13:50.707 INFO: train_f_mae: 0.037877
train_f_rmse: 0.062979
2025-06-25 05:13:50.707 INFO: train_f_rmse: 0.062979
val_e/atom_mae: 0.009769
2025-06-25 05:13:50.710 INFO: val_e/atom_mae: 0.009769
val_e/atom_rmse: 0.010704
2025-06-25 05:13:50.710 INFO: val_e/atom_rmse: 0.010704
val_f_mae: 0.038947
2025-06-25 05:13:50.710 INFO: val_f_mae: 0.038947
val_f_rmse: 0.063806
2025-06-25 05:13:50.711 INFO: val_f_rmse: 0.063806
##### Step: 39 Learning rate: 0.005 #####
2025-06-25 05:14:20.918 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 3.9848, Val Loss: 4.5116
2025-06-25 05:14:20.918 INFO: Epoch 40, Train Loss: 3.9848, Val Loss: 4.5116
train_e/atom_mae: 0.004937
2025-06-25 05:14:20.919 INFO: train_e/atom_mae: 0.004937
train_e/atom_rmse: 0.006121
2025-06-25 05:14:20.919 INFO: train_e/atom_rmse: 0.006121
train_f_mae: 0.037731
2025-06-25 05:14:20.922 INFO: train_f_mae: 0.037731
train_f_rmse: 0.062765
2025-06-25 05:14:20.923 INFO: train_f_rmse: 0.062765
val_e/atom_mae: 0.005851
2025-06-25 05:14:20.925 INFO: val_e/atom_mae: 0.005851
val_e/atom_rmse: 0.006983
2025-06-25 05:14:20.926 INFO: val_e/atom_rmse: 0.006983
val_f_mae: 0.041958
2025-06-25 05:14:20.926 INFO: val_f_mae: 0.041958
val_f_rmse: 0.066728
2025-06-25 05:14:20.926 INFO: val_f_rmse: 0.066728
2025-06-25 05:14:20.931 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-25 05:14:51.300 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 3.8135, Val Loss: 3.7666
2025-06-25 05:14:51.300 INFO: Epoch 1, Train Loss: 3.8135, Val Loss: 3.7666
train_e/atom_mae: 0.004118
2025-06-25 05:14:51.301 INFO: train_e/atom_mae: 0.004118
train_e/atom_rmse: 0.005135
2025-06-25 05:14:51.301 INFO: train_e/atom_rmse: 0.005135
train_f_mae: 0.036496
2025-06-25 05:14:51.305 INFO: train_f_mae: 0.036496
train_f_rmse: 0.061495
2025-06-25 05:14:51.305 INFO: train_f_rmse: 0.061495
val_e/atom_mae: 0.003131
2025-06-25 05:14:51.307 INFO: val_e/atom_mae: 0.003131
val_e/atom_rmse: 0.003669
2025-06-25 05:14:51.308 INFO: val_e/atom_rmse: 0.003669
val_f_mae: 0.036810
2025-06-25 05:14:51.308 INFO: val_f_mae: 0.036810
val_f_rmse: 0.061240
2025-06-25 05:14:51.308 INFO: val_f_rmse: 0.061240
##### Step: 1 Learning rate: 0.004 #####
2025-06-25 05:15:21.493 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 3.9115, Val Loss: 4.0325
2025-06-25 05:15:21.494 INFO: Epoch 2, Train Loss: 3.9115, Val Loss: 4.0325
train_e/atom_mae: 0.004855
2025-06-25 05:15:21.494 INFO: train_e/atom_mae: 0.004855
train_e/atom_rmse: 0.006123
2025-06-25 05:15:21.495 INFO: train_e/atom_rmse: 0.006123
train_f_mae: 0.037268
2025-06-25 05:15:21.498 INFO: train_f_mae: 0.037268
train_f_rmse: 0.062178
2025-06-25 05:15:21.498 INFO: train_f_rmse: 0.062178
val_e/atom_mae: 0.003506
2025-06-25 05:15:21.501 INFO: val_e/atom_mae: 0.003506
val_e/atom_rmse: 0.004166
2025-06-25 05:15:21.501 INFO: val_e/atom_rmse: 0.004166
val_f_mae: 0.038877
2025-06-25 05:15:21.502 INFO: val_f_mae: 0.038877
val_f_rmse: 0.063336
2025-06-25 05:15:21.502 INFO: val_f_rmse: 0.063336
##### Step: 2 Learning rate: 0.006 #####
2025-06-25 05:15:51.811 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 4.0023, Val Loss: 4.4591
2025-06-25 05:15:51.811 INFO: Epoch 3, Train Loss: 4.0023, Val Loss: 4.4591
train_e/atom_mae: 0.004792
2025-06-25 05:15:51.812 INFO: train_e/atom_mae: 0.004792
train_e/atom_rmse: 0.006024
2025-06-25 05:15:51.812 INFO: train_e/atom_rmse: 0.006024
train_f_mae: 0.037776
2025-06-25 05:15:51.816 INFO: train_f_mae: 0.037776
train_f_rmse: 0.062916
2025-06-25 05:15:51.816 INFO: train_f_rmse: 0.062916
val_e/atom_mae: 0.006005
2025-06-25 05:15:51.818 INFO: val_e/atom_mae: 0.006005
val_e/atom_rmse: 0.006522
2025-06-25 05:15:51.819 INFO: val_e/atom_rmse: 0.006522
val_f_mae: 0.039343
2025-06-25 05:15:51.819 INFO: val_f_mae: 0.039343
val_f_rmse: 0.066390
2025-06-25 05:15:51.819 INFO: val_f_rmse: 0.066390
##### Step: 3 Learning rate: 0.008 #####
2025-06-25 05:16:22.102 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 4.5160, Val Loss: 4.2505
2025-06-25 05:16:22.102 INFO: Epoch 4, Train Loss: 4.5160, Val Loss: 4.2505
train_e/atom_mae: 0.007918
2025-06-25 05:16:22.103 INFO: train_e/atom_mae: 0.007918
train_e/atom_rmse: 0.009866
2025-06-25 05:16:22.104 INFO: train_e/atom_rmse: 0.009866
train_f_mae: 0.040457
2025-06-25 05:16:22.107 INFO: train_f_mae: 0.040457
train_f_rmse: 0.066319
2025-06-25 05:16:22.107 INFO: train_f_rmse: 0.066319
val_e/atom_mae: 0.007145
2025-06-25 05:16:22.110 INFO: val_e/atom_mae: 0.007145
val_e/atom_rmse: 0.007522
2025-06-25 05:16:22.110 INFO: val_e/atom_rmse: 0.007522
val_f_mae: 0.039317
2025-06-25 05:16:22.111 INFO: val_f_mae: 0.039317
val_f_rmse: 0.064669
2025-06-25 05:16:22.111 INFO: val_f_rmse: 0.064669
##### Step: 4 Learning rate: 0.01 #####
2025-06-25 05:16:52.337 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 4.5652, Val Loss: 4.1728
2025-06-25 05:16:52.337 INFO: Epoch 5, Train Loss: 4.5652, Val Loss: 4.1728
train_e/atom_mae: 0.007784
2025-06-25 05:16:52.338 INFO: train_e/atom_mae: 0.007784
train_e/atom_rmse: 0.009802
2025-06-25 05:16:52.338 INFO: train_e/atom_rmse: 0.009802
train_f_mae: 0.039396
2025-06-25 05:16:52.342 INFO: train_f_mae: 0.039396
train_f_rmse: 0.066700
2025-06-25 05:16:52.342 INFO: train_f_rmse: 0.066700
val_e/atom_mae: 0.002883
2025-06-25 05:16:52.345 INFO: val_e/atom_mae: 0.002883
val_e/atom_rmse: 0.003588
2025-06-25 05:16:52.345 INFO: val_e/atom_rmse: 0.003588
val_f_mae: 0.039287
2025-06-25 05:16:52.346 INFO: val_f_mae: 0.039287
val_f_rmse: 0.064476
2025-06-25 05:16:52.346 INFO: val_f_rmse: 0.064476
##### Step: 5 Learning rate: 0.01 #####
2025-06-25 05:17:22.718 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 4.5679, Val Loss: 4.7722
2025-06-25 05:17:22.718 INFO: Epoch 6, Train Loss: 4.5679, Val Loss: 4.7722
train_e/atom_mae: 0.006706
2025-06-25 05:17:22.719 INFO: train_e/atom_mae: 0.006706
train_e/atom_rmse: 0.008669
2025-06-25 05:17:22.719 INFO: train_e/atom_rmse: 0.008669
train_f_mae: 0.040652
2025-06-25 05:17:22.723 INFO: train_f_mae: 0.040652
train_f_rmse: 0.066910
2025-06-25 05:17:22.723 INFO: train_f_rmse: 0.066910
val_e/atom_mae: 0.003931
2025-06-25 05:17:22.725 INFO: val_e/atom_mae: 0.003931
val_e/atom_rmse: 0.004713
2025-06-25 05:17:22.726 INFO: val_e/atom_rmse: 0.004713
val_f_mae: 0.041293
2025-06-25 05:17:22.726 INFO: val_f_mae: 0.041293
val_f_rmse: 0.068886
2025-06-25 05:17:22.726 INFO: val_f_rmse: 0.068886
##### Step: 6 Learning rate: 0.01 #####
2025-06-25 05:17:52.884 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 4.4732, Val Loss: 4.1647
2025-06-25 05:17:52.884 INFO: Epoch 7, Train Loss: 4.4732, Val Loss: 4.1647
train_e/atom_mae: 0.008255
2025-06-25 05:17:52.885 INFO: train_e/atom_mae: 0.008255
train_e/atom_rmse: 0.010222
2025-06-25 05:17:52.886 INFO: train_e/atom_rmse: 0.010222
train_f_mae: 0.040050
2025-06-25 05:17:52.889 INFO: train_f_mae: 0.040050
train_f_rmse: 0.065930
2025-06-25 05:17:52.889 INFO: train_f_rmse: 0.065930
val_e/atom_mae: 0.002096
2025-06-25 05:17:52.892 INFO: val_e/atom_mae: 0.002096
val_e/atom_rmse: 0.002346
2025-06-25 05:17:52.892 INFO: val_e/atom_rmse: 0.002346
val_f_mae: 0.038697
2025-06-25 05:17:52.893 INFO: val_f_mae: 0.038697
val_f_rmse: 0.064483
2025-06-25 05:17:52.893 INFO: val_f_rmse: 0.064483
##### Step: 7 Learning rate: 0.01 #####
2025-06-25 05:18:23.279 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 4.4638, Val Loss: 4.3469
2025-06-25 05:18:23.279 INFO: Epoch 8, Train Loss: 4.4638, Val Loss: 4.3469
train_e/atom_mae: 0.006894
2025-06-25 05:18:23.280 INFO: train_e/atom_mae: 0.006894
train_e/atom_rmse: 0.008841
2025-06-25 05:18:23.280 INFO: train_e/atom_rmse: 0.008841
train_f_mae: 0.040369
2025-06-25 05:18:23.284 INFO: train_f_mae: 0.040369
train_f_rmse: 0.066100
2025-06-25 05:18:23.284 INFO: train_f_rmse: 0.066100
val_e/atom_mae: 0.002362
2025-06-25 05:18:23.286 INFO: val_e/atom_mae: 0.002362
val_e/atom_rmse: 0.002981
2025-06-25 05:18:23.287 INFO: val_e/atom_rmse: 0.002981
val_f_mae: 0.040576
2025-06-25 05:18:23.287 INFO: val_f_mae: 0.040576
val_f_rmse: 0.065850
2025-06-25 05:18:23.287 INFO: val_f_rmse: 0.065850
##### Step: 8 Learning rate: 0.01 #####
2025-06-25 05:18:53.579 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 4.5464, Val Loss: 4.1708
2025-06-25 05:18:53.580 INFO: Epoch 9, Train Loss: 4.5464, Val Loss: 4.1708
train_e/atom_mae: 0.008275
2025-06-25 05:18:53.581 INFO: train_e/atom_mae: 0.008275
train_e/atom_rmse: 0.010500
2025-06-25 05:18:53.581 INFO: train_e/atom_rmse: 0.010500
train_f_mae: 0.039909
2025-06-25 05:18:53.585 INFO: train_f_mae: 0.039909
train_f_rmse: 0.066430
2025-06-25 05:18:53.585 INFO: train_f_rmse: 0.066430
val_e/atom_mae: 0.005329
2025-06-25 05:18:53.587 INFO: val_e/atom_mae: 0.005329
val_e/atom_rmse: 0.005799
2025-06-25 05:18:53.588 INFO: val_e/atom_rmse: 0.005799
val_f_mae: 0.039980
2025-06-25 05:18:53.588 INFO: val_f_mae: 0.039980
val_f_rmse: 0.064266
2025-06-25 05:18:53.588 INFO: val_f_rmse: 0.064266
##### Step: 9 Learning rate: 0.01 #####
2025-06-25 05:19:23.967 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 4.4616, Val Loss: 4.1557
2025-06-25 05:19:23.967 INFO: Epoch 10, Train Loss: 4.4616, Val Loss: 4.1557
train_e/atom_mae: 0.007994
2025-06-25 05:19:23.968 INFO: train_e/atom_mae: 0.007994
train_e/atom_rmse: 0.010072
2025-06-25 05:19:23.968 INFO: train_e/atom_rmse: 0.010072
train_f_mae: 0.040181
2025-06-25 05:19:23.972 INFO: train_f_mae: 0.040181
train_f_rmse: 0.065870
2025-06-25 05:19:23.972 INFO: train_f_rmse: 0.065870
val_e/atom_mae: 0.006760
2025-06-25 05:19:23.974 INFO: val_e/atom_mae: 0.006760
val_e/atom_rmse: 0.007200
2025-06-25 05:19:23.975 INFO: val_e/atom_rmse: 0.007200
val_f_mae: 0.038844
2025-06-25 05:19:23.975 INFO: val_f_mae: 0.038844
val_f_rmse: 0.063976
2025-06-25 05:19:23.975 INFO: val_f_rmse: 0.063976
##### Step: 10 Learning rate: 0.01 #####
2025-06-25 05:19:54.338 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 4.8606, Val Loss: 4.6253
2025-06-25 05:19:54.339 INFO: Epoch 11, Train Loss: 4.8606, Val Loss: 4.6253
train_e/atom_mae: 0.007938
2025-06-25 05:19:54.340 INFO: train_e/atom_mae: 0.007938
train_e/atom_rmse: 0.010039
2025-06-25 05:19:54.340 INFO: train_e/atom_rmse: 0.010039
train_f_mae: 0.040647
2025-06-25 05:19:54.343 INFO: train_f_mae: 0.040647
train_f_rmse: 0.068838
2025-06-25 05:19:54.344 INFO: train_f_rmse: 0.068838
val_e/atom_mae: 0.009435
2025-06-25 05:19:54.346 INFO: val_e/atom_mae: 0.009435
val_e/atom_rmse: 0.011586
2025-06-25 05:19:54.347 INFO: val_e/atom_rmse: 0.011586
val_f_mae: 0.040365
2025-06-25 05:19:54.347 INFO: val_f_mae: 0.040365
val_f_rmse: 0.066805
2025-06-25 05:19:54.347 INFO: val_f_rmse: 0.066805
##### Step: 11 Learning rate: 0.01 #####
2025-06-25 05:20:24.656 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 4.4055, Val Loss: 4.3783
2025-06-25 05:20:24.656 INFO: Epoch 12, Train Loss: 4.4055, Val Loss: 4.3783
train_e/atom_mae: 0.006175
2025-06-25 05:20:24.657 INFO: train_e/atom_mae: 0.006175
train_e/atom_rmse: 0.008157
2025-06-25 05:20:24.657 INFO: train_e/atom_rmse: 0.008157
train_f_mae: 0.039706
2025-06-25 05:20:24.660 INFO: train_f_mae: 0.039706
train_f_rmse: 0.065764
2025-06-25 05:20:24.661 INFO: train_f_rmse: 0.065764
val_e/atom_mae: 0.004962
2025-06-25 05:20:24.663 INFO: val_e/atom_mae: 0.004962
val_e/atom_rmse: 0.005661
2025-06-25 05:20:24.664 INFO: val_e/atom_rmse: 0.005661
val_f_mae: 0.041146
2025-06-25 05:20:24.664 INFO: val_f_mae: 0.041146
val_f_rmse: 0.065875
2025-06-25 05:20:24.664 INFO: val_f_rmse: 0.065875
##### Step: 12 Learning rate: 0.01 #####
2025-06-25 05:20:55.072 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 4.6442, Val Loss: 4.5515
2025-06-25 05:20:55.072 INFO: Epoch 13, Train Loss: 4.6442, Val Loss: 4.5515
train_e/atom_mae: 0.008028
2025-06-25 05:20:55.073 INFO: train_e/atom_mae: 0.008028
train_e/atom_rmse: 0.009817
2025-06-25 05:20:55.073 INFO: train_e/atom_rmse: 0.009817
train_f_mae: 0.040961
2025-06-25 05:20:55.077 INFO: train_f_mae: 0.040961
train_f_rmse: 0.067288
2025-06-25 05:20:55.077 INFO: train_f_rmse: 0.067288
val_e/atom_mae: 0.012485
2025-06-25 05:20:55.080 INFO: val_e/atom_mae: 0.012485
val_e/atom_rmse: 0.013585
2025-06-25 05:20:55.080 INFO: val_e/atom_rmse: 0.013585
val_f_mae: 0.040284
2025-06-25 05:20:55.080 INFO: val_f_mae: 0.040284
val_f_rmse: 0.065789
2025-06-25 05:20:55.080 INFO: val_f_rmse: 0.065789
##### Step: 13 Learning rate: 0.01 #####
2025-06-25 05:21:25.322 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 4.2932, Val Loss: 4.9488
2025-06-25 05:21:25.322 INFO: Epoch 14, Train Loss: 4.2932, Val Loss: 4.9488
train_e/atom_mae: 0.007556
2025-06-25 05:21:25.323 INFO: train_e/atom_mae: 0.007556
train_e/atom_rmse: 0.009501
2025-06-25 05:21:25.323 INFO: train_e/atom_rmse: 0.009501
train_f_mae: 0.039195
2025-06-25 05:21:25.326 INFO: train_f_mae: 0.039195
train_f_rmse: 0.064684
2025-06-25 05:21:25.327 INFO: train_f_rmse: 0.064684
val_e/atom_mae: 0.015448
2025-06-25 05:21:25.329 INFO: val_e/atom_mae: 0.015448
val_e/atom_rmse: 0.016591
2025-06-25 05:21:25.330 INFO: val_e/atom_rmse: 0.016591
val_f_mae: 0.040732
2025-06-25 05:21:25.330 INFO: val_f_mae: 0.040732
val_f_rmse: 0.067939
2025-06-25 05:21:25.330 INFO: val_f_rmse: 0.067939
##### Step: 14 Learning rate: 0.01 #####
2025-06-25 05:21:55.733 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 4.4005, Val Loss: 4.0395
2025-06-25 05:21:55.733 INFO: Epoch 15, Train Loss: 4.4005, Val Loss: 4.0395
train_e/atom_mae: 0.007837
2025-06-25 05:21:55.734 INFO: train_e/atom_mae: 0.007837
train_e/atom_rmse: 0.009646
2025-06-25 05:21:55.734 INFO: train_e/atom_rmse: 0.009646
train_f_mae: 0.039375
2025-06-25 05:21:55.738 INFO: train_f_mae: 0.039375
train_f_rmse: 0.065482
2025-06-25 05:21:55.738 INFO: train_f_rmse: 0.065482
val_e/atom_mae: 0.002520
2025-06-25 05:21:55.741 INFO: val_e/atom_mae: 0.002520
val_e/atom_rmse: 0.003142
2025-06-25 05:21:55.741 INFO: val_e/atom_rmse: 0.003142
val_f_mae: 0.039327
2025-06-25 05:21:55.742 INFO: val_f_mae: 0.039327
val_f_rmse: 0.063463
2025-06-25 05:21:55.742 INFO: val_f_rmse: 0.063463
##### Step: 15 Learning rate: 0.01 #####
2025-06-25 05:22:26.037 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 4.3596, Val Loss: 3.9511
2025-06-25 05:22:26.037 INFO: Epoch 16, Train Loss: 4.3596, Val Loss: 3.9511
train_e/atom_mae: 0.005896
2025-06-25 05:22:26.038 INFO: train_e/atom_mae: 0.005896
train_e/atom_rmse: 0.007681
2025-06-25 05:22:26.038 INFO: train_e/atom_rmse: 0.007681
train_f_mae: 0.038984
2025-06-25 05:22:26.042 INFO: train_f_mae: 0.038984
train_f_rmse: 0.065485
2025-06-25 05:22:26.042 INFO: train_f_rmse: 0.065485
val_e/atom_mae: 0.002376
2025-06-25 05:22:26.044 INFO: val_e/atom_mae: 0.002376
val_e/atom_rmse: 0.003073
2025-06-25 05:22:26.045 INFO: val_e/atom_rmse: 0.003073
val_f_mae: 0.037655
2025-06-25 05:22:26.045 INFO: val_f_mae: 0.037655
val_f_rmse: 0.062767
2025-06-25 05:22:26.045 INFO: val_f_rmse: 0.062767
##### Step: 16 Learning rate: 0.01 #####
2025-06-25 05:22:56.319 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 4.4706, Val Loss: 4.5944
2025-06-25 05:22:56.319 INFO: Epoch 17, Train Loss: 4.4706, Val Loss: 4.5944
train_e/atom_mae: 0.007666
2025-06-25 05:22:56.320 INFO: train_e/atom_mae: 0.007666
train_e/atom_rmse: 0.009614
2025-06-25 05:22:56.320 INFO: train_e/atom_rmse: 0.009614
train_f_mae: 0.040067
2025-06-25 05:22:56.324 INFO: train_f_mae: 0.040067
train_f_rmse: 0.066021
2025-06-25 05:22:56.324 INFO: train_f_rmse: 0.066021
val_e/atom_mae: 0.002094
2025-06-25 05:22:56.327 INFO: val_e/atom_mae: 0.002094
val_e/atom_rmse: 0.002317
2025-06-25 05:22:56.327 INFO: val_e/atom_rmse: 0.002317
val_f_mae: 0.042194
2025-06-25 05:22:56.328 INFO: val_f_mae: 0.042194
val_f_rmse: 0.067734
2025-06-25 05:22:56.328 INFO: val_f_rmse: 0.067734
##### Step: 17 Learning rate: 0.01 #####
2025-06-25 05:23:26.686 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 4.3723, Val Loss: 4.4111
2025-06-25 05:23:26.687 INFO: Epoch 18, Train Loss: 4.3723, Val Loss: 4.4111
train_e/atom_mae: 0.007930
2025-06-25 05:23:26.687 INFO: train_e/atom_mae: 0.007930
train_e/atom_rmse: 0.010161
2025-06-25 05:23:26.688 INFO: train_e/atom_rmse: 0.010161
train_f_mae: 0.039569
2025-06-25 05:23:26.691 INFO: train_f_mae: 0.039569
train_f_rmse: 0.065172
2025-06-25 05:23:26.691 INFO: train_f_rmse: 0.065172
val_e/atom_mae: 0.003150
2025-06-25 05:23:26.694 INFO: val_e/atom_mae: 0.003150
val_e/atom_rmse: 0.003865
2025-06-25 05:23:26.694 INFO: val_e/atom_rmse: 0.003865
val_f_mae: 0.040503
2025-06-25 05:23:26.695 INFO: val_f_mae: 0.040503
val_f_rmse: 0.066280
2025-06-25 05:23:26.695 INFO: val_f_rmse: 0.066280
##### Step: 18 Learning rate: 0.01 #####
2025-06-25 05:23:56.963 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 4.4641, Val Loss: 4.6048
2025-06-25 05:23:56.964 INFO: Epoch 19, Train Loss: 4.4641, Val Loss: 4.6048
train_e/atom_mae: 0.007648
2025-06-25 05:23:56.964 INFO: train_e/atom_mae: 0.007648
train_e/atom_rmse: 0.009700
2025-06-25 05:23:56.965 INFO: train_e/atom_rmse: 0.009700
train_f_mae: 0.039814
2025-06-25 05:23:56.968 INFO: train_f_mae: 0.039814
train_f_rmse: 0.065956
2025-06-25 05:23:56.968 INFO: train_f_rmse: 0.065956
val_e/atom_mae: 0.007251
2025-06-25 05:23:56.971 INFO: val_e/atom_mae: 0.007251
val_e/atom_rmse: 0.008999
2025-06-25 05:23:56.971 INFO: val_e/atom_rmse: 0.008999
val_f_mae: 0.040498
2025-06-25 05:23:56.972 INFO: val_f_mae: 0.040498
val_f_rmse: 0.067133
2025-06-25 05:23:56.972 INFO: val_f_rmse: 0.067133
##### Step: 19 Learning rate: 0.01 #####
2025-06-25 05:24:27.394 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 4.5064, Val Loss: 4.5351
2025-06-25 05:24:27.394 INFO: Epoch 20, Train Loss: 4.5064, Val Loss: 4.5351
train_e/atom_mae: 0.008427
2025-06-25 05:24:27.395 INFO: train_e/atom_mae: 0.008427
train_e/atom_rmse: 0.010611
2025-06-25 05:24:27.396 INFO: train_e/atom_rmse: 0.010611
train_f_mae: 0.040124
2025-06-25 05:24:27.399 INFO: train_f_mae: 0.040124
train_f_rmse: 0.066107
2025-06-25 05:24:27.399 INFO: train_f_rmse: 0.066107
val_e/atom_mae: 0.003602
2025-06-25 05:24:27.402 INFO: val_e/atom_mae: 0.003602
val_e/atom_rmse: 0.004539
2025-06-25 05:24:27.402 INFO: val_e/atom_rmse: 0.004539
val_f_mae: 0.041625
2025-06-25 05:24:27.403 INFO: val_f_mae: 0.041625
val_f_rmse: 0.067158
2025-06-25 05:24:27.403 INFO: val_f_rmse: 0.067158
##### Step: 20 Learning rate: 0.005 #####
2025-06-25 05:24:57.661 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 4.0156, Val Loss: 3.9913
2025-06-25 05:24:57.661 INFO: Epoch 21, Train Loss: 4.0156, Val Loss: 3.9913
train_e/atom_mae: 0.005243
2025-06-25 05:24:57.662 INFO: train_e/atom_mae: 0.005243
train_e/atom_rmse: 0.006623
2025-06-25 05:24:57.662 INFO: train_e/atom_rmse: 0.006623
train_f_mae: 0.037865
2025-06-25 05:24:57.666 INFO: train_f_mae: 0.037865
train_f_rmse: 0.062948
2025-06-25 05:24:57.666 INFO: train_f_rmse: 0.062948
val_e/atom_mae: 0.003869
2025-06-25 05:24:57.669 INFO: val_e/atom_mae: 0.003869
val_e/atom_rmse: 0.004954
2025-06-25 05:24:57.669 INFO: val_e/atom_rmse: 0.004954
val_f_mae: 0.038365
2025-06-25 05:24:57.670 INFO: val_f_mae: 0.038365
val_f_rmse: 0.062942
2025-06-25 05:24:57.670 INFO: val_f_rmse: 0.062942
##### Step: 21 Learning rate: 0.005 #####
2025-06-25 05:25:28.037 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 4.0416, Val Loss: 3.8766
2025-06-25 05:25:28.037 INFO: Epoch 22, Train Loss: 4.0416, Val Loss: 3.8766
train_e/atom_mae: 0.005158
2025-06-25 05:25:28.038 INFO: train_e/atom_mae: 0.005158
train_e/atom_rmse: 0.006494
2025-06-25 05:25:28.038 INFO: train_e/atom_rmse: 0.006494
train_f_mae: 0.037832
2025-06-25 05:25:28.041 INFO: train_f_mae: 0.037832
train_f_rmse: 0.063171
2025-06-25 05:25:28.041 INFO: train_f_rmse: 0.063171
val_e/atom_mae: 0.002526
2025-06-25 05:25:28.044 INFO: val_e/atom_mae: 0.002526
val_e/atom_rmse: 0.003048
2025-06-25 05:25:28.044 INFO: val_e/atom_rmse: 0.003048
val_f_mae: 0.037502
2025-06-25 05:25:28.045 INFO: val_f_mae: 0.037502
val_f_rmse: 0.062172
2025-06-25 05:25:28.045 INFO: val_f_rmse: 0.062172
##### Step: 22 Learning rate: 0.005 #####
2025-06-25 05:25:58.424 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 4.0388, Val Loss: 4.3212
2025-06-25 05:25:58.424 INFO: Epoch 23, Train Loss: 4.0388, Val Loss: 4.3212
train_e/atom_mae: 0.004932
2025-06-25 05:25:58.425 INFO: train_e/atom_mae: 0.004932
train_e/atom_rmse: 0.006257
2025-06-25 05:25:58.425 INFO: train_e/atom_rmse: 0.006257
train_f_mae: 0.038024
2025-06-25 05:25:58.429 INFO: train_f_mae: 0.038024
train_f_rmse: 0.063178
2025-06-25 05:25:58.429 INFO: train_f_rmse: 0.063178
val_e/atom_mae: 0.005059
2025-06-25 05:25:58.432 INFO: val_e/atom_mae: 0.005059
val_e/atom_rmse: 0.005602
2025-06-25 05:25:58.432 INFO: val_e/atom_rmse: 0.005602
val_f_mae: 0.039502
2025-06-25 05:25:58.433 INFO: val_f_mae: 0.039502
val_f_rmse: 0.065446
2025-06-25 05:25:58.433 INFO: val_f_rmse: 0.065446
##### Step: 23 Learning rate: 0.005 #####
2025-06-25 05:26:28.717 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 3.9387, Val Loss: 4.0553
2025-06-25 05:26:28.717 INFO: Epoch 24, Train Loss: 3.9387, Val Loss: 4.0553
train_e/atom_mae: 0.004148
2025-06-25 05:26:28.718 INFO: train_e/atom_mae: 0.004148
train_e/atom_rmse: 0.005221
2025-06-25 05:26:28.719 INFO: train_e/atom_rmse: 0.005221
train_f_mae: 0.037597
2025-06-25 05:26:28.722 INFO: train_f_mae: 0.037597
train_f_rmse: 0.062496
2025-06-25 05:26:28.722 INFO: train_f_rmse: 0.062496
val_e/atom_mae: 0.006844
2025-06-25 05:26:28.725 INFO: val_e/atom_mae: 0.006844
val_e/atom_rmse: 0.008113
2025-06-25 05:26:28.725 INFO: val_e/atom_rmse: 0.008113
val_f_mae: 0.037855
2025-06-25 05:26:28.726 INFO: val_f_mae: 0.037855
val_f_rmse: 0.063053
2025-06-25 05:26:28.726 INFO: val_f_rmse: 0.063053
##### Step: 24 Learning rate: 0.005 #####
2025-06-25 05:26:59.158 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 3.9906, Val Loss: 4.2516
2025-06-25 05:26:59.159 INFO: Epoch 25, Train Loss: 3.9906, Val Loss: 4.2516
train_e/atom_mae: 0.005429
2025-06-25 05:26:59.160 INFO: train_e/atom_mae: 0.005429
train_e/atom_rmse: 0.006670
2025-06-25 05:26:59.160 INFO: train_e/atom_rmse: 0.006670
train_f_mae: 0.037511
2025-06-25 05:26:59.163 INFO: train_f_mae: 0.037511
train_f_rmse: 0.062743
2025-06-25 05:26:59.164 INFO: train_f_rmse: 0.062743
val_e/atom_mae: 0.006605
2025-06-25 05:26:59.166 INFO: val_e/atom_mae: 0.006605
val_e/atom_rmse: 0.007283
2025-06-25 05:26:59.167 INFO: val_e/atom_rmse: 0.007283
val_f_mae: 0.038804
2025-06-25 05:26:59.167 INFO: val_f_mae: 0.038804
val_f_rmse: 0.064710
2025-06-25 05:26:59.167 INFO: val_f_rmse: 0.064710
##### Step: 25 Learning rate: 0.005 #####
2025-06-25 05:27:29.404 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 4.0479, Val Loss: 4.1230
2025-06-25 05:27:29.404 INFO: Epoch 26, Train Loss: 4.0479, Val Loss: 4.1230
train_e/atom_mae: 0.005071
2025-06-25 05:27:29.405 INFO: train_e/atom_mae: 0.005071
train_e/atom_rmse: 0.006208
2025-06-25 05:27:29.405 INFO: train_e/atom_rmse: 0.006208
train_f_mae: 0.037816
2025-06-25 05:27:29.409 INFO: train_f_mae: 0.037816
train_f_rmse: 0.063256
2025-06-25 05:27:29.409 INFO: train_f_rmse: 0.063256
val_e/atom_mae: 0.003315
2025-06-25 05:27:29.411 INFO: val_e/atom_mae: 0.003315
val_e/atom_rmse: 0.004005
2025-06-25 05:27:29.412 INFO: val_e/atom_rmse: 0.004005
val_f_mae: 0.038557
2025-06-25 05:27:29.412 INFO: val_f_mae: 0.038557
val_f_rmse: 0.064059
2025-06-25 05:27:29.412 INFO: val_f_rmse: 0.064059
##### Step: 26 Learning rate: 0.005 #####
2025-06-25 05:27:59.848 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 3.9333, Val Loss: 3.8417
2025-06-25 05:27:59.848 INFO: Epoch 27, Train Loss: 3.9333, Val Loss: 3.8417
train_e/atom_mae: 0.004980
2025-06-25 05:27:59.849 INFO: train_e/atom_mae: 0.004980
train_e/atom_rmse: 0.006359
2025-06-25 05:27:59.849 INFO: train_e/atom_rmse: 0.006359
train_f_mae: 0.037463
2025-06-25 05:27:59.853 INFO: train_f_mae: 0.037463
train_f_rmse: 0.062325
2025-06-25 05:27:59.853 INFO: train_f_rmse: 0.062325
val_e/atom_mae: 0.002210
2025-06-25 05:27:59.856 INFO: val_e/atom_mae: 0.002210
val_e/atom_rmse: 0.002678
2025-06-25 05:27:59.856 INFO: val_e/atom_rmse: 0.002678
val_f_mae: 0.037754
2025-06-25 05:27:59.856 INFO: val_f_mae: 0.037754
val_f_rmse: 0.061912
2025-06-25 05:27:59.857 INFO: val_f_rmse: 0.061912
##### Step: 27 Learning rate: 0.005 #####
2025-06-25 05:28:30.112 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 4.0437, Val Loss: 3.9152
2025-06-25 05:28:30.112 INFO: Epoch 28, Train Loss: 4.0437, Val Loss: 3.9152
train_e/atom_mae: 0.005158
2025-06-25 05:28:30.113 INFO: train_e/atom_mae: 0.005158
train_e/atom_rmse: 0.006562
2025-06-25 05:28:30.113 INFO: train_e/atom_rmse: 0.006562
train_f_mae: 0.037352
2025-06-25 05:28:30.117 INFO: train_f_mae: 0.037352
train_f_rmse: 0.063179
2025-06-25 05:28:30.117 INFO: train_f_rmse: 0.063179
val_e/atom_mae: 0.007387
2025-06-25 05:28:30.120 INFO: val_e/atom_mae: 0.007387
val_e/atom_rmse: 0.008676
2025-06-25 05:28:30.120 INFO: val_e/atom_rmse: 0.008676
val_f_mae: 0.037401
2025-06-25 05:28:30.121 INFO: val_f_mae: 0.037401
val_f_rmse: 0.061840
2025-06-25 05:28:30.121 INFO: val_f_rmse: 0.061840
##### Step: 28 Learning rate: 0.005 #####
2025-06-25 05:29:00.477 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 3.9377, Val Loss: 4.0526
2025-06-25 05:29:00.477 INFO: Epoch 29, Train Loss: 3.9377, Val Loss: 4.0526
train_e/atom_mae: 0.005737
2025-06-25 05:29:00.478 INFO: train_e/atom_mae: 0.005737
train_e/atom_rmse: 0.007164
2025-06-25 05:29:00.478 INFO: train_e/atom_rmse: 0.007164
train_f_mae: 0.037484
2025-06-25 05:29:00.482 INFO: train_f_mae: 0.037484
train_f_rmse: 0.062254
2025-06-25 05:29:00.482 INFO: train_f_rmse: 0.062254
val_e/atom_mae: 0.005224
2025-06-25 05:29:00.484 INFO: val_e/atom_mae: 0.005224
val_e/atom_rmse: 0.006201
2025-06-25 05:29:00.485 INFO: val_e/atom_rmse: 0.006201
val_f_mae: 0.037774
2025-06-25 05:29:00.485 INFO: val_f_mae: 0.037774
val_f_rmse: 0.063293
2025-06-25 05:29:00.485 INFO: val_f_rmse: 0.063293
##### Step: 29 Learning rate: 0.005 #####
2025-06-25 05:29:30.836 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 4.0574, Val Loss: 3.8052
2025-06-25 05:29:30.836 INFO: Epoch 30, Train Loss: 4.0574, Val Loss: 3.8052
train_e/atom_mae: 0.005017
2025-06-25 05:29:30.837 INFO: train_e/atom_mae: 0.005017
train_e/atom_rmse: 0.006251
2025-06-25 05:29:30.837 INFO: train_e/atom_rmse: 0.006251
train_f_mae: 0.037863
2025-06-25 05:29:30.841 INFO: train_f_mae: 0.037863
train_f_rmse: 0.063325
2025-06-25 05:29:30.841 INFO: train_f_rmse: 0.063325
val_e/atom_mae: 0.004391
2025-06-25 05:29:30.844 INFO: val_e/atom_mae: 0.004391
val_e/atom_rmse: 0.004987
2025-06-25 05:29:30.844 INFO: val_e/atom_rmse: 0.004987
val_f_mae: 0.037038
2025-06-25 05:29:30.845 INFO: val_f_mae: 0.037038
val_f_rmse: 0.061442
2025-06-25 05:29:30.845 INFO: val_f_rmse: 0.061442
##### Step: 30 Learning rate: 0.005 #####
2025-06-25 05:30:01.125 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 3.9079, Val Loss: 4.0569
2025-06-25 05:30:01.126 INFO: Epoch 31, Train Loss: 3.9079, Val Loss: 4.0569
train_e/atom_mae: 0.005484
2025-06-25 05:30:01.127 INFO: train_e/atom_mae: 0.005484
train_e/atom_rmse: 0.006838
2025-06-25 05:30:01.127 INFO: train_e/atom_rmse: 0.006838
train_f_mae: 0.037301
2025-06-25 05:30:01.130 INFO: train_f_mae: 0.037301
train_f_rmse: 0.062059
2025-06-25 05:30:01.130 INFO: train_f_rmse: 0.062059
val_e/atom_mae: 0.008491
2025-06-25 05:30:01.133 INFO: val_e/atom_mae: 0.008491
val_e/atom_rmse: 0.010102
2025-06-25 05:30:01.133 INFO: val_e/atom_rmse: 0.010102
val_f_mae: 0.037886
2025-06-25 05:30:01.134 INFO: val_f_mae: 0.037886
val_f_rmse: 0.062717
2025-06-25 05:30:01.134 INFO: val_f_rmse: 0.062717
##### Step: 31 Learning rate: 0.005 #####
2025-06-25 05:30:31.545 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 4.0058, Val Loss: 4.0532
2025-06-25 05:30:31.545 INFO: Epoch 32, Train Loss: 4.0058, Val Loss: 4.0532
train_e/atom_mae: 0.006076
2025-06-25 05:30:31.546 INFO: train_e/atom_mae: 0.006076
train_e/atom_rmse: 0.007810
2025-06-25 05:30:31.546 INFO: train_e/atom_rmse: 0.007810
train_f_mae: 0.037519
2025-06-25 05:30:31.550 INFO: train_f_mae: 0.037519
train_f_rmse: 0.062706
2025-06-25 05:30:31.550 INFO: train_f_rmse: 0.062706
val_e/atom_mae: 0.008148
2025-06-25 05:30:31.553 INFO: val_e/atom_mae: 0.008148
val_e/atom_rmse: 0.009879
2025-06-25 05:30:31.553 INFO: val_e/atom_rmse: 0.009879
val_f_mae: 0.037650
2025-06-25 05:30:31.553 INFO: val_f_mae: 0.037650
val_f_rmse: 0.062731
2025-06-25 05:30:31.554 INFO: val_f_rmse: 0.062731
##### Step: 32 Learning rate: 0.005 #####
2025-06-25 05:31:01.797 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 3.9057, Val Loss: 3.9165
2025-06-25 05:31:01.797 INFO: Epoch 33, Train Loss: 3.9057, Val Loss: 3.9165
train_e/atom_mae: 0.005136
2025-06-25 05:31:01.798 INFO: train_e/atom_mae: 0.005136
train_e/atom_rmse: 0.006330
2025-06-25 05:31:01.798 INFO: train_e/atom_rmse: 0.006330
train_f_mae: 0.037438
2025-06-25 05:31:01.802 INFO: train_f_mae: 0.037438
train_f_rmse: 0.062106
2025-06-25 05:31:01.802 INFO: train_f_rmse: 0.062106
val_e/atom_mae: 0.006081
2025-06-25 05:31:01.804 INFO: val_e/atom_mae: 0.006081
val_e/atom_rmse: 0.006907
2025-06-25 05:31:01.805 INFO: val_e/atom_rmse: 0.006907
val_f_mae: 0.037726
2025-06-25 05:31:01.805 INFO: val_f_mae: 0.037726
val_f_rmse: 0.062119
2025-06-25 05:31:01.805 INFO: val_f_rmse: 0.062119
##### Step: 33 Learning rate: 0.005 #####
2025-06-25 05:31:32.211 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 4.0143, Val Loss: 4.3619
2025-06-25 05:31:32.211 INFO: Epoch 34, Train Loss: 4.0143, Val Loss: 4.3619
train_e/atom_mae: 0.005112
2025-06-25 05:31:32.212 INFO: train_e/atom_mae: 0.005112
train_e/atom_rmse: 0.006517
2025-06-25 05:31:32.212 INFO: train_e/atom_rmse: 0.006517
train_f_mae: 0.037833
2025-06-25 05:31:32.215 INFO: train_f_mae: 0.037833
train_f_rmse: 0.062952
2025-06-25 05:31:32.215 INFO: train_f_rmse: 0.062952
val_e/atom_mae: 0.007000
2025-06-25 05:31:32.218 INFO: val_e/atom_mae: 0.007000
val_e/atom_rmse: 0.008834
2025-06-25 05:31:32.219 INFO: val_e/atom_rmse: 0.008834
val_f_mae: 0.040027
2025-06-25 05:31:32.219 INFO: val_f_mae: 0.040027
val_f_rmse: 0.065326
2025-06-25 05:31:32.219 INFO: val_f_rmse: 0.065326
##### Step: 34 Learning rate: 0.005 #####
2025-06-25 05:32:02.521 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 3.8815, Val Loss: 4.0039
2025-06-25 05:32:02.521 INFO: Epoch 35, Train Loss: 3.8815, Val Loss: 4.0039
train_e/atom_mae: 0.005419
2025-06-25 05:32:02.522 INFO: train_e/atom_mae: 0.005419
train_e/atom_rmse: 0.006784
2025-06-25 05:32:02.522 INFO: train_e/atom_rmse: 0.006784
train_f_mae: 0.037152
2025-06-25 05:32:02.526 INFO: train_f_mae: 0.037152
train_f_rmse: 0.061853
2025-06-25 05:32:02.526 INFO: train_f_rmse: 0.061853
val_e/atom_mae: 0.002163
2025-06-25 05:32:02.529 INFO: val_e/atom_mae: 0.002163
val_e/atom_rmse: 0.002371
2025-06-25 05:32:02.529 INFO: val_e/atom_rmse: 0.002371
val_f_mae: 0.038830
2025-06-25 05:32:02.529 INFO: val_f_mae: 0.038830
val_f_rmse: 0.063222
2025-06-25 05:32:02.530 INFO: val_f_rmse: 0.063222
##### Step: 35 Learning rate: 0.005 #####
2025-06-25 05:32:32.872 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 3.9978, Val Loss: 3.9659
2025-06-25 05:32:32.872 INFO: Epoch 36, Train Loss: 3.9978, Val Loss: 3.9659
train_e/atom_mae: 0.004745
2025-06-25 05:32:32.873 INFO: train_e/atom_mae: 0.004745
train_e/atom_rmse: 0.005868
2025-06-25 05:32:32.874 INFO: train_e/atom_rmse: 0.005868
train_f_mae: 0.037600
2025-06-25 05:32:32.877 INFO: train_f_mae: 0.037600
train_f_rmse: 0.062898
2025-06-25 05:32:32.877 INFO: train_f_rmse: 0.062898
val_e/atom_mae: 0.003421
2025-06-25 05:32:32.880 INFO: val_e/atom_mae: 0.003421
val_e/atom_rmse: 0.004031
2025-06-25 05:32:32.880 INFO: val_e/atom_rmse: 0.004031
val_f_mae: 0.038341
2025-06-25 05:32:32.881 INFO: val_f_mae: 0.038341
val_f_rmse: 0.062819
2025-06-25 05:32:32.881 INFO: val_f_rmse: 0.062819
##### Step: 36 Learning rate: 0.005 #####
2025-06-25 05:33:03.319 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 3.9664, Val Loss: 3.9786
2025-06-25 05:33:03.319 INFO: Epoch 37, Train Loss: 3.9664, Val Loss: 3.9786
train_e/atom_mae: 0.004691
2025-06-25 05:33:03.320 INFO: train_e/atom_mae: 0.004691
train_e/atom_rmse: 0.005936
2025-06-25 05:33:03.321 INFO: train_e/atom_rmse: 0.005936
train_f_mae: 0.037834
2025-06-25 05:33:03.324 INFO: train_f_mae: 0.037834
train_f_rmse: 0.062640
2025-06-25 05:33:03.324 INFO: train_f_rmse: 0.062640
val_e/atom_mae: 0.004988
2025-06-25 05:33:03.327 INFO: val_e/atom_mae: 0.004988
val_e/atom_rmse: 0.005957
2025-06-25 05:33:03.327 INFO: val_e/atom_rmse: 0.005957
val_f_mae: 0.037431
2025-06-25 05:33:03.328 INFO: val_f_mae: 0.037431
val_f_rmse: 0.062735
2025-06-25 05:33:03.328 INFO: val_f_rmse: 0.062735
##### Step: 37 Learning rate: 0.005 #####
2025-06-25 05:33:33.577 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 4.1421, Val Loss: 3.9753
2025-06-25 05:33:33.577 INFO: Epoch 38, Train Loss: 4.1421, Val Loss: 3.9753
train_e/atom_mae: 0.005161
2025-06-25 05:33:33.578 INFO: train_e/atom_mae: 0.005161
train_e/atom_rmse: 0.006515
2025-06-25 05:33:33.578 INFO: train_e/atom_rmse: 0.006515
train_f_mae: 0.037414
2025-06-25 05:33:33.582 INFO: train_f_mae: 0.037414
train_f_rmse: 0.063959
2025-06-25 05:33:33.582 INFO: train_f_rmse: 0.063959
val_e/atom_mae: 0.002261
2025-06-25 05:33:33.585 INFO: val_e/atom_mae: 0.002261
val_e/atom_rmse: 0.002586
2025-06-25 05:33:33.585 INFO: val_e/atom_rmse: 0.002586
val_f_mae: 0.038366
2025-06-25 05:33:33.586 INFO: val_f_mae: 0.038366
val_f_rmse: 0.062986
2025-06-25 05:33:33.586 INFO: val_f_rmse: 0.062986
##### Step: 38 Learning rate: 0.005 #####
2025-06-25 05:34:04.040 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 3.8781, Val Loss: 3.9370
2025-06-25 05:34:04.040 INFO: Epoch 39, Train Loss: 3.8781, Val Loss: 3.9370
train_e/atom_mae: 0.004592
2025-06-25 05:34:04.041 INFO: train_e/atom_mae: 0.004592
train_e/atom_rmse: 0.005884
2025-06-25 05:34:04.041 INFO: train_e/atom_rmse: 0.005884
train_f_mae: 0.037047
2025-06-25 05:34:04.045 INFO: train_f_mae: 0.037047
train_f_rmse: 0.061937
2025-06-25 05:34:04.045 INFO: train_f_rmse: 0.061937
val_e/atom_mae: 0.003077
2025-06-25 05:34:04.047 INFO: val_e/atom_mae: 0.003077
val_e/atom_rmse: 0.003655
2025-06-25 05:34:04.048 INFO: val_e/atom_rmse: 0.003655
val_f_mae: 0.037682
2025-06-25 05:34:04.048 INFO: val_f_mae: 0.037682
val_f_rmse: 0.062617
2025-06-25 05:34:04.048 INFO: val_f_rmse: 0.062617
##### Step: 39 Learning rate: 0.005 #####
2025-06-25 05:34:34.284 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 3.9582, Val Loss: 3.9221
2025-06-25 05:34:34.284 INFO: Epoch 40, Train Loss: 3.9582, Val Loss: 3.9221
train_e/atom_mae: 0.005429
2025-06-25 05:34:34.285 INFO: train_e/atom_mae: 0.005429
train_e/atom_rmse: 0.006801
2025-06-25 05:34:34.285 INFO: train_e/atom_rmse: 0.006801
train_f_mae: 0.037529
2025-06-25 05:34:34.289 INFO: train_f_mae: 0.037529
train_f_rmse: 0.062468
2025-06-25 05:34:34.289 INFO: train_f_rmse: 0.062468
val_e/atom_mae: 0.006017
2025-06-25 05:34:34.291 INFO: val_e/atom_mae: 0.006017
val_e/atom_rmse: 0.006448
2025-06-25 05:34:34.292 INFO: val_e/atom_rmse: 0.006448
val_f_mae: 0.037914
2025-06-25 05:34:34.292 INFO: val_f_mae: 0.037914
val_f_rmse: 0.062224
2025-06-25 05:34:34.292 INFO: val_f_rmse: 0.062224
2025-06-25 05:34:34.311 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-25 05:35:04.681 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 3.7466, Val Loss: 3.8020
2025-06-25 05:35:04.682 INFO: Epoch 1, Train Loss: 3.7466, Val Loss: 3.8020
train_e/atom_mae: 0.003583
2025-06-25 05:35:04.682 INFO: train_e/atom_mae: 0.003583
train_e/atom_rmse: 0.004689
2025-06-25 05:35:04.683 INFO: train_e/atom_rmse: 0.004689
train_f_mae: 0.036454
2025-06-25 05:35:04.686 INFO: train_f_mae: 0.036454
train_f_rmse: 0.060992
2025-06-25 05:35:04.686 INFO: train_f_rmse: 0.060992
val_e/atom_mae: 0.003228
2025-06-25 05:35:04.689 INFO: val_e/atom_mae: 0.003228
val_e/atom_rmse: 0.003879
2025-06-25 05:35:04.689 INFO: val_e/atom_rmse: 0.003879
val_f_mae: 0.036690
2025-06-25 05:35:04.690 INFO: val_f_mae: 0.036690
val_f_rmse: 0.061512
2025-06-25 05:35:04.690 INFO: val_f_rmse: 0.061512
##### Step: 1 Learning rate: 0.004 #####
2025-06-25 05:35:35.005 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 3.8300, Val Loss: 3.9142
2025-06-25 05:35:35.005 INFO: Epoch 2, Train Loss: 3.8300, Val Loss: 3.9142
train_e/atom_mae: 0.004852
2025-06-25 05:35:35.006 INFO: train_e/atom_mae: 0.004852
train_e/atom_rmse: 0.006014
2025-06-25 05:35:35.006 INFO: train_e/atom_rmse: 0.006014
train_f_mae: 0.036925
2025-06-25 05:35:35.010 INFO: train_f_mae: 0.036925
train_f_rmse: 0.061532
2025-06-25 05:35:35.010 INFO: train_f_rmse: 0.061532
val_e/atom_mae: 0.003481
2025-06-25 05:35:35.012 INFO: val_e/atom_mae: 0.003481
val_e/atom_rmse: 0.004051
2025-06-25 05:35:35.013 INFO: val_e/atom_rmse: 0.004051
val_f_mae: 0.037517
2025-06-25 05:35:35.013 INFO: val_f_mae: 0.037517
val_f_rmse: 0.062405
2025-06-25 05:35:35.013 INFO: val_f_rmse: 0.062405
##### Step: 2 Learning rate: 0.006 #####
2025-06-25 05:36:05.294 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 3.9592, Val Loss: 4.0242
2025-06-25 05:36:05.294 INFO: Epoch 3, Train Loss: 3.9592, Val Loss: 4.0242
train_e/atom_mae: 0.005910
2025-06-25 05:36:05.295 INFO: train_e/atom_mae: 0.005910
train_e/atom_rmse: 0.007439
2025-06-25 05:36:05.295 INFO: train_e/atom_rmse: 0.007439
train_f_mae: 0.037462
2025-06-25 05:36:05.298 INFO: train_f_mae: 0.037462
train_f_rmse: 0.062388
2025-06-25 05:36:05.299 INFO: train_f_rmse: 0.062388
val_e/atom_mae: 0.006306
2025-06-25 05:36:05.301 INFO: val_e/atom_mae: 0.006306
val_e/atom_rmse: 0.006698
2025-06-25 05:36:05.302 INFO: val_e/atom_rmse: 0.006698
val_f_mae: 0.037699
2025-06-25 05:36:05.302 INFO: val_f_mae: 0.037699
val_f_rmse: 0.063007
2025-06-25 05:36:05.302 INFO: val_f_rmse: 0.063007
##### Step: 3 Learning rate: 0.008 #####
2025-06-25 05:36:35.659 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 4.9449, Val Loss: 4.0224
2025-06-25 05:36:35.659 INFO: Epoch 4, Train Loss: 4.9449, Val Loss: 4.0224
train_e/atom_mae: 0.006478
2025-06-25 05:36:35.660 INFO: train_e/atom_mae: 0.006478
train_e/atom_rmse: 0.008145
2025-06-25 05:36:35.660 INFO: train_e/atom_rmse: 0.008145
train_f_mae: 0.038259
2025-06-25 05:36:35.664 INFO: train_f_mae: 0.038259
train_f_rmse: 0.069747
2025-06-25 05:36:35.664 INFO: train_f_rmse: 0.069747
val_e/atom_mae: 0.006943
2025-06-25 05:36:35.667 INFO: val_e/atom_mae: 0.006943
val_e/atom_rmse: 0.008689
2025-06-25 05:36:35.667 INFO: val_e/atom_rmse: 0.008689
val_f_mae: 0.038380
2025-06-25 05:36:35.667 INFO: val_f_mae: 0.038380
val_f_rmse: 0.062698
2025-06-25 05:36:35.668 INFO: val_f_rmse: 0.062698
##### Step: 4 Learning rate: 0.01 #####
2025-06-25 05:37:05.907 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 4.4146, Val Loss: 5.0643
2025-06-25 05:37:05.907 INFO: Epoch 5, Train Loss: 4.4146, Val Loss: 5.0643
train_e/atom_mae: 0.007538
2025-06-25 05:37:05.908 INFO: train_e/atom_mae: 0.007538
train_e/atom_rmse: 0.009436
2025-06-25 05:37:05.908 INFO: train_e/atom_rmse: 0.009436
train_f_mae: 0.039746
2025-06-25 05:37:05.911 INFO: train_f_mae: 0.039746
train_f_rmse: 0.065627
2025-06-25 05:37:05.912 INFO: train_f_rmse: 0.065627
val_e/atom_mae: 0.014521
2025-06-25 05:37:05.914 INFO: val_e/atom_mae: 0.014521
val_e/atom_rmse: 0.015943
2025-06-25 05:37:05.915 INFO: val_e/atom_rmse: 0.015943
val_f_mae: 0.040724
2025-06-25 05:37:05.915 INFO: val_f_mae: 0.040724
val_f_rmse: 0.068969
2025-06-25 05:37:05.915 INFO: val_f_rmse: 0.068969
##### Step: 5 Learning rate: 0.01 #####
2025-06-25 05:37:36.346 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 4.6116, Val Loss: 4.6823
2025-06-25 05:37:36.346 INFO: Epoch 6, Train Loss: 4.6116, Val Loss: 4.6823
train_e/atom_mae: 0.008195
2025-06-25 05:37:36.347 INFO: train_e/atom_mae: 0.008195
train_e/atom_rmse: 0.010282
2025-06-25 05:37:36.347 INFO: train_e/atom_rmse: 0.010282
train_f_mae: 0.039707
2025-06-25 05:37:36.350 INFO: train_f_mae: 0.039707
train_f_rmse: 0.066960
2025-06-25 05:37:36.350 INFO: train_f_rmse: 0.066960
val_e/atom_mae: 0.002632
2025-06-25 05:37:36.353 INFO: val_e/atom_mae: 0.002632
val_e/atom_rmse: 0.003419
2025-06-25 05:37:36.354 INFO: val_e/atom_rmse: 0.003419
val_f_mae: 0.042017
2025-06-25 05:37:36.354 INFO: val_f_mae: 0.042017
val_f_rmse: 0.068324
2025-06-25 05:37:36.354 INFO: val_f_rmse: 0.068324
##### Step: 6 Learning rate: 0.01 #####
2025-06-25 05:38:06.637 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 4.4040, Val Loss: 4.3344
2025-06-25 05:38:06.637 INFO: Epoch 7, Train Loss: 4.4040, Val Loss: 4.3344
train_e/atom_mae: 0.008673
2025-06-25 05:38:06.638 INFO: train_e/atom_mae: 0.008673
train_e/atom_rmse: 0.010861
2025-06-25 05:38:06.638 INFO: train_e/atom_rmse: 0.010861
train_f_mae: 0.039463
2025-06-25 05:38:06.642 INFO: train_f_mae: 0.039463
train_f_rmse: 0.065278
2025-06-25 05:38:06.642 INFO: train_f_rmse: 0.065278
val_e/atom_mae: 0.003967
2025-06-25 05:38:06.645 INFO: val_e/atom_mae: 0.003967
val_e/atom_rmse: 0.004771
2025-06-25 05:38:06.645 INFO: val_e/atom_rmse: 0.004771
val_f_mae: 0.040146
2025-06-25 05:38:06.645 INFO: val_f_mae: 0.040146
val_f_rmse: 0.065626
2025-06-25 05:38:06.646 INFO: val_f_rmse: 0.065626
##### Step: 7 Learning rate: 0.01 #####
2025-06-25 05:38:36.972 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 4.6026, Val Loss: 4.3512
2025-06-25 05:38:36.972 INFO: Epoch 8, Train Loss: 4.6026, Val Loss: 4.3512
train_e/atom_mae: 0.006666
2025-06-25 05:38:36.973 INFO: train_e/atom_mae: 0.006666
train_e/atom_rmse: 0.008281
2025-06-25 05:38:36.973 INFO: train_e/atom_rmse: 0.008281
train_f_mae: 0.039808
2025-06-25 05:38:36.977 INFO: train_f_mae: 0.039808
train_f_rmse: 0.067228
2025-06-25 05:38:36.977 INFO: train_f_rmse: 0.067228
val_e/atom_mae: 0.003796
2025-06-25 05:38:36.979 INFO: val_e/atom_mae: 0.003796
val_e/atom_rmse: 0.004411
2025-06-25 05:38:36.980 INFO: val_e/atom_rmse: 0.004411
val_f_mae: 0.040124
2025-06-25 05:38:36.980 INFO: val_f_mae: 0.040124
val_f_rmse: 0.065785
2025-06-25 05:38:36.980 INFO: val_f_rmse: 0.065785
##### Step: 8 Learning rate: 0.01 #####
2025-06-25 05:39:07.363 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 4.3463, Val Loss: 3.9303
2025-06-25 05:39:07.363 INFO: Epoch 9, Train Loss: 4.3463, Val Loss: 3.9303
train_e/atom_mae: 0.006961
2025-06-25 05:39:07.364 INFO: train_e/atom_mae: 0.006961
train_e/atom_rmse: 0.008729
2025-06-25 05:39:07.364 INFO: train_e/atom_rmse: 0.008729
train_f_mae: 0.039567
2025-06-25 05:39:07.368 INFO: train_f_mae: 0.039567
train_f_rmse: 0.065224
2025-06-25 05:39:07.368 INFO: train_f_rmse: 0.065224
val_e/atom_mae: 0.002968
2025-06-25 05:39:07.371 INFO: val_e/atom_mae: 0.002968
val_e/atom_rmse: 0.003527
2025-06-25 05:39:07.371 INFO: val_e/atom_rmse: 0.003527
val_f_mae: 0.037896
2025-06-25 05:39:07.372 INFO: val_f_mae: 0.037896
val_f_rmse: 0.062572
2025-06-25 05:39:07.372 INFO: val_f_rmse: 0.062572
##### Step: 9 Learning rate: 0.01 #####
2025-06-25 05:39:37.661 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 4.7450, Val Loss: 4.4744
2025-06-25 05:39:37.661 INFO: Epoch 10, Train Loss: 4.7450, Val Loss: 4.4744
train_e/atom_mae: 0.006908
2025-06-25 05:39:37.662 INFO: train_e/atom_mae: 0.006908
train_e/atom_rmse: 0.008970
2025-06-25 05:39:37.662 INFO: train_e/atom_rmse: 0.008970
train_f_mae: 0.040210
2025-06-25 05:39:37.666 INFO: train_f_mae: 0.040210
train_f_rmse: 0.068173
2025-06-25 05:39:37.666 INFO: train_f_rmse: 0.068173
val_e/atom_mae: 0.008190
2025-06-25 05:39:37.669 INFO: val_e/atom_mae: 0.008190
val_e/atom_rmse: 0.008838
2025-06-25 05:39:37.669 INFO: val_e/atom_rmse: 0.008838
val_f_mae: 0.041227
2025-06-25 05:39:37.670 INFO: val_f_mae: 0.041227
val_f_rmse: 0.066181
2025-06-25 05:39:37.670 INFO: val_f_rmse: 0.066181
##### Step: 10 Learning rate: 0.01 #####
2025-06-25 05:40:08.117 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 4.3361, Val Loss: 4.6759
2025-06-25 05:40:08.117 INFO: Epoch 11, Train Loss: 4.3361, Val Loss: 4.6759
train_e/atom_mae: 0.007794
2025-06-25 05:40:08.118 INFO: train_e/atom_mae: 0.007794
train_e/atom_rmse: 0.009967
2025-06-25 05:40:08.118 INFO: train_e/atom_rmse: 0.009967
train_f_mae: 0.039404
2025-06-25 05:40:08.122 INFO: train_f_mae: 0.039404
train_f_rmse: 0.064930
2025-06-25 05:40:08.122 INFO: train_f_rmse: 0.064930
val_e/atom_mae: 0.013557
2025-06-25 05:40:08.125 INFO: val_e/atom_mae: 0.013557
val_e/atom_rmse: 0.013955
2025-06-25 05:40:08.125 INFO: val_e/atom_rmse: 0.013955
val_f_mae: 0.040090
2025-06-25 05:40:08.126 INFO: val_f_mae: 0.040090
val_f_rmse: 0.066635
2025-06-25 05:40:08.126 INFO: val_f_rmse: 0.066635
##### Step: 11 Learning rate: 0.01 #####
2025-06-25 05:40:38.412 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 4.4198, Val Loss: 4.0276
2025-06-25 05:40:38.412 INFO: Epoch 12, Train Loss: 4.4198, Val Loss: 4.0276
train_e/atom_mae: 0.008595
2025-06-25 05:40:38.413 INFO: train_e/atom_mae: 0.008595
train_e/atom_rmse: 0.010405
2025-06-25 05:40:38.414 INFO: train_e/atom_rmse: 0.010405
train_f_mae: 0.038707
2025-06-25 05:40:38.417 INFO: train_f_mae: 0.038707
train_f_rmse: 0.065489
2025-06-25 05:40:38.417 INFO: train_f_rmse: 0.065489
val_e/atom_mae: 0.003673
2025-06-25 05:40:38.420 INFO: val_e/atom_mae: 0.003673
val_e/atom_rmse: 0.004254
2025-06-25 05:40:38.420 INFO: val_e/atom_rmse: 0.004254
val_f_mae: 0.038676
2025-06-25 05:40:38.421 INFO: val_f_mae: 0.038676
val_f_rmse: 0.063290
2025-06-25 05:40:38.421 INFO: val_f_rmse: 0.063290
##### Step: 12 Learning rate: 0.01 #####
2025-06-25 05:41:08.848 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 4.3495, Val Loss: 4.3155
2025-06-25 05:41:08.848 INFO: Epoch 13, Train Loss: 4.3495, Val Loss: 4.3155
train_e/atom_mae: 0.006109
2025-06-25 05:41:08.849 INFO: train_e/atom_mae: 0.006109
train_e/atom_rmse: 0.007732
2025-06-25 05:41:08.849 INFO: train_e/atom_rmse: 0.007732
train_f_mae: 0.039002
2025-06-25 05:41:08.853 INFO: train_f_mae: 0.039002
train_f_rmse: 0.065400
2025-06-25 05:41:08.853 INFO: train_f_rmse: 0.065400
val_e/atom_mae: 0.010093
2025-06-25 05:41:08.855 INFO: val_e/atom_mae: 0.010093
val_e/atom_rmse: 0.010729
2025-06-25 05:41:08.856 INFO: val_e/atom_rmse: 0.010729
val_f_mae: 0.039262
2025-06-25 05:41:08.856 INFO: val_f_mae: 0.039262
val_f_rmse: 0.064623
2025-06-25 05:41:08.856 INFO: val_f_rmse: 0.064623
##### Step: 13 Learning rate: 0.01 #####
2025-06-25 05:41:39.158 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 4.2953, Val Loss: 4.6433
2025-06-25 05:41:39.158 INFO: Epoch 14, Train Loss: 4.2953, Val Loss: 4.6433
train_e/atom_mae: 0.006405
2025-06-25 05:41:39.159 INFO: train_e/atom_mae: 0.006405
train_e/atom_rmse: 0.007772
2025-06-25 05:41:39.159 INFO: train_e/atom_rmse: 0.007772
train_f_mae: 0.039260
2025-06-25 05:41:39.163 INFO: train_f_mae: 0.039260
train_f_rmse: 0.064978
2025-06-25 05:41:39.163 INFO: train_f_rmse: 0.064978
val_e/atom_mae: 0.004952
2025-06-25 05:41:39.166 INFO: val_e/atom_mae: 0.004952
val_e/atom_rmse: 0.006056
2025-06-25 05:41:39.166 INFO: val_e/atom_rmse: 0.006056
val_f_mae: 0.041735
2025-06-25 05:41:39.167 INFO: val_f_mae: 0.041735
val_f_rmse: 0.067815
2025-06-25 05:41:39.167 INFO: val_f_rmse: 0.067815
##### Step: 14 Learning rate: 0.01 #####
2025-06-25 05:42:09.514 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 4.2930, Val Loss: 4.1338
2025-06-25 05:42:09.514 INFO: Epoch 15, Train Loss: 4.2930, Val Loss: 4.1338
train_e/atom_mae: 0.006160
2025-06-25 05:42:09.515 INFO: train_e/atom_mae: 0.006160
train_e/atom_rmse: 0.008012
2025-06-25 05:42:09.515 INFO: train_e/atom_rmse: 0.008012
train_f_mae: 0.039473
2025-06-25 05:42:09.519 INFO: train_f_mae: 0.039473
train_f_rmse: 0.064926
2025-06-25 05:42:09.519 INFO: train_f_rmse: 0.064926
val_e/atom_mae: 0.005488
2025-06-25 05:42:09.522 INFO: val_e/atom_mae: 0.005488
val_e/atom_rmse: 0.006051
2025-06-25 05:42:09.522 INFO: val_e/atom_rmse: 0.006051
val_f_mae: 0.038773
2025-06-25 05:42:09.523 INFO: val_f_mae: 0.038773
val_f_rmse: 0.063949
2025-06-25 05:42:09.523 INFO: val_f_rmse: 0.063949
##### Step: 15 Learning rate: 0.01 #####
2025-06-25 05:42:39.949 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 4.3609, Val Loss: 4.6634
2025-06-25 05:42:39.949 INFO: Epoch 16, Train Loss: 4.3609, Val Loss: 4.6634
train_e/atom_mae: 0.007471
2025-06-25 05:42:39.950 INFO: train_e/atom_mae: 0.007471
train_e/atom_rmse: 0.009777
2025-06-25 05:42:39.950 INFO: train_e/atom_rmse: 0.009777
train_f_mae: 0.039408
2025-06-25 05:42:39.954 INFO: train_f_mae: 0.039408
train_f_rmse: 0.065156
2025-06-25 05:42:39.954 INFO: train_f_rmse: 0.065156
val_e/atom_mae: 0.003145
2025-06-25 05:42:39.957 INFO: val_e/atom_mae: 0.003145
val_e/atom_rmse: 0.003663
2025-06-25 05:42:39.957 INFO: val_e/atom_rmse: 0.003663
val_f_mae: 0.042440
2025-06-25 05:42:39.958 INFO: val_f_mae: 0.042440
val_f_rmse: 0.068170
2025-06-25 05:42:39.958 INFO: val_f_rmse: 0.068170
##### Step: 16 Learning rate: 0.01 #####
2025-06-25 05:43:10.451 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 4.3184, Val Loss: 4.4162
2025-06-25 05:43:10.451 INFO: Epoch 17, Train Loss: 4.3184, Val Loss: 4.4162
train_e/atom_mae: 0.007345
2025-06-25 05:43:10.452 INFO: train_e/atom_mae: 0.007345
train_e/atom_rmse: 0.009427
2025-06-25 05:43:10.452 INFO: train_e/atom_rmse: 0.009427
train_f_mae: 0.039162
2025-06-25 05:43:10.456 INFO: train_f_mae: 0.039162
train_f_rmse: 0.064891
2025-06-25 05:43:10.456 INFO: train_f_rmse: 0.064891
val_e/atom_mae: 0.004381
2025-06-25 05:43:10.459 INFO: val_e/atom_mae: 0.004381
val_e/atom_rmse: 0.005039
2025-06-25 05:43:10.459 INFO: val_e/atom_rmse: 0.005039
val_f_mae: 0.041246
2025-06-25 05:43:10.460 INFO: val_f_mae: 0.041246
val_f_rmse: 0.066223
2025-06-25 05:43:10.460 INFO: val_f_rmse: 0.066223
##### Step: 17 Learning rate: 0.01 #####
2025-06-25 05:43:40.875 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 4.4053, Val Loss: 4.2857
2025-06-25 05:43:40.876 INFO: Epoch 18, Train Loss: 4.4053, Val Loss: 4.2857
train_e/atom_mae: 0.006827
2025-06-25 05:43:40.876 INFO: train_e/atom_mae: 0.006827
train_e/atom_rmse: 0.008400
2025-06-25 05:43:40.877 INFO: train_e/atom_rmse: 0.008400
train_f_mae: 0.039298
2025-06-25 05:43:40.880 INFO: train_f_mae: 0.039298
train_f_rmse: 0.065726
2025-06-25 05:43:40.880 INFO: train_f_rmse: 0.065726
val_e/atom_mae: 0.006169
2025-06-25 05:43:40.883 INFO: val_e/atom_mae: 0.006169
val_e/atom_rmse: 0.006762
2025-06-25 05:43:40.883 INFO: val_e/atom_rmse: 0.006762
val_f_mae: 0.039030
2025-06-25 05:43:40.884 INFO: val_f_mae: 0.039030
val_f_rmse: 0.065042
2025-06-25 05:43:40.884 INFO: val_f_rmse: 0.065042
##### Step: 18 Learning rate: 0.01 #####
2025-06-25 05:44:11.123 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 4.3059, Val Loss: 3.9846
2025-06-25 05:44:11.124 INFO: Epoch 19, Train Loss: 4.3059, Val Loss: 3.9846
train_e/atom_mae: 0.006188
2025-06-25 05:44:11.125 INFO: train_e/atom_mae: 0.006188
train_e/atom_rmse: 0.007930
2025-06-25 05:44:11.125 INFO: train_e/atom_rmse: 0.007930
train_f_mae: 0.038945
2025-06-25 05:44:11.128 INFO: train_f_mae: 0.038945
train_f_rmse: 0.065037
2025-06-25 05:44:11.128 INFO: train_f_rmse: 0.065037
val_e/atom_mae: 0.003674
2025-06-25 05:44:11.131 INFO: val_e/atom_mae: 0.003674
val_e/atom_rmse: 0.004311
2025-06-25 05:44:11.131 INFO: val_e/atom_rmse: 0.004311
val_f_mae: 0.038189
2025-06-25 05:44:11.132 INFO: val_f_mae: 0.038189
val_f_rmse: 0.062945
2025-06-25 05:44:11.132 INFO: val_f_rmse: 0.062945
##### Step: 19 Learning rate: 0.01 #####
2025-06-25 05:44:41.526 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 4.6243, Val Loss: 4.4250
2025-06-25 05:44:41.526 INFO: Epoch 20, Train Loss: 4.6243, Val Loss: 4.4250
train_e/atom_mae: 0.007108
2025-06-25 05:44:41.527 INFO: train_e/atom_mae: 0.007108
train_e/atom_rmse: 0.009107
2025-06-25 05:44:41.527 INFO: train_e/atom_rmse: 0.009107
train_f_mae: 0.039644
2025-06-25 05:44:41.531 INFO: train_f_mae: 0.039644
train_f_rmse: 0.067260
2025-06-25 05:44:41.531 INFO: train_f_rmse: 0.067260
val_e/atom_mae: 0.006732
2025-06-25 05:44:41.534 INFO: val_e/atom_mae: 0.006732
val_e/atom_rmse: 0.008536
2025-06-25 05:44:41.534 INFO: val_e/atom_rmse: 0.008536
val_f_mae: 0.040367
2025-06-25 05:44:41.535 INFO: val_f_mae: 0.040367
val_f_rmse: 0.065855
2025-06-25 05:44:41.535 INFO: val_f_rmse: 0.065855
##### Step: 20 Learning rate: 0.005 #####
2025-06-25 05:45:11.890 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 3.9543, Val Loss: 3.8789
2025-06-25 05:45:11.891 INFO: Epoch 21, Train Loss: 3.9543, Val Loss: 3.8789
train_e/atom_mae: 0.005700
2025-06-25 05:45:11.892 INFO: train_e/atom_mae: 0.005700
train_e/atom_rmse: 0.006951
2025-06-25 05:45:11.892 INFO: train_e/atom_rmse: 0.006951
train_f_mae: 0.037499
2025-06-25 05:45:11.895 INFO: train_f_mae: 0.037499
train_f_rmse: 0.062417
2025-06-25 05:45:11.896 INFO: train_f_rmse: 0.062417
val_e/atom_mae: 0.004561
2025-06-25 05:45:11.898 INFO: val_e/atom_mae: 0.004561
val_e/atom_rmse: 0.005720
2025-06-25 05:45:11.899 INFO: val_e/atom_rmse: 0.005720
val_f_mae: 0.037590
2025-06-25 05:45:11.899 INFO: val_f_mae: 0.037590
val_f_rmse: 0.061962
2025-06-25 05:45:11.899 INFO: val_f_rmse: 0.061962
##### Step: 21 Learning rate: 0.005 #####
2025-06-25 05:45:42.201 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 3.9692, Val Loss: 3.9383
2025-06-25 05:45:42.201 INFO: Epoch 22, Train Loss: 3.9692, Val Loss: 3.9383
train_e/atom_mae: 0.005074
2025-06-25 05:45:42.202 INFO: train_e/atom_mae: 0.005074
train_e/atom_rmse: 0.006451
2025-06-25 05:45:42.202 INFO: train_e/atom_rmse: 0.006451
train_f_mae: 0.037315
2025-06-25 05:45:42.206 INFO: train_f_mae: 0.037315
train_f_rmse: 0.062601
2025-06-25 05:45:42.206 INFO: train_f_rmse: 0.062601
val_e/atom_mae: 0.007257
2025-06-25 05:45:42.208 INFO: val_e/atom_mae: 0.007257
val_e/atom_rmse: 0.008191
2025-06-25 05:45:42.209 INFO: val_e/atom_rmse: 0.008191
val_f_mae: 0.037130
2025-06-25 05:45:42.209 INFO: val_f_mae: 0.037130
val_f_rmse: 0.062106
2025-06-25 05:45:42.209 INFO: val_f_rmse: 0.062106
##### Step: 22 Learning rate: 0.005 #####
2025-06-25 05:46:12.636 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 3.9305, Val Loss: 3.8913
2025-06-25 05:46:12.637 INFO: Epoch 23, Train Loss: 3.9305, Val Loss: 3.8913
train_e/atom_mae: 0.005456
2025-06-25 05:46:12.638 INFO: train_e/atom_mae: 0.005456
train_e/atom_rmse: 0.006748
2025-06-25 05:46:12.638 INFO: train_e/atom_rmse: 0.006748
train_f_mae: 0.037212
2025-06-25 05:46:12.641 INFO: train_f_mae: 0.037212
train_f_rmse: 0.062252
2025-06-25 05:46:12.641 INFO: train_f_rmse: 0.062252
val_e/atom_mae: 0.002773
2025-06-25 05:46:12.644 INFO: val_e/atom_mae: 0.002773
val_e/atom_rmse: 0.003554
2025-06-25 05:46:12.645 INFO: val_e/atom_rmse: 0.003554
val_f_mae: 0.038646
2025-06-25 05:46:12.645 INFO: val_f_mae: 0.038646
val_f_rmse: 0.062258
2025-06-25 05:46:12.645 INFO: val_f_rmse: 0.062258
##### Step: 23 Learning rate: 0.005 #####
2025-06-25 05:46:42.925 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 3.8808, Val Loss: 3.8074
2025-06-25 05:46:42.925 INFO: Epoch 24, Train Loss: 3.8808, Val Loss: 3.8074
train_e/atom_mae: 0.004069
2025-06-25 05:46:42.926 INFO: train_e/atom_mae: 0.004069
train_e/atom_rmse: 0.005110
2025-06-25 05:46:42.926 INFO: train_e/atom_rmse: 0.005110
train_f_mae: 0.037182
2025-06-25 05:46:42.930 INFO: train_f_mae: 0.037182
train_f_rmse: 0.062042
2025-06-25 05:46:42.930 INFO: train_f_rmse: 0.062042
val_e/atom_mae: 0.002255
2025-06-25 05:46:42.933 INFO: val_e/atom_mae: 0.002255
val_e/atom_rmse: 0.002761
2025-06-25 05:46:42.933 INFO: val_e/atom_rmse: 0.002761
val_f_mae: 0.037225
2025-06-25 05:46:42.933 INFO: val_f_mae: 0.037225
val_f_rmse: 0.061629
2025-06-25 05:46:42.933 INFO: val_f_rmse: 0.061629
##### Step: 24 Learning rate: 0.005 #####
2025-06-25 05:47:13.376 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 3.8913, Val Loss: 3.9265
2025-06-25 05:47:13.376 INFO: Epoch 25, Train Loss: 3.8913, Val Loss: 3.9265
train_e/atom_mae: 0.004664
2025-06-25 05:47:13.377 INFO: train_e/atom_mae: 0.004664
train_e/atom_rmse: 0.005734
2025-06-25 05:47:13.377 INFO: train_e/atom_rmse: 0.005734
train_f_mae: 0.037248
2025-06-25 05:47:13.381 INFO: train_f_mae: 0.037248
train_f_rmse: 0.062060
2025-06-25 05:47:13.381 INFO: train_f_rmse: 0.062060
val_e/atom_mae: 0.003499
2025-06-25 05:47:13.384 INFO: val_e/atom_mae: 0.003499
val_e/atom_rmse: 0.004158
2025-06-25 05:47:13.384 INFO: val_e/atom_rmse: 0.004158
val_f_mae: 0.038099
2025-06-25 05:47:13.385 INFO: val_f_mae: 0.038099
val_f_rmse: 0.062495
2025-06-25 05:47:13.385 INFO: val_f_rmse: 0.062495
##### Step: 25 Learning rate: 0.005 #####
2025-06-25 05:47:43.602 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 3.9285, Val Loss: 3.9415
2025-06-25 05:47:43.602 INFO: Epoch 26, Train Loss: 3.9285, Val Loss: 3.9415
train_e/atom_mae: 0.004295
2025-06-25 05:47:43.603 INFO: train_e/atom_mae: 0.004295
train_e/atom_rmse: 0.005381
2025-06-25 05:47:43.603 INFO: train_e/atom_rmse: 0.005381
train_f_mae: 0.037253
2025-06-25 05:47:43.607 INFO: train_f_mae: 0.037253
train_f_rmse: 0.062398
2025-06-25 05:47:43.607 INFO: train_f_rmse: 0.062398
val_e/atom_mae: 0.002186
2025-06-25 05:47:43.610 INFO: val_e/atom_mae: 0.002186
val_e/atom_rmse: 0.002565
2025-06-25 05:47:43.610 INFO: val_e/atom_rmse: 0.002565
val_f_mae: 0.038333
2025-06-25 05:47:43.610 INFO: val_f_mae: 0.038333
val_f_rmse: 0.062718
2025-06-25 05:47:43.611 INFO: val_f_rmse: 0.062718
##### Step: 26 Learning rate: 0.005 #####
2025-06-25 05:48:14.005 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 3.9611, Val Loss: 4.0117
2025-06-25 05:48:14.005 INFO: Epoch 27, Train Loss: 3.9611, Val Loss: 4.0117
train_e/atom_mae: 0.004639
2025-06-25 05:48:14.006 INFO: train_e/atom_mae: 0.004639
train_e/atom_rmse: 0.005911
2025-06-25 05:48:14.006 INFO: train_e/atom_rmse: 0.005911
train_f_mae: 0.037217
2025-06-25 05:48:14.010 INFO: train_f_mae: 0.037217
train_f_rmse: 0.062600
2025-06-25 05:48:14.010 INFO: train_f_rmse: 0.062600
val_e/atom_mae: 0.004708
2025-06-25 05:48:14.013 INFO: val_e/atom_mae: 0.004708
val_e/atom_rmse: 0.005235
2025-06-25 05:48:14.013 INFO: val_e/atom_rmse: 0.005235
val_f_mae: 0.037542
2025-06-25 05:48:14.013 INFO: val_f_mae: 0.037542
val_f_rmse: 0.063075
2025-06-25 05:48:14.014 INFO: val_f_rmse: 0.063075
##### Step: 27 Learning rate: 0.005 #####
2025-06-25 05:48:44.417 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 3.9280, Val Loss: 3.8521
2025-06-25 05:48:44.417 INFO: Epoch 28, Train Loss: 3.9280, Val Loss: 3.8521
train_e/atom_mae: 0.005127
2025-06-25 05:48:44.418 INFO: train_e/atom_mae: 0.005127
train_e/atom_rmse: 0.006286
2025-06-25 05:48:44.418 INFO: train_e/atom_rmse: 0.006286
train_f_mae: 0.037210
2025-06-25 05:48:44.422 INFO: train_f_mae: 0.037210
train_f_rmse: 0.062291
2025-06-25 05:48:44.422 INFO: train_f_rmse: 0.062291
val_e/atom_mae: 0.004354
2025-06-25 05:48:44.425 INFO: val_e/atom_mae: 0.004354
val_e/atom_rmse: 0.005046
2025-06-25 05:48:44.425 INFO: val_e/atom_rmse: 0.005046
val_f_mae: 0.037489
2025-06-25 05:48:44.426 INFO: val_f_mae: 0.037489
val_f_rmse: 0.061817
2025-06-25 05:48:44.426 INFO: val_f_rmse: 0.061817
##### Step: 28 Learning rate: 0.005 #####
2025-06-25 05:49:14.740 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 3.8800, Val Loss: 3.8462
2025-06-25 05:49:14.740 INFO: Epoch 29, Train Loss: 3.8800, Val Loss: 3.8462
train_e/atom_mae: 0.004868
2025-06-25 05:49:14.741 INFO: train_e/atom_mae: 0.004868
train_e/atom_rmse: 0.006053
2025-06-25 05:49:14.741 INFO: train_e/atom_rmse: 0.006053
train_f_mae: 0.037150
2025-06-25 05:49:14.744 INFO: train_f_mae: 0.037150
train_f_rmse: 0.061933
2025-06-25 05:49:14.744 INFO: train_f_rmse: 0.061933
val_e/atom_mae: 0.004327
2025-06-25 05:49:14.747 INFO: val_e/atom_mae: 0.004327
val_e/atom_rmse: 0.005079
2025-06-25 05:49:14.748 INFO: val_e/atom_rmse: 0.005079
val_f_mae: 0.037394
2025-06-25 05:49:14.748 INFO: val_f_mae: 0.037394
val_f_rmse: 0.061766
2025-06-25 05:49:14.748 INFO: val_f_rmse: 0.061766
##### Step: 29 Learning rate: 0.005 #####
2025-06-25 05:49:45.180 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 3.8751, Val Loss: 4.1139
2025-06-25 05:49:45.181 INFO: Epoch 30, Train Loss: 3.8751, Val Loss: 4.1139
train_e/atom_mae: 0.004590
2025-06-25 05:49:45.182 INFO: train_e/atom_mae: 0.004590
train_e/atom_rmse: 0.005783
2025-06-25 05:49:45.182 INFO: train_e/atom_rmse: 0.005783
train_f_mae: 0.037012
2025-06-25 05:49:45.185 INFO: train_f_mae: 0.037012
train_f_rmse: 0.061924
2025-06-25 05:49:45.185 INFO: train_f_rmse: 0.061924
val_e/atom_mae: 0.007098
2025-06-25 05:49:45.188 INFO: val_e/atom_mae: 0.007098
val_e/atom_rmse: 0.007469
2025-06-25 05:49:45.188 INFO: val_e/atom_rmse: 0.007469
val_f_mae: 0.038266
2025-06-25 05:49:45.189 INFO: val_f_mae: 0.038266
val_f_rmse: 0.063612
2025-06-25 05:49:45.189 INFO: val_f_rmse: 0.063612
##### Step: 30 Learning rate: 0.005 #####
2025-06-25 05:50:15.494 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 3.9212, Val Loss: 3.9415
2025-06-25 05:50:15.494 INFO: Epoch 31, Train Loss: 3.9212, Val Loss: 3.9415
train_e/atom_mae: 0.004376
2025-06-25 05:50:15.495 INFO: train_e/atom_mae: 0.004376
train_e/atom_rmse: 0.005387
2025-06-25 05:50:15.495 INFO: train_e/atom_rmse: 0.005387
train_f_mae: 0.037017
2025-06-25 05:50:15.499 INFO: train_f_mae: 0.037017
train_f_rmse: 0.062339
2025-06-25 05:50:15.499 INFO: train_f_rmse: 0.062339
val_e/atom_mae: 0.004263
2025-06-25 05:50:15.502 INFO: val_e/atom_mae: 0.004263
val_e/atom_rmse: 0.004904
2025-06-25 05:50:15.502 INFO: val_e/atom_rmse: 0.004904
val_f_mae: 0.037918
2025-06-25 05:50:15.502 INFO: val_f_mae: 0.037918
val_f_rmse: 0.062549
2025-06-25 05:50:15.503 INFO: val_f_rmse: 0.062549
##### Step: 31 Learning rate: 0.005 #####
2025-06-25 05:50:45.945 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 3.8993, Val Loss: 3.8769
2025-06-25 05:50:45.945 INFO: Epoch 32, Train Loss: 3.8993, Val Loss: 3.8769
train_e/atom_mae: 0.006448
2025-06-25 05:50:45.946 INFO: train_e/atom_mae: 0.006448
train_e/atom_rmse: 0.007968
2025-06-25 05:50:45.946 INFO: train_e/atom_rmse: 0.007968
train_f_mae: 0.036918
2025-06-25 05:50:45.949 INFO: train_f_mae: 0.036918
train_f_rmse: 0.061826
2025-06-25 05:50:45.950 INFO: train_f_rmse: 0.061826
val_e/atom_mae: 0.003030
2025-06-25 05:50:45.952 INFO: val_e/atom_mae: 0.003030
val_e/atom_rmse: 0.003923
2025-06-25 05:50:45.953 INFO: val_e/atom_rmse: 0.003923
val_f_mae: 0.036865
2025-06-25 05:50:45.953 INFO: val_f_mae: 0.036865
val_f_rmse: 0.062115
2025-06-25 05:50:45.953 INFO: val_f_rmse: 0.062115
##### Step: 32 Learning rate: 0.005 #####
2025-06-25 05:51:16.237 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 3.9092, Val Loss: 3.8569
2025-06-25 05:51:16.237 INFO: Epoch 33, Train Loss: 3.9092, Val Loss: 3.8569
train_e/atom_mae: 0.004339
2025-06-25 05:51:16.238 INFO: train_e/atom_mae: 0.004339
train_e/atom_rmse: 0.005651
2025-06-25 05:51:16.238 INFO: train_e/atom_rmse: 0.005651
train_f_mae: 0.037206
2025-06-25 05:51:16.242 INFO: train_f_mae: 0.037206
train_f_rmse: 0.062213
2025-06-25 05:51:16.242 INFO: train_f_rmse: 0.062213
val_e/atom_mae: 0.003675
2025-06-25 05:51:16.244 INFO: val_e/atom_mae: 0.003675
val_e/atom_rmse: 0.004584
2025-06-25 05:51:16.245 INFO: val_e/atom_rmse: 0.004584
val_f_mae: 0.037786
2025-06-25 05:51:16.245 INFO: val_f_mae: 0.037786
val_f_rmse: 0.061899
2025-06-25 05:51:16.245 INFO: val_f_rmse: 0.061899
##### Step: 33 Learning rate: 0.005 #####
2025-06-25 05:51:46.575 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 3.8296, Val Loss: 3.7540
2025-06-25 05:51:46.575 INFO: Epoch 34, Train Loss: 3.8296, Val Loss: 3.7540
train_e/atom_mae: 0.004624
2025-06-25 05:51:46.576 INFO: train_e/atom_mae: 0.004624
train_e/atom_rmse: 0.005735
2025-06-25 05:51:46.576 INFO: train_e/atom_rmse: 0.005735
train_f_mae: 0.036853
2025-06-25 05:51:46.579 INFO: train_f_mae: 0.036853
train_f_rmse: 0.061562
2025-06-25 05:51:46.580 INFO: train_f_rmse: 0.061562
val_e/atom_mae: 0.003017
2025-06-25 05:51:46.582 INFO: val_e/atom_mae: 0.003017
val_e/atom_rmse: 0.003561
2025-06-25 05:51:46.583 INFO: val_e/atom_rmse: 0.003561
val_f_mae: 0.036535
2025-06-25 05:51:46.583 INFO: val_f_mae: 0.036535
val_f_rmse: 0.061145
2025-06-25 05:51:46.583 INFO: val_f_rmse: 0.061145
##### Step: 34 Learning rate: 0.005 #####
2025-06-25 05:52:17.011 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 3.9491, Val Loss: 3.9366
2025-06-25 05:52:17.011 INFO: Epoch 35, Train Loss: 3.9491, Val Loss: 3.9366
train_e/atom_mae: 0.004889
2025-06-25 05:52:17.012 INFO: train_e/atom_mae: 0.004889
train_e/atom_rmse: 0.006113
2025-06-25 05:52:17.012 INFO: train_e/atom_rmse: 0.006113
train_f_mae: 0.037260
2025-06-25 05:52:17.016 INFO: train_f_mae: 0.037260
train_f_rmse: 0.062481
2025-06-25 05:52:17.016 INFO: train_f_rmse: 0.062481
val_e/atom_mae: 0.003414
2025-06-25 05:52:17.018 INFO: val_e/atom_mae: 0.003414
val_e/atom_rmse: 0.003799
2025-06-25 05:52:17.019 INFO: val_e/atom_rmse: 0.003799
val_f_mae: 0.038066
2025-06-25 05:52:17.019 INFO: val_f_mae: 0.038066
val_f_rmse: 0.062603
2025-06-25 05:52:17.019 INFO: val_f_rmse: 0.062603
##### Step: 35 Learning rate: 0.005 #####
2025-06-25 05:52:47.321 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 3.9241, Val Loss: 3.7941
2025-06-25 05:52:47.321 INFO: Epoch 36, Train Loss: 3.9241, Val Loss: 3.7941
train_e/atom_mae: 0.004943
2025-06-25 05:52:47.322 INFO: train_e/atom_mae: 0.004943
train_e/atom_rmse: 0.006139
2025-06-25 05:52:47.322 INFO: train_e/atom_rmse: 0.006139
train_f_mae: 0.037351
2025-06-25 05:52:47.326 INFO: train_f_mae: 0.037351
train_f_rmse: 0.062278
2025-06-25 05:52:47.326 INFO: train_f_rmse: 0.062278
val_e/atom_mae: 0.003991
2025-06-25 05:52:47.329 INFO: val_e/atom_mae: 0.003991
val_e/atom_rmse: 0.004622
2025-06-25 05:52:47.329 INFO: val_e/atom_rmse: 0.004622
val_f_mae: 0.037322
2025-06-25 05:52:47.329 INFO: val_f_mae: 0.037322
val_f_rmse: 0.061386
2025-06-25 05:52:47.330 INFO: val_f_rmse: 0.061386
##### Step: 36 Learning rate: 0.005 #####
2025-06-25 05:53:17.800 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 3.8611, Val Loss: 3.8529
2025-06-25 05:53:17.800 INFO: Epoch 37, Train Loss: 3.8611, Val Loss: 3.8529
train_e/atom_mae: 0.004981
2025-06-25 05:53:17.801 INFO: train_e/atom_mae: 0.004981
train_e/atom_rmse: 0.006399
2025-06-25 05:53:17.801 INFO: train_e/atom_rmse: 0.006399
train_f_mae: 0.037019
2025-06-25 05:53:17.805 INFO: train_f_mae: 0.037019
train_f_rmse: 0.061738
2025-06-25 05:53:17.805 INFO: train_f_rmse: 0.061738
val_e/atom_mae: 0.002195
2025-06-25 05:53:17.807 INFO: val_e/atom_mae: 0.002195
val_e/atom_rmse: 0.002530
2025-06-25 05:53:17.808 INFO: val_e/atom_rmse: 0.002530
val_f_mae: 0.037213
2025-06-25 05:53:17.808 INFO: val_f_mae: 0.037213
val_f_rmse: 0.062009
2025-06-25 05:53:17.808 INFO: val_f_rmse: 0.062009
##### Step: 37 Learning rate: 0.005 #####
2025-06-25 05:53:48.081 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 3.9135, Val Loss: 3.9781
2025-06-25 05:53:48.081 INFO: Epoch 38, Train Loss: 3.9135, Val Loss: 3.9781
train_e/atom_mae: 0.004559
2025-06-25 05:53:48.082 INFO: train_e/atom_mae: 0.004559
train_e/atom_rmse: 0.005707
2025-06-25 05:53:48.082 INFO: train_e/atom_rmse: 0.005707
train_f_mae: 0.037062
2025-06-25 05:53:48.086 INFO: train_f_mae: 0.037062
train_f_rmse: 0.062243
2025-06-25 05:53:48.086 INFO: train_f_rmse: 0.062243
val_e/atom_mae: 0.007536
2025-06-25 05:53:48.089 INFO: val_e/atom_mae: 0.007536
val_e/atom_rmse: 0.009295
2025-06-25 05:53:48.089 INFO: val_e/atom_rmse: 0.009295
val_f_mae: 0.037699
2025-06-25 05:53:48.090 INFO: val_f_mae: 0.037699
val_f_rmse: 0.062238
2025-06-25 05:53:48.090 INFO: val_f_rmse: 0.062238
##### Step: 38 Learning rate: 0.005 #####
2025-06-25 05:54:18.475 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 3.8374, Val Loss: 3.8037
2025-06-25 05:54:18.476 INFO: Epoch 39, Train Loss: 3.8374, Val Loss: 3.8037
train_e/atom_mae: 0.003699
2025-06-25 05:54:18.477 INFO: train_e/atom_mae: 0.003699
train_e/atom_rmse: 0.004637
2025-06-25 05:54:18.477 INFO: train_e/atom_rmse: 0.004637
train_f_mae: 0.036908
2025-06-25 05:54:18.480 INFO: train_f_mae: 0.036908
train_f_rmse: 0.061736
2025-06-25 05:54:18.480 INFO: train_f_rmse: 0.061736
val_e/atom_mae: 0.003177
2025-06-25 05:54:18.483 INFO: val_e/atom_mae: 0.003177
val_e/atom_rmse: 0.003754
2025-06-25 05:54:18.483 INFO: val_e/atom_rmse: 0.003754
val_f_mae: 0.037711
2025-06-25 05:54:18.484 INFO: val_f_mae: 0.037711
val_f_rmse: 0.061536
2025-06-25 05:54:18.484 INFO: val_f_rmse: 0.061536
##### Step: 39 Learning rate: 0.005 #####
2025-06-25 05:54:48.850 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 3.8490, Val Loss: 3.9519
2025-06-25 05:54:48.850 INFO: Epoch 40, Train Loss: 3.8490, Val Loss: 3.9519
train_e/atom_mae: 0.004728
2025-06-25 05:54:48.851 INFO: train_e/atom_mae: 0.004728
train_e/atom_rmse: 0.005928
2025-06-25 05:54:48.851 INFO: train_e/atom_rmse: 0.005928
train_f_mae: 0.036916
2025-06-25 05:54:48.855 INFO: train_f_mae: 0.036916
train_f_rmse: 0.061696
2025-06-25 05:54:48.855 INFO: train_f_rmse: 0.061696
val_e/atom_mae: 0.003822
2025-06-25 05:54:48.858 INFO: val_e/atom_mae: 0.003822
val_e/atom_rmse: 0.004449
2025-06-25 05:54:48.858 INFO: val_e/atom_rmse: 0.004449
val_f_mae: 0.037645
2025-06-25 05:54:48.858 INFO: val_f_mae: 0.037645
val_f_rmse: 0.062673
2025-06-25 05:54:48.859 INFO: val_f_rmse: 0.062673
2025-06-25 05:54:48.867 INFO: Second train loop:
2025-06-25 05:54:48.868 INFO: training
##### Step: 40 Learning rate: 0.0025 #####
2025-06-25 05:55:19.223 INFO: ##### Step: 40 Learning rate: 0.0025 #####
Epoch 1, Train Loss: 3.9232, Val Loss: 4.1602
2025-06-25 05:55:19.224 INFO: Epoch 1, Train Loss: 3.9232, Val Loss: 4.1602
train_e/atom_mae: 0.003497
2025-06-25 05:55:19.225 INFO: train_e/atom_mae: 0.003497
train_e/atom_rmse: 0.004326
2025-06-25 05:55:19.225 INFO: train_e/atom_rmse: 0.004326
train_f_mae: 0.035981
2025-06-25 05:55:19.228 INFO: train_f_mae: 0.035981
train_f_rmse: 0.060801
2025-06-25 05:55:19.228 INFO: train_f_rmse: 0.060801
val_e/atom_mae: 0.006218
2025-06-25 05:55:19.231 INFO: val_e/atom_mae: 0.006218
val_e/atom_rmse: 0.006615
2025-06-25 05:55:19.231 INFO: val_e/atom_rmse: 0.006615
val_f_mae: 0.036363
2025-06-25 05:55:19.232 INFO: val_f_mae: 0.036363
val_f_rmse: 0.060256
2025-06-25 05:55:19.232 INFO: val_f_rmse: 0.060256
##### Step: 41 Learning rate: 0.0025 #####
2025-06-25 05:55:49.709 INFO: ##### Step: 41 Learning rate: 0.0025 #####
Epoch 2, Train Loss: 3.9385, Val Loss: 4.0076
2025-06-25 05:55:49.709 INFO: Epoch 2, Train Loss: 3.9385, Val Loss: 4.0076
train_e/atom_mae: 0.004148
2025-06-25 05:55:49.710 INFO: train_e/atom_mae: 0.004148
train_e/atom_rmse: 0.005224
2025-06-25 05:55:49.710 INFO: train_e/atom_rmse: 0.005224
train_f_mae: 0.035696
2025-06-25 05:55:49.714 INFO: train_f_mae: 0.035696
train_f_rmse: 0.060069
2025-06-25 05:55:49.714 INFO: train_f_rmse: 0.060069
val_e/atom_mae: 0.004354
2025-06-25 05:55:49.717 INFO: val_e/atom_mae: 0.004354
val_e/atom_rmse: 0.004811
2025-06-25 05:55:49.717 INFO: val_e/atom_rmse: 0.004811
val_f_mae: 0.036948
2025-06-25 05:55:49.717 INFO: val_f_mae: 0.036948
val_f_rmse: 0.061053
2025-06-25 05:55:49.718 INFO: val_f_rmse: 0.061053
##### Step: 42 Learning rate: 0.0025 #####
2025-06-25 05:56:20.051 INFO: ##### Step: 42 Learning rate: 0.0025 #####
Epoch 3, Train Loss: 4.0084, Val Loss: 3.8266
2025-06-25 05:56:20.051 INFO: Epoch 3, Train Loss: 4.0084, Val Loss: 3.8266
train_e/atom_mae: 0.004244
2025-06-25 05:56:20.052 INFO: train_e/atom_mae: 0.004244
train_e/atom_rmse: 0.005343
2025-06-25 05:56:20.052 INFO: train_e/atom_rmse: 0.005343
train_f_mae: 0.036023
2025-06-25 05:56:20.056 INFO: train_f_mae: 0.036023
train_f_rmse: 0.060522
2025-06-25 05:56:20.056 INFO: train_f_rmse: 0.060522
val_e/atom_mae: 0.002646
2025-06-25 05:56:20.058 INFO: val_e/atom_mae: 0.002646
val_e/atom_rmse: 0.003377
2025-06-25 05:56:20.059 INFO: val_e/atom_rmse: 0.003377
val_f_mae: 0.036220
2025-06-25 05:56:20.059 INFO: val_f_mae: 0.036220
val_f_rmse: 0.060734
2025-06-25 05:56:20.059 INFO: val_f_rmse: 0.060734
##### Step: 43 Learning rate: 0.0025 #####
2025-06-25 05:56:50.539 INFO: ##### Step: 43 Learning rate: 0.0025 #####
Epoch 4, Train Loss: 3.9519, Val Loss: 3.7691
2025-06-25 05:56:50.539 INFO: Epoch 4, Train Loss: 3.9519, Val Loss: 3.7691
train_e/atom_mae: 0.003825
2025-06-25 05:56:50.540 INFO: train_e/atom_mae: 0.003825
train_e/atom_rmse: 0.004788
2025-06-25 05:56:50.540 INFO: train_e/atom_rmse: 0.004788
train_f_mae: 0.035876
2025-06-25 05:56:50.544 INFO: train_f_mae: 0.035876
train_f_rmse: 0.060618
2025-06-25 05:56:50.544 INFO: train_f_rmse: 0.060618
val_e/atom_mae: 0.002287
2025-06-25 05:56:50.547 INFO: val_e/atom_mae: 0.002287
val_e/atom_rmse: 0.002908
2025-06-25 05:56:50.547 INFO: val_e/atom_rmse: 0.002908
val_f_mae: 0.036248
2025-06-25 05:56:50.548 INFO: val_f_mae: 0.036248
val_f_rmse: 0.060553
2025-06-25 05:56:50.548 INFO: val_f_rmse: 0.060553
##### Step: 44 Learning rate: 0.0025 #####
2025-06-25 05:57:20.898 INFO: ##### Step: 44 Learning rate: 0.0025 #####
Epoch 5, Train Loss: 3.9739, Val Loss: 3.9298
2025-06-25 05:57:20.898 INFO: Epoch 5, Train Loss: 3.9739, Val Loss: 3.9298
train_e/atom_mae: 0.004109
2025-06-25 05:57:20.899 INFO: train_e/atom_mae: 0.004109
train_e/atom_rmse: 0.005149
2025-06-25 05:57:20.899 INFO: train_e/atom_rmse: 0.005149
train_f_mae: 0.035876
2025-06-25 05:57:20.903 INFO: train_f_mae: 0.035876
train_f_rmse: 0.060440
2025-06-25 05:57:20.903 INFO: train_f_rmse: 0.060440
val_e/atom_mae: 0.002495
2025-06-25 05:57:20.905 INFO: val_e/atom_mae: 0.002495
val_e/atom_rmse: 0.003085
2025-06-25 05:57:20.906 INFO: val_e/atom_rmse: 0.003085
val_f_mae: 0.036852
2025-06-25 05:57:20.906 INFO: val_f_mae: 0.036852
val_f_rmse: 0.061763
2025-06-25 05:57:20.906 INFO: val_f_rmse: 0.061763
##### Step: 45 Learning rate: 0.0025 #####
2025-06-25 05:57:51.311 INFO: ##### Step: 45 Learning rate: 0.0025 #####
Epoch 6, Train Loss: 3.8921, Val Loss: 3.8809
2025-06-25 05:57:51.311 INFO: Epoch 6, Train Loss: 3.8921, Val Loss: 3.8809
train_e/atom_mae: 0.003418
2025-06-25 05:57:51.312 INFO: train_e/atom_mae: 0.003418
train_e/atom_rmse: 0.004229
2025-06-25 05:57:51.312 INFO: train_e/atom_rmse: 0.004229
train_f_mae: 0.036228
2025-06-25 05:57:51.316 INFO: train_f_mae: 0.036228
train_f_rmse: 0.060628
2025-06-25 05:57:51.316 INFO: train_f_rmse: 0.060628
val_e/atom_mae: 0.002333
2025-06-25 05:57:51.319 INFO: val_e/atom_mae: 0.002333
val_e/atom_rmse: 0.002960
2025-06-25 05:57:51.319 INFO: val_e/atom_rmse: 0.002960
val_f_mae: 0.036698
2025-06-25 05:57:51.320 INFO: val_f_mae: 0.036698
val_f_rmse: 0.061440
2025-06-25 05:57:51.320 INFO: val_f_rmse: 0.061440
##### Step: 46 Learning rate: 0.0025 #####
2025-06-25 05:58:21.715 INFO: ##### Step: 46 Learning rate: 0.0025 #####
Epoch 7, Train Loss: 3.9020, Val Loss: 4.6665
2025-06-25 05:58:21.716 INFO: Epoch 7, Train Loss: 3.9020, Val Loss: 4.6665
train_e/atom_mae: 0.003155
2025-06-25 05:58:21.717 INFO: train_e/atom_mae: 0.003155
train_e/atom_rmse: 0.003913
2025-06-25 05:58:21.717 INFO: train_e/atom_rmse: 0.003913
train_f_mae: 0.036385
2025-06-25 05:58:21.720 INFO: train_f_mae: 0.036385
train_f_rmse: 0.060965
2025-06-25 05:58:21.720 INFO: train_f_rmse: 0.060965
val_e/atom_mae: 0.006430
2025-06-25 05:58:21.723 INFO: val_e/atom_mae: 0.006430
val_e/atom_rmse: 0.008279
2025-06-25 05:58:21.723 INFO: val_e/atom_rmse: 0.008279
val_f_mae: 0.037659
2025-06-25 05:58:21.724 INFO: val_f_mae: 0.037659
val_f_rmse: 0.061945
2025-06-25 05:58:21.724 INFO: val_f_rmse: 0.061945
##### Step: 47 Learning rate: 0.0025 #####
2025-06-25 05:58:52.056 INFO: ##### Step: 47 Learning rate: 0.0025 #####
Epoch 8, Train Loss: 3.9732, Val Loss: 3.9809
2025-06-25 05:58:52.056 INFO: Epoch 8, Train Loss: 3.9732, Val Loss: 3.9809
train_e/atom_mae: 0.003810
2025-06-25 05:58:52.057 INFO: train_e/atom_mae: 0.003810
train_e/atom_rmse: 0.004846
2025-06-25 05:58:52.057 INFO: train_e/atom_rmse: 0.004846
train_f_mae: 0.036145
2025-06-25 05:58:52.061 INFO: train_f_mae: 0.036145
train_f_rmse: 0.060738
2025-06-25 05:58:52.061 INFO: train_f_rmse: 0.060738
val_e/atom_mae: 0.002387
2025-06-25 05:58:52.063 INFO: val_e/atom_mae: 0.002387
val_e/atom_rmse: 0.002986
2025-06-25 05:58:52.064 INFO: val_e/atom_rmse: 0.002986
val_f_mae: 0.037797
2025-06-25 05:58:52.064 INFO: val_f_mae: 0.037797
val_f_rmse: 0.062234
2025-06-25 05:58:52.064 INFO: val_f_rmse: 0.062234
##### Step: 48 Learning rate: 0.0025 #####
2025-06-25 05:59:22.486 INFO: ##### Step: 48 Learning rate: 0.0025 #####
Epoch 9, Train Loss: 4.1139, Val Loss: 3.9253
2025-06-25 05:59:22.487 INFO: Epoch 9, Train Loss: 4.1139, Val Loss: 3.9253
train_e/atom_mae: 0.004526
2025-06-25 05:59:22.488 INFO: train_e/atom_mae: 0.004526
train_e/atom_rmse: 0.005691
2025-06-25 05:59:22.488 INFO: train_e/atom_rmse: 0.005691
train_f_mae: 0.036285
2025-06-25 05:59:22.492 INFO: train_f_mae: 0.036285
train_f_rmse: 0.061008
2025-06-25 05:59:22.492 INFO: train_f_rmse: 0.061008
val_e/atom_mae: 0.003733
2025-06-25 05:59:22.494 INFO: val_e/atom_mae: 0.003733
val_e/atom_rmse: 0.004394
2025-06-25 05:59:22.495 INFO: val_e/atom_rmse: 0.004394
val_f_mae: 0.036484
2025-06-25 05:59:22.495 INFO: val_f_mae: 0.036484
val_f_rmse: 0.060760
2025-06-25 05:59:22.495 INFO: val_f_rmse: 0.060760
##### Step: 49 Learning rate: 0.0025 #####
2025-06-25 05:59:52.725 INFO: ##### Step: 49 Learning rate: 0.0025 #####
Epoch 10, Train Loss: 4.0986, Val Loss: 3.8186
2025-06-25 05:59:52.726 INFO: Epoch 10, Train Loss: 4.0986, Val Loss: 3.8186
train_e/atom_mae: 0.004730
2025-06-25 05:59:52.727 INFO: train_e/atom_mae: 0.004730
train_e/atom_rmse: 0.005954
2025-06-25 05:59:52.727 INFO: train_e/atom_rmse: 0.005954
train_f_mae: 0.036023
2025-06-25 05:59:52.730 INFO: train_f_mae: 0.036023
train_f_rmse: 0.060577
2025-06-25 05:59:52.730 INFO: train_f_rmse: 0.060577
val_e/atom_mae: 0.002485
2025-06-25 05:59:52.733 INFO: val_e/atom_mae: 0.002485
val_e/atom_rmse: 0.003102
2025-06-25 05:59:52.734 INFO: val_e/atom_rmse: 0.003102
val_f_mae: 0.037101
2025-06-25 05:59:52.734 INFO: val_f_mae: 0.037101
val_f_rmse: 0.060846
2025-06-25 05:59:52.734 INFO: val_f_rmse: 0.060846
##### Step: 50 Learning rate: 0.0025 #####
2025-06-25 06:00:23.216 INFO: ##### Step: 50 Learning rate: 0.0025 #####
Epoch 11, Train Loss: 3.9080, Val Loss: 3.8713
2025-06-25 06:00:23.216 INFO: Epoch 11, Train Loss: 3.9080, Val Loss: 3.8713
train_e/atom_mae: 0.003427
2025-06-25 06:00:23.217 INFO: train_e/atom_mae: 0.003427
train_e/atom_rmse: 0.004241
2025-06-25 06:00:23.217 INFO: train_e/atom_rmse: 0.004241
train_f_mae: 0.036262
2025-06-25 06:00:23.221 INFO: train_f_mae: 0.036262
train_f_rmse: 0.060749
2025-06-25 06:00:23.221 INFO: train_f_rmse: 0.060749
val_e/atom_mae: 0.003209
2025-06-25 06:00:23.224 INFO: val_e/atom_mae: 0.003209
val_e/atom_rmse: 0.003865
2025-06-25 06:00:23.224 INFO: val_e/atom_rmse: 0.003865
val_f_mae: 0.036609
2025-06-25 06:00:23.224 INFO: val_f_mae: 0.036609
val_f_rmse: 0.060750
2025-06-25 06:00:23.224 INFO: val_f_rmse: 0.060750
##### Step: 51 Learning rate: 0.0025 #####
2025-06-25 06:00:53.572 INFO: ##### Step: 51 Learning rate: 0.0025 #####
Epoch 12, Train Loss: 3.8956, Val Loss: 3.7821
2025-06-25 06:00:53.572 INFO: Epoch 12, Train Loss: 3.8956, Val Loss: 3.7821
train_e/atom_mae: 0.003444
2025-06-25 06:00:53.573 INFO: train_e/atom_mae: 0.003444
train_e/atom_rmse: 0.004273
2025-06-25 06:00:53.573 INFO: train_e/atom_rmse: 0.004273
train_f_mae: 0.036108
2025-06-25 06:00:53.577 INFO: train_f_mae: 0.036108
train_f_rmse: 0.060619
2025-06-25 06:00:53.577 INFO: train_f_rmse: 0.060619
val_e/atom_mae: 0.002170
2025-06-25 06:00:53.580 INFO: val_e/atom_mae: 0.002170
val_e/atom_rmse: 0.002323
2025-06-25 06:00:53.580 INFO: val_e/atom_rmse: 0.002323
val_f_mae: 0.036438
2025-06-25 06:00:53.581 INFO: val_f_mae: 0.036438
val_f_rmse: 0.060966
2025-06-25 06:00:53.581 INFO: val_f_rmse: 0.060966
##### Step: 52 Learning rate: 0.0025 #####
2025-06-25 06:01:23.981 INFO: ##### Step: 52 Learning rate: 0.0025 #####
Epoch 13, Train Loss: 4.0249, Val Loss: 3.7893
2025-06-25 06:01:23.981 INFO: Epoch 13, Train Loss: 4.0249, Val Loss: 3.7893
train_e/atom_mae: 0.004392
2025-06-25 06:01:23.982 INFO: train_e/atom_mae: 0.004392
train_e/atom_rmse: 0.005582
2025-06-25 06:01:23.982 INFO: train_e/atom_rmse: 0.005582
train_f_mae: 0.036019
2025-06-25 06:01:23.985 INFO: train_f_mae: 0.036019
train_f_rmse: 0.060398
2025-06-25 06:01:23.985 INFO: train_f_rmse: 0.060398
val_e/atom_mae: 0.002185
2025-06-25 06:01:23.988 INFO: val_e/atom_mae: 0.002185
val_e/atom_rmse: 0.002306
2025-06-25 06:01:23.988 INFO: val_e/atom_rmse: 0.002306
val_f_mae: 0.036487
2025-06-25 06:01:23.989 INFO: val_f_mae: 0.036487
val_f_rmse: 0.061033
2025-06-25 06:01:23.989 INFO: val_f_rmse: 0.061033
##### Step: 53 Learning rate: 0.0025 #####
2025-06-25 06:01:54.415 INFO: ##### Step: 53 Learning rate: 0.0025 #####
Epoch 14, Train Loss: 3.9458, Val Loss: 3.9609
2025-06-25 06:01:54.415 INFO: Epoch 14, Train Loss: 3.9458, Val Loss: 3.9609
train_e/atom_mae: 0.003495
2025-06-25 06:01:54.416 INFO: train_e/atom_mae: 0.003495
train_e/atom_rmse: 0.004369
2025-06-25 06:01:54.416 INFO: train_e/atom_rmse: 0.004369
train_f_mae: 0.036335
2025-06-25 06:01:54.420 INFO: train_f_mae: 0.036335
train_f_rmse: 0.060949
2025-06-25 06:01:54.420 INFO: train_f_rmse: 0.060949
val_e/atom_mae: 0.002959
2025-06-25 06:01:54.423 INFO: val_e/atom_mae: 0.002959
val_e/atom_rmse: 0.003637
2025-06-25 06:01:54.423 INFO: val_e/atom_rmse: 0.003637
val_f_mae: 0.037373
2025-06-25 06:01:54.424 INFO: val_f_mae: 0.037373
val_f_rmse: 0.061651
2025-06-25 06:01:54.424 INFO: val_f_rmse: 0.061651
##### Step: 54 Learning rate: 0.0025 #####
2025-06-25 06:02:24.727 INFO: ##### Step: 54 Learning rate: 0.0025 #####
Epoch 15, Train Loss: 3.9207, Val Loss: 3.8279
2025-06-25 06:02:24.727 INFO: Epoch 15, Train Loss: 3.9207, Val Loss: 3.8279
train_e/atom_mae: 0.003758
2025-06-25 06:02:24.728 INFO: train_e/atom_mae: 0.003758
train_e/atom_rmse: 0.004661
2025-06-25 06:02:24.728 INFO: train_e/atom_rmse: 0.004661
train_f_mae: 0.036110
2025-06-25 06:02:24.732 INFO: train_f_mae: 0.036110
train_f_rmse: 0.060481
2025-06-25 06:02:24.732 INFO: train_f_rmse: 0.060481
val_e/atom_mae: 0.002173
2025-06-25 06:02:24.735 INFO: val_e/atom_mae: 0.002173
val_e/atom_rmse: 0.002523
2025-06-25 06:02:24.735 INFO: val_e/atom_rmse: 0.002523
val_f_mae: 0.036575
2025-06-25 06:02:24.736 INFO: val_f_mae: 0.036575
val_f_rmse: 0.061244
2025-06-25 06:02:24.736 INFO: val_f_rmse: 0.061244
##### Step: 55 Learning rate: 0.0025 #####
2025-06-25 06:02:55.228 INFO: ##### Step: 55 Learning rate: 0.0025 #####
Epoch 16, Train Loss: 3.8685, Val Loss: 3.9174
2025-06-25 06:02:55.228 INFO: Epoch 16, Train Loss: 3.8685, Val Loss: 3.9174
train_e/atom_mae: 0.003108
2025-06-25 06:02:55.229 INFO: train_e/atom_mae: 0.003108
train_e/atom_rmse: 0.003816
2025-06-25 06:02:55.230 INFO: train_e/atom_rmse: 0.003816
train_f_mae: 0.036174
2025-06-25 06:02:55.233 INFO: train_f_mae: 0.036174
train_f_rmse: 0.060764
2025-06-25 06:02:55.233 INFO: train_f_rmse: 0.060764
val_e/atom_mae: 0.002650
2025-06-25 06:02:55.236 INFO: val_e/atom_mae: 0.002650
val_e/atom_rmse: 0.003192
2025-06-25 06:02:55.236 INFO: val_e/atom_rmse: 0.003192
val_f_mae: 0.037321
2025-06-25 06:02:55.237 INFO: val_f_mae: 0.037321
val_f_rmse: 0.061597
2025-06-25 06:02:55.237 INFO: val_f_rmse: 0.061597
##### Step: 56 Learning rate: 0.0025 #####
2025-06-25 06:03:25.555 INFO: ##### Step: 56 Learning rate: 0.0025 #####
Epoch 17, Train Loss: 4.0591, Val Loss: 3.8773
2025-06-25 06:03:25.555 INFO: Epoch 17, Train Loss: 4.0591, Val Loss: 3.8773
train_e/atom_mae: 0.003957
2025-06-25 06:03:25.556 INFO: train_e/atom_mae: 0.003957
train_e/atom_rmse: 0.004928
2025-06-25 06:03:25.556 INFO: train_e/atom_rmse: 0.004928
train_f_mae: 0.036767
2025-06-25 06:03:25.560 INFO: train_f_mae: 0.036767
train_f_rmse: 0.061361
2025-06-25 06:03:25.560 INFO: train_f_rmse: 0.061361
val_e/atom_mae: 0.002245
2025-06-25 06:03:25.563 INFO: val_e/atom_mae: 0.002245
val_e/atom_rmse: 0.002662
2025-06-25 06:03:25.563 INFO: val_e/atom_rmse: 0.002662
val_f_mae: 0.036854
2025-06-25 06:03:25.564 INFO: val_f_mae: 0.036854
val_f_rmse: 0.061576
2025-06-25 06:03:25.564 INFO: val_f_rmse: 0.061576
##### Step: 57 Learning rate: 0.0025 #####
2025-06-25 06:03:55.998 INFO: ##### Step: 57 Learning rate: 0.0025 #####
Epoch 18, Train Loss: 3.8432, Val Loss: 4.0624
2025-06-25 06:03:55.998 INFO: Epoch 18, Train Loss: 3.8432, Val Loss: 4.0624
train_e/atom_mae: 0.002925
2025-06-25 06:03:55.999 INFO: train_e/atom_mae: 0.002925
train_e/atom_rmse: 0.003625
2025-06-25 06:03:55.999 INFO: train_e/atom_rmse: 0.003625
train_f_mae: 0.035984
2025-06-25 06:03:56.003 INFO: train_f_mae: 0.035984
train_f_rmse: 0.060697
2025-06-25 06:03:56.003 INFO: train_f_rmse: 0.060697
val_e/atom_mae: 0.004714
2025-06-25 06:03:56.006 INFO: val_e/atom_mae: 0.004714
val_e/atom_rmse: 0.005481
2025-06-25 06:03:56.006 INFO: val_e/atom_rmse: 0.005481
val_f_mae: 0.036485
2025-06-25 06:03:56.007 INFO: val_f_mae: 0.036485
val_f_rmse: 0.060819
2025-06-25 06:03:56.007 INFO: val_f_rmse: 0.060819
##### Step: 58 Learning rate: 0.0025 #####
2025-06-25 06:04:26.378 INFO: ##### Step: 58 Learning rate: 0.0025 #####
Epoch 19, Train Loss: 4.0339, Val Loss: 4.0296
2025-06-25 06:04:26.379 INFO: Epoch 19, Train Loss: 4.0339, Val Loss: 4.0296
train_e/atom_mae: 0.004119
2025-06-25 06:04:26.380 INFO: train_e/atom_mae: 0.004119
train_e/atom_rmse: 0.005078
2025-06-25 06:04:26.380 INFO: train_e/atom_rmse: 0.005078
train_f_mae: 0.036559
2025-06-25 06:04:26.383 INFO: train_f_mae: 0.036559
train_f_rmse: 0.061007
2025-06-25 06:04:26.383 INFO: train_f_rmse: 0.061007
val_e/atom_mae: 0.004252
2025-06-25 06:04:26.386 INFO: val_e/atom_mae: 0.004252
val_e/atom_rmse: 0.005008
2025-06-25 06:04:26.387 INFO: val_e/atom_rmse: 0.005008
val_f_mae: 0.036817
2025-06-25 06:04:26.387 INFO: val_f_mae: 0.036817
val_f_rmse: 0.061043
2025-06-25 06:04:26.387 INFO: val_f_rmse: 0.061043
##### Step: 59 Learning rate: 0.0025 #####
2025-06-25 06:04:56.748 INFO: ##### Step: 59 Learning rate: 0.0025 #####
Epoch 20, Train Loss: 3.9182, Val Loss: 3.9846
2025-06-25 06:04:56.748 INFO: Epoch 20, Train Loss: 3.9182, Val Loss: 3.9846
train_e/atom_mae: 0.003577
2025-06-25 06:04:56.749 INFO: train_e/atom_mae: 0.003577
train_e/atom_rmse: 0.004384
2025-06-25 06:04:56.749 INFO: train_e/atom_rmse: 0.004384
train_f_mae: 0.036116
2025-06-25 06:04:56.752 INFO: train_f_mae: 0.036116
train_f_rmse: 0.060710
2025-06-25 06:04:56.753 INFO: train_f_rmse: 0.060710
val_e/atom_mae: 0.002641
2025-06-25 06:04:56.755 INFO: val_e/atom_mae: 0.002641
val_e/atom_rmse: 0.003381
2025-06-25 06:04:56.756 INFO: val_e/atom_rmse: 0.003381
val_f_mae: 0.037999
2025-06-25 06:04:56.756 INFO: val_f_mae: 0.037999
val_f_rmse: 0.062018
2025-06-25 06:04:56.756 INFO: val_f_rmse: 0.062018
##### Step: 60 Learning rate: 0.00125 #####
2025-06-25 06:05:27.222 INFO: ##### Step: 60 Learning rate: 0.00125 #####
Epoch 21, Train Loss: 3.8546, Val Loss: 3.6735
2025-06-25 06:05:27.222 INFO: Epoch 21, Train Loss: 3.8546, Val Loss: 3.6735
train_e/atom_mae: 0.003118
2025-06-25 06:05:27.223 INFO: train_e/atom_mae: 0.003118
train_e/atom_rmse: 0.003828
2025-06-25 06:05:27.223 INFO: train_e/atom_rmse: 0.003828
train_f_mae: 0.035679
2025-06-25 06:05:27.227 INFO: train_f_mae: 0.035679
train_f_rmse: 0.060641
2025-06-25 06:05:27.227 INFO: train_f_rmse: 0.060641
val_e/atom_mae: 0.002182
2025-06-25 06:05:27.230 INFO: val_e/atom_mae: 0.002182
val_e/atom_rmse: 0.002501
2025-06-25 06:05:27.230 INFO: val_e/atom_rmse: 0.002501
val_f_mae: 0.035966
2025-06-25 06:05:27.231 INFO: val_f_mae: 0.035966
val_f_rmse: 0.059981
2025-06-25 06:05:27.231 INFO: val_f_rmse: 0.059981
##### Step: 61 Learning rate: 0.00125 #####
2025-06-25 06:05:57.554 INFO: ##### Step: 61 Learning rate: 0.00125 #####
Epoch 22, Train Loss: 3.7034, Val Loss: 3.7636
2025-06-25 06:05:57.555 INFO: Epoch 22, Train Loss: 3.7034, Val Loss: 3.7636
train_e/atom_mae: 0.002604
2025-06-25 06:05:57.556 INFO: train_e/atom_mae: 0.002604
train_e/atom_rmse: 0.003173
2025-06-25 06:05:57.556 INFO: train_e/atom_rmse: 0.003173
train_f_mae: 0.035535
2025-06-25 06:05:57.559 INFO: train_f_mae: 0.035535
train_f_rmse: 0.059846
2025-06-25 06:05:57.559 INFO: train_f_rmse: 0.059846
val_e/atom_mae: 0.002758
2025-06-25 06:05:57.562 INFO: val_e/atom_mae: 0.002758
val_e/atom_rmse: 0.003392
2025-06-25 06:05:57.563 INFO: val_e/atom_rmse: 0.003392
val_f_mae: 0.036216
2025-06-25 06:05:57.563 INFO: val_f_mae: 0.036216
val_f_rmse: 0.060203
2025-06-25 06:05:57.563 INFO: val_f_rmse: 0.060203
##### Step: 62 Learning rate: 0.00125 #####
2025-06-25 06:06:28.026 INFO: ##### Step: 62 Learning rate: 0.00125 #####
Epoch 23, Train Loss: 3.8093, Val Loss: 3.7187
2025-06-25 06:06:28.026 INFO: Epoch 23, Train Loss: 3.8093, Val Loss: 3.7187
train_e/atom_mae: 0.003578
2025-06-25 06:06:28.027 INFO: train_e/atom_mae: 0.003578
train_e/atom_rmse: 0.004483
2025-06-25 06:06:28.027 INFO: train_e/atom_rmse: 0.004483
train_f_mae: 0.035392
2025-06-25 06:06:28.031 INFO: train_f_mae: 0.035392
train_f_rmse: 0.059717
2025-06-25 06:06:28.031 INFO: train_f_rmse: 0.059717
val_e/atom_mae: 0.002410
2025-06-25 06:06:28.034 INFO: val_e/atom_mae: 0.002410
val_e/atom_rmse: 0.003048
2025-06-25 06:06:28.034 INFO: val_e/atom_rmse: 0.003048
val_f_mae: 0.035907
2025-06-25 06:06:28.034 INFO: val_f_mae: 0.035907
val_f_rmse: 0.060053
2025-06-25 06:06:28.035 INFO: val_f_rmse: 0.060053
##### Step: 63 Learning rate: 0.00125 #####
2025-06-25 06:06:58.348 INFO: ##### Step: 63 Learning rate: 0.00125 #####
Epoch 24, Train Loss: 3.6982, Val Loss: 3.7189
2025-06-25 06:06:58.349 INFO: Epoch 24, Train Loss: 3.6982, Val Loss: 3.7189
train_e/atom_mae: 0.002698
2025-06-25 06:06:58.350 INFO: train_e/atom_mae: 0.002698
train_e/atom_rmse: 0.003268
2025-06-25 06:06:58.350 INFO: train_e/atom_rmse: 0.003268
train_f_mae: 0.035470
2025-06-25 06:06:58.353 INFO: train_f_mae: 0.035470
train_f_rmse: 0.059741
2025-06-25 06:06:58.353 INFO: train_f_rmse: 0.059741
val_e/atom_mae: 0.002205
2025-06-25 06:06:58.356 INFO: val_e/atom_mae: 0.002205
val_e/atom_rmse: 0.002626
2025-06-25 06:06:58.356 INFO: val_e/atom_rmse: 0.002626
val_f_mae: 0.036175
2025-06-25 06:06:58.357 INFO: val_f_mae: 0.036175
val_f_rmse: 0.060295
2025-06-25 06:06:58.357 INFO: val_f_rmse: 0.060295
##### Step: 64 Learning rate: 0.00125 #####
2025-06-25 06:07:28.788 INFO: ##### Step: 64 Learning rate: 0.00125 #####
Epoch 25, Train Loss: 3.7252, Val Loss: 3.8544
2025-06-25 06:07:28.789 INFO: Epoch 25, Train Loss: 3.7252, Val Loss: 3.8544
train_e/atom_mae: 0.002763
2025-06-25 06:07:28.789 INFO: train_e/atom_mae: 0.002763
train_e/atom_rmse: 0.003403
2025-06-25 06:07:28.790 INFO: train_e/atom_rmse: 0.003403
train_f_mae: 0.035558
2025-06-25 06:07:28.793 INFO: train_f_mae: 0.035558
train_f_rmse: 0.059875
2025-06-25 06:07:28.793 INFO: train_f_rmse: 0.059875
val_e/atom_mae: 0.003369
2025-06-25 06:07:28.796 INFO: val_e/atom_mae: 0.003369
val_e/atom_rmse: 0.004053
2025-06-25 06:07:28.796 INFO: val_e/atom_rmse: 0.004053
val_f_mae: 0.036121
2025-06-25 06:07:28.797 INFO: val_f_mae: 0.036121
val_f_rmse: 0.060462
2025-06-25 06:07:28.797 INFO: val_f_rmse: 0.060462
##### Step: 65 Learning rate: 0.00125 #####
2025-06-25 06:07:59.207 INFO: ##### Step: 65 Learning rate: 0.00125 #####
Epoch 26, Train Loss: 3.7497, Val Loss: 3.9048
2025-06-25 06:07:59.207 INFO: Epoch 26, Train Loss: 3.7497, Val Loss: 3.9048
train_e/atom_mae: 0.002916
2025-06-25 06:07:59.208 INFO: train_e/atom_mae: 0.002916
train_e/atom_rmse: 0.003622
2025-06-25 06:07:59.208 INFO: train_e/atom_rmse: 0.003622
train_f_mae: 0.035538
2025-06-25 06:07:59.212 INFO: train_f_mae: 0.035538
train_f_rmse: 0.059924
2025-06-25 06:07:59.212 INFO: train_f_rmse: 0.059924
val_e/atom_mae: 0.004475
2025-06-25 06:07:59.215 INFO: val_e/atom_mae: 0.004475
val_e/atom_rmse: 0.005061
2025-06-25 06:07:59.215 INFO: val_e/atom_rmse: 0.005061
val_f_mae: 0.035848
2025-06-25 06:07:59.215 INFO: val_f_mae: 0.035848
val_f_rmse: 0.059957
2025-06-25 06:07:59.216 INFO: val_f_rmse: 0.059957
##### Step: 66 Learning rate: 0.00125 #####
2025-06-25 06:08:29.545 INFO: ##### Step: 66 Learning rate: 0.00125 #####
Epoch 27, Train Loss: 3.7302, Val Loss: 3.7260
2025-06-25 06:08:29.546 INFO: Epoch 27, Train Loss: 3.7302, Val Loss: 3.7260
train_e/atom_mae: 0.002937
2025-06-25 06:08:29.547 INFO: train_e/atom_mae: 0.002937
train_e/atom_rmse: 0.003618
2025-06-25 06:08:29.547 INFO: train_e/atom_rmse: 0.003618
train_f_mae: 0.035437
2025-06-25 06:08:29.550 INFO: train_f_mae: 0.035437
train_f_rmse: 0.059764
2025-06-25 06:08:29.550 INFO: train_f_rmse: 0.059764
val_e/atom_mae: 0.002583
2025-06-25 06:08:29.553 INFO: val_e/atom_mae: 0.002583
val_e/atom_rmse: 0.003158
2025-06-25 06:08:29.554 INFO: val_e/atom_rmse: 0.003158
val_f_mae: 0.036020
2025-06-25 06:08:29.554 INFO: val_f_mae: 0.036020
val_f_rmse: 0.060044
2025-06-25 06:08:29.554 INFO: val_f_rmse: 0.060044
##### Step: 67 Learning rate: 0.00125 #####
2025-06-25 06:09:00.035 INFO: ##### Step: 67 Learning rate: 0.00125 #####
Epoch 28, Train Loss: 3.7157, Val Loss: 3.8252
2025-06-25 06:09:00.035 INFO: Epoch 28, Train Loss: 3.7157, Val Loss: 3.8252
train_e/atom_mae: 0.002752
2025-06-25 06:09:00.036 INFO: train_e/atom_mae: 0.002752
train_e/atom_rmse: 0.003371
2025-06-25 06:09:00.036 INFO: train_e/atom_rmse: 0.003371
train_f_mae: 0.035541
2025-06-25 06:09:00.040 INFO: train_f_mae: 0.035541
train_f_rmse: 0.059818
2025-06-25 06:09:00.040 INFO: train_f_rmse: 0.059818
val_e/atom_mae: 0.003005
2025-06-25 06:09:00.043 INFO: val_e/atom_mae: 0.003005
val_e/atom_rmse: 0.003558
2025-06-25 06:09:00.043 INFO: val_e/atom_rmse: 0.003558
val_f_mae: 0.036637
2025-06-25 06:09:00.044 INFO: val_f_mae: 0.036637
val_f_rmse: 0.060597
2025-06-25 06:09:00.044 INFO: val_f_rmse: 0.060597
##### Step: 68 Learning rate: 0.00125 #####
2025-06-25 06:09:30.354 INFO: ##### Step: 68 Learning rate: 0.00125 #####
Epoch 29, Train Loss: 3.7728, Val Loss: 3.8270
2025-06-25 06:09:30.355 INFO: Epoch 29, Train Loss: 3.7728, Val Loss: 3.8270
train_e/atom_mae: 0.002926
2025-06-25 06:09:30.356 INFO: train_e/atom_mae: 0.002926
train_e/atom_rmse: 0.003664
2025-06-25 06:09:30.356 INFO: train_e/atom_rmse: 0.003664
train_f_mae: 0.035701
2025-06-25 06:09:30.359 INFO: train_f_mae: 0.035701
train_f_rmse: 0.060086
2025-06-25 06:09:30.359 INFO: train_f_rmse: 0.060086
val_e/atom_mae: 0.002317
2025-06-25 06:09:30.362 INFO: val_e/atom_mae: 0.002317
val_e/atom_rmse: 0.002885
2025-06-25 06:09:30.363 INFO: val_e/atom_rmse: 0.002885
val_f_mae: 0.036374
2025-06-25 06:09:30.363 INFO: val_f_mae: 0.036374
val_f_rmse: 0.061043
2025-06-25 06:09:30.363 INFO: val_f_rmse: 0.061043
##### Step: 69 Learning rate: 0.00125 #####
2025-06-25 06:10:00.839 INFO: ##### Step: 69 Learning rate: 0.00125 #####
Epoch 30, Train Loss: 3.7480, Val Loss: 3.7630
2025-06-25 06:10:00.840 INFO: Epoch 30, Train Loss: 3.7480, Val Loss: 3.7630
train_e/atom_mae: 0.002924
2025-06-25 06:10:00.840 INFO: train_e/atom_mae: 0.002924
train_e/atom_rmse: 0.003604
2025-06-25 06:10:00.841 INFO: train_e/atom_rmse: 0.003604
train_f_mae: 0.035658
2025-06-25 06:10:00.844 INFO: train_f_mae: 0.035658
train_f_rmse: 0.059924
2025-06-25 06:10:00.844 INFO: train_f_rmse: 0.059924
val_e/atom_mae: 0.002350
2025-06-25 06:10:00.847 INFO: val_e/atom_mae: 0.002350
val_e/atom_rmse: 0.002771
2025-06-25 06:10:00.847 INFO: val_e/atom_rmse: 0.002771
val_f_mae: 0.036061
2025-06-25 06:10:00.848 INFO: val_f_mae: 0.036061
val_f_rmse: 0.060582
2025-06-25 06:10:00.848 INFO: val_f_rmse: 0.060582
##### Step: 70 Learning rate: 0.00125 #####
2025-06-25 06:10:31.188 INFO: ##### Step: 70 Learning rate: 0.00125 #####
Epoch 31, Train Loss: 3.7278, Val Loss: 3.8397
2025-06-25 06:10:31.188 INFO: Epoch 31, Train Loss: 3.7278, Val Loss: 3.8397
train_e/atom_mae: 0.002826
2025-06-25 06:10:31.189 INFO: train_e/atom_mae: 0.002826
train_e/atom_rmse: 0.003437
2025-06-25 06:10:31.189 INFO: train_e/atom_rmse: 0.003437
train_f_mae: 0.035528
2025-06-25 06:10:31.193 INFO: train_f_mae: 0.035528
train_f_rmse: 0.059873
2025-06-25 06:10:31.193 INFO: train_f_rmse: 0.059873
val_e/atom_mae: 0.003329
2025-06-25 06:10:31.196 INFO: val_e/atom_mae: 0.003329
val_e/atom_rmse: 0.003966
2025-06-25 06:10:31.196 INFO: val_e/atom_rmse: 0.003966
val_f_mae: 0.036198
2025-06-25 06:10:31.197 INFO: val_f_mae: 0.036198
val_f_rmse: 0.060410
2025-06-25 06:10:31.197 INFO: val_f_rmse: 0.060410
##### Step: 71 Learning rate: 0.00125 #####
2025-06-25 06:11:01.599 INFO: ##### Step: 71 Learning rate: 0.00125 #####
Epoch 32, Train Loss: 3.7308, Val Loss: 3.9358
2025-06-25 06:11:01.600 INFO: Epoch 32, Train Loss: 3.7308, Val Loss: 3.9358
train_e/atom_mae: 0.002794
2025-06-25 06:11:01.600 INFO: train_e/atom_mae: 0.002794
train_e/atom_rmse: 0.003450
2025-06-25 06:11:01.601 INFO: train_e/atom_rmse: 0.003450
train_f_mae: 0.035600
2025-06-25 06:11:01.604 INFO: train_f_mae: 0.035600
train_f_rmse: 0.059889
2025-06-25 06:11:01.604 INFO: train_f_rmse: 0.059889
val_e/atom_mae: 0.002794
2025-06-25 06:11:01.607 INFO: val_e/atom_mae: 0.002794
val_e/atom_rmse: 0.003489
2025-06-25 06:11:01.607 INFO: val_e/atom_rmse: 0.003489
val_f_mae: 0.037086
2025-06-25 06:11:01.608 INFO: val_f_mae: 0.037086
val_f_rmse: 0.061551
2025-06-25 06:11:01.608 INFO: val_f_rmse: 0.061551
##### Step: 72 Learning rate: 0.00125 #####
2025-06-25 06:11:32.058 INFO: ##### Step: 72 Learning rate: 0.00125 #####
Epoch 33, Train Loss: 3.8028, Val Loss: 3.6760
2025-06-25 06:11:32.059 INFO: Epoch 33, Train Loss: 3.8028, Val Loss: 3.6760
train_e/atom_mae: 0.003408
2025-06-25 06:11:32.060 INFO: train_e/atom_mae: 0.003408
train_e/atom_rmse: 0.004306
2025-06-25 06:11:32.060 INFO: train_e/atom_rmse: 0.004306
train_f_mae: 0.035563
2025-06-25 06:11:32.064 INFO: train_f_mae: 0.035563
train_f_rmse: 0.059820
2025-06-25 06:11:32.064 INFO: train_f_rmse: 0.059820
val_e/atom_mae: 0.002217
2025-06-25 06:11:32.066 INFO: val_e/atom_mae: 0.002217
val_e/atom_rmse: 0.002617
2025-06-25 06:11:32.067 INFO: val_e/atom_rmse: 0.002617
val_f_mae: 0.035829
2025-06-25 06:11:32.067 INFO: val_f_mae: 0.035829
val_f_rmse: 0.059942
2025-06-25 06:11:32.067 INFO: val_f_rmse: 0.059942
##### Step: 73 Learning rate: 0.00125 #####
2025-06-25 06:12:02.368 INFO: ##### Step: 73 Learning rate: 0.00125 #####
Epoch 34, Train Loss: 3.7328, Val Loss: 3.8546
2025-06-25 06:12:02.368 INFO: Epoch 34, Train Loss: 3.7328, Val Loss: 3.8546
train_e/atom_mae: 0.002952
2025-06-25 06:12:02.369 INFO: train_e/atom_mae: 0.002952
train_e/atom_rmse: 0.003656
2025-06-25 06:12:02.369 INFO: train_e/atom_rmse: 0.003656
train_f_mae: 0.035499
2025-06-25 06:12:02.373 INFO: train_f_mae: 0.035499
train_f_rmse: 0.059758
2025-06-25 06:12:02.373 INFO: train_f_rmse: 0.059758
val_e/atom_mae: 0.003351
2025-06-25 06:12:02.376 INFO: val_e/atom_mae: 0.003351
val_e/atom_rmse: 0.003973
2025-06-25 06:12:02.376 INFO: val_e/atom_rmse: 0.003973
val_f_mae: 0.036183
2025-06-25 06:12:02.376 INFO: val_f_mae: 0.036183
val_f_rmse: 0.060528
2025-06-25 06:12:02.377 INFO: val_f_rmse: 0.060528
##### Step: 74 Learning rate: 0.00125 #####
2025-06-25 06:12:32.741 INFO: ##### Step: 74 Learning rate: 0.00125 #####
Epoch 35, Train Loss: 3.7233, Val Loss: 3.8916
2025-06-25 06:12:32.741 INFO: Epoch 35, Train Loss: 3.7233, Val Loss: 3.8916
train_e/atom_mae: 0.002701
2025-06-25 06:12:32.742 INFO: train_e/atom_mae: 0.002701
train_e/atom_rmse: 0.003318
2025-06-25 06:12:32.743 INFO: train_e/atom_rmse: 0.003318
train_f_mae: 0.035601
2025-06-25 06:12:32.746 INFO: train_f_mae: 0.035601
train_f_rmse: 0.059918
2025-06-25 06:12:32.746 INFO: train_f_rmse: 0.059918
val_e/atom_mae: 0.003819
2025-06-25 06:12:32.749 INFO: val_e/atom_mae: 0.003819
val_e/atom_rmse: 0.004449
2025-06-25 06:12:32.749 INFO: val_e/atom_rmse: 0.004449
val_f_mae: 0.036039
2025-06-25 06:12:32.750 INFO: val_f_mae: 0.036039
val_f_rmse: 0.060433
2025-06-25 06:12:32.750 INFO: val_f_rmse: 0.060433
##### Step: 75 Learning rate: 0.00125 #####
2025-06-25 06:13:03.037 INFO: ##### Step: 75 Learning rate: 0.00125 #####
Epoch 36, Train Loss: 3.6834, Val Loss: 3.6968
2025-06-25 06:13:03.037 INFO: Epoch 36, Train Loss: 3.6834, Val Loss: 3.6968
train_e/atom_mae: 0.002442
2025-06-25 06:13:03.038 INFO: train_e/atom_mae: 0.002442
train_e/atom_rmse: 0.002942
2025-06-25 06:13:03.038 INFO: train_e/atom_rmse: 0.002942
train_f_mae: 0.035551
2025-06-25 06:13:03.042 INFO: train_f_mae: 0.035551
train_f_rmse: 0.059823
2025-06-25 06:13:03.042 INFO: train_f_rmse: 0.059823
val_e/atom_mae: 0.002396
2025-06-25 06:13:03.045 INFO: val_e/atom_mae: 0.002396
val_e/atom_rmse: 0.002951
2025-06-25 06:13:03.045 INFO: val_e/atom_rmse: 0.002951
val_f_mae: 0.035799
2025-06-25 06:13:03.046 INFO: val_f_mae: 0.035799
val_f_rmse: 0.059929
2025-06-25 06:13:03.046 INFO: val_f_rmse: 0.059929
##### Step: 76 Learning rate: 0.00125 #####
2025-06-25 06:13:33.484 INFO: ##### Step: 76 Learning rate: 0.00125 #####
Epoch 37, Train Loss: 3.7207, Val Loss: 3.7594
2025-06-25 06:13:33.484 INFO: Epoch 37, Train Loss: 3.7207, Val Loss: 3.7594
train_e/atom_mae: 0.002879
2025-06-25 06:13:33.485 INFO: train_e/atom_mae: 0.002879
train_e/atom_rmse: 0.003536
2025-06-25 06:13:33.485 INFO: train_e/atom_rmse: 0.003536
train_f_mae: 0.035487
2025-06-25 06:13:33.489 INFO: train_f_mae: 0.035487
train_f_rmse: 0.059744
2025-06-25 06:13:33.489 INFO: train_f_rmse: 0.059744
val_e/atom_mae: 0.002327
2025-06-25 06:13:33.491 INFO: val_e/atom_mae: 0.002327
val_e/atom_rmse: 0.002845
2025-06-25 06:13:33.492 INFO: val_e/atom_rmse: 0.002845
val_f_mae: 0.036120
2025-06-25 06:13:33.492 INFO: val_f_mae: 0.036120
val_f_rmse: 0.060510
2025-06-25 06:13:33.492 INFO: val_f_rmse: 0.060510
##### Step: 77 Learning rate: 0.00125 #####
2025-06-25 06:14:03.797 INFO: ##### Step: 77 Learning rate: 0.00125 #####
Epoch 38, Train Loss: 3.7550, Val Loss: 3.9794
2025-06-25 06:14:03.797 INFO: Epoch 38, Train Loss: 3.7550, Val Loss: 3.9794
train_e/atom_mae: 0.003000
2025-06-25 06:14:03.798 INFO: train_e/atom_mae: 0.003000
train_e/atom_rmse: 0.003697
2025-06-25 06:14:03.798 INFO: train_e/atom_rmse: 0.003697
train_f_mae: 0.035498
2025-06-25 06:14:03.802 INFO: train_f_mae: 0.035498
train_f_rmse: 0.059913
2025-06-25 06:14:03.802 INFO: train_f_rmse: 0.059913
val_e/atom_mae: 0.004810
2025-06-25 06:14:03.805 INFO: val_e/atom_mae: 0.004810
val_e/atom_rmse: 0.005461
2025-06-25 06:14:03.805 INFO: val_e/atom_rmse: 0.005461
val_f_mae: 0.036105
2025-06-25 06:14:03.805 INFO: val_f_mae: 0.036105
val_f_rmse: 0.060154
2025-06-25 06:14:03.806 INFO: val_f_rmse: 0.060154
##### Step: 78 Learning rate: 0.00125 #####
2025-06-25 06:14:34.167 INFO: ##### Step: 78 Learning rate: 0.00125 #####
Epoch 39, Train Loss: 3.6790, Val Loss: 3.7221
2025-06-25 06:14:34.167 INFO: Epoch 39, Train Loss: 3.6790, Val Loss: 3.7221
train_e/atom_mae: 0.002479
2025-06-25 06:14:34.168 INFO: train_e/atom_mae: 0.002479
train_e/atom_rmse: 0.002988
2025-06-25 06:14:34.169 INFO: train_e/atom_rmse: 0.002988
train_f_mae: 0.035425
2025-06-25 06:14:34.172 INFO: train_f_mae: 0.035425
train_f_rmse: 0.059757
2025-06-25 06:14:34.172 INFO: train_f_rmse: 0.059757
val_e/atom_mae: 0.002277
2025-06-25 06:14:34.175 INFO: val_e/atom_mae: 0.002277
val_e/atom_rmse: 0.002866
2025-06-25 06:14:34.175 INFO: val_e/atom_rmse: 0.002866
val_f_mae: 0.035807
2025-06-25 06:14:34.176 INFO: val_f_mae: 0.035807
val_f_rmse: 0.060189
2025-06-25 06:14:34.176 INFO: val_f_rmse: 0.060189
##### Step: 79 Learning rate: 0.00125 #####
2025-06-25 06:15:04.609 INFO: ##### Step: 79 Learning rate: 0.00125 #####
Epoch 40, Train Loss: 3.6888, Val Loss: 3.7351
2025-06-25 06:15:04.609 INFO: Epoch 40, Train Loss: 3.6888, Val Loss: 3.7351
train_e/atom_mae: 0.002690
2025-06-25 06:15:04.610 INFO: train_e/atom_mae: 0.002690
train_e/atom_rmse: 0.003306
2025-06-25 06:15:04.610 INFO: train_e/atom_rmse: 0.003306
train_f_mae: 0.035368
2025-06-25 06:15:04.614 INFO: train_f_mae: 0.035368
train_f_rmse: 0.059637
2025-06-25 06:15:04.614 INFO: train_f_rmse: 0.059637
val_e/atom_mae: 0.002820
2025-06-25 06:15:04.617 INFO: val_e/atom_mae: 0.002820
val_e/atom_rmse: 0.003404
2025-06-25 06:15:04.617 INFO: val_e/atom_rmse: 0.003404
val_f_mae: 0.035800
2025-06-25 06:15:04.618 INFO: val_f_mae: 0.035800
val_f_rmse: 0.059957
2025-06-25 06:15:04.618 INFO: val_f_rmse: 0.059957
##### Step: 80 Learning rate: 0.000625 #####
2025-06-25 06:15:34.915 INFO: ##### Step: 80 Learning rate: 0.000625 #####
Epoch 41, Train Loss: 3.6417, Val Loss: 3.7223
2025-06-25 06:15:34.915 INFO: Epoch 41, Train Loss: 3.6417, Val Loss: 3.7223
train_e/atom_mae: 0.002437
2025-06-25 06:15:34.916 INFO: train_e/atom_mae: 0.002437
train_e/atom_rmse: 0.002929
2025-06-25 06:15:34.916 INFO: train_e/atom_rmse: 0.002929
train_f_mae: 0.035265
2025-06-25 06:15:34.920 INFO: train_f_mae: 0.035265
train_f_rmse: 0.059480
2025-06-25 06:15:34.920 INFO: train_f_rmse: 0.059480
val_e/atom_mae: 0.002364
2025-06-25 06:15:34.923 INFO: val_e/atom_mae: 0.002364
val_e/atom_rmse: 0.003036
2025-06-25 06:15:34.923 INFO: val_e/atom_rmse: 0.003036
val_f_mae: 0.035979
2025-06-25 06:15:34.923 INFO: val_f_mae: 0.035979
val_f_rmse: 0.060090
2025-06-25 06:15:34.924 INFO: val_f_rmse: 0.060090
##### Step: 81 Learning rate: 0.000625 #####
2025-06-25 06:16:05.381 INFO: ##### Step: 81 Learning rate: 0.000625 #####
Epoch 42, Train Loss: 3.6919, Val Loss: 3.6632
2025-06-25 06:16:05.381 INFO: Epoch 42, Train Loss: 3.6919, Val Loss: 3.6632
train_e/atom_mae: 0.003114
2025-06-25 06:16:05.382 INFO: train_e/atom_mae: 0.003114
train_e/atom_rmse: 0.003822
2025-06-25 06:16:05.382 INFO: train_e/atom_rmse: 0.003822
train_f_mae: 0.035114
2025-06-25 06:16:05.386 INFO: train_f_mae: 0.035114
train_f_rmse: 0.059289
2025-06-25 06:16:05.386 INFO: train_f_rmse: 0.059289
val_e/atom_mae: 0.002165
2025-06-25 06:16:05.389 INFO: val_e/atom_mae: 0.002165
val_e/atom_rmse: 0.002255
2025-06-25 06:16:05.389 INFO: val_e/atom_rmse: 0.002255
val_f_mae: 0.035770
2025-06-25 06:16:05.390 INFO: val_f_mae: 0.035770
val_f_rmse: 0.060014
2025-06-25 06:16:05.390 INFO: val_f_rmse: 0.060014
##### Step: 82 Learning rate: 0.000625 #####
2025-06-25 06:16:35.655 INFO: ##### Step: 82 Learning rate: 0.000625 #####
Epoch 43, Train Loss: 3.6479, Val Loss: 3.6565
2025-06-25 06:16:35.656 INFO: Epoch 43, Train Loss: 3.6479, Val Loss: 3.6565
train_e/atom_mae: 0.002721
2025-06-25 06:16:35.657 INFO: train_e/atom_mae: 0.002721
train_e/atom_rmse: 0.003291
2025-06-25 06:16:35.657 INFO: train_e/atom_rmse: 0.003291
train_f_mae: 0.035116
2025-06-25 06:16:35.660 INFO: train_f_mae: 0.035116
train_f_rmse: 0.059303
2025-06-25 06:16:35.661 INFO: train_f_rmse: 0.059303
val_e/atom_mae: 0.002159
2025-06-25 06:16:35.663 INFO: val_e/atom_mae: 0.002159
val_e/atom_rmse: 0.002250
2025-06-25 06:16:35.664 INFO: val_e/atom_rmse: 0.002250
val_f_mae: 0.035807
2025-06-25 06:16:35.664 INFO: val_f_mae: 0.035807
val_f_rmse: 0.059960
2025-06-25 06:16:35.664 INFO: val_f_rmse: 0.059960
##### Step: 83 Learning rate: 0.000625 #####
2025-06-25 06:17:06.045 INFO: ##### Step: 83 Learning rate: 0.000625 #####
Epoch 44, Train Loss: 3.6411, Val Loss: 3.6796
2025-06-25 06:17:06.045 INFO: Epoch 44, Train Loss: 3.6411, Val Loss: 3.6796
train_e/atom_mae: 0.002415
2025-06-25 06:17:06.046 INFO: train_e/atom_mae: 0.002415
train_e/atom_rmse: 0.002894
2025-06-25 06:17:06.046 INFO: train_e/atom_rmse: 0.002894
train_f_mae: 0.035247
2025-06-25 06:17:06.050 INFO: train_f_mae: 0.035247
train_f_rmse: 0.059496
2025-06-25 06:17:06.050 INFO: train_f_rmse: 0.059496
val_e/atom_mae: 0.002213
2025-06-25 06:17:06.053 INFO: val_e/atom_mae: 0.002213
val_e/atom_rmse: 0.002576
2025-06-25 06:17:06.053 INFO: val_e/atom_rmse: 0.002576
val_f_mae: 0.035895
2025-06-25 06:17:06.053 INFO: val_f_mae: 0.035895
val_f_rmse: 0.059994
2025-06-25 06:17:06.053 INFO: val_f_rmse: 0.059994
##### Step: 84 Learning rate: 0.000625 #####
2025-06-25 06:17:36.432 INFO: ##### Step: 84 Learning rate: 0.000625 #####
Epoch 45, Train Loss: 3.6063, Val Loss: 3.7064
2025-06-25 06:17:36.432 INFO: Epoch 45, Train Loss: 3.6063, Val Loss: 3.7064
train_e/atom_mae: 0.002320
2025-06-25 06:17:36.433 INFO: train_e/atom_mae: 0.002320
train_e/atom_rmse: 0.002723
2025-06-25 06:17:36.433 INFO: train_e/atom_rmse: 0.002723
train_f_mae: 0.035079
2025-06-25 06:17:36.437 INFO: train_f_mae: 0.035079
train_f_rmse: 0.059300
2025-06-25 06:17:36.437 INFO: train_f_rmse: 0.059300
val_e/atom_mae: 0.002376
2025-06-25 06:17:36.439 INFO: val_e/atom_mae: 0.002376
val_e/atom_rmse: 0.002838
2025-06-25 06:17:36.440 INFO: val_e/atom_rmse: 0.002838
val_f_mae: 0.035942
2025-06-25 06:17:36.440 INFO: val_f_mae: 0.035942
val_f_rmse: 0.060074
2025-06-25 06:17:36.440 INFO: val_f_rmse: 0.060074
##### Step: 85 Learning rate: 0.000625 #####
2025-06-25 06:18:06.767 INFO: ##### Step: 85 Learning rate: 0.000625 #####
Epoch 46, Train Loss: 3.6539, Val Loss: 3.6667
2025-06-25 06:18:06.767 INFO: Epoch 46, Train Loss: 3.6539, Val Loss: 3.6667
train_e/atom_mae: 0.002699
2025-06-25 06:18:06.768 INFO: train_e/atom_mae: 0.002699
train_e/atom_rmse: 0.003286
2025-06-25 06:18:06.768 INFO: train_e/atom_rmse: 0.003286
train_f_mae: 0.035171
2025-06-25 06:18:06.772 INFO: train_f_mae: 0.035171
train_f_rmse: 0.059357
2025-06-25 06:18:06.772 INFO: train_f_rmse: 0.059357
val_e/atom_mae: 0.002273
2025-06-25 06:18:06.775 INFO: val_e/atom_mae: 0.002273
val_e/atom_rmse: 0.002805
2025-06-25 06:18:06.775 INFO: val_e/atom_rmse: 0.002805
val_f_mae: 0.035770
2025-06-25 06:18:06.776 INFO: val_f_mae: 0.035770
val_f_rmse: 0.059762
2025-06-25 06:18:06.776 INFO: val_f_rmse: 0.059762
##### Step: 86 Learning rate: 0.000625 #####
2025-06-25 06:18:37.201 INFO: ##### Step: 86 Learning rate: 0.000625 #####
Epoch 47, Train Loss: 3.6073, Val Loss: 3.6988
2025-06-25 06:18:37.202 INFO: Epoch 47, Train Loss: 3.6073, Val Loss: 3.6988
train_e/atom_mae: 0.002321
2025-06-25 06:18:37.203 INFO: train_e/atom_mae: 0.002321
train_e/atom_rmse: 0.002729
2025-06-25 06:18:37.203 INFO: train_e/atom_rmse: 0.002729
train_f_mae: 0.035108
2025-06-25 06:18:37.206 INFO: train_f_mae: 0.035108
train_f_rmse: 0.059306
2025-06-25 06:18:37.206 INFO: train_f_rmse: 0.059306
val_e/atom_mae: 0.002317
2025-06-25 06:18:37.209 INFO: val_e/atom_mae: 0.002317
val_e/atom_rmse: 0.002718
2025-06-25 06:18:37.209 INFO: val_e/atom_rmse: 0.002718
val_f_mae: 0.036001
2025-06-25 06:18:37.210 INFO: val_f_mae: 0.036001
val_f_rmse: 0.060079
2025-06-25 06:18:37.210 INFO: val_f_rmse: 0.060079
##### Step: 87 Learning rate: 0.000625 #####
2025-06-25 06:19:07.513 INFO: ##### Step: 87 Learning rate: 0.000625 #####
Epoch 48, Train Loss: 3.6407, Val Loss: 3.6717
2025-06-25 06:19:07.514 INFO: Epoch 48, Train Loss: 3.6407, Val Loss: 3.6717
train_e/atom_mae: 0.002561
2025-06-25 06:19:07.515 INFO: train_e/atom_mae: 0.002561
train_e/atom_rmse: 0.003110
2025-06-25 06:19:07.515 INFO: train_e/atom_rmse: 0.003110
train_f_mae: 0.035104
2025-06-25 06:19:07.518 INFO: train_f_mae: 0.035104
train_f_rmse: 0.059360
2025-06-25 06:19:07.519 INFO: train_f_rmse: 0.059360
val_e/atom_mae: 0.002236
2025-06-25 06:19:07.521 INFO: val_e/atom_mae: 0.002236
val_e/atom_rmse: 0.002669
2025-06-25 06:19:07.522 INFO: val_e/atom_rmse: 0.002669
val_f_mae: 0.035656
2025-06-25 06:19:07.522 INFO: val_f_mae: 0.035656
val_f_rmse: 0.059879
2025-06-25 06:19:07.522 INFO: val_f_rmse: 0.059879
##### Step: 88 Learning rate: 0.000625 #####
2025-06-25 06:19:37.999 INFO: ##### Step: 88 Learning rate: 0.000625 #####
Epoch 49, Train Loss: 3.6049, Val Loss: 3.6306
2025-06-25 06:19:37.999 INFO: Epoch 49, Train Loss: 3.6049, Val Loss: 3.6306
train_e/atom_mae: 0.002340
2025-06-25 06:19:38.000 INFO: train_e/atom_mae: 0.002340
train_e/atom_rmse: 0.002737
2025-06-25 06:19:38.000 INFO: train_e/atom_rmse: 0.002737
train_f_mae: 0.035091
2025-06-25 06:19:38.003 INFO: train_f_mae: 0.035091
train_f_rmse: 0.059281
2025-06-25 06:19:38.003 INFO: train_f_rmse: 0.059281
val_e/atom_mae: 0.002134
2025-06-25 06:19:38.006 INFO: val_e/atom_mae: 0.002134
val_e/atom_rmse: 0.002304
2025-06-25 06:19:38.006 INFO: val_e/atom_rmse: 0.002304
val_f_mae: 0.035717
2025-06-25 06:19:38.007 INFO: val_f_mae: 0.035717
val_f_rmse: 0.059719
2025-06-25 06:19:38.007 INFO: val_f_rmse: 0.059719
##### Step: 89 Learning rate: 0.000625 #####
2025-06-25 06:20:08.511 INFO: ##### Step: 89 Learning rate: 0.000625 #####
Epoch 50, Train Loss: 3.6035, Val Loss: 3.7478
2025-06-25 06:20:08.511 INFO: Epoch 50, Train Loss: 3.6035, Val Loss: 3.7478
train_e/atom_mae: 0.002376
2025-06-25 06:20:08.512 INFO: train_e/atom_mae: 0.002376
train_e/atom_rmse: 0.002828
2025-06-25 06:20:08.512 INFO: train_e/atom_rmse: 0.002828
train_f_mae: 0.035056
2025-06-25 06:20:08.515 INFO: train_f_mae: 0.035056
train_f_rmse: 0.059218
2025-06-25 06:20:08.516 INFO: train_f_rmse: 0.059218
val_e/atom_mae: 0.002323
2025-06-25 06:20:08.518 INFO: val_e/atom_mae: 0.002323
val_e/atom_rmse: 0.002869
2025-06-25 06:20:08.519 INFO: val_e/atom_rmse: 0.002869
val_f_mae: 0.036094
2025-06-25 06:20:08.519 INFO: val_f_mae: 0.036094
val_f_rmse: 0.060400
2025-06-25 06:20:08.519 INFO: val_f_rmse: 0.060400
##### Step: 90 Learning rate: 0.000625 #####
2025-06-25 06:20:38.939 INFO: ##### Step: 90 Learning rate: 0.000625 #####
Epoch 51, Train Loss: 3.6303, Val Loss: 3.6742
2025-06-25 06:20:38.939 INFO: Epoch 51, Train Loss: 3.6303, Val Loss: 3.6742
train_e/atom_mae: 0.002457
2025-06-25 06:20:38.940 INFO: train_e/atom_mae: 0.002457
train_e/atom_rmse: 0.002956
2025-06-25 06:20:38.940 INFO: train_e/atom_rmse: 0.002956
train_f_mae: 0.035116
2025-06-25 06:20:38.944 INFO: train_f_mae: 0.035116
train_f_rmse: 0.059368
2025-06-25 06:20:38.944 INFO: train_f_rmse: 0.059368
val_e/atom_mae: 0.002169
2025-06-25 06:20:38.947 INFO: val_e/atom_mae: 0.002169
val_e/atom_rmse: 0.002342
2025-06-25 06:20:38.947 INFO: val_e/atom_rmse: 0.002342
val_f_mae: 0.035793
2025-06-25 06:20:38.948 INFO: val_f_mae: 0.035793
val_f_rmse: 0.060065
2025-06-25 06:20:38.948 INFO: val_f_rmse: 0.060065
##### Step: 91 Learning rate: 0.000625 #####
2025-06-25 06:21:09.384 INFO: ##### Step: 91 Learning rate: 0.000625 #####
Epoch 52, Train Loss: 3.6128, Val Loss: 3.6621
2025-06-25 06:21:09.384 INFO: Epoch 52, Train Loss: 3.6128, Val Loss: 3.6621
train_e/atom_mae: 0.002434
2025-06-25 06:21:09.385 INFO: train_e/atom_mae: 0.002434
train_e/atom_rmse: 0.002944
2025-06-25 06:21:09.385 INFO: train_e/atom_rmse: 0.002944
train_f_mae: 0.035062
2025-06-25 06:21:09.389 INFO: train_f_mae: 0.035062
train_f_rmse: 0.059228
2025-06-25 06:21:09.389 INFO: train_f_rmse: 0.059228
val_e/atom_mae: 0.002163
2025-06-25 06:21:09.391 INFO: val_e/atom_mae: 0.002163
val_e/atom_rmse: 0.002376
2025-06-25 06:21:09.392 INFO: val_e/atom_rmse: 0.002376
val_f_mae: 0.035658
2025-06-25 06:21:09.392 INFO: val_f_mae: 0.035658
val_f_rmse: 0.059948
2025-06-25 06:21:09.392 INFO: val_f_rmse: 0.059948
##### Step: 92 Learning rate: 0.000625 #####
2025-06-25 06:21:39.696 INFO: ##### Step: 92 Learning rate: 0.000625 #####
Epoch 53, Train Loss: 3.6130, Val Loss: 3.6682
2025-06-25 06:21:39.696 INFO: Epoch 53, Train Loss: 3.6130, Val Loss: 3.6682
train_e/atom_mae: 0.002328
2025-06-25 06:21:39.697 INFO: train_e/atom_mae: 0.002328
train_e/atom_rmse: 0.002772
2025-06-25 06:21:39.697 INFO: train_e/atom_rmse: 0.002772
train_f_mae: 0.035133
2025-06-25 06:21:39.701 INFO: train_f_mae: 0.035133
train_f_rmse: 0.059330
2025-06-25 06:21:39.701 INFO: train_f_rmse: 0.059330
val_e/atom_mae: 0.002499
2025-06-25 06:21:39.704 INFO: val_e/atom_mae: 0.002499
val_e/atom_rmse: 0.003073
2025-06-25 06:21:39.704 INFO: val_e/atom_rmse: 0.003073
val_f_mae: 0.035577
2025-06-25 06:21:39.704 INFO: val_f_mae: 0.035577
val_f_rmse: 0.059615
2025-06-25 06:21:39.705 INFO: val_f_rmse: 0.059615
##### Step: 93 Learning rate: 0.000625 #####
2025-06-25 06:22:10.173 INFO: ##### Step: 93 Learning rate: 0.000625 #####
Epoch 54, Train Loss: 3.6159, Val Loss: 3.6969
2025-06-25 06:22:10.174 INFO: Epoch 54, Train Loss: 3.6159, Val Loss: 3.6969
train_e/atom_mae: 0.002442
2025-06-25 06:22:10.175 INFO: train_e/atom_mae: 0.002442
train_e/atom_rmse: 0.002930
2025-06-25 06:22:10.175 INFO: train_e/atom_rmse: 0.002930
train_f_mae: 0.035070
2025-06-25 06:22:10.179 INFO: train_f_mae: 0.035070
train_f_rmse: 0.059262
2025-06-25 06:22:10.179 INFO: train_f_rmse: 0.059262
val_e/atom_mae: 0.002583
2025-06-25 06:22:10.181 INFO: val_e/atom_mae: 0.002583
val_e/atom_rmse: 0.003079
2025-06-25 06:22:10.182 INFO: val_e/atom_rmse: 0.003079
val_f_mae: 0.035622
2025-06-25 06:22:10.182 INFO: val_f_mae: 0.035622
val_f_rmse: 0.059851
2025-06-25 06:22:10.182 INFO: val_f_rmse: 0.059851
##### Step: 94 Learning rate: 0.000625 #####
2025-06-25 06:22:40.468 INFO: ##### Step: 94 Learning rate: 0.000625 #####
Epoch 55, Train Loss: 3.6342, Val Loss: 3.7125
2025-06-25 06:22:40.468 INFO: Epoch 55, Train Loss: 3.6342, Val Loss: 3.7125
train_e/atom_mae: 0.002398
2025-06-25 06:22:40.469 INFO: train_e/atom_mae: 0.002398
train_e/atom_rmse: 0.002858
2025-06-25 06:22:40.469 INFO: train_e/atom_rmse: 0.002858
train_f_mae: 0.035263
2025-06-25 06:22:40.473 INFO: train_f_mae: 0.035263
train_f_rmse: 0.059459
2025-06-25 06:22:40.473 INFO: train_f_rmse: 0.059459
val_e/atom_mae: 0.002186
2025-06-25 06:22:40.476 INFO: val_e/atom_mae: 0.002186
val_e/atom_rmse: 0.002318
2025-06-25 06:22:40.476 INFO: val_e/atom_rmse: 0.002318
val_f_mae: 0.035945
2025-06-25 06:22:40.476 INFO: val_f_mae: 0.035945
val_f_rmse: 0.060395
2025-06-25 06:22:40.477 INFO: val_f_rmse: 0.060395
##### Step: 95 Learning rate: 0.000625 #####
2025-06-25 06:23:10.940 INFO: ##### Step: 95 Learning rate: 0.000625 #####
Epoch 56, Train Loss: 3.6192, Val Loss: 3.6852
2025-06-25 06:23:10.940 INFO: Epoch 56, Train Loss: 3.6192, Val Loss: 3.6852
train_e/atom_mae: 0.002433
2025-06-25 06:23:10.941 INFO: train_e/atom_mae: 0.002433
train_e/atom_rmse: 0.002916
2025-06-25 06:23:10.941 INFO: train_e/atom_rmse: 0.002916
train_f_mae: 0.035091
2025-06-25 06:23:10.945 INFO: train_f_mae: 0.035091
train_f_rmse: 0.059299
2025-06-25 06:23:10.945 INFO: train_f_rmse: 0.059299
val_e/atom_mae: 0.002203
2025-06-25 06:23:10.948 INFO: val_e/atom_mae: 0.002203
val_e/atom_rmse: 0.002650
2025-06-25 06:23:10.948 INFO: val_e/atom_rmse: 0.002650
val_f_mae: 0.035649
2025-06-25 06:23:10.948 INFO: val_f_mae: 0.035649
val_f_rmse: 0.060002
2025-06-25 06:23:10.948 INFO: val_f_rmse: 0.060002
##### Step: 96 Learning rate: 0.000625 #####
2025-06-25 06:23:41.281 INFO: ##### Step: 96 Learning rate: 0.000625 #####
Epoch 57, Train Loss: 3.6164, Val Loss: 3.7380
2025-06-25 06:23:41.282 INFO: Epoch 57, Train Loss: 3.6164, Val Loss: 3.7380
train_e/atom_mae: 0.002363
2025-06-25 06:23:41.282 INFO: train_e/atom_mae: 0.002363
train_e/atom_rmse: 0.002800
2025-06-25 06:23:41.283 INFO: train_e/atom_rmse: 0.002800
train_f_mae: 0.035110
2025-06-25 06:23:41.286 INFO: train_f_mae: 0.035110
train_f_rmse: 0.059343
2025-06-25 06:23:41.286 INFO: train_f_rmse: 0.059343
val_e/atom_mae: 0.003046
2025-06-25 06:23:41.289 INFO: val_e/atom_mae: 0.003046
val_e/atom_rmse: 0.003588
2025-06-25 06:23:41.289 INFO: val_e/atom_rmse: 0.003588
val_f_mae: 0.035713
2025-06-25 06:23:41.290 INFO: val_f_mae: 0.035713
val_f_rmse: 0.059852
2025-06-25 06:23:41.290 INFO: val_f_rmse: 0.059852
##### Step: 97 Learning rate: 0.000625 #####
2025-06-25 06:24:11.594 INFO: ##### Step: 97 Learning rate: 0.000625 #####
Epoch 58, Train Loss: 3.6153, Val Loss: 3.6790
2025-06-25 06:24:11.594 INFO: Epoch 58, Train Loss: 3.6153, Val Loss: 3.6790
train_e/atom_mae: 0.002475
2025-06-25 06:24:11.595 INFO: train_e/atom_mae: 0.002475
train_e/atom_rmse: 0.002994
2025-06-25 06:24:11.595 INFO: train_e/atom_rmse: 0.002994
train_f_mae: 0.035097
2025-06-25 06:24:11.599 INFO: train_f_mae: 0.035097
train_f_rmse: 0.059218
2025-06-25 06:24:11.599 INFO: train_f_rmse: 0.059218
val_e/atom_mae: 0.002301
2025-06-25 06:24:11.601 INFO: val_e/atom_mae: 0.002301
val_e/atom_rmse: 0.002965
2025-06-25 06:24:11.602 INFO: val_e/atom_rmse: 0.002965
val_f_mae: 0.035604
2025-06-25 06:24:11.602 INFO: val_f_mae: 0.035604
val_f_rmse: 0.059772
2025-06-25 06:24:11.602 INFO: val_f_rmse: 0.059772
##### Step: 98 Learning rate: 0.000625 #####
2025-06-25 06:24:42.008 INFO: ##### Step: 98 Learning rate: 0.000625 #####
Epoch 59, Train Loss: 3.6215, Val Loss: 3.6984
2025-06-25 06:24:42.008 INFO: Epoch 59, Train Loss: 3.6215, Val Loss: 3.6984
train_e/atom_mae: 0.002522
2025-06-25 06:24:42.009 INFO: train_e/atom_mae: 0.002522
train_e/atom_rmse: 0.003034
2025-06-25 06:24:42.009 INFO: train_e/atom_rmse: 0.003034
train_f_mae: 0.034989
2025-06-25 06:24:42.013 INFO: train_f_mae: 0.034989
train_f_rmse: 0.059247
2025-06-25 06:24:42.013 INFO: train_f_rmse: 0.059247
val_e/atom_mae: 0.002239
2025-06-25 06:24:42.016 INFO: val_e/atom_mae: 0.002239
val_e/atom_rmse: 0.002637
2025-06-25 06:24:42.016 INFO: val_e/atom_rmse: 0.002637
val_f_mae: 0.036130
2025-06-25 06:24:42.016 INFO: val_f_mae: 0.036130
val_f_rmse: 0.060119
2025-06-25 06:24:42.016 INFO: val_f_rmse: 0.060119
##### Step: 99 Learning rate: 0.000625 #####
2025-06-25 06:25:12.325 INFO: ##### Step: 99 Learning rate: 0.000625 #####
Epoch 60, Train Loss: 3.5911, Val Loss: 3.6424
2025-06-25 06:25:12.326 INFO: Epoch 60, Train Loss: 3.5911, Val Loss: 3.6424
train_e/atom_mae: 0.002242
2025-06-25 06:25:12.327 INFO: train_e/atom_mae: 0.002242
train_e/atom_rmse: 0.002581
2025-06-25 06:25:12.327 INFO: train_e/atom_rmse: 0.002581
train_f_mae: 0.035072
2025-06-25 06:25:12.330 INFO: train_f_mae: 0.035072
train_f_rmse: 0.059250
2025-06-25 06:25:12.330 INFO: train_f_rmse: 0.059250
val_e/atom_mae: 0.002199
2025-06-25 06:25:12.333 INFO: val_e/atom_mae: 0.002199
val_e/atom_rmse: 0.002590
2025-06-25 06:25:12.333 INFO: val_e/atom_rmse: 0.002590
val_f_mae: 0.035514
2025-06-25 06:25:12.334 INFO: val_f_mae: 0.035514
val_f_rmse: 0.059676
2025-06-25 06:25:12.334 INFO: val_f_rmse: 0.059676
##### Step: 100 Learning rate: 0.0003125 #####
2025-06-25 06:25:42.745 INFO: ##### Step: 100 Learning rate: 0.0003125 #####
Epoch 61, Train Loss: 3.5726, Val Loss: 3.6213
2025-06-25 06:25:42.746 INFO: Epoch 61, Train Loss: 3.5726, Val Loss: 3.6213
train_e/atom_mae: 0.002243
2025-06-25 06:25:42.747 INFO: train_e/atom_mae: 0.002243
train_e/atom_rmse: 0.002588
2025-06-25 06:25:42.747 INFO: train_e/atom_rmse: 0.002588
train_f_mae: 0.034919
2025-06-25 06:25:42.750 INFO: train_f_mae: 0.034919
train_f_rmse: 0.059089
2025-06-25 06:25:42.750 INFO: train_f_rmse: 0.059089
val_e/atom_mae: 0.002130
2025-06-25 06:25:42.753 INFO: val_e/atom_mae: 0.002130
val_e/atom_rmse: 0.002273
2025-06-25 06:25:42.753 INFO: val_e/atom_rmse: 0.002273
val_f_mae: 0.035545
2025-06-25 06:25:42.754 INFO: val_f_mae: 0.035545
val_f_rmse: 0.059656
2025-06-25 06:25:42.754 INFO: val_f_rmse: 0.059656
##### Step: 101 Learning rate: 0.0003125 #####
2025-06-25 06:26:13.017 INFO: ##### Step: 101 Learning rate: 0.0003125 #####
Epoch 62, Train Loss: 3.5795, Val Loss: 3.6609
2025-06-25 06:26:13.018 INFO: Epoch 62, Train Loss: 3.5795, Val Loss: 3.6609
train_e/atom_mae: 0.002262
2025-06-25 06:26:13.019 INFO: train_e/atom_mae: 0.002262
train_e/atom_rmse: 0.002605
2025-06-25 06:26:13.019 INFO: train_e/atom_rmse: 0.002605
train_f_mae: 0.034958
2025-06-25 06:26:13.022 INFO: train_f_mae: 0.034958
train_f_rmse: 0.059139
2025-06-25 06:26:13.022 INFO: train_f_rmse: 0.059139
val_e/atom_mae: 0.002156
2025-06-25 06:26:13.025 INFO: val_e/atom_mae: 0.002156
val_e/atom_rmse: 0.002271
2025-06-25 06:26:13.025 INFO: val_e/atom_rmse: 0.002271
val_f_mae: 0.035689
2025-06-25 06:26:13.026 INFO: val_f_mae: 0.035689
val_f_rmse: 0.059987
2025-06-25 06:26:13.026 INFO: val_f_rmse: 0.059987
##### Step: 102 Learning rate: 0.0003125 #####
2025-06-25 06:26:43.415 INFO: ##### Step: 102 Learning rate: 0.0003125 #####
Epoch 63, Train Loss: 3.5776, Val Loss: 3.6322
2025-06-25 06:26:43.415 INFO: Epoch 63, Train Loss: 3.5776, Val Loss: 3.6322
train_e/atom_mae: 0.002214
2025-06-25 06:26:43.416 INFO: train_e/atom_mae: 0.002214
train_e/atom_rmse: 0.002538
2025-06-25 06:26:43.416 INFO: train_e/atom_rmse: 0.002538
train_f_mae: 0.035001
2025-06-25 06:26:43.420 INFO: train_f_mae: 0.035001
train_f_rmse: 0.059158
2025-06-25 06:26:43.420 INFO: train_f_rmse: 0.059158
val_e/atom_mae: 0.002152
2025-06-25 06:26:43.423 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002340
2025-06-25 06:26:43.423 INFO: val_e/atom_rmse: 0.002340
val_f_mae: 0.035605
2025-06-25 06:26:43.424 INFO: val_f_mae: 0.035605
val_f_rmse: 0.059716
2025-06-25 06:26:43.424 INFO: val_f_rmse: 0.059716
##### Step: 103 Learning rate: 0.0003125 #####
2025-06-25 06:27:13.793 INFO: ##### Step: 103 Learning rate: 0.0003125 #####
Epoch 64, Train Loss: 3.5835, Val Loss: 3.6315
2025-06-25 06:27:13.793 INFO: Epoch 64, Train Loss: 3.5835, Val Loss: 3.6315
train_e/atom_mae: 0.002302
2025-06-25 06:27:13.794 INFO: train_e/atom_mae: 0.002302
train_e/atom_rmse: 0.002681
2025-06-25 06:27:13.794 INFO: train_e/atom_rmse: 0.002681
train_f_mae: 0.034940
2025-06-25 06:27:13.798 INFO: train_f_mae: 0.034940
train_f_rmse: 0.059132
2025-06-25 06:27:13.798 INFO: train_f_rmse: 0.059132
val_e/atom_mae: 0.002151
2025-06-25 06:27:13.801 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002391
2025-06-25 06:27:13.801 INFO: val_e/atom_rmse: 0.002391
val_f_mae: 0.035683
2025-06-25 06:27:13.801 INFO: val_f_mae: 0.035683
val_f_rmse: 0.059686
2025-06-25 06:27:13.801 INFO: val_f_rmse: 0.059686
##### Step: 104 Learning rate: 0.0003125 #####
2025-06-25 06:27:44.129 INFO: ##### Step: 104 Learning rate: 0.0003125 #####
Epoch 65, Train Loss: 3.5919, Val Loss: 3.6478
2025-06-25 06:27:44.130 INFO: Epoch 65, Train Loss: 3.5919, Val Loss: 3.6478
train_e/atom_mae: 0.002321
2025-06-25 06:27:44.131 INFO: train_e/atom_mae: 0.002321
train_e/atom_rmse: 0.002747
2025-06-25 06:27:44.131 INFO: train_e/atom_rmse: 0.002747
train_f_mae: 0.035042
2025-06-25 06:27:44.134 INFO: train_f_mae: 0.035042
train_f_rmse: 0.059166
2025-06-25 06:27:44.134 INFO: train_f_rmse: 0.059166
val_e/atom_mae: 0.002157
2025-06-25 06:27:44.137 INFO: val_e/atom_mae: 0.002157
val_e/atom_rmse: 0.002266
2025-06-25 06:27:44.138 INFO: val_e/atom_rmse: 0.002266
val_f_mae: 0.035692
2025-06-25 06:27:44.138 INFO: val_f_mae: 0.035692
val_f_rmse: 0.059881
2025-06-25 06:27:44.138 INFO: val_f_rmse: 0.059881
##### Step: 105 Learning rate: 0.0003125 #####
2025-06-25 06:28:14.556 INFO: ##### Step: 105 Learning rate: 0.0003125 #####
Epoch 66, Train Loss: 3.5817, Val Loss: 3.6725
2025-06-25 06:28:14.556 INFO: Epoch 66, Train Loss: 3.5817, Val Loss: 3.6725
train_e/atom_mae: 0.002263
2025-06-25 06:28:14.557 INFO: train_e/atom_mae: 0.002263
train_e/atom_rmse: 0.002669
2025-06-25 06:28:14.557 INFO: train_e/atom_rmse: 0.002669
train_f_mae: 0.034935
2025-06-25 06:28:14.561 INFO: train_f_mae: 0.034935
train_f_rmse: 0.059122
2025-06-25 06:28:14.561 INFO: train_f_rmse: 0.059122
val_e/atom_mae: 0.002298
2025-06-25 06:28:14.564 INFO: val_e/atom_mae: 0.002298
val_e/atom_rmse: 0.002907
2025-06-25 06:28:14.564 INFO: val_e/atom_rmse: 0.002907
val_f_mae: 0.035687
2025-06-25 06:28:14.564 INFO: val_f_mae: 0.035687
val_f_rmse: 0.059752
2025-06-25 06:28:14.565 INFO: val_f_rmse: 0.059752
##### Step: 106 Learning rate: 0.0003125 #####
2025-06-25 06:28:44.852 INFO: ##### Step: 106 Learning rate: 0.0003125 #####
Epoch 67, Train Loss: 3.5814, Val Loss: 3.6496
2025-06-25 06:28:44.852 INFO: Epoch 67, Train Loss: 3.5814, Val Loss: 3.6496
train_e/atom_mae: 0.002241
2025-06-25 06:28:44.853 INFO: train_e/atom_mae: 0.002241
train_e/atom_rmse: 0.002582
2025-06-25 06:28:44.853 INFO: train_e/atom_rmse: 0.002582
train_f_mae: 0.034979
2025-06-25 06:28:44.857 INFO: train_f_mae: 0.034979
train_f_rmse: 0.059167
2025-06-25 06:28:44.857 INFO: train_f_rmse: 0.059167
val_e/atom_mae: 0.002210
2025-06-25 06:28:44.860 INFO: val_e/atom_mae: 0.002210
val_e/atom_rmse: 0.002637
2025-06-25 06:28:44.860 INFO: val_e/atom_rmse: 0.002637
val_f_mae: 0.035562
2025-06-25 06:28:44.860 INFO: val_f_mae: 0.035562
val_f_rmse: 0.059711
2025-06-25 06:28:44.861 INFO: val_f_rmse: 0.059711
##### Step: 107 Learning rate: 0.0003125 #####
2025-06-25 06:29:15.315 INFO: ##### Step: 107 Learning rate: 0.0003125 #####
Epoch 68, Train Loss: 3.5773, Val Loss: 3.6257
2025-06-25 06:29:15.316 INFO: Epoch 68, Train Loss: 3.5773, Val Loss: 3.6257
train_e/atom_mae: 0.002274
2025-06-25 06:29:15.317 INFO: train_e/atom_mae: 0.002274
train_e/atom_rmse: 0.002660
2025-06-25 06:29:15.317 INFO: train_e/atom_rmse: 0.002660
train_f_mae: 0.034943
2025-06-25 06:29:15.320 INFO: train_f_mae: 0.034943
train_f_rmse: 0.059090
2025-06-25 06:29:15.320 INFO: train_f_rmse: 0.059090
val_e/atom_mae: 0.002157
2025-06-25 06:29:15.323 INFO: val_e/atom_mae: 0.002157
val_e/atom_rmse: 0.002370
2025-06-25 06:29:15.323 INFO: val_e/atom_rmse: 0.002370
val_f_mae: 0.035547
2025-06-25 06:29:15.324 INFO: val_f_mae: 0.035547
val_f_rmse: 0.059647
2025-06-25 06:29:15.324 INFO: val_f_rmse: 0.059647
##### Step: 108 Learning rate: 0.0003125 #####
2025-06-25 06:29:45.604 INFO: ##### Step: 108 Learning rate: 0.0003125 #####
Epoch 69, Train Loss: 3.5849, Val Loss: 3.6288
2025-06-25 06:29:45.605 INFO: Epoch 69, Train Loss: 3.5849, Val Loss: 3.6288
train_e/atom_mae: 0.002302
2025-06-25 06:29:45.606 INFO: train_e/atom_mae: 0.002302
train_e/atom_rmse: 0.002700
2025-06-25 06:29:45.606 INFO: train_e/atom_rmse: 0.002700
train_f_mae: 0.034962
2025-06-25 06:29:45.609 INFO: train_f_mae: 0.034962
train_f_rmse: 0.059133
2025-06-25 06:29:45.610 INFO: train_f_rmse: 0.059133
val_e/atom_mae: 0.002238
2025-06-25 06:29:45.612 INFO: val_e/atom_mae: 0.002238
val_e/atom_rmse: 0.002755
2025-06-25 06:29:45.613 INFO: val_e/atom_rmse: 0.002755
val_f_mae: 0.035377
2025-06-25 06:29:45.613 INFO: val_f_mae: 0.035377
val_f_rmse: 0.059472
2025-06-25 06:29:45.613 INFO: val_f_rmse: 0.059472
##### Step: 109 Learning rate: 0.0003125 #####
2025-06-25 06:30:15.976 INFO: ##### Step: 109 Learning rate: 0.0003125 #####
Epoch 70, Train Loss: 3.5658, Val Loss: 3.7645
2025-06-25 06:30:15.976 INFO: Epoch 70, Train Loss: 3.5658, Val Loss: 3.7645
train_e/atom_mae: 0.002248
2025-06-25 06:30:15.977 INFO: train_e/atom_mae: 0.002248
train_e/atom_rmse: 0.002572
2025-06-25 06:30:15.977 INFO: train_e/atom_rmse: 0.002572
train_f_mae: 0.034885
2025-06-25 06:30:15.980 INFO: train_f_mae: 0.034885
train_f_rmse: 0.059040
2025-06-25 06:30:15.981 INFO: train_f_rmse: 0.059040
val_e/atom_mae: 0.003181
2025-06-25 06:30:15.983 INFO: val_e/atom_mae: 0.003181
val_e/atom_rmse: 0.003747
2025-06-25 06:30:15.984 INFO: val_e/atom_rmse: 0.003747
val_f_mae: 0.035720
2025-06-25 06:30:15.984 INFO: val_f_mae: 0.035720
val_f_rmse: 0.059955
2025-06-25 06:30:15.984 INFO: val_f_rmse: 0.059955
##### Step: 110 Learning rate: 0.0003125 #####
2025-06-25 06:30:46.349 INFO: ##### Step: 110 Learning rate: 0.0003125 #####
Epoch 71, Train Loss: 3.5855, Val Loss: 3.6368
2025-06-25 06:30:46.349 INFO: Epoch 71, Train Loss: 3.5855, Val Loss: 3.6368
train_e/atom_mae: 0.002372
2025-06-25 06:30:46.350 INFO: train_e/atom_mae: 0.002372
train_e/atom_rmse: 0.002812
2025-06-25 06:30:46.350 INFO: train_e/atom_rmse: 0.002812
train_f_mae: 0.034890
2025-06-25 06:30:46.354 INFO: train_f_mae: 0.034890
train_f_rmse: 0.059075
2025-06-25 06:30:46.354 INFO: train_f_rmse: 0.059075
val_e/atom_mae: 0.002181
2025-06-25 06:30:46.357 INFO: val_e/atom_mae: 0.002181
val_e/atom_rmse: 0.002447
2025-06-25 06:30:46.357 INFO: val_e/atom_rmse: 0.002447
val_f_mae: 0.035603
2025-06-25 06:30:46.358 INFO: val_f_mae: 0.035603
val_f_rmse: 0.059702
2025-06-25 06:30:46.358 INFO: val_f_rmse: 0.059702
##### Step: 111 Learning rate: 0.0003125 #####
2025-06-25 06:31:16.668 INFO: ##### Step: 111 Learning rate: 0.0003125 #####
Epoch 72, Train Loss: 3.5776, Val Loss: 3.6429
2025-06-25 06:31:16.669 INFO: Epoch 72, Train Loss: 3.5776, Val Loss: 3.6429
train_e/atom_mae: 0.002256
2025-06-25 06:31:16.669 INFO: train_e/atom_mae: 0.002256
train_e/atom_rmse: 0.002585
2025-06-25 06:31:16.670 INFO: train_e/atom_rmse: 0.002585
train_f_mae: 0.034949
2025-06-25 06:31:16.673 INFO: train_f_mae: 0.034949
train_f_rmse: 0.059133
2025-06-25 06:31:16.673 INFO: train_f_rmse: 0.059133
val_e/atom_mae: 0.002167
2025-06-25 06:31:16.676 INFO: val_e/atom_mae: 0.002167
val_e/atom_rmse: 0.002420
2025-06-25 06:31:16.676 INFO: val_e/atom_rmse: 0.002420
val_f_mae: 0.035585
2025-06-25 06:31:16.677 INFO: val_f_mae: 0.035585
val_f_rmse: 0.059767
2025-06-25 06:31:16.677 INFO: val_f_rmse: 0.059767
##### Step: 112 Learning rate: 0.0003125 #####
2025-06-25 06:31:47.115 INFO: ##### Step: 112 Learning rate: 0.0003125 #####
Epoch 73, Train Loss: 3.5868, Val Loss: 3.6902
2025-06-25 06:31:47.115 INFO: Epoch 73, Train Loss: 3.5868, Val Loss: 3.6902
train_e/atom_mae: 0.002256
2025-06-25 06:31:47.116 INFO: train_e/atom_mae: 0.002256
train_e/atom_rmse: 0.002653
2025-06-25 06:31:47.116 INFO: train_e/atom_rmse: 0.002653
train_f_mae: 0.034989
2025-06-25 06:31:47.120 INFO: train_f_mae: 0.034989
train_f_rmse: 0.059174
2025-06-25 06:31:47.120 INFO: train_f_rmse: 0.059174
val_e/atom_mae: 0.002294
2025-06-25 06:31:47.122 INFO: val_e/atom_mae: 0.002294
val_e/atom_rmse: 0.002957
2025-06-25 06:31:47.123 INFO: val_e/atom_rmse: 0.002957
val_f_mae: 0.035494
2025-06-25 06:31:47.123 INFO: val_f_mae: 0.035494
val_f_rmse: 0.059870
2025-06-25 06:31:47.123 INFO: val_f_rmse: 0.059870
##### Step: 113 Learning rate: 0.0003125 #####
2025-06-25 06:32:17.365 INFO: ##### Step: 113 Learning rate: 0.0003125 #####
Epoch 74, Train Loss: 3.5891, Val Loss: 3.7180
2025-06-25 06:32:17.366 INFO: Epoch 74, Train Loss: 3.5891, Val Loss: 3.7180
train_e/atom_mae: 0.002380
2025-06-25 06:32:17.367 INFO: train_e/atom_mae: 0.002380
train_e/atom_rmse: 0.002852
2025-06-25 06:32:17.367 INFO: train_e/atom_rmse: 0.002852
train_f_mae: 0.034891
2025-06-25 06:32:17.370 INFO: train_f_mae: 0.034891
train_f_rmse: 0.059082
2025-06-25 06:32:17.371 INFO: train_f_rmse: 0.059082
val_e/atom_mae: 0.002603
2025-06-25 06:32:17.373 INFO: val_e/atom_mae: 0.002603
val_e/atom_rmse: 0.003363
2025-06-25 06:32:17.374 INFO: val_e/atom_rmse: 0.003363
val_f_mae: 0.035690
2025-06-25 06:32:17.374 INFO: val_f_mae: 0.035690
val_f_rmse: 0.059843
2025-06-25 06:32:17.374 INFO: val_f_rmse: 0.059843
##### Step: 114 Learning rate: 0.0003125 #####
2025-06-25 06:32:47.780 INFO: ##### Step: 114 Learning rate: 0.0003125 #####
Epoch 75, Train Loss: 3.5869, Val Loss: 3.6726
2025-06-25 06:32:47.781 INFO: Epoch 75, Train Loss: 3.5869, Val Loss: 3.6726
train_e/atom_mae: 0.002312
2025-06-25 06:32:47.782 INFO: train_e/atom_mae: 0.002312
train_e/atom_rmse: 0.002735
2025-06-25 06:32:47.782 INFO: train_e/atom_rmse: 0.002735
train_f_mae: 0.034977
2025-06-25 06:32:47.785 INFO: train_f_mae: 0.034977
train_f_rmse: 0.059130
2025-06-25 06:32:47.785 INFO: train_f_rmse: 0.059130
val_e/atom_mae: 0.002250
2025-06-25 06:32:47.788 INFO: val_e/atom_mae: 0.002250
val_e/atom_rmse: 0.002743
2025-06-25 06:32:47.788 INFO: val_e/atom_rmse: 0.002743
val_f_mae: 0.035574
2025-06-25 06:32:47.789 INFO: val_f_mae: 0.035574
val_f_rmse: 0.059846
2025-06-25 06:32:47.789 INFO: val_f_rmse: 0.059846
##### Step: 115 Learning rate: 0.0003125 #####
2025-06-25 06:33:18.067 INFO: ##### Step: 115 Learning rate: 0.0003125 #####
Epoch 76, Train Loss: 3.5864, Val Loss: 3.6542
2025-06-25 06:33:18.068 INFO: Epoch 76, Train Loss: 3.5864, Val Loss: 3.6542
train_e/atom_mae: 0.002306
2025-06-25 06:33:18.069 INFO: train_e/atom_mae: 0.002306
train_e/atom_rmse: 0.002724
2025-06-25 06:33:18.069 INFO: train_e/atom_rmse: 0.002724
train_f_mae: 0.034943
2025-06-25 06:33:18.072 INFO: train_f_mae: 0.034943
train_f_rmse: 0.059133
2025-06-25 06:33:18.073 INFO: train_f_rmse: 0.059133
val_e/atom_mae: 0.002211
2025-06-25 06:33:18.075 INFO: val_e/atom_mae: 0.002211
val_e/atom_rmse: 0.002712
2025-06-25 06:33:18.076 INFO: val_e/atom_rmse: 0.002712
val_f_mae: 0.035529
2025-06-25 06:33:18.076 INFO: val_f_mae: 0.035529
val_f_rmse: 0.059709
2025-06-25 06:33:18.076 INFO: val_f_rmse: 0.059709
##### Step: 116 Learning rate: 0.0003125 #####
2025-06-25 06:33:48.395 INFO: ##### Step: 116 Learning rate: 0.0003125 #####
Epoch 77, Train Loss: 3.5676, Val Loss: 3.5943
2025-06-25 06:33:48.395 INFO: Epoch 77, Train Loss: 3.5676, Val Loss: 3.5943
train_e/atom_mae: 0.002207
2025-06-25 06:33:48.396 INFO: train_e/atom_mae: 0.002207
train_e/atom_rmse: 0.002502
2025-06-25 06:33:48.396 INFO: train_e/atom_rmse: 0.002502
train_f_mae: 0.034915
2025-06-25 06:33:48.399 INFO: train_f_mae: 0.034915
train_f_rmse: 0.059092
2025-06-25 06:33:48.400 INFO: train_f_rmse: 0.059092
val_e/atom_mae: 0.002146
2025-06-25 06:33:48.402 INFO: val_e/atom_mae: 0.002146
val_e/atom_rmse: 0.002260
2025-06-25 06:33:48.403 INFO: val_e/atom_rmse: 0.002260
val_f_mae: 0.035445
2025-06-25 06:33:48.403 INFO: val_f_mae: 0.035445
val_f_rmse: 0.059435
2025-06-25 06:33:48.403 INFO: val_f_rmse: 0.059435
##### Step: 117 Learning rate: 0.0003125 #####
2025-06-25 06:34:18.795 INFO: ##### Step: 117 Learning rate: 0.0003125 #####
Epoch 78, Train Loss: 3.5740, Val Loss: 3.6250
2025-06-25 06:34:18.795 INFO: Epoch 78, Train Loss: 3.5740, Val Loss: 3.6250
train_e/atom_mae: 0.002278
2025-06-25 06:34:18.796 INFO: train_e/atom_mae: 0.002278
train_e/atom_rmse: 0.002587
2025-06-25 06:34:18.796 INFO: train_e/atom_rmse: 0.002587
train_f_mae: 0.034944
2025-06-25 06:34:18.800 INFO: train_f_mae: 0.034944
train_f_rmse: 0.059102
2025-06-25 06:34:18.800 INFO: train_f_rmse: 0.059102
val_e/atom_mae: 0.002131
2025-06-25 06:34:18.803 INFO: val_e/atom_mae: 0.002131
val_e/atom_rmse: 0.002399
2025-06-25 06:34:18.803 INFO: val_e/atom_rmse: 0.002399
val_f_mae: 0.035682
2025-06-25 06:34:18.803 INFO: val_f_mae: 0.035682
val_f_rmse: 0.059626
2025-06-25 06:34:18.804 INFO: val_f_rmse: 0.059626
##### Step: 118 Learning rate: 0.0003125 #####
2025-06-25 06:34:49.097 INFO: ##### Step: 118 Learning rate: 0.0003125 #####
Epoch 79, Train Loss: 3.5872, Val Loss: 3.6367
2025-06-25 06:34:49.097 INFO: Epoch 79, Train Loss: 3.5872, Val Loss: 3.6367
train_e/atom_mae: 0.002373
2025-06-25 06:34:49.098 INFO: train_e/atom_mae: 0.002373
train_e/atom_rmse: 0.002790
2025-06-25 06:34:49.098 INFO: train_e/atom_rmse: 0.002790
train_f_mae: 0.034950
2025-06-25 06:34:49.102 INFO: train_f_mae: 0.034950
train_f_rmse: 0.059102
2025-06-25 06:34:49.102 INFO: train_f_rmse: 0.059102
val_e/atom_mae: 0.002147
2025-06-25 06:34:49.105 INFO: val_e/atom_mae: 0.002147
val_e/atom_rmse: 0.002296
2025-06-25 06:34:49.105 INFO: val_e/atom_rmse: 0.002296
val_f_mae: 0.035678
2025-06-25 06:34:49.105 INFO: val_f_mae: 0.035678
val_f_rmse: 0.059774
2025-06-25 06:34:49.106 INFO: val_f_rmse: 0.059774
##### Step: 119 Learning rate: 0.0003125 #####
2025-06-25 06:35:19.556 INFO: ##### Step: 119 Learning rate: 0.0003125 #####
Epoch 80, Train Loss: 3.5635, Val Loss: 3.6165
2025-06-25 06:35:19.556 INFO: Epoch 80, Train Loss: 3.5635, Val Loss: 3.6165
train_e/atom_mae: 0.002215
2025-06-25 06:35:19.557 INFO: train_e/atom_mae: 0.002215
train_e/atom_rmse: 0.002522
2025-06-25 06:35:19.557 INFO: train_e/atom_rmse: 0.002522
train_f_mae: 0.034883
2025-06-25 06:35:19.561 INFO: train_f_mae: 0.034883
train_f_rmse: 0.059047
2025-06-25 06:35:19.561 INFO: train_f_rmse: 0.059047
val_e/atom_mae: 0.002164
2025-06-25 06:35:19.564 INFO: val_e/atom_mae: 0.002164
val_e/atom_rmse: 0.002535
2025-06-25 06:35:19.564 INFO: val_e/atom_rmse: 0.002535
val_f_mae: 0.035477
2025-06-25 06:35:19.564 INFO: val_f_mae: 0.035477
val_f_rmse: 0.059487
2025-06-25 06:35:19.565 INFO: val_f_rmse: 0.059487
##### Step: 120 Learning rate: 0.00015625 #####
2025-06-25 06:35:49.812 INFO: ##### Step: 120 Learning rate: 0.00015625 #####
Epoch 81, Train Loss: 3.5541, Val Loss: 3.6269
2025-06-25 06:35:49.813 INFO: Epoch 81, Train Loss: 3.5541, Val Loss: 3.6269
train_e/atom_mae: 0.002188
2025-06-25 06:35:49.814 INFO: train_e/atom_mae: 0.002188
train_e/atom_rmse: 0.002468
2025-06-25 06:35:49.814 INFO: train_e/atom_rmse: 0.002468
train_f_mae: 0.034850
2025-06-25 06:35:49.817 INFO: train_f_mae: 0.034850
train_f_rmse: 0.058994
2025-06-25 06:35:49.818 INFO: train_f_rmse: 0.058994
val_e/atom_mae: 0.002155
2025-06-25 06:35:49.820 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002349
2025-06-25 06:35:49.821 INFO: val_e/atom_rmse: 0.002349
val_f_mae: 0.035527
2025-06-25 06:35:49.821 INFO: val_f_mae: 0.035527
val_f_rmse: 0.059667
2025-06-25 06:35:49.821 INFO: val_f_rmse: 0.059667
##### Step: 121 Learning rate: 0.00015625 #####
2025-06-25 06:36:20.160 INFO: ##### Step: 121 Learning rate: 0.00015625 #####
Epoch 82, Train Loss: 3.5483, Val Loss: 3.6109
2025-06-25 06:36:20.161 INFO: Epoch 82, Train Loss: 3.5483, Val Loss: 3.6109
train_e/atom_mae: 0.002162
2025-06-25 06:36:20.162 INFO: train_e/atom_mae: 0.002162
train_e/atom_rmse: 0.002360
2025-06-25 06:36:20.162 INFO: train_e/atom_rmse: 0.002360
train_f_mae: 0.034844
2025-06-25 06:36:20.165 INFO: train_f_mae: 0.034844
train_f_rmse: 0.058999
2025-06-25 06:36:20.165 INFO: train_f_rmse: 0.058999
val_e/atom_mae: 0.002156
2025-06-25 06:36:20.168 INFO: val_e/atom_mae: 0.002156
val_e/atom_rmse: 0.002248
2025-06-25 06:36:20.168 INFO: val_e/atom_rmse: 0.002248
val_f_mae: 0.035502
2025-06-25 06:36:20.169 INFO: val_f_mae: 0.035502
val_f_rmse: 0.059580
2025-06-25 06:36:20.169 INFO: val_f_rmse: 0.059580
##### Step: 122 Learning rate: 0.00015625 #####
2025-06-25 06:36:50.439 INFO: ##### Step: 122 Learning rate: 0.00015625 #####
Epoch 83, Train Loss: 3.5554, Val Loss: 3.6334
2025-06-25 06:36:50.439 INFO: Epoch 83, Train Loss: 3.5554, Val Loss: 3.6334
train_e/atom_mae: 0.002232
2025-06-25 06:36:50.440 INFO: train_e/atom_mae: 0.002232
train_e/atom_rmse: 0.002532
2025-06-25 06:36:50.440 INFO: train_e/atom_rmse: 0.002532
train_f_mae: 0.034837
2025-06-25 06:36:50.444 INFO: train_f_mae: 0.034837
train_f_rmse: 0.058973
2025-06-25 06:36:50.444 INFO: train_f_rmse: 0.058973
val_e/atom_mae: 0.002182
2025-06-25 06:36:50.447 INFO: val_e/atom_mae: 0.002182
val_e/atom_rmse: 0.002560
2025-06-25 06:36:50.447 INFO: val_e/atom_rmse: 0.002560
val_f_mae: 0.035487
2025-06-25 06:36:50.447 INFO: val_f_mae: 0.035487
val_f_rmse: 0.059616
2025-06-25 06:36:50.448 INFO: val_f_rmse: 0.059616
##### Step: 123 Learning rate: 0.00015625 #####
2025-06-25 06:37:20.709 INFO: ##### Step: 123 Learning rate: 0.00015625 #####
Epoch 84, Train Loss: 3.5478, Val Loss: 3.6629
2025-06-25 06:37:20.709 INFO: Epoch 84, Train Loss: 3.5478, Val Loss: 3.6629
train_e/atom_mae: 0.002211
2025-06-25 06:37:20.710 INFO: train_e/atom_mae: 0.002211
train_e/atom_rmse: 0.002457
2025-06-25 06:37:20.710 INFO: train_e/atom_rmse: 0.002457
train_f_mae: 0.034808
2025-06-25 06:37:20.714 INFO: train_f_mae: 0.034808
train_f_rmse: 0.058948
2025-06-25 06:37:20.714 INFO: train_f_rmse: 0.058948
val_e/atom_mae: 0.002349
2025-06-25 06:37:20.716 INFO: val_e/atom_mae: 0.002349
val_e/atom_rmse: 0.003024
2025-06-25 06:37:20.717 INFO: val_e/atom_rmse: 0.003024
val_f_mae: 0.035499
2025-06-25 06:37:20.717 INFO: val_f_mae: 0.035499
val_f_rmse: 0.059601
2025-06-25 06:37:20.717 INFO: val_f_rmse: 0.059601
##### Step: 124 Learning rate: 0.00015625 #####
2025-06-25 06:37:51.118 INFO: ##### Step: 124 Learning rate: 0.00015625 #####
Epoch 85, Train Loss: 3.5501, Val Loss: 3.6222
2025-06-25 06:37:51.118 INFO: Epoch 85, Train Loss: 3.5501, Val Loss: 3.6222
train_e/atom_mae: 0.002218
2025-06-25 06:37:51.119 INFO: train_e/atom_mae: 0.002218
train_e/atom_rmse: 0.002506
2025-06-25 06:37:51.119 INFO: train_e/atom_rmse: 0.002506
train_f_mae: 0.034796
2025-06-25 06:37:51.122 INFO: train_f_mae: 0.034796
train_f_rmse: 0.058941
2025-06-25 06:37:51.123 INFO: train_f_rmse: 0.058941
val_e/atom_mae: 0.002178
2025-06-25 06:37:51.125 INFO: val_e/atom_mae: 0.002178
val_e/atom_rmse: 0.002555
2025-06-25 06:37:51.126 INFO: val_e/atom_rmse: 0.002555
val_f_mae: 0.035351
2025-06-25 06:37:51.126 INFO: val_f_mae: 0.035351
val_f_rmse: 0.059525
2025-06-25 06:37:51.126 INFO: val_f_rmse: 0.059525
##### Step: 125 Learning rate: 0.00015625 #####
2025-06-25 06:38:21.361 INFO: ##### Step: 125 Learning rate: 0.00015625 #####
Epoch 86, Train Loss: 3.5468, Val Loss: 3.6139
2025-06-25 06:38:21.361 INFO: Epoch 86, Train Loss: 3.5468, Val Loss: 3.6139
train_e/atom_mae: 0.002186
2025-06-25 06:38:21.362 INFO: train_e/atom_mae: 0.002186
train_e/atom_rmse: 0.002409
2025-06-25 06:38:21.363 INFO: train_e/atom_rmse: 0.002409
train_f_mae: 0.034802
2025-06-25 06:38:21.366 INFO: train_f_mae: 0.034802
train_f_rmse: 0.058963
2025-06-25 06:38:21.366 INFO: train_f_rmse: 0.058963
val_e/atom_mae: 0.002152
2025-06-25 06:38:21.369 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002268
2025-06-25 06:38:21.369 INFO: val_e/atom_rmse: 0.002268
val_f_mae: 0.035495
2025-06-25 06:38:21.370 INFO: val_f_mae: 0.035495
val_f_rmse: 0.059596
2025-06-25 06:38:21.370 INFO: val_f_rmse: 0.059596
##### Step: 126 Learning rate: 0.00015625 #####
2025-06-25 06:38:51.774 INFO: ##### Step: 126 Learning rate: 0.00015625 #####
Epoch 87, Train Loss: 3.5455, Val Loss: 3.7625
2025-06-25 06:38:51.774 INFO: Epoch 87, Train Loss: 3.5455, Val Loss: 3.7625
train_e/atom_mae: 0.002215
2025-06-25 06:38:51.775 INFO: train_e/atom_mae: 0.002215
train_e/atom_rmse: 0.002465
2025-06-25 06:38:51.775 INFO: train_e/atom_rmse: 0.002465
train_f_mae: 0.034788
2025-06-25 06:38:51.779 INFO: train_f_mae: 0.034788
train_f_rmse: 0.058924
2025-06-25 06:38:51.779 INFO: train_f_rmse: 0.058924
val_e/atom_mae: 0.002923
2025-06-25 06:38:51.782 INFO: val_e/atom_mae: 0.002923
val_e/atom_rmse: 0.003621
2025-06-25 06:38:51.782 INFO: val_e/atom_rmse: 0.003621
val_f_mae: 0.035671
2025-06-25 06:38:51.782 INFO: val_f_mae: 0.035671
val_f_rmse: 0.060032
2025-06-25 06:38:51.783 INFO: val_f_rmse: 0.060032
##### Step: 127 Learning rate: 0.00015625 #####
2025-06-25 06:39:22.061 INFO: ##### Step: 127 Learning rate: 0.00015625 #####
Epoch 88, Train Loss: 3.5535, Val Loss: 3.6267
2025-06-25 06:39:22.061 INFO: Epoch 88, Train Loss: 3.5535, Val Loss: 3.6267
train_e/atom_mae: 0.002218
2025-06-25 06:39:22.062 INFO: train_e/atom_mae: 0.002218
train_e/atom_rmse: 0.002526
2025-06-25 06:39:22.062 INFO: train_e/atom_rmse: 0.002526
train_f_mae: 0.034821
2025-06-25 06:39:22.066 INFO: train_f_mae: 0.034821
train_f_rmse: 0.058960
2025-06-25 06:39:22.066 INFO: train_f_rmse: 0.058960
val_e/atom_mae: 0.002216
2025-06-25 06:39:22.069 INFO: val_e/atom_mae: 0.002216
val_e/atom_rmse: 0.002575
2025-06-25 06:39:22.069 INFO: val_e/atom_rmse: 0.002575
val_f_mae: 0.035397
2025-06-25 06:39:22.069 INFO: val_f_mae: 0.035397
val_f_rmse: 0.059553
2025-06-25 06:39:22.070 INFO: val_f_rmse: 0.059553
##### Step: 128 Learning rate: 0.00015625 #####
2025-06-25 06:39:52.396 INFO: ##### Step: 128 Learning rate: 0.00015625 #####
Epoch 89, Train Loss: 3.5526, Val Loss: 3.6173
2025-06-25 06:39:52.396 INFO: Epoch 89, Train Loss: 3.5526, Val Loss: 3.6173
train_e/atom_mae: 0.002208
2025-06-25 06:39:52.397 INFO: train_e/atom_mae: 0.002208
train_e/atom_rmse: 0.002454
2025-06-25 06:39:52.397 INFO: train_e/atom_rmse: 0.002454
train_f_mae: 0.034828
2025-06-25 06:39:52.401 INFO: train_f_mae: 0.034828
train_f_rmse: 0.058989
2025-06-25 06:39:52.401 INFO: train_f_rmse: 0.058989
val_e/atom_mae: 0.002152
2025-06-25 06:39:52.404 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002270
2025-06-25 06:39:52.404 INFO: val_e/atom_rmse: 0.002270
val_f_mae: 0.035449
2025-06-25 06:39:52.405 INFO: val_f_mae: 0.035449
val_f_rmse: 0.059624
2025-06-25 06:39:52.405 INFO: val_f_rmse: 0.059624
##### Step: 129 Learning rate: 0.00015625 #####
2025-06-25 06:40:22.772 INFO: ##### Step: 129 Learning rate: 0.00015625 #####
Epoch 90, Train Loss: 3.5513, Val Loss: 3.6275
2025-06-25 06:40:22.773 INFO: Epoch 90, Train Loss: 3.5513, Val Loss: 3.6275
train_e/atom_mae: 0.002217
2025-06-25 06:40:22.774 INFO: train_e/atom_mae: 0.002217
train_e/atom_rmse: 0.002470
2025-06-25 06:40:22.774 INFO: train_e/atom_rmse: 0.002470
train_f_mae: 0.034805
2025-06-25 06:40:22.777 INFO: train_f_mae: 0.034805
train_f_rmse: 0.058970
2025-06-25 06:40:22.778 INFO: train_f_rmse: 0.058970
val_e/atom_mae: 0.002214
2025-06-25 06:40:22.780 INFO: val_e/atom_mae: 0.002214
val_e/atom_rmse: 0.002642
2025-06-25 06:40:22.781 INFO: val_e/atom_rmse: 0.002642
val_f_mae: 0.035402
2025-06-25 06:40:22.781 INFO: val_f_mae: 0.035402
val_f_rmse: 0.059524
2025-06-25 06:40:22.781 INFO: val_f_rmse: 0.059524
##### Step: 130 Learning rate: 0.00015625 #####
2025-06-25 06:40:53.050 INFO: ##### Step: 130 Learning rate: 0.00015625 #####
Epoch 91, Train Loss: 3.5452, Val Loss: 3.6126
2025-06-25 06:40:53.051 INFO: Epoch 91, Train Loss: 3.5452, Val Loss: 3.6126
train_e/atom_mae: 0.002183
2025-06-25 06:40:53.052 INFO: train_e/atom_mae: 0.002183
train_e/atom_rmse: 0.002422
2025-06-25 06:40:53.052 INFO: train_e/atom_rmse: 0.002422
train_f_mae: 0.034794
2025-06-25 06:40:53.055 INFO: train_f_mae: 0.034794
train_f_rmse: 0.058943
2025-06-25 06:40:53.056 INFO: train_f_rmse: 0.058943
val_e/atom_mae: 0.002142
2025-06-25 06:40:53.058 INFO: val_e/atom_mae: 0.002142
val_e/atom_rmse: 0.002345
2025-06-25 06:40:53.059 INFO: val_e/atom_rmse: 0.002345
val_f_mae: 0.035476
2025-06-25 06:40:53.059 INFO: val_f_mae: 0.035476
val_f_rmse: 0.059549
2025-06-25 06:40:53.059 INFO: val_f_rmse: 0.059549
##### Step: 131 Learning rate: 0.00015625 #####
2025-06-25 06:41:23.486 INFO: ##### Step: 131 Learning rate: 0.00015625 #####
Epoch 92, Train Loss: 3.5473, Val Loss: 3.6108
2025-06-25 06:41:23.486 INFO: Epoch 92, Train Loss: 3.5473, Val Loss: 3.6108
train_e/atom_mae: 0.002200
2025-06-25 06:41:23.487 INFO: train_e/atom_mae: 0.002200
train_e/atom_rmse: 0.002429
2025-06-25 06:41:23.488 INFO: train_e/atom_rmse: 0.002429
train_f_mae: 0.034799
2025-06-25 06:41:23.491 INFO: train_f_mae: 0.034799
train_f_rmse: 0.058957
2025-06-25 06:41:23.491 INFO: train_f_rmse: 0.058957
val_e/atom_mae: 0.002150
2025-06-25 06:41:23.494 INFO: val_e/atom_mae: 0.002150
val_e/atom_rmse: 0.002238
2025-06-25 06:41:23.495 INFO: val_e/atom_rmse: 0.002238
val_f_mae: 0.035499
2025-06-25 06:41:23.495 INFO: val_f_mae: 0.035499
val_f_rmse: 0.059583
2025-06-25 06:41:23.495 INFO: val_f_rmse: 0.059583
##### Step: 132 Learning rate: 0.00015625 #####
2025-06-25 06:41:53.744 INFO: ##### Step: 132 Learning rate: 0.00015625 #####
Epoch 93, Train Loss: 3.5492, Val Loss: 3.6095
2025-06-25 06:41:53.744 INFO: Epoch 93, Train Loss: 3.5492, Val Loss: 3.6095
train_e/atom_mae: 0.002202
2025-06-25 06:41:53.745 INFO: train_e/atom_mae: 0.002202
train_e/atom_rmse: 0.002441
2025-06-25 06:41:53.745 INFO: train_e/atom_rmse: 0.002441
train_f_mae: 0.034825
2025-06-25 06:41:53.749 INFO: train_f_mae: 0.034825
train_f_rmse: 0.058966
2025-06-25 06:41:53.749 INFO: train_f_rmse: 0.058966
val_e/atom_mae: 0.002181
2025-06-25 06:41:53.752 INFO: val_e/atom_mae: 0.002181
val_e/atom_rmse: 0.002373
2025-06-25 06:41:53.752 INFO: val_e/atom_rmse: 0.002373
val_f_mae: 0.035404
2025-06-25 06:41:53.752 INFO: val_f_mae: 0.035404
val_f_rmse: 0.059510
2025-06-25 06:41:53.752 INFO: val_f_rmse: 0.059510
##### Step: 133 Learning rate: 0.00015625 #####
2025-06-25 06:42:24.180 INFO: ##### Step: 133 Learning rate: 0.00015625 #####
Epoch 94, Train Loss: 3.5448, Val Loss: 3.6062
2025-06-25 06:42:24.180 INFO: Epoch 94, Train Loss: 3.5448, Val Loss: 3.6062
train_e/atom_mae: 0.002182
2025-06-25 06:42:24.181 INFO: train_e/atom_mae: 0.002182
train_e/atom_rmse: 0.002401
2025-06-25 06:42:24.181 INFO: train_e/atom_rmse: 0.002401
train_f_mae: 0.034822
2025-06-25 06:42:24.185 INFO: train_f_mae: 0.034822
train_f_rmse: 0.058949
2025-06-25 06:42:24.185 INFO: train_f_rmse: 0.058949
val_e/atom_mae: 0.002170
2025-06-25 06:42:24.187 INFO: val_e/atom_mae: 0.002170
val_e/atom_rmse: 0.002299
2025-06-25 06:42:24.188 INFO: val_e/atom_rmse: 0.002299
val_f_mae: 0.035419
2025-06-25 06:42:24.188 INFO: val_f_mae: 0.035419
val_f_rmse: 0.059516
2025-06-25 06:42:24.188 INFO: val_f_rmse: 0.059516
##### Step: 134 Learning rate: 0.00015625 #####
2025-06-25 06:42:54.487 INFO: ##### Step: 134 Learning rate: 0.00015625 #####
Epoch 95, Train Loss: 3.5533, Val Loss: 3.6140
2025-06-25 06:42:54.487 INFO: Epoch 95, Train Loss: 3.5533, Val Loss: 3.6140
train_e/atom_mae: 0.002196
2025-06-25 06:42:54.488 INFO: train_e/atom_mae: 0.002196
train_e/atom_rmse: 0.002458
2025-06-25 06:42:54.488 INFO: train_e/atom_rmse: 0.002458
train_f_mae: 0.034822
2025-06-25 06:42:54.492 INFO: train_f_mae: 0.034822
train_f_rmse: 0.058994
2025-06-25 06:42:54.492 INFO: train_f_rmse: 0.058994
val_e/atom_mae: 0.002156
2025-06-25 06:42:54.494 INFO: val_e/atom_mae: 0.002156
val_e/atom_rmse: 0.002291
2025-06-25 06:42:54.495 INFO: val_e/atom_rmse: 0.002291
val_f_mae: 0.035449
2025-06-25 06:42:54.495 INFO: val_f_mae: 0.035449
val_f_rmse: 0.059586
2025-06-25 06:42:54.495 INFO: val_f_rmse: 0.059586
##### Step: 135 Learning rate: 0.00015625 #####
2025-06-25 06:43:24.826 INFO: ##### Step: 135 Learning rate: 0.00015625 #####
Epoch 96, Train Loss: 3.5540, Val Loss: 3.6321
2025-06-25 06:43:24.826 INFO: Epoch 96, Train Loss: 3.5540, Val Loss: 3.6321
train_e/atom_mae: 0.002198
2025-06-25 06:43:24.827 INFO: train_e/atom_mae: 0.002198
train_e/atom_rmse: 0.002434
2025-06-25 06:43:24.827 INFO: train_e/atom_rmse: 0.002434
train_f_mae: 0.034847
2025-06-25 06:43:24.830 INFO: train_f_mae: 0.034847
train_f_rmse: 0.059011
2025-06-25 06:43:24.830 INFO: train_f_rmse: 0.059011
val_e/atom_mae: 0.002146
2025-06-25 06:43:24.833 INFO: val_e/atom_mae: 0.002146
val_e/atom_rmse: 0.002251
2025-06-25 06:43:24.833 INFO: val_e/atom_rmse: 0.002251
val_f_mae: 0.035547
2025-06-25 06:43:24.834 INFO: val_f_mae: 0.035547
val_f_rmse: 0.059756
2025-06-25 06:43:24.834 INFO: val_f_rmse: 0.059756
##### Step: 136 Learning rate: 0.00015625 #####
2025-06-25 06:43:55.234 INFO: ##### Step: 136 Learning rate: 0.00015625 #####
Epoch 97, Train Loss: 3.5568, Val Loss: 3.6293
2025-06-25 06:43:55.234 INFO: Epoch 97, Train Loss: 3.5568, Val Loss: 3.6293
train_e/atom_mae: 0.002226
2025-06-25 06:43:55.235 INFO: train_e/atom_mae: 0.002226
train_e/atom_rmse: 0.002515
2025-06-25 06:43:55.235 INFO: train_e/atom_rmse: 0.002515
train_f_mae: 0.034835
2025-06-25 06:43:55.239 INFO: train_f_mae: 0.034835
train_f_rmse: 0.058994
2025-06-25 06:43:55.239 INFO: train_f_rmse: 0.058994
val_e/atom_mae: 0.002166
2025-06-25 06:43:55.242 INFO: val_e/atom_mae: 0.002166
val_e/atom_rmse: 0.002534
2025-06-25 06:43:55.242 INFO: val_e/atom_rmse: 0.002534
val_f_mae: 0.035520
2025-06-25 06:43:55.242 INFO: val_f_mae: 0.035520
val_f_rmse: 0.059596
2025-06-25 06:43:55.243 INFO: val_f_rmse: 0.059596
##### Step: 137 Learning rate: 0.00015625 #####
2025-06-25 06:44:25.444 INFO: ##### Step: 137 Learning rate: 0.00015625 #####
Epoch 98, Train Loss: 3.5521, Val Loss: 3.6147
2025-06-25 06:44:25.445 INFO: Epoch 98, Train Loss: 3.5521, Val Loss: 3.6147
train_e/atom_mae: 0.002176
2025-06-25 06:44:25.446 INFO: train_e/atom_mae: 0.002176
train_e/atom_rmse: 0.002426
2025-06-25 06:44:25.446 INFO: train_e/atom_rmse: 0.002426
train_f_mae: 0.034835
2025-06-25 06:44:25.449 INFO: train_f_mae: 0.034835
train_f_rmse: 0.058999
2025-06-25 06:44:25.449 INFO: train_f_rmse: 0.058999
val_e/atom_mae: 0.002158
2025-06-25 06:44:25.452 INFO: val_e/atom_mae: 0.002158
val_e/atom_rmse: 0.002327
2025-06-25 06:44:25.453 INFO: val_e/atom_rmse: 0.002327
val_f_mae: 0.035446
2025-06-25 06:44:25.453 INFO: val_f_mae: 0.035446
val_f_rmse: 0.059575
2025-06-25 06:44:25.453 INFO: val_f_rmse: 0.059575
##### Step: 138 Learning rate: 0.00015625 #####
2025-06-25 06:44:55.836 INFO: ##### Step: 138 Learning rate: 0.00015625 #####
Epoch 99, Train Loss: 3.5524, Val Loss: 3.5929
2025-06-25 06:44:55.836 INFO: Epoch 99, Train Loss: 3.5524, Val Loss: 3.5929
train_e/atom_mae: 0.002226
2025-06-25 06:44:55.837 INFO: train_e/atom_mae: 0.002226
train_e/atom_rmse: 0.002511
2025-06-25 06:44:55.837 INFO: train_e/atom_rmse: 0.002511
train_f_mae: 0.034808
2025-06-25 06:44:55.841 INFO: train_f_mae: 0.034808
train_f_rmse: 0.058959
2025-06-25 06:44:55.841 INFO: train_f_rmse: 0.058959
val_e/atom_mae: 0.002156
2025-06-25 06:44:55.843 INFO: val_e/atom_mae: 0.002156
val_e/atom_rmse: 0.002250
2025-06-25 06:44:55.844 INFO: val_e/atom_rmse: 0.002250
val_f_mae: 0.035350
2025-06-25 06:44:55.844 INFO: val_f_mae: 0.035350
val_f_rmse: 0.059428
2025-06-25 06:44:55.844 INFO: val_f_rmse: 0.059428
##### Step: 139 Learning rate: 0.00015625 #####
2025-06-25 06:45:26.064 INFO: ##### Step: 139 Learning rate: 0.00015625 #####
Epoch 100, Train Loss: 3.5472, Val Loss: 3.6258
2025-06-25 06:45:26.064 INFO: Epoch 100, Train Loss: 3.5472, Val Loss: 3.6258
train_e/atom_mae: 0.002217
2025-06-25 06:45:26.065 INFO: train_e/atom_mae: 0.002217
train_e/atom_rmse: 0.002485
2025-06-25 06:45:26.065 INFO: train_e/atom_rmse: 0.002485
train_f_mae: 0.034761
2025-06-25 06:45:26.069 INFO: train_f_mae: 0.034761
train_f_rmse: 0.058928
2025-06-25 06:45:26.069 INFO: train_f_rmse: 0.058928
val_e/atom_mae: 0.002159
2025-06-25 06:45:26.072 INFO: val_e/atom_mae: 0.002159
val_e/atom_rmse: 0.002389
2025-06-25 06:45:26.072 INFO: val_e/atom_rmse: 0.002389
val_f_mae: 0.035456
2025-06-25 06:45:26.073 INFO: val_f_mae: 0.035456
val_f_rmse: 0.059638
2025-06-25 06:45:26.073 INFO: val_f_rmse: 0.059638
2025-06-25 06:45:26.081 INFO: Third train loop:
##### Step: 140 Learning rate: 7.8125e-05 #####
2025-06-25 06:45:56.499 INFO: ##### Step: 140 Learning rate: 7.8125e-05 #####
Epoch 1, Train Loss: 4.1611, Val Loss: 4.4685
2025-06-25 06:45:56.499 INFO: Epoch 1, Train Loss: 4.1611, Val Loss: 4.4685
train_e/atom_mae: 0.002173
2025-06-25 06:45:56.500 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002386
2025-06-25 06:45:56.500 INFO: train_e/atom_rmse: 0.002386
train_f_mae: 0.034780
2025-06-25 06:45:56.504 INFO: train_f_mae: 0.034780
train_f_rmse: 0.058927
2025-06-25 06:45:56.504 INFO: train_f_rmse: 0.058927
val_e/atom_mae: 0.002214
2025-06-25 06:45:56.506 INFO: val_e/atom_mae: 0.002214
val_e/atom_rmse: 0.002776
2025-06-25 06:45:56.507 INFO: val_e/atom_rmse: 0.002776
val_f_mae: 0.035375
2025-06-25 06:45:56.507 INFO: val_f_mae: 0.035375
val_f_rmse: 0.059462
2025-06-25 06:45:56.507 INFO: val_f_rmse: 0.059462
##### Step: 141 Learning rate: 7.8125e-05 #####
2025-06-25 06:46:26.804 INFO: ##### Step: 141 Learning rate: 7.8125e-05 #####
Epoch 2, Train Loss: 4.1814, Val Loss: 4.1615
2025-06-25 06:46:26.805 INFO: Epoch 2, Train Loss: 4.1814, Val Loss: 4.1615
train_e/atom_mae: 0.002199
2025-06-25 06:46:26.806 INFO: train_e/atom_mae: 0.002199
train_e/atom_rmse: 0.002430
2025-06-25 06:46:26.806 INFO: train_e/atom_rmse: 0.002430
train_f_mae: 0.034746
2025-06-25 06:46:26.809 INFO: train_f_mae: 0.034746
train_f_rmse: 0.058881
2025-06-25 06:46:26.810 INFO: train_f_rmse: 0.058881
val_e/atom_mae: 0.002159
2025-06-25 06:46:26.812 INFO: val_e/atom_mae: 0.002159
val_e/atom_rmse: 0.002277
2025-06-25 06:46:26.813 INFO: val_e/atom_rmse: 0.002277
val_f_mae: 0.035374
2025-06-25 06:46:26.813 INFO: val_f_mae: 0.035374
val_f_rmse: 0.059451
2025-06-25 06:46:26.813 INFO: val_f_rmse: 0.059451
##### Step: 142 Learning rate: 7.8125e-05 #####
2025-06-25 06:46:57.067 INFO: ##### Step: 142 Learning rate: 7.8125e-05 #####
Epoch 3, Train Loss: 4.2109, Val Loss: 4.1573
2025-06-25 06:46:57.067 INFO: Epoch 3, Train Loss: 4.2109, Val Loss: 4.1573
train_e/atom_mae: 0.002220
2025-06-25 06:46:57.068 INFO: train_e/atom_mae: 0.002220
train_e/atom_rmse: 0.002482
2025-06-25 06:46:57.068 INFO: train_e/atom_rmse: 0.002482
train_f_mae: 0.034746
2025-06-25 06:46:57.071 INFO: train_f_mae: 0.034746
train_f_rmse: 0.058871
2025-06-25 06:46:57.072 INFO: train_f_rmse: 0.058871
val_e/atom_mae: 0.002132
2025-06-25 06:46:57.074 INFO: val_e/atom_mae: 0.002132
val_e/atom_rmse: 0.002274
2025-06-25 06:46:57.075 INFO: val_e/atom_rmse: 0.002274
val_f_mae: 0.035359
2025-06-25 06:46:57.075 INFO: val_f_mae: 0.035359
val_f_rmse: 0.059426
2025-06-25 06:46:57.075 INFO: val_f_rmse: 0.059426
##### Step: 143 Learning rate: 7.8125e-05 #####
2025-06-25 06:47:27.405 INFO: ##### Step: 143 Learning rate: 7.8125e-05 #####
Epoch 4, Train Loss: 4.1796, Val Loss: 4.1865
2025-06-25 06:47:27.405 INFO: Epoch 4, Train Loss: 4.1796, Val Loss: 4.1865
train_e/atom_mae: 0.002196
2025-06-25 06:47:27.406 INFO: train_e/atom_mae: 0.002196
train_e/atom_rmse: 0.002426
2025-06-25 06:47:27.406 INFO: train_e/atom_rmse: 0.002426
train_f_mae: 0.034740
2025-06-25 06:47:27.410 INFO: train_f_mae: 0.034740
train_f_rmse: 0.058887
2025-06-25 06:47:27.410 INFO: train_f_rmse: 0.058887
val_e/atom_mae: 0.002155
2025-06-25 06:47:27.413 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002300
2025-06-25 06:47:27.413 INFO: val_e/atom_rmse: 0.002300
val_f_mae: 0.035406
2025-06-25 06:47:27.413 INFO: val_f_mae: 0.035406
val_f_rmse: 0.059553
2025-06-25 06:47:27.414 INFO: val_f_rmse: 0.059553
##### Step: 144 Learning rate: 7.8125e-05 #####
2025-06-25 06:47:57.635 INFO: ##### Step: 144 Learning rate: 7.8125e-05 #####
Epoch 5, Train Loss: 4.1483, Val Loss: 4.1443
2025-06-25 06:47:57.635 INFO: Epoch 5, Train Loss: 4.1483, Val Loss: 4.1443
train_e/atom_mae: 0.002188
2025-06-25 06:47:57.636 INFO: train_e/atom_mae: 0.002188
train_e/atom_rmse: 0.002375
2025-06-25 06:47:57.636 INFO: train_e/atom_rmse: 0.002375
train_f_mae: 0.034734
2025-06-25 06:47:57.640 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058873
2025-06-25 06:47:57.640 INFO: train_f_rmse: 0.058873
val_e/atom_mae: 0.002143
2025-06-25 06:47:57.643 INFO: val_e/atom_mae: 0.002143
val_e/atom_rmse: 0.002233
2025-06-25 06:47:57.643 INFO: val_e/atom_rmse: 0.002233
val_f_mae: 0.035422
2025-06-25 06:47:57.643 INFO: val_f_mae: 0.035422
val_f_rmse: 0.059505
2025-06-25 06:47:57.644 INFO: val_f_rmse: 0.059505
##### Step: 145 Learning rate: 7.8125e-05 #####
2025-06-25 06:48:28.087 INFO: ##### Step: 145 Learning rate: 7.8125e-05 #####
Epoch 6, Train Loss: 4.1392, Val Loss: 4.6123
2025-06-25 06:48:28.087 INFO: Epoch 6, Train Loss: 4.1392, Val Loss: 4.6123
train_e/atom_mae: 0.002187
2025-06-25 06:48:28.088 INFO: train_e/atom_mae: 0.002187
train_e/atom_rmse: 0.002359
2025-06-25 06:48:28.088 INFO: train_e/atom_rmse: 0.002359
train_f_mae: 0.034758
2025-06-25 06:48:28.092 INFO: train_f_mae: 0.034758
train_f_rmse: 0.058873
2025-06-25 06:48:28.092 INFO: train_f_rmse: 0.058873
val_e/atom_mae: 0.002316
2025-06-25 06:48:28.095 INFO: val_e/atom_mae: 0.002316
val_e/atom_rmse: 0.002963
2025-06-25 06:48:28.095 INFO: val_e/atom_rmse: 0.002963
val_f_mae: 0.035454
2025-06-25 06:48:28.096 INFO: val_f_mae: 0.035454
val_f_rmse: 0.059584
2025-06-25 06:48:28.096 INFO: val_f_rmse: 0.059584
##### Step: 146 Learning rate: 7.8125e-05 #####
2025-06-25 06:48:58.315 INFO: ##### Step: 146 Learning rate: 7.8125e-05 #####
Epoch 7, Train Loss: 4.2058, Val Loss: 4.1788
2025-06-25 06:48:58.315 INFO: Epoch 7, Train Loss: 4.2058, Val Loss: 4.1788
train_e/atom_mae: 0.002189
2025-06-25 06:48:58.316 INFO: train_e/atom_mae: 0.002189
train_e/atom_rmse: 0.002470
2025-06-25 06:48:58.316 INFO: train_e/atom_rmse: 0.002470
train_f_mae: 0.034764
2025-06-25 06:48:58.320 INFO: train_f_mae: 0.034764
train_f_rmse: 0.058886
2025-06-25 06:48:58.320 INFO: train_f_rmse: 0.058886
val_e/atom_mae: 0.002168
2025-06-25 06:48:58.323 INFO: val_e/atom_mae: 0.002168
val_e/atom_rmse: 0.002291
2025-06-25 06:48:58.323 INFO: val_e/atom_rmse: 0.002291
val_f_mae: 0.035413
2025-06-25 06:48:58.323 INFO: val_f_mae: 0.035413
val_f_rmse: 0.059527
2025-06-25 06:48:58.324 INFO: val_f_rmse: 0.059527
##### Step: 147 Learning rate: 7.8125e-05 #####
2025-06-25 06:49:28.623 INFO: ##### Step: 147 Learning rate: 7.8125e-05 #####
Epoch 8, Train Loss: 4.1472, Val Loss: 4.1854
2025-06-25 06:49:28.623 INFO: Epoch 8, Train Loss: 4.1472, Val Loss: 4.1854
train_e/atom_mae: 0.002184
2025-06-25 06:49:28.624 INFO: train_e/atom_mae: 0.002184
train_e/atom_rmse: 0.002370
2025-06-25 06:49:28.624 INFO: train_e/atom_rmse: 0.002370
train_f_mae: 0.034753
2025-06-25 06:49:28.628 INFO: train_f_mae: 0.034753
train_f_rmse: 0.058885
2025-06-25 06:49:28.628 INFO: train_f_rmse: 0.058885
val_e/atom_mae: 0.002146
2025-06-25 06:49:28.631 INFO: val_e/atom_mae: 0.002146
val_e/atom_rmse: 0.002314
2025-06-25 06:49:28.631 INFO: val_e/atom_rmse: 0.002314
val_f_mae: 0.035395
2025-06-25 06:49:28.631 INFO: val_f_mae: 0.035395
val_f_rmse: 0.059477
2025-06-25 06:49:28.631 INFO: val_f_rmse: 0.059477
##### Step: 148 Learning rate: 7.8125e-05 #####
2025-06-25 06:49:58.974 INFO: ##### Step: 148 Learning rate: 7.8125e-05 #####
Epoch 9, Train Loss: 4.1800, Val Loss: 4.1515
2025-06-25 06:49:58.974 INFO: Epoch 9, Train Loss: 4.1800, Val Loss: 4.1515
train_e/atom_mae: 0.002197
2025-06-25 06:49:58.975 INFO: train_e/atom_mae: 0.002197
train_e/atom_rmse: 0.002426
2025-06-25 06:49:58.975 INFO: train_e/atom_rmse: 0.002426
train_f_mae: 0.034775
2025-06-25 06:49:58.979 INFO: train_f_mae: 0.034775
train_f_rmse: 0.058889
2025-06-25 06:49:58.979 INFO: train_f_rmse: 0.058889
val_e/atom_mae: 0.002153
2025-06-25 06:49:58.981 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002248
2025-06-25 06:49:58.982 INFO: val_e/atom_rmse: 0.002248
val_f_mae: 0.035431
2025-06-25 06:49:58.982 INFO: val_f_mae: 0.035431
val_f_rmse: 0.059499
2025-06-25 06:49:58.982 INFO: val_f_rmse: 0.059499
##### Step: 149 Learning rate: 7.8125e-05 #####
2025-06-25 06:50:29.250 INFO: ##### Step: 149 Learning rate: 7.8125e-05 #####
Epoch 10, Train Loss: 4.1535, Val Loss: 4.2056
2025-06-25 06:50:29.250 INFO: Epoch 10, Train Loss: 4.1535, Val Loss: 4.2056
train_e/atom_mae: 0.002189
2025-06-25 06:50:29.251 INFO: train_e/atom_mae: 0.002189
train_e/atom_rmse: 0.002383
2025-06-25 06:50:29.251 INFO: train_e/atom_rmse: 0.002383
train_f_mae: 0.034756
2025-06-25 06:50:29.255 INFO: train_f_mae: 0.034756
train_f_rmse: 0.058875
2025-06-25 06:50:29.255 INFO: train_f_rmse: 0.058875
val_e/atom_mae: 0.002161
2025-06-25 06:50:29.258 INFO: val_e/atom_mae: 0.002161
val_e/atom_rmse: 0.002320
2025-06-25 06:50:29.258 INFO: val_e/atom_rmse: 0.002320
val_f_mae: 0.035445
2025-06-25 06:50:29.258 INFO: val_f_mae: 0.035445
val_f_rmse: 0.059620
2025-06-25 06:50:29.259 INFO: val_f_rmse: 0.059620
##### Step: 150 Learning rate: 7.8125e-05 #####
2025-06-25 06:50:59.679 INFO: ##### Step: 150 Learning rate: 7.8125e-05 #####
Epoch 11, Train Loss: 4.1316, Val Loss: 4.2696
2025-06-25 06:50:59.679 INFO: Epoch 11, Train Loss: 4.1316, Val Loss: 4.2696
train_e/atom_mae: 0.002180
2025-06-25 06:50:59.680 INFO: train_e/atom_mae: 0.002180
train_e/atom_rmse: 0.002346
2025-06-25 06:50:59.681 INFO: train_e/atom_rmse: 0.002346
train_f_mae: 0.034756
2025-06-25 06:50:59.684 INFO: train_f_mae: 0.034756
train_f_rmse: 0.058870
2025-06-25 06:50:59.684 INFO: train_f_rmse: 0.058870
val_e/atom_mae: 0.002160
2025-06-25 06:50:59.687 INFO: val_e/atom_mae: 0.002160
val_e/atom_rmse: 0.002448
2025-06-25 06:50:59.688 INFO: val_e/atom_rmse: 0.002448
val_f_mae: 0.035410
2025-06-25 06:50:59.688 INFO: val_f_mae: 0.035410
val_f_rmse: 0.059537
2025-06-25 06:50:59.688 INFO: val_f_rmse: 0.059537
##### Step: 151 Learning rate: 7.8125e-05 #####
2025-06-25 06:51:29.918 INFO: ##### Step: 151 Learning rate: 7.8125e-05 #####
Epoch 12, Train Loss: 4.2163, Val Loss: 4.1528
2025-06-25 06:51:29.918 INFO: Epoch 12, Train Loss: 4.2163, Val Loss: 4.1528
train_e/atom_mae: 0.002199
2025-06-25 06:51:29.919 INFO: train_e/atom_mae: 0.002199
train_e/atom_rmse: 0.002488
2025-06-25 06:51:29.919 INFO: train_e/atom_rmse: 0.002488
train_f_mae: 0.034771
2025-06-25 06:51:29.923 INFO: train_f_mae: 0.034771
train_f_rmse: 0.058882
2025-06-25 06:51:29.923 INFO: train_f_rmse: 0.058882
val_e/atom_mae: 0.002142
2025-06-25 06:51:29.926 INFO: val_e/atom_mae: 0.002142
val_e/atom_rmse: 0.002254
2025-06-25 06:51:29.926 INFO: val_e/atom_rmse: 0.002254
val_f_mae: 0.035410
2025-06-25 06:51:29.926 INFO: val_f_mae: 0.035410
val_f_rmse: 0.059481
2025-06-25 06:51:29.927 INFO: val_f_rmse: 0.059481
##### Step: 152 Learning rate: 7.8125e-05 #####
2025-06-25 06:52:00.346 INFO: ##### Step: 152 Learning rate: 7.8125e-05 #####
Epoch 13, Train Loss: 4.1722, Val Loss: 4.2886
2025-06-25 06:52:00.347 INFO: Epoch 13, Train Loss: 4.1722, Val Loss: 4.2886
train_e/atom_mae: 0.002180
2025-06-25 06:52:00.347 INFO: train_e/atom_mae: 0.002180
train_e/atom_rmse: 0.002413
2025-06-25 06:52:00.348 INFO: train_e/atom_rmse: 0.002413
train_f_mae: 0.034767
2025-06-25 06:52:00.351 INFO: train_f_mae: 0.034767
train_f_rmse: 0.058886
2025-06-25 06:52:00.351 INFO: train_f_rmse: 0.058886
val_e/atom_mae: 0.002161
2025-06-25 06:52:00.354 INFO: val_e/atom_mae: 0.002161
val_e/atom_rmse: 0.002494
2025-06-25 06:52:00.354 INFO: val_e/atom_rmse: 0.002494
val_f_mae: 0.035407
2025-06-25 06:52:00.355 INFO: val_f_mae: 0.035407
val_f_rmse: 0.059464
2025-06-25 06:52:00.355 INFO: val_f_rmse: 0.059464
##### Step: 153 Learning rate: 7.8125e-05 #####
2025-06-25 06:52:30.612 INFO: ##### Step: 153 Learning rate: 7.8125e-05 #####
Epoch 14, Train Loss: 4.1501, Val Loss: 4.2694
2025-06-25 06:52:30.612 INFO: Epoch 14, Train Loss: 4.1501, Val Loss: 4.2694
train_e/atom_mae: 0.002164
2025-06-25 06:52:30.613 INFO: train_e/atom_mae: 0.002164
train_e/atom_rmse: 0.002374
2025-06-25 06:52:30.613 INFO: train_e/atom_rmse: 0.002374
train_f_mae: 0.034773
2025-06-25 06:52:30.617 INFO: train_f_mae: 0.034773
train_f_rmse: 0.058891
2025-06-25 06:52:30.617 INFO: train_f_rmse: 0.058891
val_e/atom_mae: 0.002157
2025-06-25 06:52:30.619 INFO: val_e/atom_mae: 0.002157
val_e/atom_rmse: 0.002466
2025-06-25 06:52:30.620 INFO: val_e/atom_rmse: 0.002466
val_f_mae: 0.035415
2025-06-25 06:52:30.620 INFO: val_f_mae: 0.035415
val_f_rmse: 0.059446
2025-06-25 06:52:30.620 INFO: val_f_rmse: 0.059446
##### Step: 154 Learning rate: 7.8125e-05 #####
2025-06-25 06:53:00.928 INFO: ##### Step: 154 Learning rate: 7.8125e-05 #####
Epoch 15, Train Loss: 4.1320, Val Loss: 4.1752
2025-06-25 06:53:00.929 INFO: Epoch 15, Train Loss: 4.1320, Val Loss: 4.1752
train_e/atom_mae: 0.002175
2025-06-25 06:53:00.930 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002345
2025-06-25 06:53:00.930 INFO: train_e/atom_rmse: 0.002345
train_f_mae: 0.034764
2025-06-25 06:53:00.933 INFO: train_f_mae: 0.034764
train_f_rmse: 0.058879
2025-06-25 06:53:00.933 INFO: train_f_rmse: 0.058879
val_e/atom_mae: 0.002148
2025-06-25 06:53:00.936 INFO: val_e/atom_mae: 0.002148
val_e/atom_rmse: 0.002307
2025-06-25 06:53:00.937 INFO: val_e/atom_rmse: 0.002307
val_f_mae: 0.035392
2025-06-25 06:53:00.937 INFO: val_f_mae: 0.035392
val_f_rmse: 0.059422
2025-06-25 06:53:00.937 INFO: val_f_rmse: 0.059422
##### Step: 155 Learning rate: 7.8125e-05 #####
2025-06-25 06:53:31.301 INFO: ##### Step: 155 Learning rate: 7.8125e-05 #####
Epoch 16, Train Loss: 4.1318, Val Loss: 4.2295
2025-06-25 06:53:31.301 INFO: Epoch 16, Train Loss: 4.1318, Val Loss: 4.2295
train_e/atom_mae: 0.002178
2025-06-25 06:53:31.302 INFO: train_e/atom_mae: 0.002178
train_e/atom_rmse: 0.002344
2025-06-25 06:53:31.303 INFO: train_e/atom_rmse: 0.002344
train_f_mae: 0.034750
2025-06-25 06:53:31.306 INFO: train_f_mae: 0.034750
train_f_rmse: 0.058883
2025-06-25 06:53:31.306 INFO: train_f_rmse: 0.058883
val_e/atom_mae: 0.002164
2025-06-25 06:53:31.309 INFO: val_e/atom_mae: 0.002164
val_e/atom_rmse: 0.002362
2025-06-25 06:53:31.309 INFO: val_e/atom_rmse: 0.002362
val_f_mae: 0.035461
2025-06-25 06:53:31.310 INFO: val_f_mae: 0.035461
val_f_rmse: 0.059620
2025-06-25 06:53:31.310 INFO: val_f_rmse: 0.059620
##### Step: 156 Learning rate: 7.8125e-05 #####
2025-06-25 06:54:01.561 INFO: ##### Step: 156 Learning rate: 7.8125e-05 #####
Epoch 17, Train Loss: 4.1488, Val Loss: 4.2655
2025-06-25 06:54:01.561 INFO: Epoch 17, Train Loss: 4.1488, Val Loss: 4.2655
train_e/atom_mae: 0.002191
2025-06-25 06:54:01.562 INFO: train_e/atom_mae: 0.002191
train_e/atom_rmse: 0.002375
2025-06-25 06:54:01.562 INFO: train_e/atom_rmse: 0.002375
train_f_mae: 0.034754
2025-06-25 06:54:01.566 INFO: train_f_mae: 0.034754
train_f_rmse: 0.058877
2025-06-25 06:54:01.566 INFO: train_f_rmse: 0.058877
val_e/atom_mae: 0.002163
2025-06-25 06:54:01.569 INFO: val_e/atom_mae: 0.002163
val_e/atom_rmse: 0.002414
2025-06-25 06:54:01.569 INFO: val_e/atom_rmse: 0.002414
val_f_mae: 0.035494
2025-06-25 06:54:01.570 INFO: val_f_mae: 0.035494
val_f_rmse: 0.059671
2025-06-25 06:54:01.570 INFO: val_f_rmse: 0.059671
##### Step: 157 Learning rate: 7.8125e-05 #####
2025-06-25 06:54:31.982 INFO: ##### Step: 157 Learning rate: 7.8125e-05 #####
Epoch 18, Train Loss: 4.1640, Val Loss: 4.1591
2025-06-25 06:54:31.982 INFO: Epoch 18, Train Loss: 4.1640, Val Loss: 4.1591
train_e/atom_mae: 0.002195
2025-06-25 06:54:31.983 INFO: train_e/atom_mae: 0.002195
train_e/atom_rmse: 0.002398
2025-06-25 06:54:31.984 INFO: train_e/atom_rmse: 0.002398
train_f_mae: 0.034768
2025-06-25 06:54:31.987 INFO: train_f_mae: 0.034768
train_f_rmse: 0.058894
2025-06-25 06:54:31.987 INFO: train_f_rmse: 0.058894
val_e/atom_mae: 0.002152
2025-06-25 06:54:31.990 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 06:54:31.990 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035477
2025-06-25 06:54:31.991 INFO: val_f_mae: 0.035477
val_f_rmse: 0.059593
2025-06-25 06:54:31.991 INFO: val_f_rmse: 0.059593
##### Step: 158 Learning rate: 7.8125e-05 #####
2025-06-25 06:55:02.235 INFO: ##### Step: 158 Learning rate: 7.8125e-05 #####
Epoch 19, Train Loss: 4.1634, Val Loss: 4.1715
2025-06-25 06:55:02.235 INFO: Epoch 19, Train Loss: 4.1634, Val Loss: 4.1715
train_e/atom_mae: 0.002188
2025-06-25 06:55:02.236 INFO: train_e/atom_mae: 0.002188
train_e/atom_rmse: 0.002401
2025-06-25 06:55:02.237 INFO: train_e/atom_rmse: 0.002401
train_f_mae: 0.034764
2025-06-25 06:55:02.240 INFO: train_f_mae: 0.034764
train_f_rmse: 0.058872
2025-06-25 06:55:02.240 INFO: train_f_rmse: 0.058872
val_e/atom_mae: 0.002135
2025-06-25 06:55:02.243 INFO: val_e/atom_mae: 0.002135
val_e/atom_rmse: 0.002292
2025-06-25 06:55:02.243 INFO: val_e/atom_rmse: 0.002292
val_f_mae: 0.035407
2025-06-25 06:55:02.244 INFO: val_f_mae: 0.035407
val_f_rmse: 0.059462
2025-06-25 06:55:02.244 INFO: val_f_rmse: 0.059462
##### Step: 159 Learning rate: 7.8125e-05 #####
2025-06-25 06:55:32.555 INFO: ##### Step: 159 Learning rate: 7.8125e-05 #####
Epoch 20, Train Loss: 4.1750, Val Loss: 4.1515
2025-06-25 06:55:32.556 INFO: Epoch 20, Train Loss: 4.1750, Val Loss: 4.1515
train_e/atom_mae: 0.002193
2025-06-25 06:55:32.557 INFO: train_e/atom_mae: 0.002193
train_e/atom_rmse: 0.002422
2025-06-25 06:55:32.557 INFO: train_e/atom_rmse: 0.002422
train_f_mae: 0.034761
2025-06-25 06:55:32.560 INFO: train_f_mae: 0.034761
train_f_rmse: 0.058868
2025-06-25 06:55:32.560 INFO: train_f_rmse: 0.058868
val_e/atom_mae: 0.002154
2025-06-25 06:55:32.563 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002248
2025-06-25 06:55:32.563 INFO: val_e/atom_rmse: 0.002248
val_f_mae: 0.035422
2025-06-25 06:55:32.564 INFO: val_f_mae: 0.035422
val_f_rmse: 0.059497
2025-06-25 06:55:32.564 INFO: val_f_rmse: 0.059497
##### Step: 160 Learning rate: 3.90625e-05 #####
2025-06-25 06:56:02.872 INFO: ##### Step: 160 Learning rate: 3.90625e-05 #####
Epoch 21, Train Loss: 4.1172, Val Loss: 4.1690
2025-06-25 06:56:02.873 INFO: Epoch 21, Train Loss: 4.1172, Val Loss: 4.1690
train_e/atom_mae: 0.002177
2025-06-25 06:56:02.874 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002322
2025-06-25 06:56:02.874 INFO: train_e/atom_rmse: 0.002322
train_f_mae: 0.034745
2025-06-25 06:56:02.877 INFO: train_f_mae: 0.034745
train_f_rmse: 0.058862
2025-06-25 06:56:02.878 INFO: train_f_rmse: 0.058862
val_e/atom_mae: 0.002164
2025-06-25 06:56:02.880 INFO: val_e/atom_mae: 0.002164
val_e/atom_rmse: 0.002274
2025-06-25 06:56:02.881 INFO: val_e/atom_rmse: 0.002274
val_f_mae: 0.035422
2025-06-25 06:56:02.881 INFO: val_f_mae: 0.035422
val_f_rmse: 0.059527
2025-06-25 06:56:02.881 INFO: val_f_rmse: 0.059527
##### Step: 161 Learning rate: 3.90625e-05 #####
2025-06-25 06:56:33.192 INFO: ##### Step: 161 Learning rate: 3.90625e-05 #####
Epoch 22, Train Loss: 4.1222, Val Loss: 4.1668
2025-06-25 06:56:33.193 INFO: Epoch 22, Train Loss: 4.1222, Val Loss: 4.1668
train_e/atom_mae: 0.002165
2025-06-25 06:56:33.194 INFO: train_e/atom_mae: 0.002165
train_e/atom_rmse: 0.002332
2025-06-25 06:56:33.194 INFO: train_e/atom_rmse: 0.002332
train_f_mae: 0.034742
2025-06-25 06:56:33.197 INFO: train_f_mae: 0.034742
train_f_rmse: 0.058857
2025-06-25 06:56:33.197 INFO: train_f_rmse: 0.058857
val_e/atom_mae: 0.002159
2025-06-25 06:56:33.200 INFO: val_e/atom_mae: 0.002159
val_e/atom_rmse: 0.002261
2025-06-25 06:56:33.200 INFO: val_e/atom_rmse: 0.002261
val_f_mae: 0.035442
2025-06-25 06:56:33.201 INFO: val_f_mae: 0.035442
val_f_rmse: 0.059565
2025-06-25 06:56:33.201 INFO: val_f_rmse: 0.059565
##### Step: 162 Learning rate: 3.90625e-05 #####
2025-06-25 06:57:03.598 INFO: ##### Step: 162 Learning rate: 3.90625e-05 #####
Epoch 23, Train Loss: 4.1288, Val Loss: 4.2630
2025-06-25 06:57:03.599 INFO: Epoch 23, Train Loss: 4.1288, Val Loss: 4.2630
train_e/atom_mae: 0.002187
2025-06-25 06:57:03.600 INFO: train_e/atom_mae: 0.002187
train_e/atom_rmse: 0.002344
2025-06-25 06:57:03.600 INFO: train_e/atom_rmse: 0.002344
train_f_mae: 0.034753
2025-06-25 06:57:03.603 INFO: train_f_mae: 0.034753
train_f_rmse: 0.058857
2025-06-25 06:57:03.603 INFO: train_f_rmse: 0.058857
val_e/atom_mae: 0.002165
2025-06-25 06:57:03.606 INFO: val_e/atom_mae: 0.002165
val_e/atom_rmse: 0.002445
2025-06-25 06:57:03.607 INFO: val_e/atom_rmse: 0.002445
val_f_mae: 0.035419
2025-06-25 06:57:03.607 INFO: val_f_mae: 0.035419
val_f_rmse: 0.059495
2025-06-25 06:57:03.607 INFO: val_f_rmse: 0.059495
##### Step: 163 Learning rate: 3.90625e-05 #####
2025-06-25 06:57:33.846 INFO: ##### Step: 163 Learning rate: 3.90625e-05 #####
Epoch 24, Train Loss: 4.1181, Val Loss: 4.2106
2025-06-25 06:57:33.846 INFO: Epoch 24, Train Loss: 4.1181, Val Loss: 4.2106
train_e/atom_mae: 0.002182
2025-06-25 06:57:33.847 INFO: train_e/atom_mae: 0.002182
train_e/atom_rmse: 0.002325
2025-06-25 06:57:33.847 INFO: train_e/atom_rmse: 0.002325
train_f_mae: 0.034756
2025-06-25 06:57:33.851 INFO: train_f_mae: 0.034756
train_f_rmse: 0.058855
2025-06-25 06:57:33.851 INFO: train_f_rmse: 0.058855
val_e/atom_mae: 0.002157
2025-06-25 06:57:33.854 INFO: val_e/atom_mae: 0.002157
val_e/atom_rmse: 0.002355
2025-06-25 06:57:33.854 INFO: val_e/atom_rmse: 0.002355
val_f_mae: 0.035415
2025-06-25 06:57:33.855 INFO: val_f_mae: 0.035415
val_f_rmse: 0.059492
2025-06-25 06:57:33.855 INFO: val_f_rmse: 0.059492
##### Step: 164 Learning rate: 3.90625e-05 #####
2025-06-25 06:58:04.210 INFO: ##### Step: 164 Learning rate: 3.90625e-05 #####
Epoch 25, Train Loss: 4.1161, Val Loss: 4.1665
2025-06-25 06:58:04.210 INFO: Epoch 25, Train Loss: 4.1161, Val Loss: 4.1665
train_e/atom_mae: 0.002177
2025-06-25 06:58:04.211 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002321
2025-06-25 06:58:04.211 INFO: train_e/atom_rmse: 0.002321
train_f_mae: 0.034751
2025-06-25 06:58:04.215 INFO: train_f_mae: 0.034751
train_f_rmse: 0.058858
2025-06-25 06:58:04.215 INFO: train_f_rmse: 0.058858
val_e/atom_mae: 0.002155
2025-06-25 06:58:04.217 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002290
2025-06-25 06:58:04.218 INFO: val_e/atom_rmse: 0.002290
val_f_mae: 0.035393
2025-06-25 06:58:04.218 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059433
2025-06-25 06:58:04.218 INFO: val_f_rmse: 0.059433
##### Step: 165 Learning rate: 3.90625e-05 #####
2025-06-25 06:58:34.452 INFO: ##### Step: 165 Learning rate: 3.90625e-05 #####
Epoch 26, Train Loss: 4.1302, Val Loss: 4.1598
2025-06-25 06:58:34.452 INFO: Epoch 26, Train Loss: 4.1302, Val Loss: 4.1598
train_e/atom_mae: 0.002185
2025-06-25 06:58:34.453 INFO: train_e/atom_mae: 0.002185
train_e/atom_rmse: 0.002348
2025-06-25 06:58:34.453 INFO: train_e/atom_rmse: 0.002348
train_f_mae: 0.034749
2025-06-25 06:58:34.456 INFO: train_f_mae: 0.034749
train_f_rmse: 0.058848
2025-06-25 06:58:34.457 INFO: train_f_rmse: 0.058848
val_e/atom_mae: 0.002148
2025-06-25 06:58:34.459 INFO: val_e/atom_mae: 0.002148
val_e/atom_rmse: 0.002272
2025-06-25 06:58:34.460 INFO: val_e/atom_rmse: 0.002272
val_f_mae: 0.035393
2025-06-25 06:58:34.460 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059458
2025-06-25 06:58:34.460 INFO: val_f_rmse: 0.059458
##### Step: 166 Learning rate: 3.90625e-05 #####
2025-06-25 06:59:04.798 INFO: ##### Step: 166 Learning rate: 3.90625e-05 #####
Epoch 27, Train Loss: 4.1332, Val Loss: 4.1660
2025-06-25 06:59:04.798 INFO: Epoch 27, Train Loss: 4.1332, Val Loss: 4.1660
train_e/atom_mae: 0.002178
2025-06-25 06:59:04.799 INFO: train_e/atom_mae: 0.002178
train_e/atom_rmse: 0.002353
2025-06-25 06:59:04.799 INFO: train_e/atom_rmse: 0.002353
train_f_mae: 0.034731
2025-06-25 06:59:04.802 INFO: train_f_mae: 0.034731
train_f_rmse: 0.058852
2025-06-25 06:59:04.803 INFO: train_f_rmse: 0.058852
val_e/atom_mae: 0.002158
2025-06-25 06:59:04.805 INFO: val_e/atom_mae: 0.002158
val_e/atom_rmse: 0.002284
2025-06-25 06:59:04.806 INFO: val_e/atom_rmse: 0.002284
val_f_mae: 0.035387
2025-06-25 06:59:04.806 INFO: val_f_mae: 0.035387
val_f_rmse: 0.059454
2025-06-25 06:59:04.806 INFO: val_f_rmse: 0.059454
##### Step: 167 Learning rate: 3.90625e-05 #####
2025-06-25 06:59:35.146 INFO: ##### Step: 167 Learning rate: 3.90625e-05 #####
Epoch 28, Train Loss: 4.1060, Val Loss: 4.1532
2025-06-25 06:59:35.146 INFO: Epoch 28, Train Loss: 4.1060, Val Loss: 4.1532
train_e/atom_mae: 0.002169
2025-06-25 06:59:35.147 INFO: train_e/atom_mae: 0.002169
train_e/atom_rmse: 0.002304
2025-06-25 06:59:35.147 INFO: train_e/atom_rmse: 0.002304
train_f_mae: 0.034739
2025-06-25 06:59:35.151 INFO: train_f_mae: 0.034739
train_f_rmse: 0.058851
2025-06-25 06:59:35.151 INFO: train_f_rmse: 0.058851
val_e/atom_mae: 0.002156
2025-06-25 06:59:35.154 INFO: val_e/atom_mae: 0.002156
val_e/atom_rmse: 0.002253
2025-06-25 06:59:35.154 INFO: val_e/atom_rmse: 0.002253
val_f_mae: 0.035398
2025-06-25 06:59:35.154 INFO: val_f_mae: 0.035398
val_f_rmse: 0.059489
2025-06-25 06:59:35.155 INFO: val_f_rmse: 0.059489
##### Step: 168 Learning rate: 3.90625e-05 #####
2025-06-25 07:00:05.437 INFO: ##### Step: 168 Learning rate: 3.90625e-05 #####
Epoch 29, Train Loss: 4.1147, Val Loss: 4.2762
2025-06-25 07:00:05.437 INFO: Epoch 29, Train Loss: 4.1147, Val Loss: 4.2762
train_e/atom_mae: 0.002177
2025-06-25 07:00:05.438 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002319
2025-06-25 07:00:05.438 INFO: train_e/atom_rmse: 0.002319
train_f_mae: 0.034741
2025-06-25 07:00:05.442 INFO: train_f_mae: 0.034741
train_f_rmse: 0.058855
2025-06-25 07:00:05.442 INFO: train_f_rmse: 0.058855
val_e/atom_mae: 0.002168
2025-06-25 07:00:05.445 INFO: val_e/atom_mae: 0.002168
val_e/atom_rmse: 0.002466
2025-06-25 07:00:05.445 INFO: val_e/atom_rmse: 0.002466
val_f_mae: 0.035401
2025-06-25 07:00:05.446 INFO: val_f_mae: 0.035401
val_f_rmse: 0.059501
2025-06-25 07:00:05.446 INFO: val_f_rmse: 0.059501
##### Step: 169 Learning rate: 3.90625e-05 #####
2025-06-25 07:00:35.865 INFO: ##### Step: 169 Learning rate: 3.90625e-05 #####
Epoch 30, Train Loss: 4.1022, Val Loss: 4.1510
2025-06-25 07:00:35.866 INFO: Epoch 30, Train Loss: 4.1022, Val Loss: 4.1510
train_e/atom_mae: 0.002175
2025-06-25 07:00:35.866 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002297
2025-06-25 07:00:35.867 INFO: train_e/atom_rmse: 0.002297
train_f_mae: 0.034747
2025-06-25 07:00:35.870 INFO: train_f_mae: 0.034747
train_f_rmse: 0.058856
2025-06-25 07:00:35.870 INFO: train_f_rmse: 0.058856
val_e/atom_mae: 0.002157
2025-06-25 07:00:35.873 INFO: val_e/atom_mae: 0.002157
val_e/atom_rmse: 0.002247
2025-06-25 07:00:35.873 INFO: val_e/atom_rmse: 0.002247
val_f_mae: 0.035407
2025-06-25 07:00:35.874 INFO: val_f_mae: 0.035407
val_f_rmse: 0.059499
2025-06-25 07:00:35.874 INFO: val_f_rmse: 0.059499
##### Step: 170 Learning rate: 3.90625e-05 #####
2025-06-25 07:01:06.048 INFO: ##### Step: 170 Learning rate: 3.90625e-05 #####
Epoch 31, Train Loss: 4.1066, Val Loss: 4.2242
2025-06-25 07:01:06.048 INFO: Epoch 31, Train Loss: 4.1066, Val Loss: 4.2242
train_e/atom_mae: 0.002174
2025-06-25 07:01:06.049 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002306
2025-06-25 07:01:06.049 INFO: train_e/atom_rmse: 0.002306
train_f_mae: 0.034733
2025-06-25 07:01:06.053 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058850
2025-06-25 07:01:06.053 INFO: train_f_rmse: 0.058850
val_e/atom_mae: 0.002154
2025-06-25 07:01:06.056 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002379
2025-06-25 07:01:06.056 INFO: val_e/atom_rmse: 0.002379
val_f_mae: 0.035402
2025-06-25 07:01:06.056 INFO: val_f_mae: 0.035402
val_f_rmse: 0.059494
2025-06-25 07:01:06.057 INFO: val_f_rmse: 0.059494
##### Step: 171 Learning rate: 3.90625e-05 #####
2025-06-25 07:01:36.416 INFO: ##### Step: 171 Learning rate: 3.90625e-05 #####
Epoch 32, Train Loss: 4.1034, Val Loss: 4.1694
2025-06-25 07:01:36.416 INFO: Epoch 32, Train Loss: 4.1034, Val Loss: 4.1694
train_e/atom_mae: 0.002177
2025-06-25 07:01:36.417 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002300
2025-06-25 07:01:36.417 INFO: train_e/atom_rmse: 0.002300
train_f_mae: 0.034736
2025-06-25 07:01:36.421 INFO: train_f_mae: 0.034736
train_f_rmse: 0.058852
2025-06-25 07:01:36.421 INFO: train_f_rmse: 0.058852
val_e/atom_mae: 0.002148
2025-06-25 07:01:36.424 INFO: val_e/atom_mae: 0.002148
val_e/atom_rmse: 0.002281
2025-06-25 07:01:36.424 INFO: val_e/atom_rmse: 0.002281
val_f_mae: 0.035406
2025-06-25 07:01:36.424 INFO: val_f_mae: 0.035406
val_f_rmse: 0.059498
2025-06-25 07:01:36.424 INFO: val_f_rmse: 0.059498
##### Step: 172 Learning rate: 3.90625e-05 #####
2025-06-25 07:02:06.637 INFO: ##### Step: 172 Learning rate: 3.90625e-05 #####
Epoch 33, Train Loss: 4.1332, Val Loss: 4.1528
2025-06-25 07:02:06.638 INFO: Epoch 33, Train Loss: 4.1332, Val Loss: 4.1528
train_e/atom_mae: 0.002177
2025-06-25 07:02:06.638 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002351
2025-06-25 07:02:06.639 INFO: train_e/atom_rmse: 0.002351
train_f_mae: 0.034738
2025-06-25 07:02:06.642 INFO: train_f_mae: 0.034738
train_f_rmse: 0.058858
2025-06-25 07:02:06.642 INFO: train_f_rmse: 0.058858
val_e/atom_mae: 0.002152
2025-06-25 07:02:06.645 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002250
2025-06-25 07:02:06.645 INFO: val_e/atom_rmse: 0.002250
val_f_mae: 0.035396
2025-06-25 07:02:06.646 INFO: val_f_mae: 0.035396
val_f_rmse: 0.059500
2025-06-25 07:02:06.646 INFO: val_f_rmse: 0.059500
##### Step: 173 Learning rate: 3.90625e-05 #####
2025-06-25 07:02:36.888 INFO: ##### Step: 173 Learning rate: 3.90625e-05 #####
Epoch 34, Train Loss: 4.1201, Val Loss: 4.1947
2025-06-25 07:02:36.889 INFO: Epoch 34, Train Loss: 4.1201, Val Loss: 4.1947
train_e/atom_mae: 0.002179
2025-06-25 07:02:36.890 INFO: train_e/atom_mae: 0.002179
train_e/atom_rmse: 0.002326
2025-06-25 07:02:36.890 INFO: train_e/atom_rmse: 0.002326
train_f_mae: 0.034742
2025-06-25 07:02:36.893 INFO: train_f_mae: 0.034742
train_f_rmse: 0.058868
2025-06-25 07:02:36.893 INFO: train_f_rmse: 0.058868
val_e/atom_mae: 0.002165
2025-06-25 07:02:36.896 INFO: val_e/atom_mae: 0.002165
val_e/atom_rmse: 0.002320
2025-06-25 07:02:36.896 INFO: val_e/atom_rmse: 0.002320
val_f_mae: 0.035420
2025-06-25 07:02:36.897 INFO: val_f_mae: 0.035420
val_f_rmse: 0.059526
2025-06-25 07:02:36.897 INFO: val_f_rmse: 0.059526
##### Step: 174 Learning rate: 3.90625e-05 #####
2025-06-25 07:03:07.223 INFO: ##### Step: 174 Learning rate: 3.90625e-05 #####
Epoch 35, Train Loss: 4.1135, Val Loss: 4.1513
2025-06-25 07:03:07.223 INFO: Epoch 35, Train Loss: 4.1135, Val Loss: 4.1513
train_e/atom_mae: 0.002174
2025-06-25 07:03:07.224 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002315
2025-06-25 07:03:07.224 INFO: train_e/atom_rmse: 0.002315
train_f_mae: 0.034738
2025-06-25 07:03:07.228 INFO: train_f_mae: 0.034738
train_f_rmse: 0.058862
2025-06-25 07:03:07.228 INFO: train_f_rmse: 0.058862
val_e/atom_mae: 0.002152
2025-06-25 07:03:07.231 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002250
2025-06-25 07:03:07.231 INFO: val_e/atom_rmse: 0.002250
val_f_mae: 0.035405
2025-06-25 07:03:07.232 INFO: val_f_mae: 0.035405
val_f_rmse: 0.059486
2025-06-25 07:03:07.232 INFO: val_f_rmse: 0.059486
##### Step: 175 Learning rate: 3.90625e-05 #####
2025-06-25 07:03:37.421 INFO: ##### Step: 175 Learning rate: 3.90625e-05 #####
Epoch 36, Train Loss: 4.1156, Val Loss: 4.1626
2025-06-25 07:03:37.421 INFO: Epoch 36, Train Loss: 4.1156, Val Loss: 4.1626
train_e/atom_mae: 0.002171
2025-06-25 07:03:37.422 INFO: train_e/atom_mae: 0.002171
train_e/atom_rmse: 0.002321
2025-06-25 07:03:37.422 INFO: train_e/atom_rmse: 0.002321
train_f_mae: 0.034738
2025-06-25 07:03:37.426 INFO: train_f_mae: 0.034738
train_f_rmse: 0.058851
2025-06-25 07:03:37.426 INFO: train_f_rmse: 0.058851
val_e/atom_mae: 0.002156
2025-06-25 07:03:37.428 INFO: val_e/atom_mae: 0.002156
val_e/atom_rmse: 0.002261
2025-06-25 07:03:37.429 INFO: val_e/atom_rmse: 0.002261
val_f_mae: 0.035424
2025-06-25 07:03:37.429 INFO: val_f_mae: 0.035424
val_f_rmse: 0.059530
2025-06-25 07:03:37.429 INFO: val_f_rmse: 0.059530
##### Step: 176 Learning rate: 3.90625e-05 #####
2025-06-25 07:04:07.749 INFO: ##### Step: 176 Learning rate: 3.90625e-05 #####
Epoch 37, Train Loss: 4.1177, Val Loss: 4.2087
2025-06-25 07:04:07.749 INFO: Epoch 37, Train Loss: 4.1177, Val Loss: 4.2087
train_e/atom_mae: 0.002174
2025-06-25 07:04:07.750 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002325
2025-06-25 07:04:07.750 INFO: train_e/atom_rmse: 0.002325
train_f_mae: 0.034738
2025-06-25 07:04:07.754 INFO: train_f_mae: 0.034738
train_f_rmse: 0.058853
2025-06-25 07:04:07.754 INFO: train_f_rmse: 0.058853
val_e/atom_mae: 0.002165
2025-06-25 07:04:07.756 INFO: val_e/atom_mae: 0.002165
val_e/atom_rmse: 0.002342
2025-06-25 07:04:07.757 INFO: val_e/atom_rmse: 0.002342
val_f_mae: 0.035418
2025-06-25 07:04:07.757 INFO: val_f_mae: 0.035418
val_f_rmse: 0.059540
2025-06-25 07:04:07.757 INFO: val_f_rmse: 0.059540
##### Step: 177 Learning rate: 3.90625e-05 #####
2025-06-25 07:04:37.964 INFO: ##### Step: 177 Learning rate: 3.90625e-05 #####
Epoch 38, Train Loss: 4.1295, Val Loss: 4.1512
2025-06-25 07:04:37.965 INFO: Epoch 38, Train Loss: 4.1295, Val Loss: 4.1512
train_e/atom_mae: 0.002180
2025-06-25 07:04:37.966 INFO: train_e/atom_mae: 0.002180
train_e/atom_rmse: 0.002347
2025-06-25 07:04:37.966 INFO: train_e/atom_rmse: 0.002347
train_f_mae: 0.034734
2025-06-25 07:04:37.969 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058847
2025-06-25 07:04:37.969 INFO: train_f_rmse: 0.058847
val_e/atom_mae: 0.002144
2025-06-25 07:04:37.972 INFO: val_e/atom_mae: 0.002144
val_e/atom_rmse: 0.002248
2025-06-25 07:04:37.972 INFO: val_e/atom_rmse: 0.002248
val_f_mae: 0.035395
2025-06-25 07:04:37.973 INFO: val_f_mae: 0.035395
val_f_rmse: 0.059498
2025-06-25 07:04:37.973 INFO: val_f_rmse: 0.059498
##### Step: 178 Learning rate: 3.90625e-05 #####
2025-06-25 07:05:08.322 INFO: ##### Step: 178 Learning rate: 3.90625e-05 #####
Epoch 39, Train Loss: 4.1105, Val Loss: 4.1470
2025-06-25 07:05:08.322 INFO: Epoch 39, Train Loss: 4.1105, Val Loss: 4.1470
train_e/atom_mae: 0.002155
2025-06-25 07:05:08.323 INFO: train_e/atom_mae: 0.002155
train_e/atom_rmse: 0.002311
2025-06-25 07:05:08.323 INFO: train_e/atom_rmse: 0.002311
train_f_mae: 0.034740
2025-06-25 07:05:08.327 INFO: train_f_mae: 0.034740
train_f_rmse: 0.058857
2025-06-25 07:05:08.327 INFO: train_f_rmse: 0.058857
val_e/atom_mae: 0.002149
2025-06-25 07:05:08.330 INFO: val_e/atom_mae: 0.002149
val_e/atom_rmse: 0.002247
2025-06-25 07:05:08.330 INFO: val_e/atom_rmse: 0.002247
val_f_mae: 0.035396
2025-06-25 07:05:08.330 INFO: val_f_mae: 0.035396
val_f_rmse: 0.059464
2025-06-25 07:05:08.330 INFO: val_f_rmse: 0.059464
##### Step: 179 Learning rate: 3.90625e-05 #####
2025-06-25 07:05:38.589 INFO: ##### Step: 179 Learning rate: 3.90625e-05 #####
Epoch 40, Train Loss: 4.1304, Val Loss: 4.1555
2025-06-25 07:05:38.589 INFO: Epoch 40, Train Loss: 4.1304, Val Loss: 4.1555
train_e/atom_mae: 0.002185
2025-06-25 07:05:38.590 INFO: train_e/atom_mae: 0.002185
train_e/atom_rmse: 0.002347
2025-06-25 07:05:38.590 INFO: train_e/atom_rmse: 0.002347
train_f_mae: 0.034734
2025-06-25 07:05:38.594 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058853
2025-06-25 07:05:38.594 INFO: train_f_rmse: 0.058853
val_e/atom_mae: 0.002143
2025-06-25 07:05:38.596 INFO: val_e/atom_mae: 0.002143
val_e/atom_rmse: 0.002261
2025-06-25 07:05:38.597 INFO: val_e/atom_rmse: 0.002261
val_f_mae: 0.035390
2025-06-25 07:05:38.597 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059473
2025-06-25 07:05:38.597 INFO: val_f_rmse: 0.059473
##### Step: 180 Learning rate: 1.953125e-05 #####
2025-06-25 07:06:08.906 INFO: ##### Step: 180 Learning rate: 1.953125e-05 #####
Epoch 41, Train Loss: 4.1017, Val Loss: 4.1538
2025-06-25 07:06:08.906 INFO: Epoch 41, Train Loss: 4.1017, Val Loss: 4.1538
train_e/atom_mae: 0.002160
2025-06-25 07:06:08.907 INFO: train_e/atom_mae: 0.002160
train_e/atom_rmse: 0.002297
2025-06-25 07:06:08.907 INFO: train_e/atom_rmse: 0.002297
train_f_mae: 0.034730
2025-06-25 07:06:08.911 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058851
2025-06-25 07:06:08.911 INFO: train_f_rmse: 0.058851
val_e/atom_mae: 0.002155
2025-06-25 07:06:08.913 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002258
2025-06-25 07:06:08.914 INFO: val_e/atom_rmse: 0.002258
val_f_mae: 0.035393
2025-06-25 07:06:08.914 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059470
2025-06-25 07:06:08.914 INFO: val_f_rmse: 0.059470
##### Step: 181 Learning rate: 1.953125e-05 #####
2025-06-25 07:06:39.347 INFO: ##### Step: 181 Learning rate: 1.953125e-05 #####
Epoch 42, Train Loss: 4.1035, Val Loss: 4.1575
2025-06-25 07:06:39.347 INFO: Epoch 42, Train Loss: 4.1035, Val Loss: 4.1575
train_e/atom_mae: 0.002179
2025-06-25 07:06:39.348 INFO: train_e/atom_mae: 0.002179
train_e/atom_rmse: 0.002303
2025-06-25 07:06:39.348 INFO: train_e/atom_rmse: 0.002303
train_f_mae: 0.034727
2025-06-25 07:06:39.352 INFO: train_f_mae: 0.034727
train_f_rmse: 0.058836
2025-06-25 07:06:39.352 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002155
2025-06-25 07:06:39.355 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002262
2025-06-25 07:06:39.355 INFO: val_e/atom_rmse: 0.002262
val_f_mae: 0.035398
2025-06-25 07:06:39.356 INFO: val_f_mae: 0.035398
val_f_rmse: 0.059483
2025-06-25 07:06:39.356 INFO: val_f_rmse: 0.059483
##### Step: 182 Learning rate: 1.953125e-05 #####
2025-06-25 07:07:09.627 INFO: ##### Step: 182 Learning rate: 1.953125e-05 #####
Epoch 43, Train Loss: 4.0971, Val Loss: 4.1482
2025-06-25 07:07:09.627 INFO: Epoch 43, Train Loss: 4.0971, Val Loss: 4.1482
train_e/atom_mae: 0.002179
2025-06-25 07:07:09.628 INFO: train_e/atom_mae: 0.002179
train_e/atom_rmse: 0.002291
2025-06-25 07:07:09.628 INFO: train_e/atom_rmse: 0.002291
train_f_mae: 0.034729
2025-06-25 07:07:09.632 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 07:07:09.632 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002155
2025-06-25 07:07:09.635 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002243
2025-06-25 07:07:09.635 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035402
2025-06-25 07:07:09.636 INFO: val_f_mae: 0.035402
val_f_rmse: 0.059494
2025-06-25 07:07:09.636 INFO: val_f_rmse: 0.059494
##### Step: 183 Learning rate: 1.953125e-05 #####
2025-06-25 07:07:40.108 INFO: ##### Step: 183 Learning rate: 1.953125e-05 #####
Epoch 44, Train Loss: 4.1261, Val Loss: 4.1367
2025-06-25 07:07:40.109 INFO: Epoch 44, Train Loss: 4.1261, Val Loss: 4.1367
train_e/atom_mae: 0.002185
2025-06-25 07:07:40.109 INFO: train_e/atom_mae: 0.002185
train_e/atom_rmse: 0.002342
2025-06-25 07:07:40.110 INFO: train_e/atom_rmse: 0.002342
train_f_mae: 0.034733
2025-06-25 07:07:40.113 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058841
2025-06-25 07:07:40.113 INFO: train_f_rmse: 0.058841
val_e/atom_mae: 0.002145
2025-06-25 07:07:40.116 INFO: val_e/atom_mae: 0.002145
val_e/atom_rmse: 0.002234
2025-06-25 07:07:40.116 INFO: val_e/atom_rmse: 0.002234
val_f_mae: 0.035379
2025-06-25 07:07:40.117 INFO: val_f_mae: 0.035379
val_f_rmse: 0.059436
2025-06-25 07:07:40.117 INFO: val_f_rmse: 0.059436
##### Step: 184 Learning rate: 1.953125e-05 #####
2025-06-25 07:08:10.428 INFO: ##### Step: 184 Learning rate: 1.953125e-05 #####
Epoch 45, Train Loss: 4.1096, Val Loss: 4.1493
2025-06-25 07:08:10.428 INFO: Epoch 45, Train Loss: 4.1096, Val Loss: 4.1493
train_e/atom_mae: 0.002177
2025-06-25 07:08:10.429 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002311
2025-06-25 07:08:10.429 INFO: train_e/atom_rmse: 0.002311
train_f_mae: 0.034740
2025-06-25 07:08:10.433 INFO: train_f_mae: 0.034740
train_f_rmse: 0.058850
2025-06-25 07:08:10.433 INFO: train_f_rmse: 0.058850
val_e/atom_mae: 0.002152
2025-06-25 07:08:10.436 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002254
2025-06-25 07:08:10.436 INFO: val_e/atom_rmse: 0.002254
val_f_mae: 0.035388
2025-06-25 07:08:10.437 INFO: val_f_mae: 0.035388
val_f_rmse: 0.059450
2025-06-25 07:08:10.437 INFO: val_f_rmse: 0.059450
##### Step: 185 Learning rate: 1.953125e-05 #####
2025-06-25 07:08:41.015 INFO: ##### Step: 185 Learning rate: 1.953125e-05 #####
Epoch 46, Train Loss: 4.1053, Val Loss: 4.1665
2025-06-25 07:08:41.015 INFO: Epoch 46, Train Loss: 4.1053, Val Loss: 4.1665
train_e/atom_mae: 0.002175
2025-06-25 07:08:41.016 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002306
2025-06-25 07:08:41.017 INFO: train_e/atom_rmse: 0.002306
train_f_mae: 0.034730
2025-06-25 07:08:41.020 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058839
2025-06-25 07:08:41.020 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002160
2025-06-25 07:08:41.023 INFO: val_e/atom_mae: 0.002160
val_e/atom_rmse: 0.002280
2025-06-25 07:08:41.023 INFO: val_e/atom_rmse: 0.002280
val_f_mae: 0.035399
2025-06-25 07:08:41.024 INFO: val_f_mae: 0.035399
val_f_rmse: 0.059476
2025-06-25 07:08:41.024 INFO: val_f_rmse: 0.059476
##### Step: 186 Learning rate: 1.953125e-05 #####
2025-06-25 07:09:11.383 INFO: ##### Step: 186 Learning rate: 1.953125e-05 #####
Epoch 47, Train Loss: 4.1008, Val Loss: 4.1447
2025-06-25 07:09:11.383 INFO: Epoch 47, Train Loss: 4.1008, Val Loss: 4.1447
train_e/atom_mae: 0.002176
2025-06-25 07:09:11.384 INFO: train_e/atom_mae: 0.002176
train_e/atom_rmse: 0.002296
2025-06-25 07:09:11.384 INFO: train_e/atom_rmse: 0.002296
train_f_mae: 0.034739
2025-06-25 07:09:11.388 INFO: train_f_mae: 0.034739
train_f_rmse: 0.058846
2025-06-25 07:09:11.388 INFO: train_f_rmse: 0.058846
val_e/atom_mae: 0.002152
2025-06-25 07:09:11.390 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002239
2025-06-25 07:09:11.391 INFO: val_e/atom_rmse: 0.002239
val_f_mae: 0.035402
2025-06-25 07:09:11.391 INFO: val_f_mae: 0.035402
val_f_rmse: 0.059482
2025-06-25 07:09:11.391 INFO: val_f_rmse: 0.059482
##### Step: 187 Learning rate: 1.953125e-05 #####
2025-06-25 07:09:41.661 INFO: ##### Step: 187 Learning rate: 1.953125e-05 #####
Epoch 48, Train Loss: 4.1021, Val Loss: 4.1385
2025-06-25 07:09:41.661 INFO: Epoch 48, Train Loss: 4.1021, Val Loss: 4.1385
train_e/atom_mae: 0.002171
2025-06-25 07:09:41.662 INFO: train_e/atom_mae: 0.002171
train_e/atom_rmse: 0.002299
2025-06-25 07:09:41.662 INFO: train_e/atom_rmse: 0.002299
train_f_mae: 0.034740
2025-06-25 07:09:41.665 INFO: train_f_mae: 0.034740
train_f_rmse: 0.058845
2025-06-25 07:09:41.666 INFO: train_f_rmse: 0.058845
val_e/atom_mae: 0.002150
2025-06-25 07:09:41.668 INFO: val_e/atom_mae: 0.002150
val_e/atom_rmse: 0.002238
2025-06-25 07:09:41.669 INFO: val_e/atom_rmse: 0.002238
val_f_mae: 0.035385
2025-06-25 07:09:41.669 INFO: val_f_mae: 0.035385
val_f_rmse: 0.059434
2025-06-25 07:09:41.669 INFO: val_f_rmse: 0.059434
##### Step: 188 Learning rate: 1.953125e-05 #####
2025-06-25 07:10:12.112 INFO: ##### Step: 188 Learning rate: 1.953125e-05 #####
Epoch 49, Train Loss: 4.1005, Val Loss: 4.1417
2025-06-25 07:10:12.113 INFO: Epoch 49, Train Loss: 4.1005, Val Loss: 4.1417
train_e/atom_mae: 0.002170
2025-06-25 07:10:12.114 INFO: train_e/atom_mae: 0.002170
train_e/atom_rmse: 0.002295
2025-06-25 07:10:12.114 INFO: train_e/atom_rmse: 0.002295
train_f_mae: 0.034742
2025-06-25 07:10:12.118 INFO: train_f_mae: 0.034742
train_f_rmse: 0.058849
2025-06-25 07:10:12.118 INFO: train_f_rmse: 0.058849
val_e/atom_mae: 0.002140
2025-06-25 07:10:12.120 INFO: val_e/atom_mae: 0.002140
val_e/atom_rmse: 0.002240
2025-06-25 07:10:12.121 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035393
2025-06-25 07:10:12.121 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059453
2025-06-25 07:10:12.121 INFO: val_f_rmse: 0.059453
##### Step: 189 Learning rate: 1.953125e-05 #####
2025-06-25 07:10:42.406 INFO: ##### Step: 189 Learning rate: 1.953125e-05 #####
Epoch 50, Train Loss: 4.1070, Val Loss: 4.1396
2025-06-25 07:10:42.406 INFO: Epoch 50, Train Loss: 4.1070, Val Loss: 4.1396
train_e/atom_mae: 0.002178
2025-06-25 07:10:42.407 INFO: train_e/atom_mae: 0.002178
train_e/atom_rmse: 0.002307
2025-06-25 07:10:42.407 INFO: train_e/atom_rmse: 0.002307
train_f_mae: 0.034740
2025-06-25 07:10:42.411 INFO: train_f_mae: 0.034740
train_f_rmse: 0.058848
2025-06-25 07:10:42.411 INFO: train_f_rmse: 0.058848
val_e/atom_mae: 0.002148
2025-06-25 07:10:42.413 INFO: val_e/atom_mae: 0.002148
val_e/atom_rmse: 0.002238
2025-06-25 07:10:42.414 INFO: val_e/atom_rmse: 0.002238
val_f_mae: 0.035386
2025-06-25 07:10:42.414 INFO: val_f_mae: 0.035386
val_f_rmse: 0.059443
2025-06-25 07:10:42.414 INFO: val_f_rmse: 0.059443
##### Step: 190 Learning rate: 1.953125e-05 #####
2025-06-25 07:11:12.902 INFO: ##### Step: 190 Learning rate: 1.953125e-05 #####
Epoch 51, Train Loss: 4.0966, Val Loss: 4.1503
2025-06-25 07:11:12.902 INFO: Epoch 51, Train Loss: 4.0966, Val Loss: 4.1503
train_e/atom_mae: 0.002170
2025-06-25 07:11:12.903 INFO: train_e/atom_mae: 0.002170
train_e/atom_rmse: 0.002290
2025-06-25 07:11:12.903 INFO: train_e/atom_rmse: 0.002290
train_f_mae: 0.034738
2025-06-25 07:11:12.907 INFO: train_f_mae: 0.034738
train_f_rmse: 0.058841
2025-06-25 07:11:12.907 INFO: train_f_rmse: 0.058841
val_e/atom_mae: 0.002152
2025-06-25 07:11:12.909 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002248
2025-06-25 07:11:12.910 INFO: val_e/atom_rmse: 0.002248
val_f_mae: 0.035408
2025-06-25 07:11:12.910 INFO: val_f_mae: 0.035408
val_f_rmse: 0.059490
2025-06-25 07:11:12.910 INFO: val_f_rmse: 0.059490
##### Step: 191 Learning rate: 1.953125e-05 #####
2025-06-25 07:11:43.227 INFO: ##### Step: 191 Learning rate: 1.953125e-05 #####
Epoch 52, Train Loss: 4.0982, Val Loss: 4.1974
2025-06-25 07:11:43.227 INFO: Epoch 52, Train Loss: 4.0982, Val Loss: 4.1974
train_e/atom_mae: 0.002167
2025-06-25 07:11:43.228 INFO: train_e/atom_mae: 0.002167
train_e/atom_rmse: 0.002290
2025-06-25 07:11:43.228 INFO: train_e/atom_rmse: 0.002290
train_f_mae: 0.034747
2025-06-25 07:11:43.232 INFO: train_f_mae: 0.034747
train_f_rmse: 0.058851
2025-06-25 07:11:43.232 INFO: train_f_rmse: 0.058851
val_e/atom_mae: 0.002158
2025-06-25 07:11:43.234 INFO: val_e/atom_mae: 0.002158
val_e/atom_rmse: 0.002327
2025-06-25 07:11:43.235 INFO: val_e/atom_rmse: 0.002327
val_f_mae: 0.035426
2025-06-25 07:11:43.235 INFO: val_f_mae: 0.035426
val_f_rmse: 0.059514
2025-06-25 07:11:43.235 INFO: val_f_rmse: 0.059514
##### Step: 192 Learning rate: 1.953125e-05 #####
2025-06-25 07:12:13.526 INFO: ##### Step: 192 Learning rate: 1.953125e-05 #####
Epoch 53, Train Loss: 4.1117, Val Loss: 4.1540
2025-06-25 07:12:13.526 INFO: Epoch 53, Train Loss: 4.1117, Val Loss: 4.1540
train_e/atom_mae: 0.002188
2025-06-25 07:12:13.527 INFO: train_e/atom_mae: 0.002188
train_e/atom_rmse: 0.002316
2025-06-25 07:12:13.527 INFO: train_e/atom_rmse: 0.002316
train_f_mae: 0.034747
2025-06-25 07:12:13.530 INFO: train_f_mae: 0.034747
train_f_rmse: 0.058844
2025-06-25 07:12:13.530 INFO: train_f_rmse: 0.058844
val_e/atom_mae: 0.002146
2025-06-25 07:12:13.533 INFO: val_e/atom_mae: 0.002146
val_e/atom_rmse: 0.002260
2025-06-25 07:12:13.534 INFO: val_e/atom_rmse: 0.002260
val_f_mae: 0.035400
2025-06-25 07:12:13.534 INFO: val_f_mae: 0.035400
val_f_rmse: 0.059464
2025-06-25 07:12:13.534 INFO: val_f_rmse: 0.059464
##### Step: 193 Learning rate: 1.953125e-05 #####
2025-06-25 07:12:43.906 INFO: ##### Step: 193 Learning rate: 1.953125e-05 #####
Epoch 54, Train Loss: 4.1088, Val Loss: 4.1504
2025-06-25 07:12:43.906 INFO: Epoch 54, Train Loss: 4.1088, Val Loss: 4.1504
train_e/atom_mae: 0.002172
2025-06-25 07:12:43.907 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002310
2025-06-25 07:12:43.907 INFO: train_e/atom_rmse: 0.002310
train_f_mae: 0.034745
2025-06-25 07:12:43.911 INFO: train_f_mae: 0.034745
train_f_rmse: 0.058847
2025-06-25 07:12:43.911 INFO: train_f_rmse: 0.058847
val_e/atom_mae: 0.002145
2025-06-25 07:12:43.914 INFO: val_e/atom_mae: 0.002145
val_e/atom_rmse: 0.002248
2025-06-25 07:12:43.914 INFO: val_e/atom_rmse: 0.002248
val_f_mae: 0.035407
2025-06-25 07:12:43.914 INFO: val_f_mae: 0.035407
val_f_rmse: 0.059490
2025-06-25 07:12:43.915 INFO: val_f_rmse: 0.059490
##### Step: 194 Learning rate: 1.953125e-05 #####
2025-06-25 07:13:14.190 INFO: ##### Step: 194 Learning rate: 1.953125e-05 #####
Epoch 55, Train Loss: 4.0959, Val Loss: 4.1557
2025-06-25 07:13:14.190 INFO: Epoch 55, Train Loss: 4.0959, Val Loss: 4.1557
train_e/atom_mae: 0.002175
2025-06-25 07:13:14.191 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002289
2025-06-25 07:13:14.191 INFO: train_e/atom_rmse: 0.002289
train_f_mae: 0.034737
2025-06-25 07:13:14.195 INFO: train_f_mae: 0.034737
train_f_rmse: 0.058837
2025-06-25 07:13:14.195 INFO: train_f_rmse: 0.058837
val_e/atom_mae: 0.002154
2025-06-25 07:13:14.198 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002253
2025-06-25 07:13:14.198 INFO: val_e/atom_rmse: 0.002253
val_f_mae: 0.035425
2025-06-25 07:13:14.199 INFO: val_f_mae: 0.035425
val_f_rmse: 0.059508
2025-06-25 07:13:14.199 INFO: val_f_rmse: 0.059508
##### Step: 195 Learning rate: 1.953125e-05 #####
2025-06-25 07:13:44.604 INFO: ##### Step: 195 Learning rate: 1.953125e-05 #####
Epoch 56, Train Loss: 4.1035, Val Loss: 4.1471
2025-06-25 07:13:44.604 INFO: Epoch 56, Train Loss: 4.1035, Val Loss: 4.1471
train_e/atom_mae: 0.002171
2025-06-25 07:13:44.605 INFO: train_e/atom_mae: 0.002171
train_e/atom_rmse: 0.002300
2025-06-25 07:13:44.606 INFO: train_e/atom_rmse: 0.002300
train_f_mae: 0.034753
2025-06-25 07:13:44.609 INFO: train_f_mae: 0.034753
train_f_rmse: 0.058853
2025-06-25 07:13:44.609 INFO: train_f_rmse: 0.058853
val_e/atom_mae: 0.002154
2025-06-25 07:13:44.612 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002247
2025-06-25 07:13:44.612 INFO: val_e/atom_rmse: 0.002247
val_f_mae: 0.035404
2025-06-25 07:13:44.613 INFO: val_f_mae: 0.035404
val_f_rmse: 0.059466
2025-06-25 07:13:44.613 INFO: val_f_rmse: 0.059466
##### Step: 196 Learning rate: 1.953125e-05 #####
2025-06-25 07:14:14.877 INFO: ##### Step: 196 Learning rate: 1.953125e-05 #####
Epoch 57, Train Loss: 4.0969, Val Loss: 4.1597
2025-06-25 07:14:14.878 INFO: Epoch 57, Train Loss: 4.0969, Val Loss: 4.1597
train_e/atom_mae: 0.002175
2025-06-25 07:14:14.879 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002289
2025-06-25 07:14:14.879 INFO: train_e/atom_rmse: 0.002289
train_f_mae: 0.034745
2025-06-25 07:14:14.882 INFO: train_f_mae: 0.034745
train_f_rmse: 0.058844
2025-06-25 07:14:14.882 INFO: train_f_rmse: 0.058844
val_e/atom_mae: 0.002158
2025-06-25 07:14:14.885 INFO: val_e/atom_mae: 0.002158
val_e/atom_rmse: 0.002261
2025-06-25 07:14:14.885 INFO: val_e/atom_rmse: 0.002261
val_f_mae: 0.035425
2025-06-25 07:14:14.886 INFO: val_f_mae: 0.035425
val_f_rmse: 0.059509
2025-06-25 07:14:14.886 INFO: val_f_rmse: 0.059509
##### Step: 197 Learning rate: 1.953125e-05 #####
2025-06-25 07:14:45.269 INFO: ##### Step: 197 Learning rate: 1.953125e-05 #####
Epoch 58, Train Loss: 4.0941, Val Loss: 4.1566
2025-06-25 07:14:45.270 INFO: Epoch 58, Train Loss: 4.0941, Val Loss: 4.1566
train_e/atom_mae: 0.002173
2025-06-25 07:14:45.271 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002284
2025-06-25 07:14:45.271 INFO: train_e/atom_rmse: 0.002284
train_f_mae: 0.034749
2025-06-25 07:14:45.274 INFO: train_f_mae: 0.034749
train_f_rmse: 0.058846
2025-06-25 07:14:45.274 INFO: train_f_rmse: 0.058846
val_e/atom_mae: 0.002156
2025-06-25 07:14:45.277 INFO: val_e/atom_mae: 0.002156
val_e/atom_rmse: 0.002255
2025-06-25 07:14:45.277 INFO: val_e/atom_rmse: 0.002255
val_f_mae: 0.035417
2025-06-25 07:14:45.278 INFO: val_f_mae: 0.035417
val_f_rmse: 0.059508
2025-06-25 07:14:45.278 INFO: val_f_rmse: 0.059508
##### Step: 198 Learning rate: 1.953125e-05 #####
2025-06-25 07:15:15.651 INFO: ##### Step: 198 Learning rate: 1.953125e-05 #####
Epoch 59, Train Loss: 4.0984, Val Loss: 4.1581
2025-06-25 07:15:15.651 INFO: Epoch 59, Train Loss: 4.0984, Val Loss: 4.1581
train_e/atom_mae: 0.002169
2025-06-25 07:15:15.652 INFO: train_e/atom_mae: 0.002169
train_e/atom_rmse: 0.002291
2025-06-25 07:15:15.652 INFO: train_e/atom_rmse: 0.002291
train_f_mae: 0.034736
2025-06-25 07:15:15.656 INFO: train_f_mae: 0.034736
train_f_rmse: 0.058849
2025-06-25 07:15:15.656 INFO: train_f_rmse: 0.058849
val_e/atom_mae: 0.002158
2025-06-25 07:15:15.659 INFO: val_e/atom_mae: 0.002158
val_e/atom_rmse: 0.002260
2025-06-25 07:15:15.659 INFO: val_e/atom_rmse: 0.002260
val_f_mae: 0.035404
2025-06-25 07:15:15.660 INFO: val_f_mae: 0.035404
val_f_rmse: 0.059499
2025-06-25 07:15:15.660 INFO: val_f_rmse: 0.059499
##### Step: 199 Learning rate: 1.953125e-05 #####
2025-06-25 07:15:45.990 INFO: ##### Step: 199 Learning rate: 1.953125e-05 #####
Epoch 60, Train Loss: 4.1080, Val Loss: 4.2990
2025-06-25 07:15:45.990 INFO: Epoch 60, Train Loss: 4.1080, Val Loss: 4.2990
train_e/atom_mae: 0.002167
2025-06-25 07:15:45.991 INFO: train_e/atom_mae: 0.002167
train_e/atom_rmse: 0.002309
2025-06-25 07:15:45.991 INFO: train_e/atom_rmse: 0.002309
train_f_mae: 0.034729
2025-06-25 07:15:45.995 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058846
2025-06-25 07:15:45.995 INFO: train_f_rmse: 0.058846
val_e/atom_mae: 0.002166
2025-06-25 07:15:45.998 INFO: val_e/atom_mae: 0.002166
val_e/atom_rmse: 0.002507
2025-06-25 07:15:45.998 INFO: val_e/atom_rmse: 0.002507
val_f_mae: 0.035397
2025-06-25 07:15:45.999 INFO: val_f_mae: 0.035397
val_f_rmse: 0.059487
2025-06-25 07:15:45.999 INFO: val_f_rmse: 0.059487
##### Step: 200 Learning rate: 9.765625e-06 #####
2025-06-25 07:16:16.455 INFO: ##### Step: 200 Learning rate: 9.765625e-06 #####
Epoch 61, Train Loss: 4.1011, Val Loss: 4.1502
2025-06-25 07:16:16.456 INFO: Epoch 61, Train Loss: 4.1011, Val Loss: 4.1502
train_e/atom_mae: 0.002175
2025-06-25 07:16:16.457 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002298
2025-06-25 07:16:16.457 INFO: train_e/atom_rmse: 0.002298
train_f_mae: 0.034731
2025-06-25 07:16:16.460 INFO: train_f_mae: 0.034731
train_f_rmse: 0.058841
2025-06-25 07:16:16.461 INFO: train_f_rmse: 0.058841
val_e/atom_mae: 0.002152
2025-06-25 07:16:16.463 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002244
2025-06-25 07:16:16.464 INFO: val_e/atom_rmse: 0.002244
val_f_mae: 0.035410
2025-06-25 07:16:16.464 INFO: val_f_mae: 0.035410
val_f_rmse: 0.059504
2025-06-25 07:16:16.464 INFO: val_f_rmse: 0.059504
##### Step: 201 Learning rate: 9.765625e-06 #####
2025-06-25 07:16:46.721 INFO: ##### Step: 201 Learning rate: 9.765625e-06 #####
Epoch 62, Train Loss: 4.0957, Val Loss: 4.1486
2025-06-25 07:16:46.721 INFO: Epoch 62, Train Loss: 4.0957, Val Loss: 4.1486
train_e/atom_mae: 0.002173
2025-06-25 07:16:46.722 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002288
2025-06-25 07:16:46.722 INFO: train_e/atom_rmse: 0.002288
train_f_mae: 0.034736
2025-06-25 07:16:46.726 INFO: train_f_mae: 0.034736
train_f_rmse: 0.058840
2025-06-25 07:16:46.726 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002151
2025-06-25 07:16:46.729 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002245
2025-06-25 07:16:46.729 INFO: val_e/atom_rmse: 0.002245
val_f_mae: 0.035409
2025-06-25 07:16:46.729 INFO: val_f_mae: 0.035409
val_f_rmse: 0.059485
2025-06-25 07:16:46.730 INFO: val_f_rmse: 0.059485
##### Step: 202 Learning rate: 9.765625e-06 #####
2025-06-25 07:17:17.175 INFO: ##### Step: 202 Learning rate: 9.765625e-06 #####
Epoch 63, Train Loss: 4.0951, Val Loss: 4.1478
2025-06-25 07:17:17.175 INFO: Epoch 63, Train Loss: 4.0951, Val Loss: 4.1478
train_e/atom_mae: 0.002175
2025-06-25 07:17:17.176 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002289
2025-06-25 07:17:17.176 INFO: train_e/atom_rmse: 0.002289
train_f_mae: 0.034734
2025-06-25 07:17:17.180 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058833
2025-06-25 07:17:17.180 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002153
2025-06-25 07:17:17.183 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002245
2025-06-25 07:17:17.183 INFO: val_e/atom_rmse: 0.002245
val_f_mae: 0.035407
2025-06-25 07:17:17.184 INFO: val_f_mae: 0.035407
val_f_rmse: 0.059481
2025-06-25 07:17:17.184 INFO: val_f_rmse: 0.059481
##### Step: 203 Learning rate: 9.765625e-06 #####
2025-06-25 07:17:47.471 INFO: ##### Step: 203 Learning rate: 9.765625e-06 #####
Epoch 64, Train Loss: 4.0936, Val Loss: 4.1713
2025-06-25 07:17:47.472 INFO: Epoch 64, Train Loss: 4.0936, Val Loss: 4.1713
train_e/atom_mae: 0.002181
2025-06-25 07:17:47.472 INFO: train_e/atom_mae: 0.002181
train_e/atom_rmse: 0.002285
2025-06-25 07:17:47.473 INFO: train_e/atom_rmse: 0.002285
train_f_mae: 0.034736
2025-06-25 07:17:47.476 INFO: train_f_mae: 0.034736
train_f_rmse: 0.058836
2025-06-25 07:17:47.476 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002153
2025-06-25 07:17:47.479 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002290
2025-06-25 07:17:47.479 INFO: val_e/atom_rmse: 0.002290
val_f_mae: 0.035398
2025-06-25 07:17:47.480 INFO: val_f_mae: 0.035398
val_f_rmse: 0.059470
2025-06-25 07:17:47.480 INFO: val_f_rmse: 0.059470
##### Step: 204 Learning rate: 9.765625e-06 #####
2025-06-25 07:18:17.879 INFO: ##### Step: 204 Learning rate: 9.765625e-06 #####
Epoch 65, Train Loss: 4.0931, Val Loss: 4.1440
2025-06-25 07:18:17.879 INFO: Epoch 65, Train Loss: 4.0931, Val Loss: 4.1440
train_e/atom_mae: 0.002170
2025-06-25 07:18:17.880 INFO: train_e/atom_mae: 0.002170
train_e/atom_rmse: 0.002284
2025-06-25 07:18:17.880 INFO: train_e/atom_rmse: 0.002284
train_f_mae: 0.034736
2025-06-25 07:18:17.884 INFO: train_f_mae: 0.034736
train_f_rmse: 0.058837
2025-06-25 07:18:17.884 INFO: train_f_rmse: 0.058837
val_e/atom_mae: 0.002149
2025-06-25 07:18:17.887 INFO: val_e/atom_mae: 0.002149
val_e/atom_rmse: 0.002238
2025-06-25 07:18:17.887 INFO: val_e/atom_rmse: 0.002238
val_f_mae: 0.035399
2025-06-25 07:18:17.888 INFO: val_f_mae: 0.035399
val_f_rmse: 0.059481
2025-06-25 07:18:17.888 INFO: val_f_rmse: 0.059481
##### Step: 205 Learning rate: 9.765625e-06 #####
2025-06-25 07:18:48.304 INFO: ##### Step: 205 Learning rate: 9.765625e-06 #####
Epoch 66, Train Loss: 4.0901, Val Loss: 4.1412
2025-06-25 07:18:48.305 INFO: Epoch 66, Train Loss: 4.0901, Val Loss: 4.1412
train_e/atom_mae: 0.002175
2025-06-25 07:18:48.306 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002279
2025-06-25 07:18:48.306 INFO: train_e/atom_rmse: 0.002279
train_f_mae: 0.034733
2025-06-25 07:18:48.309 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058836
2025-06-25 07:18:48.309 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002147
2025-06-25 07:18:48.312 INFO: val_e/atom_mae: 0.002147
val_e/atom_rmse: 0.002237
2025-06-25 07:18:48.313 INFO: val_e/atom_rmse: 0.002237
val_f_mae: 0.035389
2025-06-25 07:18:48.313 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059460
2025-06-25 07:18:48.313 INFO: val_f_rmse: 0.059460
##### Step: 206 Learning rate: 9.765625e-06 #####
2025-06-25 07:19:18.638 INFO: ##### Step: 206 Learning rate: 9.765625e-06 #####
Epoch 67, Train Loss: 4.0902, Val Loss: 4.1516
2025-06-25 07:19:18.639 INFO: Epoch 67, Train Loss: 4.0902, Val Loss: 4.1516
train_e/atom_mae: 0.002171
2025-06-25 07:19:18.640 INFO: train_e/atom_mae: 0.002171
train_e/atom_rmse: 0.002280
2025-06-25 07:19:18.640 INFO: train_e/atom_rmse: 0.002280
train_f_mae: 0.034727
2025-06-25 07:19:18.643 INFO: train_f_mae: 0.034727
train_f_rmse: 0.058830
2025-06-25 07:19:18.643 INFO: train_f_rmse: 0.058830
val_e/atom_mae: 0.002157
2025-06-25 07:19:18.646 INFO: val_e/atom_mae: 0.002157
val_e/atom_rmse: 0.002252
2025-06-25 07:19:18.647 INFO: val_e/atom_rmse: 0.002252
val_f_mae: 0.035398
2025-06-25 07:19:18.647 INFO: val_f_mae: 0.035398
val_f_rmse: 0.059482
2025-06-25 07:19:18.647 INFO: val_f_rmse: 0.059482
##### Step: 207 Learning rate: 9.765625e-06 #####
2025-06-25 07:19:49.078 INFO: ##### Step: 207 Learning rate: 9.765625e-06 #####
Epoch 68, Train Loss: 4.0913, Val Loss: 4.1488
2025-06-25 07:19:49.078 INFO: Epoch 68, Train Loss: 4.0913, Val Loss: 4.1488
train_e/atom_mae: 0.002175
2025-06-25 07:19:49.079 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002281
2025-06-25 07:19:49.079 INFO: train_e/atom_rmse: 0.002281
train_f_mae: 0.034729
2025-06-25 07:19:49.083 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058836
2025-06-25 07:19:49.083 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002155
2025-06-25 07:19:49.086 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002247
2025-06-25 07:19:49.086 INFO: val_e/atom_rmse: 0.002247
val_f_mae: 0.035399
2025-06-25 07:19:49.087 INFO: val_f_mae: 0.035399
val_f_rmse: 0.059481
2025-06-25 07:19:49.087 INFO: val_f_rmse: 0.059481
##### Step: 208 Learning rate: 9.765625e-06 #####
2025-06-25 07:20:19.341 INFO: ##### Step: 208 Learning rate: 9.765625e-06 #####
Epoch 69, Train Loss: 4.0952, Val Loss: 4.1509
2025-06-25 07:20:19.341 INFO: Epoch 69, Train Loss: 4.0952, Val Loss: 4.1509
train_e/atom_mae: 0.002174
2025-06-25 07:20:19.342 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002288
2025-06-25 07:20:19.342 INFO: train_e/atom_rmse: 0.002288
train_f_mae: 0.034730
2025-06-25 07:20:19.346 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058838
2025-06-25 07:20:19.346 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002155
2025-06-25 07:20:19.348 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002252
2025-06-25 07:20:19.349 INFO: val_e/atom_rmse: 0.002252
val_f_mae: 0.035396
2025-06-25 07:20:19.349 INFO: val_f_mae: 0.035396
val_f_rmse: 0.059476
2025-06-25 07:20:19.349 INFO: val_f_rmse: 0.059476
##### Step: 209 Learning rate: 9.765625e-06 #####
2025-06-25 07:20:49.783 INFO: ##### Step: 209 Learning rate: 9.765625e-06 #####
Epoch 70, Train Loss: 4.0920, Val Loss: 4.1466
2025-06-25 07:20:49.783 INFO: Epoch 70, Train Loss: 4.0920, Val Loss: 4.1466
train_e/atom_mae: 0.002171
2025-06-25 07:20:49.784 INFO: train_e/atom_mae: 0.002171
train_e/atom_rmse: 0.002283
2025-06-25 07:20:49.784 INFO: train_e/atom_rmse: 0.002283
train_f_mae: 0.034728
2025-06-25 07:20:49.788 INFO: train_f_mae: 0.034728
train_f_rmse: 0.058833
2025-06-25 07:20:49.788 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002152
2025-06-25 07:20:49.791 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 07:20:49.791 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035403
2025-06-25 07:20:49.792 INFO: val_f_mae: 0.035403
val_f_rmse: 0.059492
2025-06-25 07:20:49.792 INFO: val_f_rmse: 0.059492
##### Step: 210 Learning rate: 9.765625e-06 #####
2025-06-25 07:21:20.111 INFO: ##### Step: 210 Learning rate: 9.765625e-06 #####
Epoch 71, Train Loss: 4.0934, Val Loss: 4.1676
2025-06-25 07:21:20.111 INFO: Epoch 71, Train Loss: 4.0934, Val Loss: 4.1676
train_e/atom_mae: 0.002177
2025-06-25 07:21:20.112 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002284
2025-06-25 07:21:20.112 INFO: train_e/atom_rmse: 0.002284
train_f_mae: 0.034731
2025-06-25 07:21:20.116 INFO: train_f_mae: 0.034731
train_f_rmse: 0.058839
2025-06-25 07:21:20.116 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002155
2025-06-25 07:21:20.119 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002273
2025-06-25 07:21:20.119 INFO: val_e/atom_rmse: 0.002273
val_f_mae: 0.035416
2025-06-25 07:21:20.120 INFO: val_f_mae: 0.035416
val_f_rmse: 0.059519
2025-06-25 07:21:20.120 INFO: val_f_rmse: 0.059519
##### Step: 211 Learning rate: 9.765625e-06 #####
2025-06-25 07:21:50.501 INFO: ##### Step: 211 Learning rate: 9.765625e-06 #####
Epoch 72, Train Loss: 4.0928, Val Loss: 4.1660
2025-06-25 07:21:50.501 INFO: Epoch 72, Train Loss: 4.0928, Val Loss: 4.1660
train_e/atom_mae: 0.002176
2025-06-25 07:21:50.502 INFO: train_e/atom_mae: 0.002176
train_e/atom_rmse: 0.002283
2025-06-25 07:21:50.502 INFO: train_e/atom_rmse: 0.002283
train_f_mae: 0.034734
2025-06-25 07:21:50.505 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058838
2025-06-25 07:21:50.506 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002155
2025-06-25 07:21:50.508 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002276
2025-06-25 07:21:50.509 INFO: val_e/atom_rmse: 0.002276
val_f_mae: 0.035409
2025-06-25 07:21:50.509 INFO: val_f_mae: 0.035409
val_f_rmse: 0.059492
2025-06-25 07:21:50.509 INFO: val_f_rmse: 0.059492
##### Step: 212 Learning rate: 9.765625e-06 #####
2025-06-25 07:22:20.924 INFO: ##### Step: 212 Learning rate: 9.765625e-06 #####
Epoch 73, Train Loss: 4.0897, Val Loss: 4.1490
2025-06-25 07:22:20.925 INFO: Epoch 73, Train Loss: 4.0897, Val Loss: 4.1490
train_e/atom_mae: 0.002168
2025-06-25 07:22:20.925 INFO: train_e/atom_mae: 0.002168
train_e/atom_rmse: 0.002278
2025-06-25 07:22:20.926 INFO: train_e/atom_rmse: 0.002278
train_f_mae: 0.034735
2025-06-25 07:22:20.929 INFO: train_f_mae: 0.034735
train_f_rmse: 0.058835
2025-06-25 07:22:20.929 INFO: train_f_rmse: 0.058835
val_e/atom_mae: 0.002154
2025-06-25 07:22:20.932 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002245
2025-06-25 07:22:20.933 INFO: val_e/atom_rmse: 0.002245
val_f_mae: 0.035409
2025-06-25 07:22:20.933 INFO: val_f_mae: 0.035409
val_f_rmse: 0.059492
2025-06-25 07:22:20.933 INFO: val_f_rmse: 0.059492
##### Step: 213 Learning rate: 9.765625e-06 #####
2025-06-25 07:22:51.222 INFO: ##### Step: 213 Learning rate: 9.765625e-06 #####
Epoch 74, Train Loss: 4.0949, Val Loss: 4.1487
2025-06-25 07:22:51.223 INFO: Epoch 74, Train Loss: 4.0949, Val Loss: 4.1487
train_e/atom_mae: 0.002173
2025-06-25 07:22:51.224 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002288
2025-06-25 07:22:51.224 INFO: train_e/atom_rmse: 0.002288
train_f_mae: 0.034734
2025-06-25 07:22:51.227 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058836
2025-06-25 07:22:51.227 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002155
2025-06-25 07:22:51.230 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002244
2025-06-25 07:22:51.230 INFO: val_e/atom_rmse: 0.002244
val_f_mae: 0.035407
2025-06-25 07:22:51.231 INFO: val_f_mae: 0.035407
val_f_rmse: 0.059491
2025-06-25 07:22:51.231 INFO: val_f_rmse: 0.059491
##### Step: 214 Learning rate: 9.765625e-06 #####
2025-06-25 07:23:21.676 INFO: ##### Step: 214 Learning rate: 9.765625e-06 #####
Epoch 75, Train Loss: 4.0930, Val Loss: 4.1587
2025-06-25 07:23:21.676 INFO: Epoch 75, Train Loss: 4.0930, Val Loss: 4.1587
train_e/atom_mae: 0.002170
2025-06-25 07:23:21.677 INFO: train_e/atom_mae: 0.002170
train_e/atom_rmse: 0.002284
2025-06-25 07:23:21.678 INFO: train_e/atom_rmse: 0.002284
train_f_mae: 0.034730
2025-06-25 07:23:21.681 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058835
2025-06-25 07:23:21.681 INFO: train_f_rmse: 0.058835
val_e/atom_mae: 0.002161
2025-06-25 07:23:21.684 INFO: val_e/atom_mae: 0.002161
val_e/atom_rmse: 0.002258
2025-06-25 07:23:21.684 INFO: val_e/atom_rmse: 0.002258
val_f_mae: 0.035417
2025-06-25 07:23:21.685 INFO: val_f_mae: 0.035417
val_f_rmse: 0.059513
2025-06-25 07:23:21.685 INFO: val_f_rmse: 0.059513
##### Step: 215 Learning rate: 9.765625e-06 #####
2025-06-25 07:23:51.954 INFO: ##### Step: 215 Learning rate: 9.765625e-06 #####
Epoch 76, Train Loss: 4.0982, Val Loss: 4.1556
2025-06-25 07:23:51.954 INFO: Epoch 76, Train Loss: 4.0982, Val Loss: 4.1556
train_e/atom_mae: 0.002177
2025-06-25 07:23:51.955 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002293
2025-06-25 07:23:51.955 INFO: train_e/atom_rmse: 0.002293
train_f_mae: 0.034733
2025-06-25 07:23:51.959 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058841
2025-06-25 07:23:51.959 INFO: train_f_rmse: 0.058841
val_e/atom_mae: 0.002154
2025-06-25 07:23:51.962 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002249
2025-06-25 07:23:51.962 INFO: val_e/atom_rmse: 0.002249
val_f_mae: 0.035419
2025-06-25 07:23:51.963 INFO: val_f_mae: 0.035419
val_f_rmse: 0.059528
2025-06-25 07:23:51.963 INFO: val_f_rmse: 0.059528
##### Step: 216 Learning rate: 9.765625e-06 #####
2025-06-25 07:24:22.378 INFO: ##### Step: 216 Learning rate: 9.765625e-06 #####
Epoch 77, Train Loss: 4.0941, Val Loss: 4.1617
2025-06-25 07:24:22.378 INFO: Epoch 77, Train Loss: 4.0941, Val Loss: 4.1617
train_e/atom_mae: 0.002176
2025-06-25 07:24:22.379 INFO: train_e/atom_mae: 0.002176
train_e/atom_rmse: 0.002286
2025-06-25 07:24:22.379 INFO: train_e/atom_rmse: 0.002286
train_f_mae: 0.034728
2025-06-25 07:24:22.383 INFO: train_f_mae: 0.034728
train_f_rmse: 0.058839
2025-06-25 07:24:22.383 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002151
2025-06-25 07:24:22.386 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002264
2025-06-25 07:24:22.386 INFO: val_e/atom_rmse: 0.002264
val_f_mae: 0.035409
2025-06-25 07:24:22.386 INFO: val_f_mae: 0.035409
val_f_rmse: 0.059511
2025-06-25 07:24:22.386 INFO: val_f_rmse: 0.059511
##### Step: 217 Learning rate: 9.765625e-06 #####
2025-06-25 07:24:52.709 INFO: ##### Step: 217 Learning rate: 9.765625e-06 #####
Epoch 78, Train Loss: 4.0913, Val Loss: 4.1651
2025-06-25 07:24:52.710 INFO: Epoch 78, Train Loss: 4.0913, Val Loss: 4.1651
train_e/atom_mae: 0.002175
2025-06-25 07:24:52.711 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002281
2025-06-25 07:24:52.711 INFO: train_e/atom_rmse: 0.002281
train_f_mae: 0.034731
2025-06-25 07:24:52.714 INFO: train_f_mae: 0.034731
train_f_rmse: 0.058838
2025-06-25 07:24:52.715 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002159
2025-06-25 07:24:52.717 INFO: val_e/atom_mae: 0.002159
val_e/atom_rmse: 0.002268
2025-06-25 07:24:52.718 INFO: val_e/atom_rmse: 0.002268
val_f_mae: 0.035419
2025-06-25 07:24:52.718 INFO: val_f_mae: 0.035419
val_f_rmse: 0.059518
2025-06-25 07:24:52.718 INFO: val_f_rmse: 0.059518
##### Step: 218 Learning rate: 9.765625e-06 #####
2025-06-25 07:25:23.041 INFO: ##### Step: 218 Learning rate: 9.765625e-06 #####
Epoch 79, Train Loss: 4.0967, Val Loss: 4.1487
2025-06-25 07:25:23.042 INFO: Epoch 79, Train Loss: 4.0967, Val Loss: 4.1487
train_e/atom_mae: 0.002172
2025-06-25 07:25:23.043 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002290
2025-06-25 07:25:23.043 INFO: train_e/atom_rmse: 0.002290
train_f_mae: 0.034738
2025-06-25 07:25:23.046 INFO: train_f_mae: 0.034738
train_f_rmse: 0.058842
2025-06-25 07:25:23.046 INFO: train_f_rmse: 0.058842
val_e/atom_mae: 0.002154
2025-06-25 07:25:23.049 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002246
2025-06-25 07:25:23.049 INFO: val_e/atom_rmse: 0.002246
val_f_mae: 0.035414
2025-06-25 07:25:23.050 INFO: val_f_mae: 0.035414
val_f_rmse: 0.059486
2025-06-25 07:25:23.050 INFO: val_f_rmse: 0.059486
##### Step: 219 Learning rate: 9.765625e-06 #####
2025-06-25 07:25:53.509 INFO: ##### Step: 219 Learning rate: 9.765625e-06 #####
Epoch 80, Train Loss: 4.0908, Val Loss: 4.1541
2025-06-25 07:25:53.509 INFO: Epoch 80, Train Loss: 4.0908, Val Loss: 4.1541
train_e/atom_mae: 0.002178
2025-06-25 07:25:53.510 INFO: train_e/atom_mae: 0.002178
train_e/atom_rmse: 0.002280
2025-06-25 07:25:53.510 INFO: train_e/atom_rmse: 0.002280
train_f_mae: 0.034741
2025-06-25 07:25:53.514 INFO: train_f_mae: 0.034741
train_f_rmse: 0.058836
2025-06-25 07:25:53.514 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002157
2025-06-25 07:25:53.517 INFO: val_e/atom_mae: 0.002157
val_e/atom_rmse: 0.002254
2025-06-25 07:25:53.517 INFO: val_e/atom_rmse: 0.002254
val_f_mae: 0.035414
2025-06-25 07:25:53.518 INFO: val_f_mae: 0.035414
val_f_rmse: 0.059493
2025-06-25 07:25:53.518 INFO: val_f_rmse: 0.059493
##### Step: 220 Learning rate: 4.8828125e-06 #####
2025-06-25 07:26:23.805 INFO: ##### Step: 220 Learning rate: 4.8828125e-06 #####
Epoch 81, Train Loss: 4.0876, Val Loss: 4.1492
2025-06-25 07:26:23.806 INFO: Epoch 81, Train Loss: 4.0876, Val Loss: 4.1492
train_e/atom_mae: 0.002174
2025-06-25 07:26:23.807 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002275
2025-06-25 07:26:23.807 INFO: train_e/atom_rmse: 0.002275
train_f_mae: 0.034737
2025-06-25 07:26:23.810 INFO: train_f_mae: 0.034737
train_f_rmse: 0.058834
2025-06-25 07:26:23.811 INFO: train_f_rmse: 0.058834
val_e/atom_mae: 0.002155
2025-06-25 07:26:23.813 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002245
2025-06-25 07:26:23.814 INFO: val_e/atom_rmse: 0.002245
val_f_mae: 0.035412
2025-06-25 07:26:23.814 INFO: val_f_mae: 0.035412
val_f_rmse: 0.059494
2025-06-25 07:26:23.814 INFO: val_f_rmse: 0.059494
##### Step: 221 Learning rate: 4.8828125e-06 #####
2025-06-25 07:26:54.254 INFO: ##### Step: 221 Learning rate: 4.8828125e-06 #####
Epoch 82, Train Loss: 4.0876, Val Loss: 4.1636
2025-06-25 07:26:54.254 INFO: Epoch 82, Train Loss: 4.0876, Val Loss: 4.1636
train_e/atom_mae: 0.002172
2025-06-25 07:26:54.255 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002275
2025-06-25 07:26:54.255 INFO: train_e/atom_rmse: 0.002275
train_f_mae: 0.034735
2025-06-25 07:26:54.259 INFO: train_f_mae: 0.034735
train_f_rmse: 0.058834
2025-06-25 07:26:54.259 INFO: train_f_rmse: 0.058834
val_e/atom_mae: 0.002156
2025-06-25 07:26:54.262 INFO: val_e/atom_mae: 0.002156
val_e/atom_rmse: 0.002271
2025-06-25 07:26:54.262 INFO: val_e/atom_rmse: 0.002271
val_f_mae: 0.035409
2025-06-25 07:26:54.263 INFO: val_f_mae: 0.035409
val_f_rmse: 0.059492
2025-06-25 07:26:54.263 INFO: val_f_rmse: 0.059492
##### Step: 222 Learning rate: 4.8828125e-06 #####
2025-06-25 07:27:24.576 INFO: ##### Step: 222 Learning rate: 4.8828125e-06 #####
Epoch 83, Train Loss: 4.0891, Val Loss: 4.1631
2025-06-25 07:27:24.576 INFO: Epoch 83, Train Loss: 4.0891, Val Loss: 4.1631
train_e/atom_mae: 0.002175
2025-06-25 07:27:24.577 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002277
2025-06-25 07:27:24.577 INFO: train_e/atom_rmse: 0.002277
train_f_mae: 0.034735
2025-06-25 07:27:24.581 INFO: train_f_mae: 0.034735
train_f_rmse: 0.058835
2025-06-25 07:27:24.581 INFO: train_f_rmse: 0.058835
val_e/atom_mae: 0.002156
2025-06-25 07:27:24.583 INFO: val_e/atom_mae: 0.002156
val_e/atom_rmse: 0.002270
2025-06-25 07:27:24.584 INFO: val_e/atom_rmse: 0.002270
val_f_mae: 0.035413
2025-06-25 07:27:24.584 INFO: val_f_mae: 0.035413
val_f_rmse: 0.059495
2025-06-25 07:27:24.584 INFO: val_f_rmse: 0.059495
##### Step: 223 Learning rate: 4.8828125e-06 #####
2025-06-25 07:27:54.979 INFO: ##### Step: 223 Learning rate: 4.8828125e-06 #####
Epoch 84, Train Loss: 4.0893, Val Loss: 4.1522
2025-06-25 07:27:54.979 INFO: Epoch 84, Train Loss: 4.0893, Val Loss: 4.1522
train_e/atom_mae: 0.002175
2025-06-25 07:27:54.980 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002277
2025-06-25 07:27:54.981 INFO: train_e/atom_rmse: 0.002277
train_f_mae: 0.034740
2025-06-25 07:27:54.984 INFO: train_f_mae: 0.034740
train_f_rmse: 0.058839
2025-06-25 07:27:54.984 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002155
2025-06-25 07:27:54.987 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002248
2025-06-25 07:27:54.987 INFO: val_e/atom_rmse: 0.002248
val_f_mae: 0.035419
2025-06-25 07:27:54.988 INFO: val_f_mae: 0.035419
val_f_rmse: 0.059502
2025-06-25 07:27:54.988 INFO: val_f_rmse: 0.059502
##### Step: 224 Learning rate: 4.8828125e-06 #####
2025-06-25 07:28:25.405 INFO: ##### Step: 224 Learning rate: 4.8828125e-06 #####
Epoch 85, Train Loss: 4.0889, Val Loss: 4.1452
2025-06-25 07:28:25.405 INFO: Epoch 85, Train Loss: 4.0889, Val Loss: 4.1452
train_e/atom_mae: 0.002168
2025-06-25 07:28:25.406 INFO: train_e/atom_mae: 0.002168
train_e/atom_rmse: 0.002276
2025-06-25 07:28:25.406 INFO: train_e/atom_rmse: 0.002276
train_f_mae: 0.034740
2025-06-25 07:28:25.410 INFO: train_f_mae: 0.034740
train_f_rmse: 0.058841
2025-06-25 07:28:25.410 INFO: train_f_rmse: 0.058841
val_e/atom_mae: 0.002151
2025-06-25 07:28:25.413 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002239
2025-06-25 07:28:25.413 INFO: val_e/atom_rmse: 0.002239
val_f_mae: 0.035411
2025-06-25 07:28:25.414 INFO: val_f_mae: 0.035411
val_f_rmse: 0.059485
2025-06-25 07:28:25.414 INFO: val_f_rmse: 0.059485
##### Step: 225 Learning rate: 4.8828125e-06 #####
2025-06-25 07:28:55.859 INFO: ##### Step: 225 Learning rate: 4.8828125e-06 #####
Epoch 86, Train Loss: 4.0903, Val Loss: 4.1436
2025-06-25 07:28:55.859 INFO: Epoch 86, Train Loss: 4.0903, Val Loss: 4.1436
train_e/atom_mae: 0.002177
2025-06-25 07:28:55.860 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002279
2025-06-25 07:28:55.860 INFO: train_e/atom_rmse: 0.002279
train_f_mae: 0.034738
2025-06-25 07:28:55.864 INFO: train_f_mae: 0.034738
train_f_rmse: 0.058838
2025-06-25 07:28:55.864 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002151
2025-06-25 07:28:55.867 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002239
2025-06-25 07:28:55.867 INFO: val_e/atom_rmse: 0.002239
val_f_mae: 0.035403
2025-06-25 07:28:55.867 INFO: val_f_mae: 0.035403
val_f_rmse: 0.059471
2025-06-25 07:28:55.867 INFO: val_f_rmse: 0.059471
##### Step: 226 Learning rate: 4.8828125e-06 #####
2025-06-25 07:29:26.467 INFO: ##### Step: 226 Learning rate: 4.8828125e-06 #####
Epoch 87, Train Loss: 4.0888, Val Loss: 4.1433
2025-06-25 07:29:26.467 INFO: Epoch 87, Train Loss: 4.0888, Val Loss: 4.1433
train_e/atom_mae: 0.002177
2025-06-25 07:29:26.468 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002276
2025-06-25 07:29:26.469 INFO: train_e/atom_rmse: 0.002276
train_f_mae: 0.034738
2025-06-25 07:29:26.472 INFO: train_f_mae: 0.034738
train_f_rmse: 0.058837
2025-06-25 07:29:26.472 INFO: train_f_rmse: 0.058837
val_e/atom_mae: 0.002150
2025-06-25 07:29:26.475 INFO: val_e/atom_mae: 0.002150
val_e/atom_rmse: 0.002237
2025-06-25 07:29:26.476 INFO: val_e/atom_rmse: 0.002237
val_f_mae: 0.035406
2025-06-25 07:29:26.476 INFO: val_f_mae: 0.035406
val_f_rmse: 0.059478
2025-06-25 07:29:26.476 INFO: val_f_rmse: 0.059478
##### Step: 227 Learning rate: 4.8828125e-06 #####
2025-06-25 07:29:56.940 INFO: ##### Step: 227 Learning rate: 4.8828125e-06 #####
Epoch 88, Train Loss: 4.0881, Val Loss: 4.1692
2025-06-25 07:29:56.940 INFO: Epoch 88, Train Loss: 4.0881, Val Loss: 4.1692
train_e/atom_mae: 0.002172
2025-06-25 07:29:56.941 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002275
2025-06-25 07:29:56.941 INFO: train_e/atom_rmse: 0.002275
train_f_mae: 0.034737
2025-06-25 07:29:56.945 INFO: train_f_mae: 0.034737
train_f_rmse: 0.058835
2025-06-25 07:29:56.945 INFO: train_f_rmse: 0.058835
val_e/atom_mae: 0.002157
2025-06-25 07:29:56.948 INFO: val_e/atom_mae: 0.002157
val_e/atom_rmse: 0.002284
2025-06-25 07:29:56.948 INFO: val_e/atom_rmse: 0.002284
val_f_mae: 0.035407
2025-06-25 07:29:56.948 INFO: val_f_mae: 0.035407
val_f_rmse: 0.059480
2025-06-25 07:29:56.949 INFO: val_f_rmse: 0.059480
##### Step: 228 Learning rate: 4.8828125e-06 #####
2025-06-25 07:30:27.590 INFO: ##### Step: 228 Learning rate: 4.8828125e-06 #####
Epoch 89, Train Loss: 4.0945, Val Loss: 4.1514
2025-06-25 07:30:27.590 INFO: Epoch 89, Train Loss: 4.0945, Val Loss: 4.1514
train_e/atom_mae: 0.002172
2025-06-25 07:30:27.591 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002287
2025-06-25 07:30:27.591 INFO: train_e/atom_rmse: 0.002287
train_f_mae: 0.034739
2025-06-25 07:30:27.595 INFO: train_f_mae: 0.034739
train_f_rmse: 0.058837
2025-06-25 07:30:27.595 INFO: train_f_rmse: 0.058837
val_e/atom_mae: 0.002152
2025-06-25 07:30:27.598 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002251
2025-06-25 07:30:27.598 INFO: val_e/atom_rmse: 0.002251
val_f_mae: 0.035411
2025-06-25 07:30:27.599 INFO: val_f_mae: 0.035411
val_f_rmse: 0.059483
2025-06-25 07:30:27.599 INFO: val_f_rmse: 0.059483
##### Step: 229 Learning rate: 4.8828125e-06 #####
2025-06-25 07:30:58.091 INFO: ##### Step: 229 Learning rate: 4.8828125e-06 #####
Epoch 90, Train Loss: 4.0945, Val Loss: 4.1536
2025-06-25 07:30:58.092 INFO: Epoch 90, Train Loss: 4.0945, Val Loss: 4.1536
train_e/atom_mae: 0.002179
2025-06-25 07:30:58.093 INFO: train_e/atom_mae: 0.002179
train_e/atom_rmse: 0.002286
2025-06-25 07:30:58.093 INFO: train_e/atom_rmse: 0.002286
train_f_mae: 0.034740
2025-06-25 07:30:58.096 INFO: train_f_mae: 0.034740
train_f_rmse: 0.058839
2025-06-25 07:30:58.097 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002149
2025-06-25 07:30:58.099 INFO: val_e/atom_mae: 0.002149
val_e/atom_rmse: 0.002257
2025-06-25 07:30:58.100 INFO: val_e/atom_rmse: 0.002257
val_f_mae: 0.035408
2025-06-25 07:30:58.100 INFO: val_f_mae: 0.035408
val_f_rmse: 0.059473
2025-06-25 07:30:58.100 INFO: val_f_rmse: 0.059473
##### Step: 230 Learning rate: 4.8828125e-06 #####
2025-06-25 07:31:28.677 INFO: ##### Step: 230 Learning rate: 4.8828125e-06 #####
Epoch 91, Train Loss: 4.0899, Val Loss: 4.1417
2025-06-25 07:31:28.678 INFO: Epoch 91, Train Loss: 4.0899, Val Loss: 4.1417
train_e/atom_mae: 0.002175
2025-06-25 07:31:28.679 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002278
2025-06-25 07:31:28.679 INFO: train_e/atom_rmse: 0.002278
train_f_mae: 0.034738
2025-06-25 07:31:28.682 INFO: train_f_mae: 0.034738
train_f_rmse: 0.058838
2025-06-25 07:31:28.682 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002150
2025-06-25 07:31:28.685 INFO: val_e/atom_mae: 0.002150
val_e/atom_rmse: 0.002238
2025-06-25 07:31:28.686 INFO: val_e/atom_rmse: 0.002238
val_f_mae: 0.035400
2025-06-25 07:31:28.686 INFO: val_f_mae: 0.035400
val_f_rmse: 0.059461
2025-06-25 07:31:28.686 INFO: val_f_rmse: 0.059461
##### Step: 231 Learning rate: 4.8828125e-06 #####
2025-06-25 07:31:59.297 INFO: ##### Step: 231 Learning rate: 4.8828125e-06 #####
Epoch 92, Train Loss: 4.0887, Val Loss: 4.1554
2025-06-25 07:31:59.297 INFO: Epoch 92, Train Loss: 4.0887, Val Loss: 4.1554
train_e/atom_mae: 0.002173
2025-06-25 07:31:59.298 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002277
2025-06-25 07:31:59.298 INFO: train_e/atom_rmse: 0.002277
train_f_mae: 0.034735
2025-06-25 07:31:59.302 INFO: train_f_mae: 0.034735
train_f_rmse: 0.058836
2025-06-25 07:31:59.302 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002152
2025-06-25 07:31:59.305 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002262
2025-06-25 07:31:59.305 INFO: val_e/atom_rmse: 0.002262
val_f_mae: 0.035401
2025-06-25 07:31:59.306 INFO: val_f_mae: 0.035401
val_f_rmse: 0.059469
2025-06-25 07:31:59.306 INFO: val_f_rmse: 0.059469
##### Step: 232 Learning rate: 4.8828125e-06 #####
2025-06-25 07:32:29.706 INFO: ##### Step: 232 Learning rate: 4.8828125e-06 #####
Epoch 93, Train Loss: 4.0960, Val Loss: 4.1447
2025-06-25 07:32:29.706 INFO: Epoch 93, Train Loss: 4.0960, Val Loss: 4.1447
train_e/atom_mae: 0.002173
2025-06-25 07:32:29.707 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002289
2025-06-25 07:32:29.707 INFO: train_e/atom_rmse: 0.002289
train_f_mae: 0.034736
2025-06-25 07:32:29.711 INFO: train_f_mae: 0.034736
train_f_rmse: 0.058839
2025-06-25 07:32:29.711 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002148
2025-06-25 07:32:29.713 INFO: val_e/atom_mae: 0.002148
val_e/atom_rmse: 0.002242
2025-06-25 07:32:29.714 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035398
2025-06-25 07:32:29.714 INFO: val_f_mae: 0.035398
val_f_rmse: 0.059467
2025-06-25 07:32:29.715 INFO: val_f_rmse: 0.059467
##### Step: 233 Learning rate: 4.8828125e-06 #####
2025-06-25 07:33:00.263 INFO: ##### Step: 233 Learning rate: 4.8828125e-06 #####
Epoch 94, Train Loss: 4.0876, Val Loss: 4.1461
2025-06-25 07:33:00.263 INFO: Epoch 94, Train Loss: 4.0876, Val Loss: 4.1461
train_e/atom_mae: 0.002176
2025-06-25 07:33:00.264 INFO: train_e/atom_mae: 0.002176
train_e/atom_rmse: 0.002275
2025-06-25 07:33:00.264 INFO: train_e/atom_rmse: 0.002275
train_f_mae: 0.034734
2025-06-25 07:33:00.267 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058835
2025-06-25 07:33:00.268 INFO: train_f_rmse: 0.058835
val_e/atom_mae: 0.002153
2025-06-25 07:33:00.270 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 07:33:00.271 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035404
2025-06-25 07:33:00.271 INFO: val_f_mae: 0.035404
val_f_rmse: 0.059480
2025-06-25 07:33:00.271 INFO: val_f_rmse: 0.059480
##### Step: 234 Learning rate: 4.8828125e-06 #####
2025-06-25 07:33:30.723 INFO: ##### Step: 234 Learning rate: 4.8828125e-06 #####
Epoch 95, Train Loss: 4.0869, Val Loss: 4.1462
2025-06-25 07:33:30.723 INFO: Epoch 95, Train Loss: 4.0869, Val Loss: 4.1462
train_e/atom_mae: 0.002176
2025-06-25 07:33:30.724 INFO: train_e/atom_mae: 0.002176
train_e/atom_rmse: 0.002273
2025-06-25 07:33:30.724 INFO: train_e/atom_rmse: 0.002273
train_f_mae: 0.034733
2025-06-25 07:33:30.728 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058835
2025-06-25 07:33:30.728 INFO: train_f_rmse: 0.058835
val_e/atom_mae: 0.002152
2025-06-25 07:33:30.732 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002243
2025-06-25 07:33:30.732 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035401
2025-06-25 07:33:30.733 INFO: val_f_mae: 0.035401
val_f_rmse: 0.059478
2025-06-25 07:33:30.733 INFO: val_f_rmse: 0.059478
##### Step: 235 Learning rate: 4.8828125e-06 #####
2025-06-25 07:34:01.336 INFO: ##### Step: 235 Learning rate: 4.8828125e-06 #####
Epoch 96, Train Loss: 4.0909, Val Loss: 4.1441
2025-06-25 07:34:01.336 INFO: Epoch 96, Train Loss: 4.0909, Val Loss: 4.1441
train_e/atom_mae: 0.002175
2025-06-25 07:34:01.337 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002281
2025-06-25 07:34:01.337 INFO: train_e/atom_rmse: 0.002281
train_f_mae: 0.034733
2025-06-25 07:34:01.340 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058835
2025-06-25 07:34:01.341 INFO: train_f_rmse: 0.058835
val_e/atom_mae: 0.002151
2025-06-25 07:34:01.343 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002239
2025-06-25 07:34:01.344 INFO: val_e/atom_rmse: 0.002239
val_f_mae: 0.035402
2025-06-25 07:34:01.344 INFO: val_f_mae: 0.035402
val_f_rmse: 0.059475
2025-06-25 07:34:01.344 INFO: val_f_rmse: 0.059475
##### Step: 236 Learning rate: 4.8828125e-06 #####
2025-06-25 07:34:31.851 INFO: ##### Step: 236 Learning rate: 4.8828125e-06 #####
Epoch 97, Train Loss: 4.0894, Val Loss: 4.1592
2025-06-25 07:34:31.852 INFO: Epoch 97, Train Loss: 4.0894, Val Loss: 4.1592
train_e/atom_mae: 0.002174
2025-06-25 07:34:31.852 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002277
2025-06-25 07:34:31.853 INFO: train_e/atom_rmse: 0.002277
train_f_mae: 0.034738
2025-06-25 07:34:31.856 INFO: train_f_mae: 0.034738
train_f_rmse: 0.058838
2025-06-25 07:34:31.856 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002154
2025-06-25 07:34:31.859 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002265
2025-06-25 07:34:31.859 INFO: val_e/atom_rmse: 0.002265
val_f_mae: 0.035409
2025-06-25 07:34:31.860 INFO: val_f_mae: 0.035409
val_f_rmse: 0.059483
2025-06-25 07:34:31.860 INFO: val_f_rmse: 0.059483
##### Step: 237 Learning rate: 4.8828125e-06 #####
2025-06-25 07:35:02.366 INFO: ##### Step: 237 Learning rate: 4.8828125e-06 #####
Epoch 98, Train Loss: 4.0886, Val Loss: 4.1461
2025-06-25 07:35:02.366 INFO: Epoch 98, Train Loss: 4.0886, Val Loss: 4.1461
train_e/atom_mae: 0.002175
2025-06-25 07:35:02.367 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002276
2025-06-25 07:35:02.367 INFO: train_e/atom_rmse: 0.002276
train_f_mae: 0.034737
2025-06-25 07:35:02.371 INFO: train_f_mae: 0.034737
train_f_rmse: 0.058837
2025-06-25 07:35:02.371 INFO: train_f_rmse: 0.058837
val_e/atom_mae: 0.002152
2025-06-25 07:35:02.374 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002247
2025-06-25 07:35:02.374 INFO: val_e/atom_rmse: 0.002247
val_f_mae: 0.035399
2025-06-25 07:35:02.374 INFO: val_f_mae: 0.035399
val_f_rmse: 0.059460
2025-06-25 07:35:02.374 INFO: val_f_rmse: 0.059460
##### Step: 238 Learning rate: 4.8828125e-06 #####
2025-06-25 07:35:32.983 INFO: ##### Step: 238 Learning rate: 4.8828125e-06 #####
Epoch 99, Train Loss: 4.0863, Val Loss: 4.1480
2025-06-25 07:35:32.984 INFO: Epoch 99, Train Loss: 4.0863, Val Loss: 4.1480
train_e/atom_mae: 0.002171
2025-06-25 07:35:32.985 INFO: train_e/atom_mae: 0.002171
train_e/atom_rmse: 0.002272
2025-06-25 07:35:32.985 INFO: train_e/atom_rmse: 0.002272
train_f_mae: 0.034736
2025-06-25 07:35:32.988 INFO: train_f_mae: 0.034736
train_f_rmse: 0.058836
2025-06-25 07:35:32.989 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002149
2025-06-25 07:35:32.991 INFO: val_e/atom_mae: 0.002149
val_e/atom_rmse: 0.002251
2025-06-25 07:35:32.992 INFO: val_e/atom_rmse: 0.002251
val_f_mae: 0.035396
2025-06-25 07:35:32.992 INFO: val_f_mae: 0.035396
val_f_rmse: 0.059456
2025-06-25 07:35:32.992 INFO: val_f_rmse: 0.059456
##### Step: 239 Learning rate: 4.8828125e-06 #####
2025-06-25 07:36:03.472 INFO: ##### Step: 239 Learning rate: 4.8828125e-06 #####
Epoch 100, Train Loss: 4.0888, Val Loss: 4.1394
2025-06-25 07:36:03.472 INFO: Epoch 100, Train Loss: 4.0888, Val Loss: 4.1394
train_e/atom_mae: 0.002176
2025-06-25 07:36:03.473 INFO: train_e/atom_mae: 0.002176
train_e/atom_rmse: 0.002277
2025-06-25 07:36:03.473 INFO: train_e/atom_rmse: 0.002277
train_f_mae: 0.034736
2025-06-25 07:36:03.477 INFO: train_f_mae: 0.034736
train_f_rmse: 0.058836
2025-06-25 07:36:03.477 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002147
2025-06-25 07:36:03.480 INFO: val_e/atom_mae: 0.002147
val_e/atom_rmse: 0.002237
2025-06-25 07:36:03.480 INFO: val_e/atom_rmse: 0.002237
val_f_mae: 0.035390
2025-06-25 07:36:03.480 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059445
2025-06-25 07:36:03.481 INFO: val_f_rmse: 0.059445
2025-06-25 07:36:03.489 INFO: Fourth train loop:
##### Step: 240 Learning rate: 2.44140625e-06 #####
2025-06-25 07:36:34.070 INFO: ##### Step: 240 Learning rate: 2.44140625e-06 #####
Epoch 1, Train Loss: 66.0835, Val Loss: 64.2526
2025-06-25 07:36:34.071 INFO: Epoch 1, Train Loss: 66.0835, Val Loss: 64.2526
train_e/atom_mae: 0.002177
2025-06-25 07:36:34.071 INFO: train_e/atom_mae: 0.002177
train_e/atom_rmse: 0.002275
2025-06-25 07:36:34.072 INFO: train_e/atom_rmse: 0.002275
train_f_mae: 0.034735
2025-06-25 07:36:34.075 INFO: train_f_mae: 0.034735
train_f_rmse: 0.058833
2025-06-25 07:36:34.075 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002152
2025-06-25 07:36:34.078 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 07:36:34.078 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035393
2025-06-25 07:36:34.079 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059450
2025-06-25 07:36:34.079 INFO: val_f_rmse: 0.059450
##### Step: 241 Learning rate: 2.44140625e-06 #####
2025-06-25 07:37:04.522 INFO: ##### Step: 241 Learning rate: 2.44140625e-06 #####
Epoch 2, Train Loss: 65.9260, Val Loss: 64.3786
2025-06-25 07:37:04.522 INFO: Epoch 2, Train Loss: 65.9260, Val Loss: 64.3786
train_e/atom_mae: 0.002175
2025-06-25 07:37:04.523 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002272
2025-06-25 07:37:04.523 INFO: train_e/atom_rmse: 0.002272
train_f_mae: 0.034733
2025-06-25 07:37:04.527 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058833
2025-06-25 07:37:04.527 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002151
2025-06-25 07:37:04.529 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002242
2025-06-25 07:37:04.530 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035393
2025-06-25 07:37:04.530 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059450
2025-06-25 07:37:04.530 INFO: val_f_rmse: 0.059450
##### Step: 242 Learning rate: 2.44140625e-06 #####
2025-06-25 07:37:35.084 INFO: ##### Step: 242 Learning rate: 2.44140625e-06 #####
Epoch 3, Train Loss: 66.2053, Val Loss: 64.7754
2025-06-25 07:37:35.085 INFO: Epoch 3, Train Loss: 66.2053, Val Loss: 64.7754
train_e/atom_mae: 0.002176
2025-06-25 07:37:35.086 INFO: train_e/atom_mae: 0.002176
train_e/atom_rmse: 0.002277
2025-06-25 07:37:35.086 INFO: train_e/atom_rmse: 0.002277
train_f_mae: 0.034734
2025-06-25 07:37:35.089 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058833
2025-06-25 07:37:35.089 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002151
2025-06-25 07:37:35.092 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002250
2025-06-25 07:37:35.092 INFO: val_e/atom_rmse: 0.002250
val_f_mae: 0.035391
2025-06-25 07:37:35.093 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059450
2025-06-25 07:37:35.093 INFO: val_f_rmse: 0.059450
##### Step: 243 Learning rate: 2.44140625e-06 #####
2025-06-25 07:38:05.645 INFO: ##### Step: 243 Learning rate: 2.44140625e-06 #####
Epoch 4, Train Loss: 65.9820, Val Loss: 64.2312
2025-06-25 07:38:05.646 INFO: Epoch 4, Train Loss: 65.9820, Val Loss: 64.2312
train_e/atom_mae: 0.002175
2025-06-25 07:38:05.647 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002273
2025-06-25 07:38:05.647 INFO: train_e/atom_rmse: 0.002273
train_f_mae: 0.034734
2025-06-25 07:38:05.650 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058833
2025-06-25 07:38:05.651 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002150
2025-06-25 07:38:05.653 INFO: val_e/atom_mae: 0.002150
val_e/atom_rmse: 0.002240
2025-06-25 07:38:05.654 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035390
2025-06-25 07:38:05.654 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059447
2025-06-25 07:38:05.654 INFO: val_f_rmse: 0.059447
##### Step: 244 Learning rate: 2.44140625e-06 #####
2025-06-25 07:38:36.178 INFO: ##### Step: 244 Learning rate: 2.44140625e-06 #####
Epoch 5, Train Loss: 66.0666, Val Loss: 64.2849
2025-06-25 07:38:36.178 INFO: Epoch 5, Train Loss: 66.0666, Val Loss: 64.2849
train_e/atom_mae: 0.002172
2025-06-25 07:38:36.179 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002275
2025-06-25 07:38:36.179 INFO: train_e/atom_rmse: 0.002275
train_f_mae: 0.034733
2025-06-25 07:38:36.183 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058833
2025-06-25 07:38:36.183 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002153
2025-06-25 07:38:36.185 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002241
2025-06-25 07:38:36.186 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035393
2025-06-25 07:38:36.186 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059453
2025-06-25 07:38:36.186 INFO: val_f_rmse: 0.059453
##### Step: 245 Learning rate: 2.44140625e-06 #####
2025-06-25 07:39:06.798 INFO: ##### Step: 245 Learning rate: 2.44140625e-06 #####
Epoch 6, Train Loss: 66.2007, Val Loss: 64.3735
2025-06-25 07:39:06.798 INFO: Epoch 6, Train Loss: 66.2007, Val Loss: 64.3735
train_e/atom_mae: 0.002176
2025-06-25 07:39:06.799 INFO: train_e/atom_mae: 0.002176
train_e/atom_rmse: 0.002277
2025-06-25 07:39:06.799 INFO: train_e/atom_rmse: 0.002277
train_f_mae: 0.034734
2025-06-25 07:39:06.803 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058833
2025-06-25 07:39:06.803 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002153
2025-06-25 07:39:06.806 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 07:39:06.806 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035395
2025-06-25 07:39:06.806 INFO: val_f_mae: 0.035395
val_f_rmse: 0.059457
2025-06-25 07:39:06.807 INFO: val_f_rmse: 0.059457
##### Step: 246 Learning rate: 2.44140625e-06 #####
2025-06-25 07:39:37.257 INFO: ##### Step: 246 Learning rate: 2.44140625e-06 #####
Epoch 7, Train Loss: 65.9539, Val Loss: 64.5683
2025-06-25 07:39:37.257 INFO: Epoch 7, Train Loss: 65.9539, Val Loss: 64.5683
train_e/atom_mae: 0.002174
2025-06-25 07:39:37.258 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002273
2025-06-25 07:39:37.258 INFO: train_e/atom_rmse: 0.002273
train_f_mae: 0.034735
2025-06-25 07:39:37.262 INFO: train_f_mae: 0.034735
train_f_rmse: 0.058833
2025-06-25 07:39:37.262 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002154
2025-06-25 07:39:37.265 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002246
2025-06-25 07:39:37.265 INFO: val_e/atom_rmse: 0.002246
val_f_mae: 0.035395
2025-06-25 07:39:37.266 INFO: val_f_mae: 0.035395
val_f_rmse: 0.059456
2025-06-25 07:39:37.266 INFO: val_f_rmse: 0.059456
##### Step: 247 Learning rate: 2.44140625e-06 #####
2025-06-25 07:40:07.901 INFO: ##### Step: 247 Learning rate: 2.44140625e-06 #####
Epoch 8, Train Loss: 65.9622, Val Loss: 64.3181
2025-06-25 07:40:07.901 INFO: Epoch 8, Train Loss: 65.9622, Val Loss: 64.3181
train_e/atom_mae: 0.002174
2025-06-25 07:40:07.902 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002273
2025-06-25 07:40:07.902 INFO: train_e/atom_rmse: 0.002273
train_f_mae: 0.034734
2025-06-25 07:40:07.906 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058833
2025-06-25 07:40:07.906 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002153
2025-06-25 07:40:07.909 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002241
2025-06-25 07:40:07.909 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035394
2025-06-25 07:40:07.910 INFO: val_f_mae: 0.035394
val_f_rmse: 0.059456
2025-06-25 07:40:07.910 INFO: val_f_rmse: 0.059456
##### Step: 248 Learning rate: 2.44140625e-06 #####
2025-06-25 07:40:38.389 INFO: ##### Step: 248 Learning rate: 2.44140625e-06 #####
Epoch 9, Train Loss: 65.9196, Val Loss: 64.5712
2025-06-25 07:40:38.389 INFO: Epoch 9, Train Loss: 65.9196, Val Loss: 64.5712
train_e/atom_mae: 0.002174
2025-06-25 07:40:38.390 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002272
2025-06-25 07:40:38.390 INFO: train_e/atom_rmse: 0.002272
train_f_mae: 0.034735
2025-06-25 07:40:38.394 INFO: train_f_mae: 0.034735
train_f_rmse: 0.058834
2025-06-25 07:40:38.394 INFO: train_f_rmse: 0.058834
val_e/atom_mae: 0.002151
2025-06-25 07:40:38.397 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002246
2025-06-25 07:40:38.397 INFO: val_e/atom_rmse: 0.002246
val_f_mae: 0.035392
2025-06-25 07:40:38.397 INFO: val_f_mae: 0.035392
val_f_rmse: 0.059453
2025-06-25 07:40:38.398 INFO: val_f_rmse: 0.059453
##### Step: 249 Learning rate: 2.44140625e-06 #####
2025-06-25 07:41:08.940 INFO: ##### Step: 249 Learning rate: 2.44140625e-06 #####
Epoch 10, Train Loss: 65.9254, Val Loss: 64.5495
2025-06-25 07:41:08.941 INFO: Epoch 10, Train Loss: 65.9254, Val Loss: 64.5495
train_e/atom_mae: 0.002173
2025-06-25 07:41:08.942 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002272
2025-06-25 07:41:08.942 INFO: train_e/atom_rmse: 0.002272
train_f_mae: 0.034734
2025-06-25 07:41:08.945 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058833
2025-06-25 07:41:08.945 INFO: train_f_rmse: 0.058833
val_e/atom_mae: 0.002153
2025-06-25 07:41:08.948 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002246
2025-06-25 07:41:08.948 INFO: val_e/atom_rmse: 0.002246
val_f_mae: 0.035394
2025-06-25 07:41:08.949 INFO: val_f_mae: 0.035394
val_f_rmse: 0.059456
2025-06-25 07:41:08.949 INFO: val_f_rmse: 0.059456
##### Step: 250 Learning rate: 2.44140625e-06 #####
2025-06-25 07:41:39.491 INFO: ##### Step: 250 Learning rate: 2.44140625e-06 #####
Epoch 11, Train Loss: 66.0751, Val Loss: 64.6087
2025-06-25 07:41:39.491 INFO: Epoch 11, Train Loss: 66.0751, Val Loss: 64.6087
train_e/atom_mae: 0.002176
2025-06-25 07:41:39.492 INFO: train_e/atom_mae: 0.002176
train_e/atom_rmse: 0.002275
2025-06-25 07:41:39.492 INFO: train_e/atom_rmse: 0.002275
train_f_mae: 0.034734
2025-06-25 07:41:39.496 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058834
2025-06-25 07:41:39.496 INFO: train_f_rmse: 0.058834
val_e/atom_mae: 0.002155
2025-06-25 07:41:39.499 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002247
2025-06-25 07:41:39.499 INFO: val_e/atom_rmse: 0.002247
val_f_mae: 0.035397
2025-06-25 07:41:39.500 INFO: val_f_mae: 0.035397
val_f_rmse: 0.059459
2025-06-25 07:41:39.500 INFO: val_f_rmse: 0.059459
##### Step: 251 Learning rate: 2.44140625e-06 #####
2025-06-25 07:42:09.813 INFO: ##### Step: 251 Learning rate: 2.44140625e-06 #####
Epoch 12, Train Loss: 65.9256, Val Loss: 64.5280
2025-06-25 07:42:09.813 INFO: Epoch 12, Train Loss: 65.9256, Val Loss: 64.5280
train_e/atom_mae: 0.002174
2025-06-25 07:42:09.814 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002272
2025-06-25 07:42:09.814 INFO: train_e/atom_rmse: 0.002272
train_f_mae: 0.034735
2025-06-25 07:42:09.818 INFO: train_f_mae: 0.034735
train_f_rmse: 0.058834
2025-06-25 07:42:09.818 INFO: train_f_rmse: 0.058834
val_e/atom_mae: 0.002154
2025-06-25 07:42:09.820 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002245
2025-06-25 07:42:09.821 INFO: val_e/atom_rmse: 0.002245
val_f_mae: 0.035394
2025-06-25 07:42:09.821 INFO: val_f_mae: 0.035394
val_f_rmse: 0.059456
2025-06-25 07:42:09.821 INFO: val_f_rmse: 0.059456
##### Step: 252 Learning rate: 2.44140625e-06 #####
2025-06-25 07:42:40.221 INFO: ##### Step: 252 Learning rate: 2.44140625e-06 #####
Epoch 13, Train Loss: 66.0081, Val Loss: 65.6849
2025-06-25 07:42:40.221 INFO: Epoch 13, Train Loss: 66.0081, Val Loss: 65.6849
train_e/atom_mae: 0.002174
2025-06-25 07:42:40.222 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002274
2025-06-25 07:42:40.222 INFO: train_e/atom_rmse: 0.002274
train_f_mae: 0.034734
2025-06-25 07:42:40.226 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058834
2025-06-25 07:42:40.226 INFO: train_f_rmse: 0.058834
val_e/atom_mae: 0.002156
2025-06-25 07:42:40.229 INFO: val_e/atom_mae: 0.002156
val_e/atom_rmse: 0.002266
2025-06-25 07:42:40.229 INFO: val_e/atom_rmse: 0.002266
val_f_mae: 0.035396
2025-06-25 07:42:40.229 INFO: val_f_mae: 0.035396
val_f_rmse: 0.059461
2025-06-25 07:42:40.229 INFO: val_f_rmse: 0.059461
##### Step: 253 Learning rate: 2.44140625e-06 #####
2025-06-25 07:43:10.435 INFO: ##### Step: 253 Learning rate: 2.44140625e-06 #####
Epoch 14, Train Loss: 65.8229, Val Loss: 64.5716
2025-06-25 07:43:10.435 INFO: Epoch 14, Train Loss: 65.8229, Val Loss: 64.5716
train_e/atom_mae: 0.002171
2025-06-25 07:43:10.436 INFO: train_e/atom_mae: 0.002171
train_e/atom_rmse: 0.002270
2025-06-25 07:43:10.436 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034735
2025-06-25 07:43:10.440 INFO: train_f_mae: 0.034735
train_f_rmse: 0.058834
2025-06-25 07:43:10.440 INFO: train_f_rmse: 0.058834
val_e/atom_mae: 0.002155
2025-06-25 07:43:10.442 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002246
2025-06-25 07:43:10.443 INFO: val_e/atom_rmse: 0.002246
val_f_mae: 0.035396
2025-06-25 07:43:10.443 INFO: val_f_mae: 0.035396
val_f_rmse: 0.059462
2025-06-25 07:43:10.443 INFO: val_f_rmse: 0.059462
##### Step: 254 Learning rate: 2.44140625e-06 #####
2025-06-25 07:43:40.740 INFO: ##### Step: 254 Learning rate: 2.44140625e-06 #####
Epoch 15, Train Loss: 65.8504, Val Loss: 64.7785
2025-06-25 07:43:40.741 INFO: Epoch 15, Train Loss: 65.8504, Val Loss: 64.7785
train_e/atom_mae: 0.002172
2025-06-25 07:43:40.742 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002271
2025-06-25 07:43:40.742 INFO: train_e/atom_rmse: 0.002271
train_f_mae: 0.034733
2025-06-25 07:43:40.745 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058834
2025-06-25 07:43:40.745 INFO: train_f_rmse: 0.058834
val_e/atom_mae: 0.002158
2025-06-25 07:43:40.748 INFO: val_e/atom_mae: 0.002158
val_e/atom_rmse: 0.002250
2025-06-25 07:43:40.748 INFO: val_e/atom_rmse: 0.002250
val_f_mae: 0.035398
2025-06-25 07:43:40.749 INFO: val_f_mae: 0.035398
val_f_rmse: 0.059466
2025-06-25 07:43:40.749 INFO: val_f_rmse: 0.059466
##### Step: 255 Learning rate: 2.44140625e-06 #####
2025-06-25 07:44:10.966 INFO: ##### Step: 255 Learning rate: 2.44140625e-06 #####
Epoch 16, Train Loss: 66.0974, Val Loss: 64.3556
2025-06-25 07:44:10.966 INFO: Epoch 16, Train Loss: 66.0974, Val Loss: 64.3556
train_e/atom_mae: 0.002175
2025-06-25 07:44:10.967 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002275
2025-06-25 07:44:10.968 INFO: train_e/atom_rmse: 0.002275
train_f_mae: 0.034734
2025-06-25 07:44:10.971 INFO: train_f_mae: 0.034734
train_f_rmse: 0.058835
2025-06-25 07:44:10.971 INFO: train_f_rmse: 0.058835
val_e/atom_mae: 0.002152
2025-06-25 07:44:10.974 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002242
2025-06-25 07:44:10.974 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035393
2025-06-25 07:44:10.975 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059457
2025-06-25 07:44:10.975 INFO: val_f_rmse: 0.059457
##### Step: 256 Learning rate: 2.44140625e-06 #####
2025-06-25 07:44:41.175 INFO: ##### Step: 256 Learning rate: 2.44140625e-06 #####
Epoch 17, Train Loss: 65.9188, Val Loss: 64.1544
2025-06-25 07:44:41.175 INFO: Epoch 17, Train Loss: 65.9188, Val Loss: 64.1544
train_e/atom_mae: 0.002172
2025-06-25 07:44:41.176 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002272
2025-06-25 07:44:41.176 INFO: train_e/atom_rmse: 0.002272
train_f_mae: 0.034732
2025-06-25 07:44:41.180 INFO: train_f_mae: 0.034732
train_f_rmse: 0.058835
2025-06-25 07:44:41.180 INFO: train_f_rmse: 0.058835
val_e/atom_mae: 0.002151
2025-06-25 07:44:41.183 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002238
2025-06-25 07:44:41.183 INFO: val_e/atom_rmse: 0.002238
val_f_mae: 0.035392
2025-06-25 07:44:41.183 INFO: val_f_mae: 0.035392
val_f_rmse: 0.059456
2025-06-25 07:44:41.184 INFO: val_f_rmse: 0.059456
##### Step: 257 Learning rate: 2.44140625e-06 #####
2025-06-25 07:45:11.508 INFO: ##### Step: 257 Learning rate: 2.44140625e-06 #####
Epoch 18, Train Loss: 66.1043, Val Loss: 64.4042
2025-06-25 07:45:11.508 INFO: Epoch 18, Train Loss: 66.1043, Val Loss: 64.4042
train_e/atom_mae: 0.002175
2025-06-25 07:45:11.509 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002275
2025-06-25 07:45:11.509 INFO: train_e/atom_rmse: 0.002275
train_f_mae: 0.034733
2025-06-25 07:45:11.513 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058836
2025-06-25 07:45:11.513 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002154
2025-06-25 07:45:11.515 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002243
2025-06-25 07:45:11.516 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035394
2025-06-25 07:45:11.516 INFO: val_f_mae: 0.035394
val_f_rmse: 0.059459
2025-06-25 07:45:11.516 INFO: val_f_rmse: 0.059459
##### Step: 258 Learning rate: 2.44140625e-06 #####
2025-06-25 07:45:41.647 INFO: ##### Step: 258 Learning rate: 2.44140625e-06 #####
Epoch 19, Train Loss: 65.8824, Val Loss: 64.5646
2025-06-25 07:45:41.647 INFO: Epoch 19, Train Loss: 65.8824, Val Loss: 64.5646
train_e/atom_mae: 0.002171
2025-06-25 07:45:41.648 INFO: train_e/atom_mae: 0.002171
train_e/atom_rmse: 0.002271
2025-06-25 07:45:41.648 INFO: train_e/atom_rmse: 0.002271
train_f_mae: 0.034733
2025-06-25 07:45:41.652 INFO: train_f_mae: 0.034733
train_f_rmse: 0.058836
2025-06-25 07:45:41.652 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002154
2025-06-25 07:45:41.655 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002246
2025-06-25 07:45:41.655 INFO: val_e/atom_rmse: 0.002246
val_f_mae: 0.035395
2025-06-25 07:45:41.656 INFO: val_f_mae: 0.035395
val_f_rmse: 0.059460
2025-06-25 07:45:41.656 INFO: val_f_rmse: 0.059460
##### Step: 259 Learning rate: 2.44140625e-06 #####
2025-06-25 07:46:11.915 INFO: ##### Step: 259 Learning rate: 2.44140625e-06 #####
Epoch 20, Train Loss: 65.9195, Val Loss: 64.1733
2025-06-25 07:46:11.915 INFO: Epoch 20, Train Loss: 65.9195, Val Loss: 64.1733
train_e/atom_mae: 0.002174
2025-06-25 07:46:11.916 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002272
2025-06-25 07:46:11.916 INFO: train_e/atom_rmse: 0.002272
train_f_mae: 0.034732
2025-06-25 07:46:11.919 INFO: train_f_mae: 0.034732
train_f_rmse: 0.058836
2025-06-25 07:46:11.920 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002151
2025-06-25 07:46:11.922 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002239
2025-06-25 07:46:11.923 INFO: val_e/atom_rmse: 0.002239
val_f_mae: 0.035391
2025-06-25 07:46:11.923 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059456
2025-06-25 07:46:11.923 INFO: val_f_rmse: 0.059456
##### Step: 260 Learning rate: 1.220703125e-06 #####
2025-06-25 07:46:42.177 INFO: ##### Step: 260 Learning rate: 1.220703125e-06 #####
Epoch 21, Train Loss: 65.7862, Val Loss: 64.3382
2025-06-25 07:46:42.177 INFO: Epoch 21, Train Loss: 65.7862, Val Loss: 64.3382
train_e/atom_mae: 0.002170
2025-06-25 07:46:42.178 INFO: train_e/atom_mae: 0.002170
train_e/atom_rmse: 0.002270
2025-06-25 07:46:42.179 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034732
2025-06-25 07:46:42.182 INFO: train_f_mae: 0.034732
train_f_rmse: 0.058836
2025-06-25 07:46:42.182 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002153
2025-06-25 07:46:42.185 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 07:46:42.185 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035393
2025-06-25 07:46:42.185 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059459
2025-06-25 07:46:42.186 INFO: val_f_rmse: 0.059459
##### Step: 261 Learning rate: 1.220703125e-06 #####
2025-06-25 07:47:12.300 INFO: ##### Step: 261 Learning rate: 1.220703125e-06 #####
Epoch 22, Train Loss: 65.7837, Val Loss: 64.5908
2025-06-25 07:47:12.300 INFO: Epoch 22, Train Loss: 65.7837, Val Loss: 64.5908
train_e/atom_mae: 0.002173
2025-06-25 07:47:12.301 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 07:47:12.301 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034732
2025-06-25 07:47:12.305 INFO: train_f_mae: 0.034732
train_f_rmse: 0.058836
2025-06-25 07:47:12.305 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002154
2025-06-25 07:47:12.308 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002246
2025-06-25 07:47:12.308 INFO: val_e/atom_rmse: 0.002246
val_f_mae: 0.035394
2025-06-25 07:47:12.308 INFO: val_f_mae: 0.035394
val_f_rmse: 0.059460
2025-06-25 07:47:12.309 INFO: val_f_rmse: 0.059460
##### Step: 262 Learning rate: 1.220703125e-06 #####
2025-06-25 07:47:42.598 INFO: ##### Step: 262 Learning rate: 1.220703125e-06 #####
Epoch 23, Train Loss: 65.8438, Val Loss: 64.4317
2025-06-25 07:47:42.598 INFO: Epoch 23, Train Loss: 65.8438, Val Loss: 64.4317
train_e/atom_mae: 0.002172
2025-06-25 07:47:42.599 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002271
2025-06-25 07:47:42.599 INFO: train_e/atom_rmse: 0.002271
train_f_mae: 0.034732
2025-06-25 07:47:42.602 INFO: train_f_mae: 0.034732
train_f_rmse: 0.058836
2025-06-25 07:47:42.603 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002154
2025-06-25 07:47:42.605 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002243
2025-06-25 07:47:42.606 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035393
2025-06-25 07:47:42.606 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059459
2025-06-25 07:47:42.606 INFO: val_f_rmse: 0.059459
##### Step: 263 Learning rate: 1.220703125e-06 #####
2025-06-25 07:48:12.708 INFO: ##### Step: 263 Learning rate: 1.220703125e-06 #####
Epoch 24, Train Loss: 65.8260, Val Loss: 64.3382
2025-06-25 07:48:12.708 INFO: Epoch 24, Train Loss: 65.8260, Val Loss: 64.3382
train_e/atom_mae: 0.002174
2025-06-25 07:48:12.709 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002270
2025-06-25 07:48:12.709 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034732
2025-06-25 07:48:12.713 INFO: train_f_mae: 0.034732
train_f_rmse: 0.058836
2025-06-25 07:48:12.713 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002152
2025-06-25 07:48:12.716 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002242
2025-06-25 07:48:12.716 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035392
2025-06-25 07:48:12.717 INFO: val_f_mae: 0.035392
val_f_rmse: 0.059457
2025-06-25 07:48:12.717 INFO: val_f_rmse: 0.059457
##### Step: 264 Learning rate: 1.220703125e-06 #####
2025-06-25 07:48:42.867 INFO: ##### Step: 264 Learning rate: 1.220703125e-06 #####
Epoch 25, Train Loss: 65.8080, Val Loss: 64.2819
2025-06-25 07:48:42.867 INFO: Epoch 25, Train Loss: 65.8080, Val Loss: 64.2819
train_e/atom_mae: 0.002174
2025-06-25 07:48:42.868 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002270
2025-06-25 07:48:42.868 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034732
2025-06-25 07:48:42.872 INFO: train_f_mae: 0.034732
train_f_rmse: 0.058836
2025-06-25 07:48:42.872 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002152
2025-06-25 07:48:42.874 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 07:48:42.875 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035393
2025-06-25 07:48:42.875 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059459
2025-06-25 07:48:42.875 INFO: val_f_rmse: 0.059459
##### Step: 265 Learning rate: 1.220703125e-06 #####
2025-06-25 07:49:13.165 INFO: ##### Step: 265 Learning rate: 1.220703125e-06 #####
Epoch 26, Train Loss: 65.7925, Val Loss: 64.2047
2025-06-25 07:49:13.165 INFO: Epoch 26, Train Loss: 65.7925, Val Loss: 64.2047
train_e/atom_mae: 0.002174
2025-06-25 07:49:13.166 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002270
2025-06-25 07:49:13.166 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034732
2025-06-25 07:49:13.170 INFO: train_f_mae: 0.034732
train_f_rmse: 0.058836
2025-06-25 07:49:13.170 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002151
2025-06-25 07:49:13.172 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002239
2025-06-25 07:49:13.173 INFO: val_e/atom_rmse: 0.002239
val_f_mae: 0.035391
2025-06-25 07:49:13.173 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059456
2025-06-25 07:49:13.173 INFO: val_f_rmse: 0.059456
##### Step: 266 Learning rate: 1.220703125e-06 #####
2025-06-25 07:49:43.260 INFO: ##### Step: 266 Learning rate: 1.220703125e-06 #####
Epoch 27, Train Loss: 65.8147, Val Loss: 64.3004
2025-06-25 07:49:43.261 INFO: Epoch 27, Train Loss: 65.8147, Val Loss: 64.3004
train_e/atom_mae: 0.002174
2025-06-25 07:49:43.262 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002270
2025-06-25 07:49:43.262 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034732
2025-06-25 07:49:43.265 INFO: train_f_mae: 0.034732
train_f_rmse: 0.058836
2025-06-25 07:49:43.265 INFO: train_f_rmse: 0.058836
val_e/atom_mae: 0.002152
2025-06-25 07:49:43.268 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 07:49:43.268 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035392
2025-06-25 07:49:43.269 INFO: val_f_mae: 0.035392
val_f_rmse: 0.059457
2025-06-25 07:49:43.269 INFO: val_f_rmse: 0.059457
##### Step: 267 Learning rate: 1.220703125e-06 #####
2025-06-25 07:50:13.545 INFO: ##### Step: 267 Learning rate: 1.220703125e-06 #####
Epoch 28, Train Loss: 65.7713, Val Loss: 64.4470
2025-06-25 07:50:13.546 INFO: Epoch 28, Train Loss: 65.7713, Val Loss: 64.4470
train_e/atom_mae: 0.002172
2025-06-25 07:50:13.547 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002269
2025-06-25 07:50:13.547 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034731
2025-06-25 07:50:13.550 INFO: train_f_mae: 0.034731
train_f_rmse: 0.058837
2025-06-25 07:50:13.550 INFO: train_f_rmse: 0.058837
val_e/atom_mae: 0.002153
2025-06-25 07:50:13.553 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002244
2025-06-25 07:50:13.553 INFO: val_e/atom_rmse: 0.002244
val_f_mae: 0.035391
2025-06-25 07:50:13.554 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059456
2025-06-25 07:50:13.554 INFO: val_f_rmse: 0.059456
##### Step: 268 Learning rate: 1.220703125e-06 #####
2025-06-25 07:50:43.717 INFO: ##### Step: 268 Learning rate: 1.220703125e-06 #####
Epoch 29, Train Loss: 65.8113, Val Loss: 64.2337
2025-06-25 07:50:43.717 INFO: Epoch 29, Train Loss: 65.8113, Val Loss: 64.2337
train_e/atom_mae: 0.002172
2025-06-25 07:50:43.718 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002270
2025-06-25 07:50:43.718 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034731
2025-06-25 07:50:43.722 INFO: train_f_mae: 0.034731
train_f_rmse: 0.058837
2025-06-25 07:50:43.722 INFO: train_f_rmse: 0.058837
val_e/atom_mae: 0.002152
2025-06-25 07:50:43.725 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 07:50:43.725 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035391
2025-06-25 07:50:43.725 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059456
2025-06-25 07:50:43.726 INFO: val_f_rmse: 0.059456
##### Step: 269 Learning rate: 1.220703125e-06 #####
2025-06-25 07:51:13.917 INFO: ##### Step: 269 Learning rate: 1.220703125e-06 #####
Epoch 30, Train Loss: 65.8454, Val Loss: 64.2446
2025-06-25 07:51:13.917 INFO: Epoch 30, Train Loss: 65.8454, Val Loss: 64.2446
train_e/atom_mae: 0.002174
2025-06-25 07:51:13.918 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002271
2025-06-25 07:51:13.918 INFO: train_e/atom_rmse: 0.002271
train_f_mae: 0.034730
2025-06-25 07:51:13.922 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058837
2025-06-25 07:51:13.922 INFO: train_f_rmse: 0.058837
val_e/atom_mae: 0.002151
2025-06-25 07:51:13.925 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002240
2025-06-25 07:51:13.925 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035390
2025-06-25 07:51:13.925 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059455
2025-06-25 07:51:13.926 INFO: val_f_rmse: 0.059455
##### Step: 270 Learning rate: 1.220703125e-06 #####
2025-06-25 07:51:44.143 INFO: ##### Step: 270 Learning rate: 1.220703125e-06 #####
Epoch 31, Train Loss: 65.7732, Val Loss: 64.4717
2025-06-25 07:51:44.143 INFO: Epoch 31, Train Loss: 65.7732, Val Loss: 64.4717
train_e/atom_mae: 0.002173
2025-06-25 07:51:44.144 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 07:51:44.144 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034730
2025-06-25 07:51:44.148 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058837
2025-06-25 07:51:44.148 INFO: train_f_rmse: 0.058837
val_e/atom_mae: 0.002154
2025-06-25 07:51:44.151 INFO: val_e/atom_mae: 0.002154
val_e/atom_rmse: 0.002244
2025-06-25 07:51:44.151 INFO: val_e/atom_rmse: 0.002244
val_f_mae: 0.035392
2025-06-25 07:51:44.152 INFO: val_f_mae: 0.035392
val_f_rmse: 0.059459
2025-06-25 07:51:44.152 INFO: val_f_rmse: 0.059459
##### Step: 271 Learning rate: 1.220703125e-06 #####
2025-06-25 07:52:14.161 INFO: ##### Step: 271 Learning rate: 1.220703125e-06 #####
Epoch 32, Train Loss: 65.8793, Val Loss: 64.7288
2025-06-25 07:52:14.161 INFO: Epoch 32, Train Loss: 65.8793, Val Loss: 64.7288
train_e/atom_mae: 0.002172
2025-06-25 07:52:14.162 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002271
2025-06-25 07:52:14.162 INFO: train_e/atom_rmse: 0.002271
train_f_mae: 0.034732
2025-06-25 07:52:14.166 INFO: train_f_mae: 0.034732
train_f_rmse: 0.058838
2025-06-25 07:52:14.166 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002155
2025-06-25 07:52:14.168 INFO: val_e/atom_mae: 0.002155
val_e/atom_rmse: 0.002249
2025-06-25 07:52:14.169 INFO: val_e/atom_rmse: 0.002249
val_f_mae: 0.035393
2025-06-25 07:52:14.169 INFO: val_f_mae: 0.035393
val_f_rmse: 0.059461
2025-06-25 07:52:14.169 INFO: val_f_rmse: 0.059461
##### Step: 272 Learning rate: 1.220703125e-06 #####
2025-06-25 07:52:44.328 INFO: ##### Step: 272 Learning rate: 1.220703125e-06 #####
Epoch 33, Train Loss: 65.8146, Val Loss: 64.3093
2025-06-25 07:52:44.328 INFO: Epoch 33, Train Loss: 65.8146, Val Loss: 64.3093
train_e/atom_mae: 0.002174
2025-06-25 07:52:44.329 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002270
2025-06-25 07:52:44.330 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034731
2025-06-25 07:52:44.333 INFO: train_f_mae: 0.034731
train_f_rmse: 0.058838
2025-06-25 07:52:44.333 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002153
2025-06-25 07:52:44.336 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002241
2025-06-25 07:52:44.336 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035391
2025-06-25 07:52:44.337 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059457
2025-06-25 07:52:44.337 INFO: val_f_rmse: 0.059457
##### Step: 273 Learning rate: 1.220703125e-06 #####
2025-06-25 07:53:14.337 INFO: ##### Step: 273 Learning rate: 1.220703125e-06 #####
Epoch 34, Train Loss: 65.7836, Val Loss: 64.3000
2025-06-25 07:53:14.337 INFO: Epoch 34, Train Loss: 65.7836, Val Loss: 64.3000
train_e/atom_mae: 0.002173
2025-06-25 07:53:14.338 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 07:53:14.338 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034731
2025-06-25 07:53:14.341 INFO: train_f_mae: 0.034731
train_f_rmse: 0.058838
2025-06-25 07:53:14.342 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002152
2025-06-25 07:53:14.344 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 07:53:14.345 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035391
2025-06-25 07:53:14.345 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059457
2025-06-25 07:53:14.345 INFO: val_f_rmse: 0.059457
##### Step: 274 Learning rate: 1.220703125e-06 #####
2025-06-25 07:53:44.430 INFO: ##### Step: 274 Learning rate: 1.220703125e-06 #####
Epoch 35, Train Loss: 65.8636, Val Loss: 64.6236
2025-06-25 07:53:44.430 INFO: Epoch 35, Train Loss: 65.8636, Val Loss: 64.6236
train_e/atom_mae: 0.002173
2025-06-25 07:53:44.431 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002271
2025-06-25 07:53:44.431 INFO: train_e/atom_rmse: 0.002271
train_f_mae: 0.034731
2025-06-25 07:53:44.434 INFO: train_f_mae: 0.034731
train_f_rmse: 0.058838
2025-06-25 07:53:44.435 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002152
2025-06-25 07:53:44.437 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002247
2025-06-25 07:53:44.438 INFO: val_e/atom_rmse: 0.002247
val_f_mae: 0.035391
2025-06-25 07:53:44.438 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059456
2025-06-25 07:53:44.438 INFO: val_f_rmse: 0.059456
##### Step: 275 Learning rate: 1.220703125e-06 #####
2025-06-25 07:54:14.551 INFO: ##### Step: 275 Learning rate: 1.220703125e-06 #####
Epoch 36, Train Loss: 65.7812, Val Loss: 64.2241
2025-06-25 07:54:14.551 INFO: Epoch 36, Train Loss: 65.7812, Val Loss: 64.2241
train_e/atom_mae: 0.002173
2025-06-25 07:54:14.552 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 07:54:14.552 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034730
2025-06-25 07:54:14.555 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058838
2025-06-25 07:54:14.556 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002151
2025-06-25 07:54:14.558 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002240
2025-06-25 07:54:14.559 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035390
2025-06-25 07:54:14.559 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059455
2025-06-25 07:54:14.559 INFO: val_f_rmse: 0.059455
##### Step: 276 Learning rate: 1.220703125e-06 #####
2025-06-25 07:54:44.538 INFO: ##### Step: 276 Learning rate: 1.220703125e-06 #####
Epoch 37, Train Loss: 65.8627, Val Loss: 64.4164
2025-06-25 07:54:44.539 INFO: Epoch 37, Train Loss: 65.8627, Val Loss: 64.4164
train_e/atom_mae: 0.002174
2025-06-25 07:54:44.540 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002271
2025-06-25 07:54:44.540 INFO: train_e/atom_rmse: 0.002271
train_f_mae: 0.034730
2025-06-25 07:54:44.543 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058838
2025-06-25 07:54:44.543 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002153
2025-06-25 07:54:44.546 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002243
2025-06-25 07:54:44.546 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035392
2025-06-25 07:54:44.547 INFO: val_f_mae: 0.035392
val_f_rmse: 0.059459
2025-06-25 07:54:44.547 INFO: val_f_rmse: 0.059459
##### Step: 277 Learning rate: 1.220703125e-06 #####
2025-06-25 07:55:14.666 INFO: ##### Step: 277 Learning rate: 1.220703125e-06 #####
Epoch 38, Train Loss: 65.7803, Val Loss: 64.2558
2025-06-25 07:55:14.666 INFO: Epoch 38, Train Loss: 65.7803, Val Loss: 64.2558
train_e/atom_mae: 0.002172
2025-06-25 07:55:14.667 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002269
2025-06-25 07:55:14.667 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034730
2025-06-25 07:55:14.670 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058838
2025-06-25 07:55:14.671 INFO: train_f_rmse: 0.058838
val_e/atom_mae: 0.002151
2025-06-25 07:55:14.673 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002240
2025-06-25 07:55:14.673 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035390
2025-06-25 07:55:14.674 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059456
2025-06-25 07:55:14.674 INFO: val_f_rmse: 0.059456
##### Step: 278 Learning rate: 1.220703125e-06 #####
2025-06-25 07:55:44.593 INFO: ##### Step: 278 Learning rate: 1.220703125e-06 #####
Epoch 39, Train Loss: 65.7628, Val Loss: 64.2441
2025-06-25 07:55:44.594 INFO: Epoch 39, Train Loss: 65.7628, Val Loss: 64.2441
train_e/atom_mae: 0.002173
2025-06-25 07:55:44.594 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 07:55:44.595 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034730
2025-06-25 07:55:44.598 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058839
2025-06-25 07:55:44.598 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002152
2025-06-25 07:55:44.601 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 07:55:44.601 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035390
2025-06-25 07:55:44.602 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059456
2025-06-25 07:55:44.602 INFO: val_f_rmse: 0.059456
##### Step: 279 Learning rate: 1.220703125e-06 #####
2025-06-25 07:56:14.645 INFO: ##### Step: 279 Learning rate: 1.220703125e-06 #####
Epoch 40, Train Loss: 65.8424, Val Loss: 64.3991
2025-06-25 07:56:14.645 INFO: Epoch 40, Train Loss: 65.8424, Val Loss: 64.3991
train_e/atom_mae: 0.002175
2025-06-25 07:56:14.646 INFO: train_e/atom_mae: 0.002175
train_e/atom_rmse: 0.002271
2025-06-25 07:56:14.646 INFO: train_e/atom_rmse: 0.002271
train_f_mae: 0.034729
2025-06-25 07:56:14.650 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058839
2025-06-25 07:56:14.650 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002153
2025-06-25 07:56:14.652 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002243
2025-06-25 07:56:14.653 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035391
2025-06-25 07:56:14.653 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059459
2025-06-25 07:56:14.653 INFO: val_f_rmse: 0.059459
##### Step: 280 Learning rate: 6.103515625e-07 #####
2025-06-25 07:56:44.811 INFO: ##### Step: 280 Learning rate: 6.103515625e-07 #####
Epoch 41, Train Loss: 65.8090, Val Loss: 64.4017
2025-06-25 07:56:44.811 INFO: Epoch 41, Train Loss: 65.8090, Val Loss: 64.4017
train_e/atom_mae: 0.002172
2025-06-25 07:56:44.812 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002270
2025-06-25 07:56:44.812 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034730
2025-06-25 07:56:44.815 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058839
2025-06-25 07:56:44.816 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002153
2025-06-25 07:56:44.818 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002243
2025-06-25 07:56:44.819 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035391
2025-06-25 07:56:44.819 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059459
2025-06-25 07:56:44.819 INFO: val_f_rmse: 0.059459
##### Step: 281 Learning rate: 6.103515625e-07 #####
2025-06-25 07:57:14.742 INFO: ##### Step: 281 Learning rate: 6.103515625e-07 #####
Epoch 42, Train Loss: 65.8722, Val Loss: 64.3850
2025-06-25 07:57:14.742 INFO: Epoch 42, Train Loss: 65.8722, Val Loss: 64.3850
train_e/atom_mae: 0.002174
2025-06-25 07:57:14.743 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002271
2025-06-25 07:57:14.743 INFO: train_e/atom_rmse: 0.002271
train_f_mae: 0.034730
2025-06-25 07:57:14.747 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058839
2025-06-25 07:57:14.747 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002153
2025-06-25 07:57:14.749 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002243
2025-06-25 07:57:14.750 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035391
2025-06-25 07:57:14.750 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059459
2025-06-25 07:57:14.750 INFO: val_f_rmse: 0.059459
##### Step: 282 Learning rate: 6.103515625e-07 #####
2025-06-25 07:57:44.824 INFO: ##### Step: 282 Learning rate: 6.103515625e-07 #####
Epoch 43, Train Loss: 65.8117, Val Loss: 64.4115
2025-06-25 07:57:44.825 INFO: Epoch 43, Train Loss: 65.8117, Val Loss: 64.4115
train_e/atom_mae: 0.002173
2025-06-25 07:57:44.826 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002270
2025-06-25 07:57:44.826 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034730
2025-06-25 07:57:44.829 INFO: train_f_mae: 0.034730
train_f_rmse: 0.058839
2025-06-25 07:57:44.829 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002153
2025-06-25 07:57:44.832 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002243
2025-06-25 07:57:44.832 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035391
2025-06-25 07:57:44.833 INFO: val_f_mae: 0.035391
val_f_rmse: 0.059459
2025-06-25 07:57:44.833 INFO: val_f_rmse: 0.059459
##### Step: 283 Learning rate: 6.103515625e-07 #####
2025-06-25 07:58:14.708 INFO: ##### Step: 283 Learning rate: 6.103515625e-07 #####
Epoch 44, Train Loss: 65.7330, Val Loss: 64.3070
2025-06-25 07:58:14.709 INFO: Epoch 44, Train Loss: 65.7330, Val Loss: 64.3070
train_e/atom_mae: 0.002172
2025-06-25 07:58:14.710 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002269
2025-06-25 07:58:14.710 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 07:58:14.713 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058839
2025-06-25 07:58:14.713 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002152
2025-06-25 07:58:14.716 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 07:58:14.716 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035389
2025-06-25 07:58:14.717 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059457
2025-06-25 07:58:14.717 INFO: val_f_rmse: 0.059457
##### Step: 284 Learning rate: 6.103515625e-07 #####
2025-06-25 07:58:44.727 INFO: ##### Step: 284 Learning rate: 6.103515625e-07 #####
Epoch 45, Train Loss: 65.7402, Val Loss: 64.2379
2025-06-25 07:58:44.727 INFO: Epoch 45, Train Loss: 65.7402, Val Loss: 64.2379
train_e/atom_mae: 0.002173
2025-06-25 07:58:44.728 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 07:58:44.728 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 07:58:44.732 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058839
2025-06-25 07:58:44.732 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002152
2025-06-25 07:58:44.735 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 07:58:44.735 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 07:58:44.735 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059457
2025-06-25 07:58:44.736 INFO: val_f_rmse: 0.059457
##### Step: 285 Learning rate: 6.103515625e-07 #####
2025-06-25 07:59:14.674 INFO: ##### Step: 285 Learning rate: 6.103515625e-07 #####
Epoch 46, Train Loss: 65.7283, Val Loss: 64.2753
2025-06-25 07:59:14.675 INFO: Epoch 46, Train Loss: 65.7283, Val Loss: 64.2753
train_e/atom_mae: 0.002173
2025-06-25 07:59:14.676 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 07:59:14.676 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 07:59:14.679 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058839
2025-06-25 07:59:14.679 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002152
2025-06-25 07:59:14.682 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 07:59:14.682 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 07:59:14.683 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059457
2025-06-25 07:59:14.683 INFO: val_f_rmse: 0.059457
##### Step: 286 Learning rate: 6.103515625e-07 #####
2025-06-25 07:59:44.555 INFO: ##### Step: 286 Learning rate: 6.103515625e-07 #####
Epoch 47, Train Loss: 65.7790, Val Loss: 64.2738
2025-06-25 07:59:44.555 INFO: Epoch 47, Train Loss: 65.7790, Val Loss: 64.2738
train_e/atom_mae: 0.002174
2025-06-25 07:59:44.556 INFO: train_e/atom_mae: 0.002174
train_e/atom_rmse: 0.002269
2025-06-25 07:59:44.556 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 07:59:44.560 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058839
2025-06-25 07:59:44.560 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002152
2025-06-25 07:59:44.562 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 07:59:44.563 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035390
2025-06-25 07:59:44.563 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 07:59:44.563 INFO: val_f_rmse: 0.059458
##### Step: 287 Learning rate: 6.103515625e-07 #####
2025-06-25 08:00:14.586 INFO: ##### Step: 287 Learning rate: 6.103515625e-07 #####
Epoch 48, Train Loss: 65.7224, Val Loss: 64.3427
2025-06-25 08:00:14.587 INFO: Epoch 48, Train Loss: 65.7224, Val Loss: 64.3427
train_e/atom_mae: 0.002173
2025-06-25 08:00:14.587 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:00:14.588 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:00:14.591 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058839
2025-06-25 08:00:14.591 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002153
2025-06-25 08:00:14.594 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:00:14.594 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:00:14.595 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:00:14.595 INFO: val_f_rmse: 0.059458
##### Step: 288 Learning rate: 6.103515625e-07 #####
2025-06-25 08:00:44.471 INFO: ##### Step: 288 Learning rate: 6.103515625e-07 #####
Epoch 49, Train Loss: 65.7591, Val Loss: 64.3751
2025-06-25 08:00:44.471 INFO: Epoch 49, Train Loss: 65.7591, Val Loss: 64.3751
train_e/atom_mae: 0.002172
2025-06-25 08:00:44.472 INFO: train_e/atom_mae: 0.002172
train_e/atom_rmse: 0.002269
2025-06-25 08:00:44.472 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:00:44.476 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:00:44.476 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:00:44.478 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:00:44.479 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:00:44.479 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:00:44.479 INFO: val_f_rmse: 0.059459
##### Step: 289 Learning rate: 6.103515625e-07 #####
2025-06-25 08:01:14.481 INFO: ##### Step: 289 Learning rate: 6.103515625e-07 #####
Epoch 50, Train Loss: 65.7667, Val Loss: 64.3579
2025-06-25 08:01:14.481 INFO: Epoch 50, Train Loss: 65.7667, Val Loss: 64.3579
train_e/atom_mae: 0.002173
2025-06-25 08:01:14.482 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:01:14.482 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:01:14.486 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058839
2025-06-25 08:01:14.486 INFO: train_f_rmse: 0.058839
val_e/atom_mae: 0.002153
2025-06-25 08:01:14.489 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:01:14.489 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:01:14.489 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:01:14.489 INFO: val_f_rmse: 0.059459
##### Step: 290 Learning rate: 6.103515625e-07 #####
2025-06-25 08:01:44.408 INFO: ##### Step: 290 Learning rate: 6.103515625e-07 #####
Epoch 51, Train Loss: 65.7363, Val Loss: 64.2881
2025-06-25 08:01:44.409 INFO: Epoch 51, Train Loss: 65.7363, Val Loss: 64.2881
train_e/atom_mae: 0.002173
2025-06-25 08:01:44.409 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:01:44.410 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:01:44.413 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:01:44.413 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:01:44.416 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:01:44.416 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:01:44.417 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:01:44.417 INFO: val_f_rmse: 0.059458
##### Step: 291 Learning rate: 6.103515625e-07 #####
2025-06-25 08:02:14.312 INFO: ##### Step: 291 Learning rate: 6.103515625e-07 #####
Epoch 52, Train Loss: 65.7506, Val Loss: 64.2413
2025-06-25 08:02:14.312 INFO: Epoch 52, Train Loss: 65.7506, Val Loss: 64.2413
train_e/atom_mae: 0.002173
2025-06-25 08:02:14.313 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:02:14.313 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:02:14.316 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:02:14.317 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:02:14.319 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:02:14.320 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035390
2025-06-25 08:02:14.320 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:02:14.320 INFO: val_f_rmse: 0.059458
##### Step: 292 Learning rate: 6.103515625e-07 #####
2025-06-25 08:02:44.285 INFO: ##### Step: 292 Learning rate: 6.103515625e-07 #####
Epoch 53, Train Loss: 65.7167, Val Loss: 64.1731
2025-06-25 08:02:44.285 INFO: Epoch 53, Train Loss: 65.7167, Val Loss: 64.1731
train_e/atom_mae: 0.002173
2025-06-25 08:02:44.286 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:02:44.286 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:02:44.290 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:02:44.290 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002151
2025-06-25 08:02:44.292 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002239
2025-06-25 08:02:44.293 INFO: val_e/atom_rmse: 0.002239
val_f_mae: 0.035389
2025-06-25 08:02:44.293 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059457
2025-06-25 08:02:44.293 INFO: val_f_rmse: 0.059457
##### Step: 293 Learning rate: 6.103515625e-07 #####
2025-06-25 08:03:14.133 INFO: ##### Step: 293 Learning rate: 6.103515625e-07 #####
Epoch 54, Train Loss: 65.7222, Val Loss: 64.2465
2025-06-25 08:03:14.133 INFO: Epoch 54, Train Loss: 65.7222, Val Loss: 64.2465
train_e/atom_mae: 0.002173
2025-06-25 08:03:14.134 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:03:14.134 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:03:14.137 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:03:14.137 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002151
2025-06-25 08:03:14.140 INFO: val_e/atom_mae: 0.002151
val_e/atom_rmse: 0.002240
2025-06-25 08:03:14.140 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:03:14.141 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059457
2025-06-25 08:03:14.141 INFO: val_f_rmse: 0.059457
##### Step: 294 Learning rate: 6.103515625e-07 #####
2025-06-25 08:03:44.076 INFO: ##### Step: 294 Learning rate: 6.103515625e-07 #####
Epoch 55, Train Loss: 65.7514, Val Loss: 64.3055
2025-06-25 08:03:44.077 INFO: Epoch 55, Train Loss: 65.7514, Val Loss: 64.3055
train_e/atom_mae: 0.002173
2025-06-25 08:03:44.078 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:03:44.078 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:03:44.081 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:03:44.081 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:03:44.084 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:03:44.084 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:03:44.085 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:03:44.085 INFO: val_f_rmse: 0.059458
##### Step: 295 Learning rate: 6.103515625e-07 #####
2025-06-25 08:04:13.928 INFO: ##### Step: 295 Learning rate: 6.103515625e-07 #####
Epoch 56, Train Loss: 65.7850, Val Loss: 64.3442
2025-06-25 08:04:13.929 INFO: Epoch 56, Train Loss: 65.7850, Val Loss: 64.3442
train_e/atom_mae: 0.002173
2025-06-25 08:04:13.930 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002270
2025-06-25 08:04:13.930 INFO: train_e/atom_rmse: 0.002270
train_f_mae: 0.034729
2025-06-25 08:04:13.933 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:04:13.933 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:04:13.936 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:04:13.936 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:04:13.937 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:04:13.937 INFO: val_f_rmse: 0.059459
##### Step: 296 Learning rate: 6.103515625e-07 #####
2025-06-25 08:04:43.828 INFO: ##### Step: 296 Learning rate: 6.103515625e-07 #####
Epoch 57, Train Loss: 65.7786, Val Loss: 64.3386
2025-06-25 08:04:43.829 INFO: Epoch 57, Train Loss: 65.7786, Val Loss: 64.3386
train_e/atom_mae: 0.002173
2025-06-25 08:04:43.830 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:04:43.830 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:04:43.833 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:04:43.833 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:04:43.836 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:04:43.836 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:04:43.837 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:04:43.837 INFO: val_f_rmse: 0.059459
##### Step: 297 Learning rate: 6.103515625e-07 #####
2025-06-25 08:05:13.706 INFO: ##### Step: 297 Learning rate: 6.103515625e-07 #####
Epoch 58, Train Loss: 65.7696, Val Loss: 64.3682
2025-06-25 08:05:13.706 INFO: Epoch 58, Train Loss: 65.7696, Val Loss: 64.3682
train_e/atom_mae: 0.002173
2025-06-25 08:05:13.707 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:05:13.707 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:05:13.711 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:05:13.711 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:05:13.713 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:05:13.714 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:05:13.714 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:05:13.714 INFO: val_f_rmse: 0.059459
##### Step: 298 Learning rate: 6.103515625e-07 #####
2025-06-25 08:05:43.535 INFO: ##### Step: 298 Learning rate: 6.103515625e-07 #####
Epoch 59, Train Loss: 65.7754, Val Loss: 64.4017
2025-06-25 08:05:43.535 INFO: Epoch 59, Train Loss: 65.7754, Val Loss: 64.4017
train_e/atom_mae: 0.002173
2025-06-25 08:05:43.536 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:05:43.536 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:05:43.540 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:05:43.540 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:05:43.542 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002243
2025-06-25 08:05:43.543 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035390
2025-06-25 08:05:43.543 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059460
2025-06-25 08:05:43.543 INFO: val_f_rmse: 0.059460
##### Step: 299 Learning rate: 6.103515625e-07 #####
2025-06-25 08:06:13.448 INFO: ##### Step: 299 Learning rate: 6.103515625e-07 #####
Epoch 60, Train Loss: 65.7729, Val Loss: 64.3900
2025-06-25 08:06:13.448 INFO: Epoch 60, Train Loss: 65.7729, Val Loss: 64.3900
train_e/atom_mae: 0.002173
2025-06-25 08:06:13.449 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:06:13.449 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:06:13.453 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:06:13.453 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:06:13.455 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002243
2025-06-25 08:06:13.456 INFO: val_e/atom_rmse: 0.002243
val_f_mae: 0.035390
2025-06-25 08:06:13.456 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059460
2025-06-25 08:06:13.456 INFO: val_f_rmse: 0.059460
##### Step: 300 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:06:43.296 INFO: ##### Step: 300 Learning rate: 3.0517578125e-07 #####
Epoch 61, Train Loss: 65.7684, Val Loss: 64.3607
2025-06-25 08:06:43.297 INFO: Epoch 61, Train Loss: 65.7684, Val Loss: 64.3607
train_e/atom_mae: 0.002173
2025-06-25 08:06:43.298 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:06:43.298 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:06:43.301 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:06:43.301 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:06:43.304 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:06:43.304 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:06:43.305 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059460
2025-06-25 08:06:43.305 INFO: val_f_rmse: 0.059460
##### Step: 301 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:07:13.146 INFO: ##### Step: 301 Learning rate: 3.0517578125e-07 #####
Epoch 62, Train Loss: 65.7649, Val Loss: 64.3821
2025-06-25 08:07:13.147 INFO: Epoch 62, Train Loss: 65.7649, Val Loss: 64.3821
train_e/atom_mae: 0.002173
2025-06-25 08:07:13.148 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:07:13.148 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:07:13.151 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:07:13.151 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:07:13.154 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:07:13.154 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:07:13.155 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059460
2025-06-25 08:07:13.155 INFO: val_f_rmse: 0.059460
##### Step: 302 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:07:42.980 INFO: ##### Step: 302 Learning rate: 3.0517578125e-07 #####
Epoch 63, Train Loss: 65.7648, Val Loss: 64.3600
2025-06-25 08:07:42.980 INFO: Epoch 63, Train Loss: 65.7648, Val Loss: 64.3600
train_e/atom_mae: 0.002173
2025-06-25 08:07:42.981 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:07:42.981 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:07:42.985 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:07:42.985 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:07:42.987 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:07:42.988 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:07:42.988 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:07:42.988 INFO: val_f_rmse: 0.059459
##### Step: 303 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:08:12.776 INFO: ##### Step: 303 Learning rate: 3.0517578125e-07 #####
Epoch 64, Train Loss: 65.7393, Val Loss: 64.3235
2025-06-25 08:08:12.776 INFO: Epoch 64, Train Loss: 65.7393, Val Loss: 64.3235
train_e/atom_mae: 0.002173
2025-06-25 08:08:12.777 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:08:12.777 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:08:12.781 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:08:12.781 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:08:12.783 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002241
2025-06-25 08:08:12.784 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:08:12.784 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:08:12.784 INFO: val_f_rmse: 0.059459
##### Step: 304 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:08:42.628 INFO: ##### Step: 304 Learning rate: 3.0517578125e-07 #####
Epoch 65, Train Loss: 65.7477, Val Loss: 64.3587
2025-06-25 08:08:42.628 INFO: Epoch 65, Train Loss: 65.7477, Val Loss: 64.3587
train_e/atom_mae: 0.002173
2025-06-25 08:08:42.629 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:08:42.629 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:08:42.633 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:08:42.633 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:08:42.636 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:08:42.636 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:08:42.636 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:08:42.637 INFO: val_f_rmse: 0.059459
##### Step: 305 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:09:12.450 INFO: ##### Step: 305 Learning rate: 3.0517578125e-07 #####
Epoch 66, Train Loss: 65.7618, Val Loss: 64.2954
2025-06-25 08:09:12.450 INFO: Epoch 66, Train Loss: 65.7618, Val Loss: 64.2954
train_e/atom_mae: 0.002173
2025-06-25 08:09:12.451 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:09:12.451 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:09:12.455 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:09:12.455 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:09:12.457 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:09:12.458 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:09:12.458 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:09:12.458 INFO: val_f_rmse: 0.059459
##### Step: 306 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:09:42.301 INFO: ##### Step: 306 Learning rate: 3.0517578125e-07 #####
Epoch 67, Train Loss: 65.7343, Val Loss: 64.2811
2025-06-25 08:09:42.302 INFO: Epoch 67, Train Loss: 65.7343, Val Loss: 64.2811
train_e/atom_mae: 0.002173
2025-06-25 08:09:42.303 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:09:42.303 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:09:42.306 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:09:42.306 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:09:42.309 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:09:42.309 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:09:42.310 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:09:42.310 INFO: val_f_rmse: 0.059459
##### Step: 307 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:10:12.134 INFO: ##### Step: 307 Learning rate: 3.0517578125e-07 #####
Epoch 68, Train Loss: 65.7349, Val Loss: 64.3227
2025-06-25 08:10:12.134 INFO: Epoch 68, Train Loss: 65.7349, Val Loss: 64.3227
train_e/atom_mae: 0.002173
2025-06-25 08:10:12.135 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:10:12.135 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:10:12.139 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:10:12.139 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:10:12.141 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002241
2025-06-25 08:10:12.142 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:10:12.142 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:10:12.142 INFO: val_f_rmse: 0.059459
##### Step: 308 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:10:41.950 INFO: ##### Step: 308 Learning rate: 3.0517578125e-07 #####
Epoch 69, Train Loss: 65.7369, Val Loss: 64.2647
2025-06-25 08:10:41.950 INFO: Epoch 69, Train Loss: 65.7369, Val Loss: 64.2647
train_e/atom_mae: 0.002173
2025-06-25 08:10:41.951 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:10:41.951 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:10:41.954 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:10:41.955 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:10:41.957 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:10:41.957 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035390
2025-06-25 08:10:41.958 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:10:41.958 INFO: val_f_rmse: 0.059458
##### Step: 309 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:11:11.749 INFO: ##### Step: 309 Learning rate: 3.0517578125e-07 #####
Epoch 70, Train Loss: 65.7245, Val Loss: 64.2774
2025-06-25 08:11:11.749 INFO: Epoch 70, Train Loss: 65.7245, Val Loss: 64.2774
train_e/atom_mae: 0.002173
2025-06-25 08:11:11.750 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:11:11.750 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:11:11.754 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:11:11.754 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:11:11.757 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:11:11.757 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:11:11.757 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:11:11.758 INFO: val_f_rmse: 0.059459
##### Step: 310 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:11:41.565 INFO: ##### Step: 310 Learning rate: 3.0517578125e-07 #####
Epoch 71, Train Loss: 65.7261, Val Loss: 64.2854
2025-06-25 08:11:41.565 INFO: Epoch 71, Train Loss: 65.7261, Val Loss: 64.2854
train_e/atom_mae: 0.002173
2025-06-25 08:11:41.566 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:11:41.566 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:11:41.570 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:11:41.570 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:11:41.572 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:11:41.573 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:11:41.573 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:11:41.573 INFO: val_f_rmse: 0.059459
##### Step: 311 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:12:11.325 INFO: ##### Step: 311 Learning rate: 3.0517578125e-07 #####
Epoch 72, Train Loss: 65.7274, Val Loss: 64.3349
2025-06-25 08:12:11.325 INFO: Epoch 72, Train Loss: 65.7274, Val Loss: 64.3349
train_e/atom_mae: 0.002173
2025-06-25 08:12:11.326 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:12:11.326 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:12:11.329 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:12:11.330 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:12:11.332 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002242
2025-06-25 08:12:11.332 INFO: val_e/atom_rmse: 0.002242
val_f_mae: 0.035390
2025-06-25 08:12:11.333 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:12:11.333 INFO: val_f_rmse: 0.059459
##### Step: 312 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:12:40.951 INFO: ##### Step: 312 Learning rate: 3.0517578125e-07 #####
Epoch 73, Train Loss: 65.7566, Val Loss: 64.3010
2025-06-25 08:12:40.951 INFO: Epoch 73, Train Loss: 65.7566, Val Loss: 64.3010
train_e/atom_mae: 0.002173
2025-06-25 08:12:40.952 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:12:40.952 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:12:40.955 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:12:40.955 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:12:40.958 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:12:40.958 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:12:40.959 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:12:40.959 INFO: val_f_rmse: 0.059459
##### Step: 313 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:13:10.576 INFO: ##### Step: 313 Learning rate: 3.0517578125e-07 #####
Epoch 74, Train Loss: 65.7442, Val Loss: 64.3172
2025-06-25 08:13:10.576 INFO: Epoch 74, Train Loss: 65.7442, Val Loss: 64.3172
train_e/atom_mae: 0.002173
2025-06-25 08:13:10.577 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:13:10.577 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:13:10.581 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:13:10.581 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:13:10.583 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:13:10.584 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:13:10.584 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:13:10.584 INFO: val_f_rmse: 0.059459
##### Step: 314 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:13:40.208 INFO: ##### Step: 314 Learning rate: 3.0517578125e-07 #####
Epoch 75, Train Loss: 65.7537, Val Loss: 64.3219
2025-06-25 08:13:40.208 INFO: Epoch 75, Train Loss: 65.7537, Val Loss: 64.3219
train_e/atom_mae: 0.002173
2025-06-25 08:13:40.209 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:13:40.209 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:13:40.212 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:13:40.213 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002153
2025-06-25 08:13:40.215 INFO: val_e/atom_mae: 0.002153
val_e/atom_rmse: 0.002241
2025-06-25 08:13:40.215 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:13:40.216 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:13:40.216 INFO: val_f_rmse: 0.059459
##### Step: 315 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:14:09.829 INFO: ##### Step: 315 Learning rate: 3.0517578125e-07 #####
Epoch 76, Train Loss: 65.7353, Val Loss: 64.2861
2025-06-25 08:14:09.829 INFO: Epoch 76, Train Loss: 65.7353, Val Loss: 64.2861
train_e/atom_mae: 0.002173
2025-06-25 08:14:09.830 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:14:09.830 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:14:09.834 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:14:09.834 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:14:09.836 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:14:09.837 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:14:09.837 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:14:09.837 INFO: val_f_rmse: 0.059459
##### Step: 316 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:14:39.434 INFO: ##### Step: 316 Learning rate: 3.0517578125e-07 #####
Epoch 77, Train Loss: 65.7280, Val Loss: 64.2962
2025-06-25 08:14:39.434 INFO: Epoch 77, Train Loss: 65.7280, Val Loss: 64.2962
train_e/atom_mae: 0.002173
2025-06-25 08:14:39.435 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:14:39.435 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:14:39.439 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:14:39.439 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:14:39.441 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:14:39.442 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035389
2025-06-25 08:14:39.442 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:14:39.442 INFO: val_f_rmse: 0.059458
##### Step: 317 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:15:09.041 INFO: ##### Step: 317 Learning rate: 3.0517578125e-07 #####
Epoch 78, Train Loss: 65.7344, Val Loss: 64.2568
2025-06-25 08:15:09.041 INFO: Epoch 78, Train Loss: 65.7344, Val Loss: 64.2568
train_e/atom_mae: 0.002173
2025-06-25 08:15:09.042 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:15:09.042 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:15:09.045 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:15:09.045 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:15:09.048 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:15:09.048 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:15:09.048 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:15:09.049 INFO: val_f_rmse: 0.059458
##### Step: 318 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:15:38.652 INFO: ##### Step: 318 Learning rate: 3.0517578125e-07 #####
Epoch 79, Train Loss: 65.7472, Val Loss: 64.2698
2025-06-25 08:15:38.652 INFO: Epoch 79, Train Loss: 65.7472, Val Loss: 64.2698
train_e/atom_mae: 0.002173
2025-06-25 08:15:38.653 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:15:38.653 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:15:38.657 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:15:38.657 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:15:38.659 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:15:38.660 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:15:38.660 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:15:38.660 INFO: val_f_rmse: 0.059458
##### Step: 319 Learning rate: 3.0517578125e-07 #####
2025-06-25 08:16:08.307 INFO: ##### Step: 319 Learning rate: 3.0517578125e-07 #####
Epoch 80, Train Loss: 65.7297, Val Loss: 64.2852
2025-06-25 08:16:08.307 INFO: Epoch 80, Train Loss: 65.7297, Val Loss: 64.2852
train_e/atom_mae: 0.002173
2025-06-25 08:16:08.308 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:16:08.308 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:16:08.312 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:16:08.312 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:16:08.314 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:16:08.315 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:16:08.315 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:16:08.315 INFO: val_f_rmse: 0.059458
##### Step: 320 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:16:37.948 INFO: ##### Step: 320 Learning rate: 1.52587890625e-07 #####
Epoch 81, Train Loss: 65.7271, Val Loss: 64.2977
2025-06-25 08:16:37.949 INFO: Epoch 81, Train Loss: 65.7271, Val Loss: 64.2977
train_e/atom_mae: 0.002173
2025-06-25 08:16:37.950 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:16:37.950 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:16:37.953 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:16:37.953 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:16:37.956 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:16:37.956 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:16:37.956 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:16:37.957 INFO: val_f_rmse: 0.059459
##### Step: 321 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:17:07.592 INFO: ##### Step: 321 Learning rate: 1.52587890625e-07 #####
Epoch 82, Train Loss: 65.7290, Val Loss: 64.2925
2025-06-25 08:17:07.593 INFO: Epoch 82, Train Loss: 65.7290, Val Loss: 64.2925
train_e/atom_mae: 0.002173
2025-06-25 08:17:07.594 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:17:07.594 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:17:07.597 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:17:07.597 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:17:07.600 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:17:07.600 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:17:07.600 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:17:07.600 INFO: val_f_rmse: 0.059459
##### Step: 322 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:17:37.252 INFO: ##### Step: 322 Learning rate: 1.52587890625e-07 #####
Epoch 83, Train Loss: 65.7192, Val Loss: 64.2858
2025-06-25 08:17:37.253 INFO: Epoch 83, Train Loss: 65.7192, Val Loss: 64.2858
train_e/atom_mae: 0.002173
2025-06-25 08:17:37.254 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:17:37.254 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:17:37.257 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:17:37.257 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:17:37.260 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:17:37.260 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:17:37.260 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:17:37.261 INFO: val_f_rmse: 0.059458
##### Step: 323 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:18:06.934 INFO: ##### Step: 323 Learning rate: 1.52587890625e-07 #####
Epoch 84, Train Loss: 65.7198, Val Loss: 64.2821
2025-06-25 08:18:06.934 INFO: Epoch 84, Train Loss: 65.7198, Val Loss: 64.2821
train_e/atom_mae: 0.002173
2025-06-25 08:18:06.935 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:18:06.935 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:18:06.939 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:18:06.939 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:18:06.941 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:18:06.941 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:18:06.942 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:18:06.942 INFO: val_f_rmse: 0.059458
##### Step: 324 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:18:36.584 INFO: ##### Step: 324 Learning rate: 1.52587890625e-07 #####
Epoch 85, Train Loss: 65.7180, Val Loss: 64.2835
2025-06-25 08:18:36.584 INFO: Epoch 85, Train Loss: 65.7180, Val Loss: 64.2835
train_e/atom_mae: 0.002173
2025-06-25 08:18:36.585 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:18:36.585 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:18:36.588 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:18:36.588 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:18:36.591 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:18:36.591 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:18:36.592 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:18:36.592 INFO: val_f_rmse: 0.059458
##### Step: 325 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:19:06.207 INFO: ##### Step: 325 Learning rate: 1.52587890625e-07 #####
Epoch 86, Train Loss: 65.7191, Val Loss: 64.2814
2025-06-25 08:19:06.207 INFO: Epoch 86, Train Loss: 65.7191, Val Loss: 64.2814
train_e/atom_mae: 0.002173
2025-06-25 08:19:06.208 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:19:06.208 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:19:06.211 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:19:06.212 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:19:06.214 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:19:06.214 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:19:06.215 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:19:06.215 INFO: val_f_rmse: 0.059458
##### Step: 326 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:19:35.850 INFO: ##### Step: 326 Learning rate: 1.52587890625e-07 #####
Epoch 87, Train Loss: 65.7161, Val Loss: 64.2651
2025-06-25 08:19:35.850 INFO: Epoch 87, Train Loss: 65.7161, Val Loss: 64.2651
train_e/atom_mae: 0.002173
2025-06-25 08:19:35.851 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:19:35.851 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:19:35.855 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:19:35.855 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:19:35.857 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:19:35.858 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:19:35.858 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:19:35.858 INFO: val_f_rmse: 0.059458
##### Step: 327 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:20:05.494 INFO: ##### Step: 327 Learning rate: 1.52587890625e-07 #####
Epoch 88, Train Loss: 65.7173, Val Loss: 64.2944
2025-06-25 08:20:05.494 INFO: Epoch 88, Train Loss: 65.7173, Val Loss: 64.2944
train_e/atom_mae: 0.002173
2025-06-25 08:20:05.495 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:20:05.495 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:20:05.499 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:20:05.499 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:20:05.501 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:20:05.502 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:20:05.502 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:20:05.502 INFO: val_f_rmse: 0.059459
##### Step: 328 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:20:35.145 INFO: ##### Step: 328 Learning rate: 1.52587890625e-07 #####
Epoch 89, Train Loss: 65.7391, Val Loss: 64.3030
2025-06-25 08:20:35.145 INFO: Epoch 89, Train Loss: 65.7391, Val Loss: 64.3030
train_e/atom_mae: 0.002173
2025-06-25 08:20:35.146 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002269
2025-06-25 08:20:35.146 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.034729
2025-06-25 08:20:35.149 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:20:35.149 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:20:35.152 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:20:35.152 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:20:35.153 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:20:35.153 INFO: val_f_rmse: 0.059459
##### Step: 329 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:21:04.767 INFO: ##### Step: 329 Learning rate: 1.52587890625e-07 #####
Epoch 90, Train Loss: 65.7227, Val Loss: 64.2998
2025-06-25 08:21:04.767 INFO: Epoch 90, Train Loss: 65.7227, Val Loss: 64.2998
train_e/atom_mae: 0.002173
2025-06-25 08:21:04.768 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:21:04.768 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:21:04.772 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:21:04.772 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:21:04.774 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:21:04.775 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:21:04.775 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059459
2025-06-25 08:21:04.775 INFO: val_f_rmse: 0.059459
##### Step: 330 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:21:34.431 INFO: ##### Step: 330 Learning rate: 1.52587890625e-07 #####
Epoch 91, Train Loss: 65.7242, Val Loss: 64.2860
2025-06-25 08:21:34.432 INFO: Epoch 91, Train Loss: 65.7242, Val Loss: 64.2860
train_e/atom_mae: 0.002173
2025-06-25 08:21:34.432 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:21:34.433 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:21:34.436 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:21:34.436 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:21:34.439 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:21:34.439 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:21:34.439 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:21:34.440 INFO: val_f_rmse: 0.059458
##### Step: 331 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:22:04.069 INFO: ##### Step: 331 Learning rate: 1.52587890625e-07 #####
Epoch 92, Train Loss: 65.7260, Val Loss: 64.2810
2025-06-25 08:22:04.069 INFO: Epoch 92, Train Loss: 65.7260, Val Loss: 64.2810
train_e/atom_mae: 0.002173
2025-06-25 08:22:04.070 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:22:04.070 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:22:04.073 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:22:04.074 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:22:04.076 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:22:04.076 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035390
2025-06-25 08:22:04.077 INFO: val_f_mae: 0.035390
val_f_rmse: 0.059458
2025-06-25 08:22:04.077 INFO: val_f_rmse: 0.059458
##### Step: 332 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:22:33.674 INFO: ##### Step: 332 Learning rate: 1.52587890625e-07 #####
Epoch 93, Train Loss: 65.7172, Val Loss: 64.2778
2025-06-25 08:22:33.674 INFO: Epoch 93, Train Loss: 65.7172, Val Loss: 64.2778
train_e/atom_mae: 0.002173
2025-06-25 08:22:33.675 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:22:33.675 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:22:33.679 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:22:33.679 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:22:33.681 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002241
2025-06-25 08:22:33.682 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.035389
2025-06-25 08:22:33.682 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:22:33.682 INFO: val_f_rmse: 0.059458
##### Step: 333 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:23:03.281 INFO: ##### Step: 333 Learning rate: 1.52587890625e-07 #####
Epoch 94, Train Loss: 65.7120, Val Loss: 64.2622
2025-06-25 08:23:03.281 INFO: Epoch 94, Train Loss: 65.7120, Val Loss: 64.2622
train_e/atom_mae: 0.002173
2025-06-25 08:23:03.282 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:23:03.282 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:23:03.285 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:23:03.286 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:23:03.288 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:23:03.288 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:23:03.289 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:23:03.289 INFO: val_f_rmse: 0.059458
##### Step: 334 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:23:32.925 INFO: ##### Step: 334 Learning rate: 1.52587890625e-07 #####
Epoch 95, Train Loss: 65.7184, Val Loss: 64.2621
2025-06-25 08:23:32.925 INFO: Epoch 95, Train Loss: 65.7184, Val Loss: 64.2621
train_e/atom_mae: 0.002173
2025-06-25 08:23:32.926 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:23:32.926 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:23:32.930 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:23:32.930 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:23:32.932 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:23:32.933 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:23:32.933 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:23:32.933 INFO: val_f_rmse: 0.059458
##### Step: 335 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:24:02.574 INFO: ##### Step: 335 Learning rate: 1.52587890625e-07 #####
Epoch 96, Train Loss: 65.7184, Val Loss: 64.2716
2025-06-25 08:24:02.574 INFO: Epoch 96, Train Loss: 65.7184, Val Loss: 64.2716
train_e/atom_mae: 0.002173
2025-06-25 08:24:02.575 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:24:02.575 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:24:02.578 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:24:02.578 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:24:02.581 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:24:02.581 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:24:02.581 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:24:02.582 INFO: val_f_rmse: 0.059458
##### Step: 336 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:24:32.214 INFO: ##### Step: 336 Learning rate: 1.52587890625e-07 #####
Epoch 97, Train Loss: 65.7184, Val Loss: 64.2613
2025-06-25 08:24:32.214 INFO: Epoch 97, Train Loss: 65.7184, Val Loss: 64.2613
train_e/atom_mae: 0.002173
2025-06-25 08:24:32.215 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:24:32.215 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:24:32.218 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:24:32.218 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:24:32.221 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:24:32.221 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:24:32.222 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:24:32.222 INFO: val_f_rmse: 0.059458
##### Step: 337 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:25:01.866 INFO: ##### Step: 337 Learning rate: 1.52587890625e-07 #####
Epoch 98, Train Loss: 65.7120, Val Loss: 64.2581
2025-06-25 08:25:01.867 INFO: Epoch 98, Train Loss: 65.7120, Val Loss: 64.2581
train_e/atom_mae: 0.002173
2025-06-25 08:25:01.868 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:25:01.868 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:25:01.871 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:25:01.871 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:25:01.874 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:25:01.874 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:25:01.874 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:25:01.875 INFO: val_f_rmse: 0.059458
##### Step: 338 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:25:31.511 INFO: ##### Step: 338 Learning rate: 1.52587890625e-07 #####
Epoch 99, Train Loss: 65.7092, Val Loss: 64.2586
2025-06-25 08:25:31.511 INFO: Epoch 99, Train Loss: 65.7092, Val Loss: 64.2586
train_e/atom_mae: 0.002173
2025-06-25 08:25:31.512 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:25:31.512 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:25:31.516 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:25:31.516 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:25:31.519 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:25:31.519 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:25:31.519 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:25:31.519 INFO: val_f_rmse: 0.059458
##### Step: 339 Learning rate: 1.52587890625e-07 #####
2025-06-25 08:26:01.176 INFO: ##### Step: 339 Learning rate: 1.52587890625e-07 #####
Epoch 100, Train Loss: 65.7273, Val Loss: 64.2527
2025-06-25 08:26:01.176 INFO: Epoch 100, Train Loss: 65.7273, Val Loss: 64.2527
train_e/atom_mae: 0.002173
2025-06-25 08:26:01.177 INFO: train_e/atom_mae: 0.002173
train_e/atom_rmse: 0.002268
2025-06-25 08:26:01.177 INFO: train_e/atom_rmse: 0.002268
train_f_mae: 0.034729
2025-06-25 08:26:01.181 INFO: train_f_mae: 0.034729
train_f_rmse: 0.058840
2025-06-25 08:26:01.181 INFO: train_f_rmse: 0.058840
val_e/atom_mae: 0.002152
2025-06-25 08:26:01.183 INFO: val_e/atom_mae: 0.002152
val_e/atom_rmse: 0.002240
2025-06-25 08:26:01.184 INFO: val_e/atom_rmse: 0.002240
val_f_mae: 0.035389
2025-06-25 08:26:01.184 INFO: val_f_mae: 0.035389
val_f_rmse: 0.059458
2025-06-25 08:26:01.184 INFO: val_f_rmse: 0.059458
2025-06-25 08:26:01.193 INFO: Finished
2025-06-25 08:26:01.193 INFO: Number of trainable parameters: 23080
