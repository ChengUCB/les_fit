Start: Wed Jun 11 09:18:48 PDT 2025
cwd: /global/scratch/users/dongjinkim/potential_training/CACE-LES/water-lr
2025-06-11 09:19:31.834 INFO: reading data
2025-06-11 09:19:34.546 INFO: Loaded 604 training configurations from '../data-sets/train-H2O_RPBE-D3.xyz'
2025-06-11 09:19:34.546 INFO: Using random 9.999999999999999e-05% of training set for validation
2025-06-11 09:19:34.970 INFO: Loaded 50 training configurations from '../data-sets/test-H2O_RPBE-D3.xyz'
2025-06-11 09:19:34.971 INFO: Using random 9.999999999999999e-05% of training set for validation
2025-06-11 09:19:53.287 INFO: CUDA version: 11.8, CUDA device: 0
2025-06-11 09:19:53.288 INFO: device: cuda
2025-06-11 09:19:53.289 INFO: building CACE representation
2025-06-11 09:19:53.755 INFO: Representation: Cace(
  (node_onehot): NodeEncoder(num_classes=2)
  (node_embedding_sender): NodeEmbedding(num_classes=2, embedding_dim=2)
  (node_embedding_receiver): NodeEmbedding(num_classes=2, embedding_dim=2)
  (edge_coding): EdgeEncoder(directed=True)
  (radial_basis): BesselRBF(cutoff=4.5, n_rbf=6, trainable=True)
  (cutoff_fn): PolynomialCutoff(p=6.0, cutoff=4.5)
  (angular_basis): AngularComponent(l_max=3)
  (radial_transform): SharedRadialLinearTransform(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 6x12x4 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 6x12x4 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 6x12x4 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 6x12x4 (cuda:0)]
    )
  )
  (symmetrizer): Symmetrizer()
  (message_passing_list): ModuleList()
)
2025-06-11 09:19:53.759 INFO: First train loop:
2025-06-11 09:19:53.760 INFO: creating training task
2025-06-11 09:19:53.764 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-11 09:22:20.009 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 1145.3570, Val Loss: 80.6992
2025-06-11 09:22:20.012 INFO: Epoch 1, Train Loss: 1145.3570, Val Loss: 80.6992
train_e/atom_mae: 0.048887
2025-06-11 09:22:20.018 INFO: train_e/atom_mae: 0.048887
train_e/atom_rmse: 0.086294
2025-06-11 09:22:20.020 INFO: train_e/atom_rmse: 0.086294
train_f_mae: 0.563705
2025-06-11 09:22:20.026 INFO: train_f_mae: 0.563705
train_f_rmse: 1.057311
2025-06-11 09:22:20.027 INFO: train_f_rmse: 1.057311
val_e/atom_mae: 0.005726
2025-06-11 09:22:20.032 INFO: val_e/atom_mae: 0.005726
val_e/atom_rmse: 0.007211
2025-06-11 09:22:20.034 INFO: val_e/atom_rmse: 0.007211
val_f_mae: 0.220117
2025-06-11 09:22:20.035 INFO: val_f_mae: 0.220117
val_f_rmse: 0.283738
2025-06-11 09:22:20.036 INFO: val_f_rmse: 0.283738
##### Step: 1 Learning rate: 0.004 #####
2025-06-11 09:24:35.539 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 44.8359, Val Loss: 24.6816
2025-06-11 09:24:35.541 INFO: Epoch 2, Train Loss: 44.8359, Val Loss: 24.6816
train_e/atom_mae: 0.006731
2025-06-11 09:24:35.543 INFO: train_e/atom_mae: 0.006731
train_e/atom_rmse: 0.008566
2025-06-11 09:24:35.544 INFO: train_e/atom_rmse: 0.008566
train_f_mae: 0.160887
2025-06-11 09:24:35.551 INFO: train_f_mae: 0.160887
train_f_rmse: 0.211105
2025-06-11 09:24:35.552 INFO: train_f_rmse: 0.211105
val_e/atom_mae: 0.003401
2025-06-11 09:24:35.556 INFO: val_e/atom_mae: 0.003401
val_e/atom_rmse: 0.004553
2025-06-11 09:24:35.558 INFO: val_e/atom_rmse: 0.004553
val_f_mae: 0.121121
2025-06-11 09:24:35.559 INFO: val_f_mae: 0.121121
val_f_rmse: 0.156860
2025-06-11 09:24:35.560 INFO: val_f_rmse: 0.156860
##### Step: 2 Learning rate: 0.006 #####
2025-06-11 09:26:51.280 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 17.9193, Val Loss: 13.0333
2025-06-11 09:26:51.283 INFO: Epoch 3, Train Loss: 17.9193, Val Loss: 13.0333
train_e/atom_mae: 0.005532
2025-06-11 09:26:51.285 INFO: train_e/atom_mae: 0.005532
train_e/atom_rmse: 0.006831
2025-06-11 09:26:51.286 INFO: train_e/atom_rmse: 0.006831
train_f_mae: 0.101330
2025-06-11 09:26:51.292 INFO: train_f_mae: 0.101330
train_f_rmse: 0.133219
2025-06-11 09:26:51.293 INFO: train_f_rmse: 0.133219
val_e/atom_mae: 0.008450
2025-06-11 09:26:51.298 INFO: val_e/atom_mae: 0.008450
val_e/atom_rmse: 0.008793
2025-06-11 09:26:51.300 INFO: val_e/atom_rmse: 0.008793
val_f_mae: 0.086528
2025-06-11 09:26:51.301 INFO: val_f_mae: 0.086528
val_f_rmse: 0.112908
2025-06-11 09:26:51.302 INFO: val_f_rmse: 0.112908
##### Step: 3 Learning rate: 0.008 #####
2025-06-11 09:29:06.944 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 10.5760, Val Loss: 8.1725
2025-06-11 09:29:06.945 INFO: Epoch 4, Train Loss: 10.5760, Val Loss: 8.1725
train_e/atom_mae: 0.006400
2025-06-11 09:29:06.947 INFO: train_e/atom_mae: 0.006400
train_e/atom_rmse: 0.007503
2025-06-11 09:29:06.948 INFO: train_e/atom_rmse: 0.007503
train_f_mae: 0.076670
2025-06-11 09:29:06.955 INFO: train_f_mae: 0.076670
train_f_rmse: 0.101826
2025-06-11 09:29:06.956 INFO: train_f_rmse: 0.101826
val_e/atom_mae: 0.001776
2025-06-11 09:29:06.961 INFO: val_e/atom_mae: 0.001776
val_e/atom_rmse: 0.002069
2025-06-11 09:29:06.962 INFO: val_e/atom_rmse: 0.002069
val_f_mae: 0.067822
2025-06-11 09:29:06.963 INFO: val_f_mae: 0.067822
val_f_rmse: 0.090315
2025-06-11 09:29:06.964 INFO: val_f_rmse: 0.090315
##### Step: 4 Learning rate: 0.01 #####
2025-06-11 09:31:22.626 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 7.4539, Val Loss: 6.5729
2025-06-11 09:31:22.627 INFO: Epoch 5, Train Loss: 7.4539, Val Loss: 6.5729
train_e/atom_mae: 0.003225
2025-06-11 09:31:22.629 INFO: train_e/atom_mae: 0.003225
train_e/atom_rmse: 0.003977
2025-06-11 09:31:22.630 INFO: train_e/atom_rmse: 0.003977
train_f_mae: 0.064256
2025-06-11 09:31:22.637 INFO: train_f_mae: 0.064256
train_f_rmse: 0.085997
2025-06-11 09:31:22.638 INFO: train_f_rmse: 0.085997
val_e/atom_mae: 0.005591
2025-06-11 09:31:22.643 INFO: val_e/atom_mae: 0.005591
val_e/atom_rmse: 0.005733
2025-06-11 09:31:22.644 INFO: val_e/atom_rmse: 0.005733
val_f_mae: 0.060211
2025-06-11 09:31:22.645 INFO: val_f_mae: 0.060211
val_f_rmse: 0.080323
2025-06-11 09:31:22.646 INFO: val_f_rmse: 0.080323
##### Step: 5 Learning rate: 0.01 #####
2025-06-11 09:33:38.330 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 6.2341, Val Loss: 5.1887
2025-06-11 09:33:38.333 INFO: Epoch 6, Train Loss: 6.2341, Val Loss: 5.1887
train_e/atom_mae: 0.003019
2025-06-11 09:33:38.335 INFO: train_e/atom_mae: 0.003019
train_e/atom_rmse: 0.003811
2025-06-11 09:33:38.336 INFO: train_e/atom_rmse: 0.003811
train_f_mae: 0.058450
2025-06-11 09:33:38.343 INFO: train_f_mae: 0.058450
train_f_rmse: 0.078617
2025-06-11 09:33:38.344 INFO: train_f_rmse: 0.078617
val_e/atom_mae: 0.001140
2025-06-11 09:33:38.349 INFO: val_e/atom_mae: 0.001140
val_e/atom_rmse: 0.001397
2025-06-11 09:33:38.350 INFO: val_e/atom_rmse: 0.001397
val_f_mae: 0.053887
2025-06-11 09:33:38.351 INFO: val_f_mae: 0.053887
val_f_rmse: 0.071983
2025-06-11 09:33:38.352 INFO: val_f_rmse: 0.071983
##### Step: 6 Learning rate: 0.01 #####
2025-06-11 09:35:54.024 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 5.5006, Val Loss: 4.7218
2025-06-11 09:35:54.025 INFO: Epoch 7, Train Loss: 5.5006, Val Loss: 4.7218
train_e/atom_mae: 0.003278
2025-06-11 09:35:54.027 INFO: train_e/atom_mae: 0.003278
train_e/atom_rmse: 0.004043
2025-06-11 09:35:54.028 INFO: train_e/atom_rmse: 0.004043
train_f_mae: 0.054679
2025-06-11 09:35:54.035 INFO: train_f_mae: 0.054679
train_f_rmse: 0.073758
2025-06-11 09:35:54.036 INFO: train_f_rmse: 0.073758
val_e/atom_mae: 0.001370
2025-06-11 09:35:54.041 INFO: val_e/atom_mae: 0.001370
val_e/atom_rmse: 0.001625
2025-06-11 09:35:54.042 INFO: val_e/atom_rmse: 0.001625
val_f_mae: 0.050897
2025-06-11 09:35:54.043 INFO: val_f_mae: 0.050897
val_f_rmse: 0.068645
2025-06-11 09:35:54.044 INFO: val_f_rmse: 0.068645
##### Step: 7 Learning rate: 0.01 #####
2025-06-11 09:38:09.745 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 4.8706, Val Loss: 4.1509
2025-06-11 09:38:09.748 INFO: Epoch 8, Train Loss: 4.8706, Val Loss: 4.1509
train_e/atom_mae: 0.002969
2025-06-11 09:38:09.750 INFO: train_e/atom_mae: 0.002969
train_e/atom_rmse: 0.003600
2025-06-11 09:38:09.751 INFO: train_e/atom_rmse: 0.003600
train_f_mae: 0.051378
2025-06-11 09:38:09.758 INFO: train_f_mae: 0.051378
train_f_rmse: 0.069447
2025-06-11 09:38:09.759 INFO: train_f_rmse: 0.069447
val_e/atom_mae: 0.001285
2025-06-11 09:38:09.764 INFO: val_e/atom_mae: 0.001285
val_e/atom_rmse: 0.001484
2025-06-11 09:38:09.765 INFO: val_e/atom_rmse: 0.001484
val_f_mae: 0.047739
2025-06-11 09:38:09.766 INFO: val_f_mae: 0.047739
val_f_rmse: 0.064364
2025-06-11 09:38:09.767 INFO: val_f_rmse: 0.064364
##### Step: 8 Learning rate: 0.01 #####
2025-06-11 09:40:25.537 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 4.5037, Val Loss: 4.7036
2025-06-11 09:40:25.538 INFO: Epoch 9, Train Loss: 4.5037, Val Loss: 4.7036
train_e/atom_mae: 0.003266
2025-06-11 09:40:25.541 INFO: train_e/atom_mae: 0.003266
train_e/atom_rmse: 0.004257
2025-06-11 09:40:25.541 INFO: train_e/atom_rmse: 0.004257
train_f_mae: 0.049186
2025-06-11 09:40:25.548 INFO: train_f_mae: 0.049186
train_f_rmse: 0.066610
2025-06-11 09:40:25.549 INFO: train_f_rmse: 0.066610
val_e/atom_mae: 0.011340
2025-06-11 09:40:25.554 INFO: val_e/atom_mae: 0.011340
val_e/atom_rmse: 0.011359
2025-06-11 09:40:25.555 INFO: val_e/atom_rmse: 0.011359
val_f_mae: 0.048801
2025-06-11 09:40:25.556 INFO: val_f_mae: 0.048801
val_f_rmse: 0.065023
2025-06-11 09:40:25.557 INFO: val_f_rmse: 0.065023
##### Step: 9 Learning rate: 0.01 #####
2025-06-11 09:42:41.275 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 3.9788, Val Loss: 3.7230
2025-06-11 09:42:41.277 INFO: Epoch 10, Train Loss: 3.9788, Val Loss: 3.7230
train_e/atom_mae: 0.004029
2025-06-11 09:42:41.280 INFO: train_e/atom_mae: 0.004029
train_e/atom_rmse: 0.004720
2025-06-11 09:42:41.281 INFO: train_e/atom_rmse: 0.004720
train_f_mae: 0.045939
2025-06-11 09:42:41.287 INFO: train_f_mae: 0.045939
train_f_rmse: 0.062423
2025-06-11 09:42:41.288 INFO: train_f_rmse: 0.062423
val_e/atom_mae: 0.003431
2025-06-11 09:42:41.293 INFO: val_e/atom_mae: 0.003431
val_e/atom_rmse: 0.003477
2025-06-11 09:42:41.294 INFO: val_e/atom_rmse: 0.003477
val_f_mae: 0.045214
2025-06-11 09:42:41.296 INFO: val_f_mae: 0.045214
val_f_rmse: 0.060650
2025-06-11 09:42:41.296 INFO: val_f_rmse: 0.060650
##### Step: 10 Learning rate: 0.01 #####
2025-06-11 09:44:57.140 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 3.7089, Val Loss: 3.1607
2025-06-11 09:44:57.142 INFO: Epoch 11, Train Loss: 3.7089, Val Loss: 3.1607
train_e/atom_mae: 0.003417
2025-06-11 09:44:57.144 INFO: train_e/atom_mae: 0.003417
train_e/atom_rmse: 0.003978
2025-06-11 09:44:57.145 INFO: train_e/atom_rmse: 0.003978
train_f_mae: 0.044615
2025-06-11 09:44:57.151 INFO: train_f_mae: 0.044615
train_f_rmse: 0.060420
2025-06-11 09:44:57.152 INFO: train_f_rmse: 0.060420
val_e/atom_mae: 0.002746
2025-06-11 09:44:57.157 INFO: val_e/atom_mae: 0.002746
val_e/atom_rmse: 0.002840
2025-06-11 09:44:57.159 INFO: val_e/atom_rmse: 0.002840
val_f_mae: 0.041257
2025-06-11 09:44:57.160 INFO: val_f_mae: 0.041257
val_f_rmse: 0.055955
2025-06-11 09:44:57.161 INFO: val_f_rmse: 0.055955
##### Step: 11 Learning rate: 0.01 #####
2025-06-11 09:47:12.922 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 4.1542, Val Loss: 4.4612
2025-06-11 09:47:12.923 INFO: Epoch 12, Train Loss: 4.1542, Val Loss: 4.4612
train_e/atom_mae: 0.004122
2025-06-11 09:47:12.925 INFO: train_e/atom_mae: 0.004122
train_e/atom_rmse: 0.005115
2025-06-11 09:47:12.926 INFO: train_e/atom_rmse: 0.005115
train_f_mae: 0.047478
2025-06-11 09:47:12.933 INFO: train_f_mae: 0.047478
train_f_rmse: 0.063701
2025-06-11 09:47:12.934 INFO: train_f_rmse: 0.063701
val_e/atom_mae: 0.003827
2025-06-11 09:47:12.939 INFO: val_e/atom_mae: 0.003827
val_e/atom_rmse: 0.003876
2025-06-11 09:47:12.940 INFO: val_e/atom_rmse: 0.003876
val_f_mae: 0.049869
2025-06-11 09:47:12.941 INFO: val_f_mae: 0.049869
val_f_rmse: 0.066377
2025-06-11 09:47:12.942 INFO: val_f_rmse: 0.066377
##### Step: 12 Learning rate: 0.01 #####
2025-06-11 09:49:28.656 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 3.5522, Val Loss: 3.2803
2025-06-11 09:49:28.658 INFO: Epoch 13, Train Loss: 3.5522, Val Loss: 3.2803
train_e/atom_mae: 0.004905
2025-06-11 09:49:28.660 INFO: train_e/atom_mae: 0.004905
train_e/atom_rmse: 0.005646
2025-06-11 09:49:28.661 INFO: train_e/atom_rmse: 0.005646
train_f_mae: 0.043233
2025-06-11 09:49:28.668 INFO: train_f_mae: 0.043233
train_f_rmse: 0.058606
2025-06-11 09:49:28.668 INFO: train_f_rmse: 0.058606
val_e/atom_mae: 0.009341
2025-06-11 09:49:28.673 INFO: val_e/atom_mae: 0.009341
val_e/atom_rmse: 0.009355
2025-06-11 09:49:28.675 INFO: val_e/atom_rmse: 0.009355
val_f_mae: 0.040249
2025-06-11 09:49:28.676 INFO: val_f_mae: 0.040249
val_f_rmse: 0.054385
2025-06-11 09:49:28.677 INFO: val_f_rmse: 0.054385
##### Step: 13 Learning rate: 0.01 #####
2025-06-11 09:51:44.331 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 3.1855, Val Loss: 2.9332
2025-06-11 09:51:44.332 INFO: Epoch 14, Train Loss: 3.1855, Val Loss: 2.9332
train_e/atom_mae: 0.003883
2025-06-11 09:51:44.335 INFO: train_e/atom_mae: 0.003883
train_e/atom_rmse: 0.004515
2025-06-11 09:51:44.335 INFO: train_e/atom_rmse: 0.004515
train_f_mae: 0.041131
2025-06-11 09:51:44.342 INFO: train_f_mae: 0.041131
train_f_rmse: 0.055771
2025-06-11 09:51:44.343 INFO: train_f_rmse: 0.055771
val_e/atom_mae: 0.007529
2025-06-11 09:51:44.348 INFO: val_e/atom_mae: 0.007529
val_e/atom_rmse: 0.007545
2025-06-11 09:51:44.349 INFO: val_e/atom_rmse: 0.007545
val_f_mae: 0.038668
2025-06-11 09:51:44.350 INFO: val_f_mae: 0.038668
val_f_rmse: 0.052186
2025-06-11 09:51:44.351 INFO: val_f_rmse: 0.052186
##### Step: 14 Learning rate: 0.01 #####
2025-06-11 09:54:00.145 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 3.0068, Val Loss: 2.5859
2025-06-11 09:54:00.148 INFO: Epoch 15, Train Loss: 3.0068, Val Loss: 2.5859
train_e/atom_mae: 0.002195
2025-06-11 09:54:00.150 INFO: train_e/atom_mae: 0.002195
train_e/atom_rmse: 0.002804
2025-06-11 09:54:00.151 INFO: train_e/atom_rmse: 0.002804
train_f_mae: 0.040476
2025-06-11 09:54:00.157 INFO: train_f_mae: 0.040476
train_f_rmse: 0.054570
2025-06-11 09:54:00.158 INFO: train_f_rmse: 0.054570
val_e/atom_mae: 0.001417
2025-06-11 09:54:00.163 INFO: val_e/atom_mae: 0.001417
val_e/atom_rmse: 0.001484
2025-06-11 09:54:00.165 INFO: val_e/atom_rmse: 0.001484
val_f_mae: 0.037808
2025-06-11 09:54:00.166 INFO: val_f_mae: 0.037808
val_f_rmse: 0.050771
2025-06-11 09:54:00.167 INFO: val_f_rmse: 0.050771
##### Step: 15 Learning rate: 0.01 #####
2025-06-11 09:56:15.988 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 3.2602, Val Loss: 3.0264
2025-06-11 09:56:15.990 INFO: Epoch 16, Train Loss: 3.2602, Val Loss: 3.0264
train_e/atom_mae: 0.002598
2025-06-11 09:56:15.993 INFO: train_e/atom_mae: 0.002598
train_e/atom_rmse: 0.003121
2025-06-11 09:56:15.994 INFO: train_e/atom_rmse: 0.003121
train_f_mae: 0.042309
2025-06-11 09:56:16.000 INFO: train_f_mae: 0.042309
train_f_rmse: 0.056783
2025-06-11 09:56:16.001 INFO: train_f_rmse: 0.056783
val_e/atom_mae: 0.002237
2025-06-11 09:56:16.006 INFO: val_e/atom_mae: 0.002237
val_e/atom_rmse: 0.002273
2025-06-11 09:56:16.008 INFO: val_e/atom_rmse: 0.002273
val_f_mae: 0.040839
2025-06-11 09:56:16.009 INFO: val_f_mae: 0.040839
val_f_rmse: 0.054840
2025-06-11 09:56:16.010 INFO: val_f_rmse: 0.054840
##### Step: 16 Learning rate: 0.01 #####
2025-06-11 09:58:31.703 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 2.9581, Val Loss: 2.7674
2025-06-11 09:58:31.705 INFO: Epoch 17, Train Loss: 2.9581, Val Loss: 2.7674
train_e/atom_mae: 0.003061
2025-06-11 09:58:31.707 INFO: train_e/atom_mae: 0.003061
train_e/atom_rmse: 0.003759
2025-06-11 09:58:31.708 INFO: train_e/atom_rmse: 0.003759
train_f_mae: 0.040041
2025-06-11 09:58:31.715 INFO: train_f_mae: 0.040041
train_f_rmse: 0.053907
2025-06-11 09:58:31.715 INFO: train_f_rmse: 0.053907
val_e/atom_mae: 0.004455
2025-06-11 09:58:31.720 INFO: val_e/atom_mae: 0.004455
val_e/atom_rmse: 0.004477
2025-06-11 09:58:31.722 INFO: val_e/atom_rmse: 0.004477
val_f_mae: 0.039019
2025-06-11 09:58:31.723 INFO: val_f_mae: 0.039019
val_f_rmse: 0.051899
2025-06-11 09:58:31.724 INFO: val_f_rmse: 0.051899
##### Step: 17 Learning rate: 0.01 #####
2025-06-11 10:00:47.374 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 2.9325, Val Loss: 2.8764
2025-06-11 10:00:47.375 INFO: Epoch 18, Train Loss: 2.9325, Val Loss: 2.8764
train_e/atom_mae: 0.003481
2025-06-11 10:00:47.378 INFO: train_e/atom_mae: 0.003481
train_e/atom_rmse: 0.004202
2025-06-11 10:00:47.379 INFO: train_e/atom_rmse: 0.004202
train_f_mae: 0.039833
2025-06-11 10:00:47.385 INFO: train_f_mae: 0.039833
train_f_rmse: 0.053548
2025-06-11 10:00:47.386 INFO: train_f_rmse: 0.053548
val_e/atom_mae: 0.004428
2025-06-11 10:00:47.391 INFO: val_e/atom_mae: 0.004428
val_e/atom_rmse: 0.004478
2025-06-11 10:00:47.392 INFO: val_e/atom_rmse: 0.004478
val_f_mae: 0.039613
2025-06-11 10:00:47.394 INFO: val_f_mae: 0.039613
val_f_rmse: 0.052939
2025-06-11 10:00:47.395 INFO: val_f_rmse: 0.052939
##### Step: 18 Learning rate: 0.01 #####
2025-06-11 10:03:03.072 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 2.9962, Val Loss: 2.8908
2025-06-11 10:03:03.074 INFO: Epoch 19, Train Loss: 2.9962, Val Loss: 2.8908
train_e/atom_mae: 0.003646
2025-06-11 10:03:03.076 INFO: train_e/atom_mae: 0.003646
train_e/atom_rmse: 0.004707
2025-06-11 10:03:03.077 INFO: train_e/atom_rmse: 0.004707
train_f_mae: 0.040371
2025-06-11 10:03:03.084 INFO: train_f_mae: 0.040371
train_f_rmse: 0.053986
2025-06-11 10:03:03.084 INFO: train_f_rmse: 0.053986
val_e/atom_mae: 0.010783
2025-06-11 10:03:03.089 INFO: val_e/atom_mae: 0.010783
val_e/atom_rmse: 0.010791
2025-06-11 10:03:03.091 INFO: val_e/atom_rmse: 0.010791
val_f_mae: 0.036957
2025-06-11 10:03:03.092 INFO: val_f_mae: 0.036957
val_f_rmse: 0.049613
2025-06-11 10:03:03.093 INFO: val_f_rmse: 0.049613
##### Step: 19 Learning rate: 0.01 #####
2025-06-11 10:05:18.837 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 3.0169, Val Loss: 2.9892
2025-06-11 10:05:18.839 INFO: Epoch 20, Train Loss: 3.0169, Val Loss: 2.9892
train_e/atom_mae: 0.005399
2025-06-11 10:05:18.842 INFO: train_e/atom_mae: 0.005399
train_e/atom_rmse: 0.006229
2025-06-11 10:05:18.842 INFO: train_e/atom_rmse: 0.006229
train_f_mae: 0.039896
2025-06-11 10:05:18.849 INFO: train_f_mae: 0.039896
train_f_rmse: 0.053609
2025-06-11 10:05:18.850 INFO: train_f_rmse: 0.053609
val_e/atom_mae: 0.006557
2025-06-11 10:05:18.855 INFO: val_e/atom_mae: 0.006557
val_e/atom_rmse: 0.006590
2025-06-11 10:05:18.856 INFO: val_e/atom_rmse: 0.006590
val_f_mae: 0.039331
2025-06-11 10:05:18.858 INFO: val_f_mae: 0.039331
val_f_rmse: 0.053190
2025-06-11 10:05:18.858 INFO: val_f_rmse: 0.053190
##### Step: 20 Learning rate: 0.005 #####
2025-06-11 10:07:34.759 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 2.6865, Val Loss: 2.6216
2025-06-11 10:07:34.761 INFO: Epoch 21, Train Loss: 2.6865, Val Loss: 2.6216
train_e/atom_mae: 0.002012
2025-06-11 10:07:34.764 INFO: train_e/atom_mae: 0.002012
train_e/atom_rmse: 0.002523
2025-06-11 10:07:34.765 INFO: train_e/atom_rmse: 0.002523
train_f_mae: 0.038300
2025-06-11 10:07:34.771 INFO: train_f_mae: 0.038300
train_f_rmse: 0.051605
2025-06-11 10:07:34.772 INFO: train_f_rmse: 0.051605
val_e/atom_mae: 0.001380
2025-06-11 10:07:34.777 INFO: val_e/atom_mae: 0.001380
val_e/atom_rmse: 0.001435
2025-06-11 10:07:34.778 INFO: val_e/atom_rmse: 0.001435
val_f_mae: 0.038090
2025-06-11 10:07:34.780 INFO: val_f_mae: 0.038090
val_f_rmse: 0.051127
2025-06-11 10:07:34.780 INFO: val_f_rmse: 0.051127
##### Step: 21 Learning rate: 0.005 #####
2025-06-11 10:09:50.532 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 2.4985, Val Loss: 2.3272
2025-06-11 10:09:50.534 INFO: Epoch 22, Train Loss: 2.4985, Val Loss: 2.3272
train_e/atom_mae: 0.001530
2025-06-11 10:09:50.536 INFO: train_e/atom_mae: 0.001530
train_e/atom_rmse: 0.001946
2025-06-11 10:09:50.537 INFO: train_e/atom_rmse: 0.001946
train_f_mae: 0.037054
2025-06-11 10:09:50.544 INFO: train_f_mae: 0.037054
train_f_rmse: 0.049846
2025-06-11 10:09:50.544 INFO: train_f_rmse: 0.049846
val_e/atom_mae: 0.000767
2025-06-11 10:09:50.549 INFO: val_e/atom_mae: 0.000767
val_e/atom_rmse: 0.000884
2025-06-11 10:09:50.551 INFO: val_e/atom_rmse: 0.000884
val_f_mae: 0.036329
2025-06-11 10:09:50.552 INFO: val_f_mae: 0.036329
val_f_rmse: 0.048211
2025-06-11 10:09:50.553 INFO: val_f_rmse: 0.048211
##### Step: 22 Learning rate: 0.005 #####
2025-06-11 10:12:06.307 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 2.4130, Val Loss: 2.3256
2025-06-11 10:12:06.310 INFO: Epoch 23, Train Loss: 2.4130, Val Loss: 2.3256
train_e/atom_mae: 0.002835
2025-06-11 10:12:06.312 INFO: train_e/atom_mae: 0.002835
train_e/atom_rmse: 0.003361
2025-06-11 10:12:06.313 INFO: train_e/atom_rmse: 0.003361
train_f_mae: 0.036203
2025-06-11 10:12:06.320 INFO: train_f_mae: 0.036203
train_f_rmse: 0.048697
2025-06-11 10:12:06.321 INFO: train_f_rmse: 0.048697
val_e/atom_mae: 0.003771
2025-06-11 10:12:06.326 INFO: val_e/atom_mae: 0.003771
val_e/atom_rmse: 0.003788
2025-06-11 10:12:06.327 INFO: val_e/atom_rmse: 0.003788
val_f_mae: 0.035664
2025-06-11 10:12:06.328 INFO: val_f_mae: 0.035664
val_f_rmse: 0.047673
2025-06-11 10:12:06.329 INFO: val_f_rmse: 0.047673
##### Step: 23 Learning rate: 0.005 #####
2025-06-11 10:14:22.223 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 2.4033, Val Loss: 2.3548
2025-06-11 10:14:22.225 INFO: Epoch 24, Train Loss: 2.4033, Val Loss: 2.3548
train_e/atom_mae: 0.002496
2025-06-11 10:14:22.227 INFO: train_e/atom_mae: 0.002496
train_e/atom_rmse: 0.003016
2025-06-11 10:14:22.228 INFO: train_e/atom_rmse: 0.003016
train_f_mae: 0.036221
2025-06-11 10:14:22.235 INFO: train_f_mae: 0.036221
train_f_rmse: 0.048680
2025-06-11 10:14:22.235 INFO: train_f_rmse: 0.048680
val_e/atom_mae: 0.003611
2025-06-11 10:14:22.240 INFO: val_e/atom_mae: 0.003611
val_e/atom_rmse: 0.003799
2025-06-11 10:14:22.242 INFO: val_e/atom_rmse: 0.003799
val_f_mae: 0.035919
2025-06-11 10:14:22.243 INFO: val_f_mae: 0.035919
val_f_rmse: 0.047975
2025-06-11 10:14:22.244 INFO: val_f_rmse: 0.047975
##### Step: 24 Learning rate: 0.005 #####
2025-06-11 10:16:38.052 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 2.3504, Val Loss: 2.2582
2025-06-11 10:16:38.055 INFO: Epoch 25, Train Loss: 2.3504, Val Loss: 2.2582
train_e/atom_mae: 0.002483
2025-06-11 10:16:38.057 INFO: train_e/atom_mae: 0.002483
train_e/atom_rmse: 0.002991
2025-06-11 10:16:38.058 INFO: train_e/atom_rmse: 0.002991
train_f_mae: 0.035834
2025-06-11 10:16:38.065 INFO: train_f_mae: 0.035834
train_f_rmse: 0.048140
2025-06-11 10:16:38.065 INFO: train_f_rmse: 0.048140
val_e/atom_mae: 0.000591
2025-06-11 10:16:38.071 INFO: val_e/atom_mae: 0.000591
val_e/atom_rmse: 0.000725
2025-06-11 10:16:38.072 INFO: val_e/atom_rmse: 0.000725
val_f_mae: 0.035689
2025-06-11 10:16:38.073 INFO: val_f_mae: 0.035689
val_f_rmse: 0.047500
2025-06-11 10:16:38.074 INFO: val_f_rmse: 0.047500
##### Step: 25 Learning rate: 0.005 #####
2025-06-11 10:18:53.778 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 2.3874, Val Loss: 2.2713
2025-06-11 10:18:53.779 INFO: Epoch 26, Train Loss: 2.3874, Val Loss: 2.2713
train_e/atom_mae: 0.001556
2025-06-11 10:18:53.782 INFO: train_e/atom_mae: 0.001556
train_e/atom_rmse: 0.001940
2025-06-11 10:18:53.782 INFO: train_e/atom_rmse: 0.001940
train_f_mae: 0.036391
2025-06-11 10:18:53.789 INFO: train_f_mae: 0.036391
train_f_rmse: 0.048719
2025-06-11 10:18:53.790 INFO: train_f_rmse: 0.048719
val_e/atom_mae: 0.000600
2025-06-11 10:18:53.795 INFO: val_e/atom_mae: 0.000600
val_e/atom_rmse: 0.000686
2025-06-11 10:18:53.796 INFO: val_e/atom_rmse: 0.000686
val_f_mae: 0.035665
2025-06-11 10:18:53.797 INFO: val_f_mae: 0.035665
val_f_rmse: 0.047640
2025-06-11 10:18:53.798 INFO: val_f_rmse: 0.047640
##### Step: 26 Learning rate: 0.005 #####
2025-06-11 10:21:09.577 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 2.2590, Val Loss: 2.6819
2025-06-11 10:21:09.579 INFO: Epoch 27, Train Loss: 2.2590, Val Loss: 2.6819
train_e/atom_mae: 0.001812
2025-06-11 10:21:09.582 INFO: train_e/atom_mae: 0.001812
train_e/atom_rmse: 0.002288
2025-06-11 10:21:09.583 INFO: train_e/atom_rmse: 0.002288
train_f_mae: 0.035231
2025-06-11 10:21:09.589 INFO: train_f_mae: 0.035231
train_f_rmse: 0.047325
2025-06-11 10:21:09.590 INFO: train_f_rmse: 0.047325
val_e/atom_mae: 0.001708
2025-06-11 10:21:09.595 INFO: val_e/atom_mae: 0.001708
val_e/atom_rmse: 0.001765
2025-06-11 10:21:09.596 INFO: val_e/atom_rmse: 0.001765
val_f_mae: 0.038976
2025-06-11 10:21:09.598 INFO: val_f_mae: 0.038976
val_f_rmse: 0.051676
2025-06-11 10:21:09.599 INFO: val_f_rmse: 0.051676
##### Step: 27 Learning rate: 0.005 #####
2025-06-11 10:23:25.418 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 2.7466, Val Loss: 3.8053
2025-06-11 10:23:25.420 INFO: Epoch 28, Train Loss: 2.7466, Val Loss: 3.8053
train_e/atom_mae: 0.002376
2025-06-11 10:23:25.422 INFO: train_e/atom_mae: 0.002376
train_e/atom_rmse: 0.002830
2025-06-11 10:23:25.423 INFO: train_e/atom_rmse: 0.002830
train_f_mae: 0.039198
2025-06-11 10:23:25.430 INFO: train_f_mae: 0.039198
train_f_rmse: 0.052125
2025-06-11 10:23:25.431 INFO: train_f_rmse: 0.052125
val_e/atom_mae: 0.002993
2025-06-11 10:23:25.436 INFO: val_e/atom_mae: 0.002993
val_e/atom_rmse: 0.003042
2025-06-11 10:23:25.437 INFO: val_e/atom_rmse: 0.003042
val_f_mae: 0.047411
2025-06-11 10:23:25.438 INFO: val_f_mae: 0.047411
val_f_rmse: 0.061410
2025-06-11 10:23:25.439 INFO: val_f_rmse: 0.061410
##### Step: 28 Learning rate: 0.005 #####
2025-06-11 10:25:41.359 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 2.3461, Val Loss: 2.0038
2025-06-11 10:25:41.362 INFO: Epoch 29, Train Loss: 2.3461, Val Loss: 2.0038
train_e/atom_mae: 0.001782
2025-06-11 10:25:41.364 INFO: train_e/atom_mae: 0.001782
train_e/atom_rmse: 0.002202
2025-06-11 10:25:41.365 INFO: train_e/atom_rmse: 0.002202
train_f_mae: 0.035989
2025-06-11 10:25:41.372 INFO: train_f_mae: 0.035989
train_f_rmse: 0.048252
2025-06-11 10:25:41.373 INFO: train_f_rmse: 0.048252
val_e/atom_mae: 0.001580
2025-06-11 10:25:41.378 INFO: val_e/atom_mae: 0.001580
val_e/atom_rmse: 0.001639
2025-06-11 10:25:41.379 INFO: val_e/atom_rmse: 0.001639
val_f_mae: 0.033217
2025-06-11 10:25:41.380 INFO: val_f_mae: 0.033217
val_f_rmse: 0.044653
2025-06-11 10:25:41.381 INFO: val_f_rmse: 0.044653
##### Step: 29 Learning rate: 0.005 #####
2025-06-11 10:27:57.447 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 2.2616, Val Loss: 2.0285
2025-06-11 10:27:57.450 INFO: Epoch 30, Train Loss: 2.2616, Val Loss: 2.0285
train_e/atom_mae: 0.002696
2025-06-11 10:27:57.452 INFO: train_e/atom_mae: 0.002696
train_e/atom_rmse: 0.003336
2025-06-11 10:27:57.453 INFO: train_e/atom_rmse: 0.003336
train_f_mae: 0.035163
2025-06-11 10:27:57.460 INFO: train_f_mae: 0.035163
train_f_rmse: 0.047123
2025-06-11 10:27:57.461 INFO: train_f_rmse: 0.047123
val_e/atom_mae: 0.001972
2025-06-11 10:27:57.465 INFO: val_e/atom_mae: 0.001972
val_e/atom_rmse: 0.002025
2025-06-11 10:27:57.467 INFO: val_e/atom_rmse: 0.002025
val_f_mae: 0.033369
2025-06-11 10:27:57.468 INFO: val_f_mae: 0.033369
val_f_rmse: 0.044870
2025-06-11 10:27:57.469 INFO: val_f_rmse: 0.044870
##### Step: 30 Learning rate: 0.005 #####
2025-06-11 10:30:13.430 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 2.2098, Val Loss: 2.2511
2025-06-11 10:30:13.433 INFO: Epoch 31, Train Loss: 2.2098, Val Loss: 2.2511
train_e/atom_mae: 0.001587
2025-06-11 10:30:13.435 INFO: train_e/atom_mae: 0.001587
train_e/atom_rmse: 0.001923
2025-06-11 10:30:13.436 INFO: train_e/atom_rmse: 0.001923
train_f_mae: 0.035039
2025-06-11 10:30:13.443 INFO: train_f_mae: 0.035039
train_f_rmse: 0.046863
2025-06-11 10:30:13.444 INFO: train_f_rmse: 0.046863
val_e/atom_mae: 0.001732
2025-06-11 10:30:13.449 INFO: val_e/atom_mae: 0.001732
val_e/atom_rmse: 0.001779
2025-06-11 10:30:13.450 INFO: val_e/atom_rmse: 0.001779
val_f_mae: 0.035769
2025-06-11 10:30:13.451 INFO: val_f_mae: 0.035769
val_f_rmse: 0.047323
2025-06-11 10:30:13.452 INFO: val_f_rmse: 0.047323
##### Step: 31 Learning rate: 0.005 #####
2025-06-11 10:32:29.118 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 2.0955, Val Loss: 2.0438
2025-06-11 10:32:29.120 INFO: Epoch 32, Train Loss: 2.0955, Val Loss: 2.0438
train_e/atom_mae: 0.002541
2025-06-11 10:32:29.123 INFO: train_e/atom_mae: 0.002541
train_e/atom_rmse: 0.003034
2025-06-11 10:32:29.124 INFO: train_e/atom_rmse: 0.003034
train_f_mae: 0.033849
2025-06-11 10:32:29.130 INFO: train_f_mae: 0.033849
train_f_rmse: 0.045404
2025-06-11 10:32:29.131 INFO: train_f_rmse: 0.045404
val_e/atom_mae: 0.003027
2025-06-11 10:32:29.136 INFO: val_e/atom_mae: 0.003027
val_e/atom_rmse: 0.003082
2025-06-11 10:32:29.137 INFO: val_e/atom_rmse: 0.003082
val_f_mae: 0.033601
2025-06-11 10:32:29.139 INFO: val_f_mae: 0.033601
val_f_rmse: 0.044820
2025-06-11 10:32:29.139 INFO: val_f_rmse: 0.044820
##### Step: 32 Learning rate: 0.005 #####
2025-06-11 10:34:44.844 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 2.0971, Val Loss: 2.0631
2025-06-11 10:34:44.846 INFO: Epoch 33, Train Loss: 2.0971, Val Loss: 2.0631
train_e/atom_mae: 0.002484
2025-06-11 10:34:44.848 INFO: train_e/atom_mae: 0.002484
train_e/atom_rmse: 0.003039
2025-06-11 10:34:44.849 INFO: train_e/atom_rmse: 0.003039
train_f_mae: 0.033872
2025-06-11 10:34:44.856 INFO: train_f_mae: 0.033872
train_f_rmse: 0.045421
2025-06-11 10:34:44.857 INFO: train_f_rmse: 0.045421
val_e/atom_mae: 0.002377
2025-06-11 10:34:44.862 INFO: val_e/atom_mae: 0.002377
val_e/atom_rmse: 0.002485
2025-06-11 10:34:44.863 INFO: val_e/atom_rmse: 0.002485
val_f_mae: 0.033710
2025-06-11 10:34:44.864 INFO: val_f_mae: 0.033710
val_f_rmse: 0.045170
2025-06-11 10:34:44.865 INFO: val_f_rmse: 0.045170
##### Step: 33 Learning rate: 0.005 #####
2025-06-11 10:37:00.555 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 2.0877, Val Loss: 2.1884
2025-06-11 10:37:00.557 INFO: Epoch 34, Train Loss: 2.0877, Val Loss: 2.1884
train_e/atom_mae: 0.002800
2025-06-11 10:37:00.559 INFO: train_e/atom_mae: 0.002800
train_e/atom_rmse: 0.003300
2025-06-11 10:37:00.560 INFO: train_e/atom_rmse: 0.003300
train_f_mae: 0.033796
2025-06-11 10:37:00.567 INFO: train_f_mae: 0.033796
train_f_rmse: 0.045250
2025-06-11 10:37:00.568 INFO: train_f_rmse: 0.045250
val_e/atom_mae: 0.006142
2025-06-11 10:37:00.573 INFO: val_e/atom_mae: 0.006142
val_e/atom_rmse: 0.006167
2025-06-11 10:37:00.574 INFO: val_e/atom_rmse: 0.006167
val_f_mae: 0.033829
2025-06-11 10:37:00.575 INFO: val_f_mae: 0.033829
val_f_rmse: 0.045257
2025-06-11 10:37:00.576 INFO: val_f_rmse: 0.045257
##### Step: 34 Learning rate: 0.005 #####
2025-06-11 10:39:16.272 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 2.0802, Val Loss: 1.8845
2025-06-11 10:39:16.275 INFO: Epoch 35, Train Loss: 2.0802, Val Loss: 1.8845
train_e/atom_mae: 0.002407
2025-06-11 10:39:16.277 INFO: train_e/atom_mae: 0.002407
train_e/atom_rmse: 0.003197
2025-06-11 10:39:16.278 INFO: train_e/atom_rmse: 0.003197
train_f_mae: 0.033643
2025-06-11 10:39:16.285 INFO: train_f_mae: 0.033643
train_f_rmse: 0.045194
2025-06-11 10:39:16.285 INFO: train_f_rmse: 0.045194
val_e/atom_mae: 0.000289
2025-06-11 10:39:16.291 INFO: val_e/atom_mae: 0.000289
val_e/atom_rmse: 0.000366
2025-06-11 10:39:16.292 INFO: val_e/atom_rmse: 0.000366
val_f_mae: 0.032382
2025-06-11 10:39:16.293 INFO: val_f_mae: 0.032382
val_f_rmse: 0.043405
2025-06-11 10:39:16.294 INFO: val_f_rmse: 0.043405
##### Step: 35 Learning rate: 0.005 #####
2025-06-11 10:41:32.222 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 2.2390, Val Loss: 1.9094
2025-06-11 10:41:32.224 INFO: Epoch 36, Train Loss: 2.2390, Val Loss: 1.9094
train_e/atom_mae: 0.003296
2025-06-11 10:41:32.226 INFO: train_e/atom_mae: 0.003296
train_e/atom_rmse: 0.003934
2025-06-11 10:41:32.227 INFO: train_e/atom_rmse: 0.003934
train_f_mae: 0.035044
2025-06-11 10:41:32.233 INFO: train_f_mae: 0.035044
train_f_rmse: 0.046711
2025-06-11 10:41:32.234 INFO: train_f_rmse: 0.046711
val_e/atom_mae: 0.004324
2025-06-11 10:41:32.239 INFO: val_e/atom_mae: 0.004324
val_e/atom_rmse: 0.004344
2025-06-11 10:41:32.241 INFO: val_e/atom_rmse: 0.004344
val_f_mae: 0.032016
2025-06-11 10:41:32.242 INFO: val_f_mae: 0.032016
val_f_rmse: 0.042893
2025-06-11 10:41:32.243 INFO: val_f_rmse: 0.042893
##### Step: 36 Learning rate: 0.005 #####
2025-06-11 10:43:48.058 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 2.1038, Val Loss: 2.3144
2025-06-11 10:43:48.060 INFO: Epoch 37, Train Loss: 2.1038, Val Loss: 2.3144
train_e/atom_mae: 0.002227
2025-06-11 10:43:48.063 INFO: train_e/atom_mae: 0.002227
train_e/atom_rmse: 0.002651
2025-06-11 10:43:48.064 INFO: train_e/atom_rmse: 0.002651
train_f_mae: 0.034054
2025-06-11 10:43:48.070 INFO: train_f_mae: 0.034054
train_f_rmse: 0.045584
2025-06-11 10:43:48.071 INFO: train_f_rmse: 0.045584
val_e/atom_mae: 0.000982
2025-06-11 10:43:48.076 INFO: val_e/atom_mae: 0.000982
val_e/atom_rmse: 0.001056
2025-06-11 10:43:48.077 INFO: val_e/atom_rmse: 0.001056
val_f_mae: 0.036514
2025-06-11 10:43:48.078 INFO: val_f_mae: 0.036514
val_f_rmse: 0.048065
2025-06-11 10:43:48.079 INFO: val_f_rmse: 0.048065
##### Step: 37 Learning rate: 0.005 #####
2025-06-11 10:46:03.950 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 2.1094, Val Loss: 2.1681
2025-06-11 10:46:03.952 INFO: Epoch 38, Train Loss: 2.1094, Val Loss: 2.1681
train_e/atom_mae: 0.001416
2025-06-11 10:46:03.955 INFO: train_e/atom_mae: 0.001416
train_e/atom_rmse: 0.001845
2025-06-11 10:46:03.956 INFO: train_e/atom_rmse: 0.001845
train_f_mae: 0.034224
2025-06-11 10:46:03.962 INFO: train_f_mae: 0.034224
train_f_rmse: 0.045791
2025-06-11 10:46:03.963 INFO: train_f_rmse: 0.045791
val_e/atom_mae: 0.001684
2025-06-11 10:46:03.968 INFO: val_e/atom_mae: 0.001684
val_e/atom_rmse: 0.001735
2025-06-11 10:46:03.969 INFO: val_e/atom_rmse: 0.001735
val_f_mae: 0.035064
2025-06-11 10:46:03.971 INFO: val_f_mae: 0.035064
val_f_rmse: 0.046444
2025-06-11 10:46:03.972 INFO: val_f_rmse: 0.046444
##### Step: 38 Learning rate: 0.005 #####
2025-06-11 10:48:19.871 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 2.0713, Val Loss: 1.9499
2025-06-11 10:48:19.874 INFO: Epoch 39, Train Loss: 2.0713, Val Loss: 1.9499
train_e/atom_mae: 0.003602
2025-06-11 10:48:19.876 INFO: train_e/atom_mae: 0.003602
train_e/atom_rmse: 0.004227
2025-06-11 10:48:19.877 INFO: train_e/atom_rmse: 0.004227
train_f_mae: 0.033465
2025-06-11 10:48:19.884 INFO: train_f_mae: 0.033465
train_f_rmse: 0.044782
2025-06-11 10:48:19.885 INFO: train_f_rmse: 0.044782
val_e/atom_mae: 0.002253
2025-06-11 10:48:19.890 INFO: val_e/atom_mae: 0.002253
val_e/atom_rmse: 0.002309
2025-06-11 10:48:19.891 INFO: val_e/atom_rmse: 0.002309
val_f_mae: 0.032897
2025-06-11 10:48:19.892 INFO: val_f_mae: 0.032897
val_f_rmse: 0.043934
2025-06-11 10:48:19.893 INFO: val_f_rmse: 0.043934
##### Step: 39 Learning rate: 0.005 #####
2025-06-11 10:50:35.907 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 2.0412, Val Loss: 1.9668
2025-06-11 10:50:35.909 INFO: Epoch 40, Train Loss: 2.0412, Val Loss: 1.9668
train_e/atom_mae: 0.002028
2025-06-11 10:50:35.912 INFO: train_e/atom_mae: 0.002028
train_e/atom_rmse: 0.002493
2025-06-11 10:50:35.913 INFO: train_e/atom_rmse: 0.002493
train_f_mae: 0.033568
2025-06-11 10:50:35.919 INFO: train_f_mae: 0.033568
train_f_rmse: 0.044925
2025-06-11 10:50:35.920 INFO: train_f_rmse: 0.044925
val_e/atom_mae: 0.001982
2025-06-11 10:50:35.925 INFO: val_e/atom_mae: 0.001982
val_e/atom_rmse: 0.002017
2025-06-11 10:50:35.927 INFO: val_e/atom_rmse: 0.002017
val_f_mae: 0.032764
2025-06-11 10:50:35.928 INFO: val_f_mae: 0.032764
val_f_rmse: 0.044179
2025-06-11 10:50:35.929 INFO: val_f_rmse: 0.044179
2025-06-11 10:50:35.978 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-11 10:52:52.450 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 51.2249, Val Loss: 3.1506
2025-06-11 10:52:52.451 INFO: Epoch 1, Train Loss: 51.2249, Val Loss: 3.1506
train_e/atom_mae: 0.016147
2025-06-11 10:52:52.453 INFO: train_e/atom_mae: 0.016147
train_e/atom_rmse: 0.030738
2025-06-11 10:52:52.454 INFO: train_e/atom_rmse: 0.030738
train_f_mae: 0.105718
2025-06-11 10:52:52.461 INFO: train_f_mae: 0.105718
train_f_rmse: 0.218499
2025-06-11 10:52:52.462 INFO: train_f_rmse: 0.218499
val_e/atom_mae: 0.000839
2025-06-11 10:52:52.466 INFO: val_e/atom_mae: 0.000839
val_e/atom_rmse: 0.001046
2025-06-11 10:52:52.468 INFO: val_e/atom_rmse: 0.001046
val_f_mae: 0.042619
2025-06-11 10:52:52.469 INFO: val_f_mae: 0.042619
val_f_rmse: 0.056094
2025-06-11 10:52:52.470 INFO: val_f_rmse: 0.056094
##### Step: 1 Learning rate: 0.004 #####
2025-06-11 10:55:08.884 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 3.9861, Val Loss: 2.6053
2025-06-11 10:55:08.886 INFO: Epoch 2, Train Loss: 3.9861, Val Loss: 2.6053
train_e/atom_mae: 0.006160
2025-06-11 10:55:08.889 INFO: train_e/atom_mae: 0.006160
train_e/atom_rmse: 0.007329
2025-06-11 10:55:08.890 INFO: train_e/atom_rmse: 0.007329
train_f_mae: 0.046516
2025-06-11 10:55:08.896 INFO: train_f_mae: 0.046516
train_f_rmse: 0.061547
2025-06-11 10:55:08.897 INFO: train_f_rmse: 0.061547
val_e/atom_mae: 0.002477
2025-06-11 10:55:08.902 INFO: val_e/atom_mae: 0.002477
val_e/atom_rmse: 0.002623
2025-06-11 10:55:08.903 INFO: val_e/atom_rmse: 0.002623
val_f_mae: 0.037904
2025-06-11 10:55:08.905 INFO: val_f_mae: 0.037904
val_f_rmse: 0.050793
2025-06-11 10:55:08.906 INFO: val_f_rmse: 0.050793
##### Step: 2 Learning rate: 0.006 #####
2025-06-11 10:57:25.471 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 4.5032, Val Loss: 3.2252
2025-06-11 10:57:25.473 INFO: Epoch 3, Train Loss: 4.5032, Val Loss: 3.2252
train_e/atom_mae: 0.010970
2025-06-11 10:57:25.475 INFO: train_e/atom_mae: 0.010970
train_e/atom_rmse: 0.013455
2025-06-11 10:57:25.476 INFO: train_e/atom_rmse: 0.013455
train_f_mae: 0.046980
2025-06-11 10:57:25.482 INFO: train_f_mae: 0.046980
train_f_rmse: 0.061934
2025-06-11 10:57:25.483 INFO: train_f_rmse: 0.061934
val_e/atom_mae: 0.007735
2025-06-11 10:57:25.488 INFO: val_e/atom_mae: 0.007735
val_e/atom_rmse: 0.007749
2025-06-11 10:57:25.490 INFO: val_e/atom_rmse: 0.007749
val_f_mae: 0.041883
2025-06-11 10:57:25.491 INFO: val_f_mae: 0.041883
val_f_rmse: 0.054807
2025-06-11 10:57:25.492 INFO: val_f_rmse: 0.054807
##### Step: 3 Learning rate: 0.008 #####
2025-06-11 10:59:42.042 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 4.0931, Val Loss: 6.0572
2025-06-11 10:59:42.043 INFO: Epoch 4, Train Loss: 4.0931, Val Loss: 6.0572
train_e/atom_mae: 0.012929
2025-06-11 10:59:42.045 INFO: train_e/atom_mae: 0.012929
train_e/atom_rmse: 0.015532
2025-06-11 10:59:42.046 INFO: train_e/atom_rmse: 0.015532
train_f_mae: 0.043288
2025-06-11 10:59:42.053 INFO: train_f_mae: 0.043288
train_f_rmse: 0.056602
2025-06-11 10:59:42.054 INFO: train_f_rmse: 0.056602
val_e/atom_mae: 0.025163
2025-06-11 10:59:42.059 INFO: val_e/atom_mae: 0.025163
val_e/atom_rmse: 0.025166
2025-06-11 10:59:42.060 INFO: val_e/atom_rmse: 0.025166
val_f_mae: 0.047634
2025-06-11 10:59:42.061 INFO: val_f_mae: 0.047634
val_f_rmse: 0.061012
2025-06-11 10:59:42.062 INFO: val_f_rmse: 0.061012
##### Step: 4 Learning rate: 0.01 #####
2025-06-11 11:01:58.386 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 4.3659, Val Loss: 3.7358
2025-06-11 11:01:58.387 INFO: Epoch 5, Train Loss: 4.3659, Val Loss: 3.7358
train_e/atom_mae: 0.017558
2025-06-11 11:01:58.390 INFO: train_e/atom_mae: 0.017558
train_e/atom_rmse: 0.020019
2025-06-11 11:01:58.391 INFO: train_e/atom_rmse: 0.020019
train_f_mae: 0.040735
2025-06-11 11:01:58.397 INFO: train_f_mae: 0.040735
train_f_rmse: 0.053746
2025-06-11 11:01:58.398 INFO: train_f_rmse: 0.053746
val_e/atom_mae: 0.006716
2025-06-11 11:01:58.403 INFO: val_e/atom_mae: 0.006716
val_e/atom_rmse: 0.006897
2025-06-11 11:01:58.404 INFO: val_e/atom_rmse: 0.006897
val_f_mae: 0.046499
2025-06-11 11:01:58.406 INFO: val_f_mae: 0.046499
val_f_rmse: 0.059669
2025-06-11 11:01:58.406 INFO: val_f_rmse: 0.059669
##### Step: 5 Learning rate: 0.01 #####
2025-06-11 11:04:14.740 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 5.3322, Val Loss: 3.3880
2025-06-11 11:04:14.743 INFO: Epoch 6, Train Loss: 5.3322, Val Loss: 3.3880
train_e/atom_mae: 0.012847
2025-06-11 11:04:14.745 INFO: train_e/atom_mae: 0.012847
train_e/atom_rmse: 0.016112
2025-06-11 11:04:14.746 INFO: train_e/atom_rmse: 0.016112
train_f_mae: 0.050109
2025-06-11 11:04:14.753 INFO: train_f_mae: 0.050109
train_f_rmse: 0.066145
2025-06-11 11:04:14.754 INFO: train_f_rmse: 0.066145
val_e/atom_mae: 0.008274
2025-06-11 11:04:14.759 INFO: val_e/atom_mae: 0.008274
val_e/atom_rmse: 0.008325
2025-06-11 11:04:14.760 INFO: val_e/atom_rmse: 0.008325
val_f_mae: 0.043124
2025-06-11 11:04:14.761 INFO: val_f_mae: 0.043124
val_f_rmse: 0.055968
2025-06-11 11:04:14.762 INFO: val_f_rmse: 0.055968
##### Step: 6 Learning rate: 0.01 #####
2025-06-11 11:06:31.164 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 2.7845, Val Loss: 2.7179
2025-06-11 11:06:31.167 INFO: Epoch 7, Train Loss: 2.7845, Val Loss: 2.7179
train_e/atom_mae: 0.005627
2025-06-11 11:06:31.169 INFO: train_e/atom_mae: 0.005627
train_e/atom_rmse: 0.006627
2025-06-11 11:06:31.170 INFO: train_e/atom_rmse: 0.006627
train_f_mae: 0.039078
2025-06-11 11:06:31.177 INFO: train_f_mae: 0.039078
train_f_rmse: 0.051211
2025-06-11 11:06:31.178 INFO: train_f_rmse: 0.051211
val_e/atom_mae: 0.005852
2025-06-11 11:06:31.182 INFO: val_e/atom_mae: 0.005852
val_e/atom_rmse: 0.005868
2025-06-11 11:06:31.184 INFO: val_e/atom_rmse: 0.005868
val_f_mae: 0.038842
2025-06-11 11:06:31.185 INFO: val_f_mae: 0.038842
val_f_rmse: 0.050901
2025-06-11 11:06:31.186 INFO: val_f_rmse: 0.050901
##### Step: 7 Learning rate: 0.01 #####
2025-06-11 11:08:47.489 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 3.1161, Val Loss: 2.9552
2025-06-11 11:08:47.492 INFO: Epoch 8, Train Loss: 3.1161, Val Loss: 2.9552
train_e/atom_mae: 0.006148
2025-06-11 11:08:47.494 INFO: train_e/atom_mae: 0.006148
train_e/atom_rmse: 0.007903
2025-06-11 11:08:47.495 INFO: train_e/atom_rmse: 0.007903
train_f_mae: 0.041217
2025-06-11 11:08:47.502 INFO: train_f_mae: 0.041217
train_f_rmse: 0.053721
2025-06-11 11:08:47.503 INFO: train_f_rmse: 0.053721
val_e/atom_mae: 0.007321
2025-06-11 11:08:47.508 INFO: val_e/atom_mae: 0.007321
val_e/atom_rmse: 0.007334
2025-06-11 11:08:47.509 INFO: val_e/atom_rmse: 0.007334
val_f_mae: 0.040126
2025-06-11 11:08:47.510 INFO: val_f_mae: 0.040126
val_f_rmse: 0.052506
2025-06-11 11:08:47.511 INFO: val_f_rmse: 0.052506
##### Step: 8 Learning rate: 0.01 #####
2025-06-11 11:11:03.748 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 2.2022, Val Loss: 1.7316
2025-06-11 11:11:03.750 INFO: Epoch 9, Train Loss: 2.2022, Val Loss: 1.7316
train_e/atom_mae: 0.005871
2025-06-11 11:11:03.753 INFO: train_e/atom_mae: 0.005871
train_e/atom_rmse: 0.007520
2025-06-11 11:11:03.754 INFO: train_e/atom_rmse: 0.007520
train_f_mae: 0.034106
2025-06-11 11:11:03.760 INFO: train_f_mae: 0.034106
train_f_rmse: 0.044652
2025-06-11 11:11:03.761 INFO: train_f_rmse: 0.044652
val_e/atom_mae: 0.009232
2025-06-11 11:11:03.766 INFO: val_e/atom_mae: 0.009232
val_e/atom_rmse: 0.009261
2025-06-11 11:11:03.767 INFO: val_e/atom_rmse: 0.009261
val_f_mae: 0.028794
2025-06-11 11:11:03.769 INFO: val_f_mae: 0.028794
val_f_rmse: 0.037622
2025-06-11 11:11:03.769 INFO: val_f_rmse: 0.037622
##### Step: 9 Learning rate: 0.01 #####
2025-06-11 11:13:20.170 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 2.8354, Val Loss: 2.7759
2025-06-11 11:13:20.172 INFO: Epoch 10, Train Loss: 2.8354, Val Loss: 2.7759
train_e/atom_mae: 0.010586
2025-06-11 11:13:20.175 INFO: train_e/atom_mae: 0.010586
train_e/atom_rmse: 0.012541
2025-06-11 11:13:20.176 INFO: train_e/atom_rmse: 0.012541
train_f_mae: 0.036491
2025-06-11 11:13:20.183 INFO: train_f_mae: 0.036491
train_f_rmse: 0.047494
2025-06-11 11:13:20.184 INFO: train_f_rmse: 0.047494
val_e/atom_mae: 0.017816
2025-06-11 11:13:20.189 INFO: val_e/atom_mae: 0.017816
val_e/atom_rmse: 0.017820
2025-06-11 11:13:20.190 INFO: val_e/atom_rmse: 0.017820
val_f_mae: 0.030956
2025-06-11 11:13:20.191 INFO: val_f_mae: 0.030956
val_f_rmse: 0.040067
2025-06-11 11:13:20.192 INFO: val_f_rmse: 0.040067
##### Step: 10 Learning rate: 0.01 #####
2025-06-11 11:15:36.442 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 2.0618, Val Loss: 2.1969
2025-06-11 11:15:36.443 INFO: Epoch 11, Train Loss: 2.0618, Val Loss: 2.1969
train_e/atom_mae: 0.010456
2025-06-11 11:15:36.445 INFO: train_e/atom_mae: 0.010456
train_e/atom_rmse: 0.011740
2025-06-11 11:15:36.446 INFO: train_e/atom_rmse: 0.011740
train_f_mae: 0.030249
2025-06-11 11:15:36.453 INFO: train_f_mae: 0.030249
train_f_rmse: 0.039417
2025-06-11 11:15:36.454 INFO: train_f_rmse: 0.039417
val_e/atom_mae: 0.015130
2025-06-11 11:15:36.458 INFO: val_e/atom_mae: 0.015130
val_e/atom_rmse: 0.015138
2025-06-11 11:15:36.460 INFO: val_e/atom_rmse: 0.015138
val_f_mae: 0.028065
2025-06-11 11:15:36.461 INFO: val_f_mae: 0.028065
val_f_rmse: 0.036772
2025-06-11 11:15:36.462 INFO: val_f_rmse: 0.036772
##### Step: 11 Learning rate: 0.01 #####
2025-06-11 11:17:52.559 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 1.7836, Val Loss: 1.3035
2025-06-11 11:17:52.560 INFO: Epoch 12, Train Loss: 1.7836, Val Loss: 1.3035
train_e/atom_mae: 0.008551
2025-06-11 11:17:52.562 INFO: train_e/atom_mae: 0.008551
train_e/atom_rmse: 0.009901
2025-06-11 11:17:52.563 INFO: train_e/atom_rmse: 0.009901
train_f_mae: 0.028911
2025-06-11 11:17:52.570 INFO: train_f_mae: 0.028911
train_f_rmse: 0.037712
2025-06-11 11:17:52.571 INFO: train_f_rmse: 0.037712
val_e/atom_mae: 0.005673
2025-06-11 11:17:52.575 INFO: val_e/atom_mae: 0.005673
val_e/atom_rmse: 0.005680
2025-06-11 11:17:52.577 INFO: val_e/atom_rmse: 0.005680
val_f_mae: 0.026437
2025-06-11 11:17:52.578 INFO: val_f_mae: 0.026437
val_f_rmse: 0.034418
2025-06-11 11:17:52.579 INFO: val_f_rmse: 0.034418
##### Step: 12 Learning rate: 0.01 #####
2025-06-11 11:20:08.583 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 1.6330, Val Loss: 2.6516
2025-06-11 11:20:08.585 INFO: Epoch 13, Train Loss: 1.6330, Val Loss: 2.6516
train_e/atom_mae: 0.004994
2025-06-11 11:20:08.587 INFO: train_e/atom_mae: 0.004994
train_e/atom_rmse: 0.006030
2025-06-11 11:20:08.588 INFO: train_e/atom_rmse: 0.006030
train_f_mae: 0.029632
2025-06-11 11:20:08.594 INFO: train_f_mae: 0.029632
train_f_rmse: 0.038717
2025-06-11 11:20:08.595 INFO: train_f_rmse: 0.038717
val_e/atom_mae: 0.003676
2025-06-11 11:20:08.600 INFO: val_e/atom_mae: 0.003676
val_e/atom_rmse: 0.003692
2025-06-11 11:20:08.601 INFO: val_e/atom_rmse: 0.003692
val_f_mae: 0.039760
2025-06-11 11:20:08.603 INFO: val_f_mae: 0.039760
val_f_rmse: 0.051004
2025-06-11 11:20:08.604 INFO: val_f_rmse: 0.051004
##### Step: 13 Learning rate: 0.01 #####
2025-06-11 11:22:24.355 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 1.6974, Val Loss: 2.1002
2025-06-11 11:22:24.359 INFO: Epoch 14, Train Loss: 1.6974, Val Loss: 2.1002
train_e/atom_mae: 0.004591
2025-06-11 11:22:24.361 INFO: train_e/atom_mae: 0.004591
train_e/atom_rmse: 0.005499
2025-06-11 11:22:24.362 INFO: train_e/atom_rmse: 0.005499
train_f_mae: 0.030545
2025-06-11 11:22:24.369 INFO: train_f_mae: 0.030545
train_f_rmse: 0.039824
2025-06-11 11:22:24.369 INFO: train_f_rmse: 0.039824
val_e/atom_mae: 0.004744
2025-06-11 11:22:24.374 INFO: val_e/atom_mae: 0.004744
val_e/atom_rmse: 0.004753
2025-06-11 11:22:24.375 INFO: val_e/atom_rmse: 0.004753
val_f_mae: 0.035627
2025-06-11 11:22:24.377 INFO: val_f_mae: 0.035627
val_f_rmse: 0.044911
2025-06-11 11:22:24.378 INFO: val_f_rmse: 0.044911
##### Step: 14 Learning rate: 0.01 #####
2025-06-11 11:24:39.305 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 1.8177, Val Loss: 2.2648
2025-06-11 11:24:39.307 INFO: Epoch 15, Train Loss: 1.8177, Val Loss: 2.2648
train_e/atom_mae: 0.003694
2025-06-11 11:24:39.310 INFO: train_e/atom_mae: 0.003694
train_e/atom_rmse: 0.004558
2025-06-11 11:24:39.311 INFO: train_e/atom_rmse: 0.004558
train_f_mae: 0.032135
2025-06-11 11:24:39.317 INFO: train_f_mae: 0.032135
train_f_rmse: 0.041726
2025-06-11 11:24:39.318 INFO: train_f_rmse: 0.041726
val_e/atom_mae: 0.007101
2025-06-11 11:24:39.322 INFO: val_e/atom_mae: 0.007101
val_e/atom_rmse: 0.007108
2025-06-11 11:24:39.323 INFO: val_e/atom_rmse: 0.007108
val_f_mae: 0.036354
2025-06-11 11:24:39.325 INFO: val_f_mae: 0.036354
val_f_rmse: 0.045590
2025-06-11 11:24:39.326 INFO: val_f_rmse: 0.045590
##### Step: 15 Learning rate: 0.01 #####
2025-06-11 11:26:54.077 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 1.7253, Val Loss: 1.4244
2025-06-11 11:26:54.080 INFO: Epoch 16, Train Loss: 1.7253, Val Loss: 1.4244
train_e/atom_mae: 0.005230
2025-06-11 11:26:54.082 INFO: train_e/atom_mae: 0.005230
train_e/atom_rmse: 0.006775
2025-06-11 11:26:54.083 INFO: train_e/atom_rmse: 0.006775
train_f_mae: 0.030399
2025-06-11 11:26:54.089 INFO: train_f_mae: 0.030399
train_f_rmse: 0.039447
2025-06-11 11:26:54.090 INFO: train_f_rmse: 0.039447
val_e/atom_mae: 0.002462
2025-06-11 11:26:54.095 INFO: val_e/atom_mae: 0.002462
val_e/atom_rmse: 0.002550
2025-06-11 11:26:54.096 INFO: val_e/atom_rmse: 0.002550
val_f_mae: 0.029314
2025-06-11 11:26:54.097 INFO: val_f_mae: 0.029314
val_f_rmse: 0.037423
2025-06-11 11:26:54.098 INFO: val_f_rmse: 0.037423
##### Step: 16 Learning rate: 0.01 #####
2025-06-11 11:29:09.031 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 1.5893, Val Loss: 2.4199
2025-06-11 11:29:09.032 INFO: Epoch 17, Train Loss: 1.5893, Val Loss: 2.4199
train_e/atom_mae: 0.003540
2025-06-11 11:29:09.035 INFO: train_e/atom_mae: 0.003540
train_e/atom_rmse: 0.004362
2025-06-11 11:29:09.035 INFO: train_e/atom_rmse: 0.004362
train_f_mae: 0.030014
2025-06-11 11:29:09.042 INFO: train_f_mae: 0.030014
train_f_rmse: 0.038976
2025-06-11 11:29:09.043 INFO: train_f_rmse: 0.038976
val_e/atom_mae: 0.003643
2025-06-11 11:29:09.047 INFO: val_e/atom_mae: 0.003643
val_e/atom_rmse: 0.003659
2025-06-11 11:29:09.049 INFO: val_e/atom_rmse: 0.003659
val_f_mae: 0.038166
2025-06-11 11:29:09.050 INFO: val_f_mae: 0.038166
val_f_rmse: 0.048689
2025-06-11 11:29:09.051 INFO: val_f_rmse: 0.048689
##### Step: 17 Learning rate: 0.01 #####
2025-06-11 11:31:24.173 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 2.0189, Val Loss: 1.2125
2025-06-11 11:31:24.175 INFO: Epoch 18, Train Loss: 2.0189, Val Loss: 1.2125
train_e/atom_mae: 0.006987
2025-06-11 11:31:24.178 INFO: train_e/atom_mae: 0.006987
train_e/atom_rmse: 0.008532
2025-06-11 11:31:24.178 INFO: train_e/atom_rmse: 0.008532
train_f_mae: 0.032454
2025-06-11 11:31:24.185 INFO: train_f_mae: 0.032454
train_f_rmse: 0.041839
2025-06-11 11:31:24.186 INFO: train_f_rmse: 0.041839
val_e/atom_mae: 0.005491
2025-06-11 11:31:24.190 INFO: val_e/atom_mae: 0.005491
val_e/atom_rmse: 0.005549
2025-06-11 11:31:24.192 INFO: val_e/atom_rmse: 0.005549
val_f_mae: 0.025447
2025-06-11 11:31:24.193 INFO: val_f_mae: 0.025447
val_f_rmse: 0.033152
2025-06-11 11:31:24.194 INFO: val_f_rmse: 0.033152
##### Step: 18 Learning rate: 0.01 #####
2025-06-11 11:33:39.311 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 1.4392, Val Loss: 2.1947
2025-06-11 11:33:39.314 INFO: Epoch 19, Train Loss: 1.4392, Val Loss: 2.1947
train_e/atom_mae: 0.004750
2025-06-11 11:33:39.316 INFO: train_e/atom_mae: 0.004750
train_e/atom_rmse: 0.005864
2025-06-11 11:33:39.317 INFO: train_e/atom_rmse: 0.005864
train_f_mae: 0.027846
2025-06-11 11:33:39.323 INFO: train_f_mae: 0.027846
train_f_rmse: 0.036227
2025-06-11 11:33:39.324 INFO: train_f_rmse: 0.036227
val_e/atom_mae: 0.002358
2025-06-11 11:33:39.329 INFO: val_e/atom_mae: 0.002358
val_e/atom_rmse: 0.002387
2025-06-11 11:33:39.330 INFO: val_e/atom_rmse: 0.002387
val_f_mae: 0.036772
2025-06-11 11:33:39.331 INFO: val_f_mae: 0.036772
val_f_rmse: 0.046623
2025-06-11 11:33:39.332 INFO: val_f_rmse: 0.046623
##### Step: 19 Learning rate: 0.01 #####
2025-06-11 11:35:54.146 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 1.5266, Val Loss: 1.0395
2025-06-11 11:35:54.147 INFO: Epoch 20, Train Loss: 1.5266, Val Loss: 1.0395
train_e/atom_mae: 0.004574
2025-06-11 11:35:54.149 INFO: train_e/atom_mae: 0.004574
train_e/atom_rmse: 0.006221
2025-06-11 11:35:54.150 INFO: train_e/atom_rmse: 0.006221
train_f_mae: 0.028503
2025-06-11 11:35:54.156 INFO: train_f_mae: 0.028503
train_f_rmse: 0.037201
2025-06-11 11:35:54.157 INFO: train_f_rmse: 0.037201
val_e/atom_mae: 0.001341
2025-06-11 11:35:54.162 INFO: val_e/atom_mae: 0.001341
val_e/atom_rmse: 0.001414
2025-06-11 11:35:54.163 INFO: val_e/atom_rmse: 0.001414
val_f_mae: 0.024729
2025-06-11 11:35:54.165 INFO: val_f_mae: 0.024729
val_f_rmse: 0.032127
2025-06-11 11:35:54.166 INFO: val_f_rmse: 0.032127
##### Step: 20 Learning rate: 0.005 #####
2025-06-11 11:38:09.092 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 1.0024, Val Loss: 0.9689
2025-06-11 11:38:09.093 INFO: Epoch 21, Train Loss: 1.0024, Val Loss: 0.9689
train_e/atom_mae: 0.002539
2025-06-11 11:38:09.096 INFO: train_e/atom_mae: 0.002539
train_e/atom_rmse: 0.003030
2025-06-11 11:38:09.097 INFO: train_e/atom_rmse: 0.003030
train_f_mae: 0.023930
2025-06-11 11:38:09.103 INFO: train_f_mae: 0.023930
train_f_rmse: 0.031122
2025-06-11 11:38:09.104 INFO: train_f_rmse: 0.031122
val_e/atom_mae: 0.005043
2025-06-11 11:38:09.108 INFO: val_e/atom_mae: 0.005043
val_e/atom_rmse: 0.005055
2025-06-11 11:38:09.110 INFO: val_e/atom_rmse: 0.005055
val_f_mae: 0.022645
2025-06-11 11:38:09.111 INFO: val_f_mae: 0.022645
val_f_rmse: 0.029575
2025-06-11 11:38:09.112 INFO: val_f_rmse: 0.029575
##### Step: 21 Learning rate: 0.005 #####
2025-06-11 11:40:24.012 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.9477, Val Loss: 1.2684
2025-06-11 11:40:24.014 INFO: Epoch 22, Train Loss: 0.9477, Val Loss: 1.2684
train_e/atom_mae: 0.001731
2025-06-11 11:40:24.017 INFO: train_e/atom_mae: 0.001731
train_e/atom_rmse: 0.002214
2025-06-11 11:40:24.018 INFO: train_e/atom_rmse: 0.002214
train_f_mae: 0.023440
2025-06-11 11:40:24.024 INFO: train_f_mae: 0.023440
train_f_rmse: 0.030490
2025-06-11 11:40:24.025 INFO: train_f_rmse: 0.030490
val_e/atom_mae: 0.002601
2025-06-11 11:40:24.029 INFO: val_e/atom_mae: 0.002601
val_e/atom_rmse: 0.002758
2025-06-11 11:40:24.031 INFO: val_e/atom_rmse: 0.002758
val_f_mae: 0.027017
2025-06-11 11:40:24.032 INFO: val_f_mae: 0.027017
val_f_rmse: 0.035219
2025-06-11 11:40:24.033 INFO: val_f_rmse: 0.035219
##### Step: 22 Learning rate: 0.005 #####
2025-06-11 11:42:38.725 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 1.0227, Val Loss: 1.1774
2025-06-11 11:42:38.728 INFO: Epoch 23, Train Loss: 1.0227, Val Loss: 1.1774
train_e/atom_mae: 0.003245
2025-06-11 11:42:38.730 INFO: train_e/atom_mae: 0.003245
train_e/atom_rmse: 0.003942
2025-06-11 11:42:38.731 INFO: train_e/atom_rmse: 0.003942
train_f_mae: 0.023902
2025-06-11 11:42:38.737 INFO: train_f_mae: 0.023902
train_f_rmse: 0.031072
2025-06-11 11:42:38.738 INFO: train_f_rmse: 0.031072
val_e/atom_mae: 0.006330
2025-06-11 11:42:38.743 INFO: val_e/atom_mae: 0.006330
val_e/atom_rmse: 0.006338
2025-06-11 11:42:38.744 INFO: val_e/atom_rmse: 0.006338
val_f_mae: 0.024841
2025-06-11 11:42:38.745 INFO: val_f_mae: 0.024841
val_f_rmse: 0.032082
2025-06-11 11:42:38.746 INFO: val_f_rmse: 0.032082
##### Step: 23 Learning rate: 0.005 #####
2025-06-11 11:44:53.401 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 1.0082, Val Loss: 1.1438
2025-06-11 11:44:53.402 INFO: Epoch 24, Train Loss: 1.0082, Val Loss: 1.1438
train_e/atom_mae: 0.002707
2025-06-11 11:44:53.405 INFO: train_e/atom_mae: 0.002707
train_e/atom_rmse: 0.003349
2025-06-11 11:44:53.406 INFO: train_e/atom_rmse: 0.003349
train_f_mae: 0.023954
2025-06-11 11:44:53.412 INFO: train_f_mae: 0.023954
train_f_rmse: 0.031095
2025-06-11 11:44:53.413 INFO: train_f_rmse: 0.031095
val_e/atom_mae: 0.006168
2025-06-11 11:44:53.418 INFO: val_e/atom_mae: 0.006168
val_e/atom_rmse: 0.006175
2025-06-11 11:44:53.418 INFO: val_e/atom_rmse: 0.006175
val_f_mae: 0.024450
2025-06-11 11:44:53.420 INFO: val_f_mae: 0.024450
val_f_rmse: 0.031674
2025-06-11 11:44:53.421 INFO: val_f_rmse: 0.031674
##### Step: 24 Learning rate: 0.005 #####
2025-06-11 11:47:08.368 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 1.0126, Val Loss: 0.9814
2025-06-11 11:47:08.370 INFO: Epoch 25, Train Loss: 1.0126, Val Loss: 0.9814
train_e/atom_mae: 0.005023
2025-06-11 11:47:08.372 INFO: train_e/atom_mae: 0.005023
train_e/atom_rmse: 0.005714
2025-06-11 11:47:08.373 INFO: train_e/atom_rmse: 0.005714
train_f_mae: 0.023037
2025-06-11 11:47:08.379 INFO: train_f_mae: 0.023037
train_f_rmse: 0.029870
2025-06-11 11:47:08.380 INFO: train_f_rmse: 0.029870
val_e/atom_mae: 0.003835
2025-06-11 11:47:08.385 INFO: val_e/atom_mae: 0.003835
val_e/atom_rmse: 0.003854
2025-06-11 11:47:08.386 INFO: val_e/atom_rmse: 0.003854
val_f_mae: 0.023694
2025-06-11 11:47:08.387 INFO: val_f_mae: 0.023694
val_f_rmse: 0.030442
2025-06-11 11:47:08.388 INFO: val_f_rmse: 0.030442
##### Step: 25 Learning rate: 0.005 #####
2025-06-11 11:49:23.201 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 1.1576, Val Loss: 1.2002
2025-06-11 11:49:23.203 INFO: Epoch 26, Train Loss: 1.1576, Val Loss: 1.2002
train_e/atom_mae: 0.003158
2025-06-11 11:49:23.205 INFO: train_e/atom_mae: 0.003158
train_e/atom_rmse: 0.003980
2025-06-11 11:49:23.206 INFO: train_e/atom_rmse: 0.003980
train_f_mae: 0.025490
2025-06-11 11:49:23.212 INFO: train_f_mae: 0.025490
train_f_rmse: 0.033155
2025-06-11 11:49:23.213 INFO: train_f_rmse: 0.033155
val_e/atom_mae: 0.003860
2025-06-11 11:49:23.218 INFO: val_e/atom_mae: 0.003860
val_e/atom_rmse: 0.003880
2025-06-11 11:49:23.219 INFO: val_e/atom_rmse: 0.003880
val_f_mae: 0.026473
2025-06-11 11:49:23.220 INFO: val_f_mae: 0.026473
val_f_rmse: 0.033833
2025-06-11 11:49:23.221 INFO: val_f_rmse: 0.033833
##### Step: 26 Learning rate: 0.005 #####
2025-06-11 11:51:37.991 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.9689, Val Loss: 1.0185
2025-06-11 11:51:37.993 INFO: Epoch 27, Train Loss: 0.9689, Val Loss: 1.0185
train_e/atom_mae: 0.003073
2025-06-11 11:51:37.995 INFO: train_e/atom_mae: 0.003073
train_e/atom_rmse: 0.003588
2025-06-11 11:51:37.996 INFO: train_e/atom_rmse: 0.003588
train_f_mae: 0.023386
2025-06-11 11:51:38.002 INFO: train_f_mae: 0.023386
train_f_rmse: 0.030356
2025-06-11 11:51:38.003 INFO: train_f_rmse: 0.030356
val_e/atom_mae: 0.007037
2025-06-11 11:51:38.008 INFO: val_e/atom_mae: 0.007037
val_e/atom_rmse: 0.007043
2025-06-11 11:51:38.009 INFO: val_e/atom_rmse: 0.007043
val_f_mae: 0.022313
2025-06-11 11:51:38.010 INFO: val_f_mae: 0.022313
val_f_rmse: 0.028907
2025-06-11 11:51:38.011 INFO: val_f_rmse: 0.028907
##### Step: 27 Learning rate: 0.005 #####
2025-06-11 11:53:52.664 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 1.0473, Val Loss: 1.0298
2025-06-11 11:53:52.665 INFO: Epoch 28, Train Loss: 1.0473, Val Loss: 1.0298
train_e/atom_mae: 0.003906
2025-06-11 11:53:52.667 INFO: train_e/atom_mae: 0.003906
train_e/atom_rmse: 0.005018
2025-06-11 11:53:52.668 INFO: train_e/atom_rmse: 0.005018
train_f_mae: 0.023855
2025-06-11 11:53:52.675 INFO: train_f_mae: 0.023855
train_f_rmse: 0.030895
2025-06-11 11:53:52.675 INFO: train_f_rmse: 0.030895
val_e/atom_mae: 0.004101
2025-06-11 11:53:52.680 INFO: val_e/atom_mae: 0.004101
val_e/atom_rmse: 0.004111
2025-06-11 11:53:52.681 INFO: val_e/atom_rmse: 0.004111
val_f_mae: 0.023911
2025-06-11 11:53:52.683 INFO: val_f_mae: 0.023911
val_f_rmse: 0.031105
2025-06-11 11:53:52.683 INFO: val_f_rmse: 0.031105
##### Step: 28 Learning rate: 0.005 #####
2025-06-11 11:56:07.408 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.8683, Val Loss: 0.8540
2025-06-11 11:56:07.411 INFO: Epoch 29, Train Loss: 0.8683, Val Loss: 0.8540
train_e/atom_mae: 0.002830
2025-06-11 11:56:07.413 INFO: train_e/atom_mae: 0.002830
train_e/atom_rmse: 0.003308
2025-06-11 11:56:07.414 INFO: train_e/atom_rmse: 0.003308
train_f_mae: 0.022141
2025-06-11 11:56:07.420 INFO: train_f_mae: 0.022141
train_f_rmse: 0.028774
2025-06-11 11:56:07.421 INFO: train_f_rmse: 0.028774
val_e/atom_mae: 0.003407
2025-06-11 11:56:07.426 INFO: val_e/atom_mae: 0.003407
val_e/atom_rmse: 0.003423
2025-06-11 11:56:07.427 INFO: val_e/atom_rmse: 0.003423
val_f_mae: 0.021975
2025-06-11 11:56:07.428 INFO: val_f_mae: 0.021975
val_f_rmse: 0.028475
2025-06-11 11:56:07.429 INFO: val_f_rmse: 0.028475
##### Step: 29 Learning rate: 0.005 #####
2025-06-11 11:58:22.195 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.8829, Val Loss: 0.9392
2025-06-11 11:58:22.196 INFO: Epoch 30, Train Loss: 0.8829, Val Loss: 0.9392
train_e/atom_mae: 0.002194
2025-06-11 11:58:22.198 INFO: train_e/atom_mae: 0.002194
train_e/atom_rmse: 0.003033
2025-06-11 11:58:22.199 INFO: train_e/atom_rmse: 0.003033
train_f_mae: 0.022466
2025-06-11 11:58:22.206 INFO: train_f_mae: 0.022466
train_f_rmse: 0.029137
2025-06-11 11:58:22.207 INFO: train_f_rmse: 0.029137
val_e/atom_mae: 0.000417
2025-06-11 11:58:22.211 INFO: val_e/atom_mae: 0.000417
val_e/atom_rmse: 0.000510
2025-06-11 11:58:22.212 INFO: val_e/atom_rmse: 0.000510
val_f_mae: 0.023548
2025-06-11 11:58:22.214 INFO: val_f_mae: 0.023548
val_f_rmse: 0.030631
2025-06-11 11:58:22.215 INFO: val_f_rmse: 0.030631
##### Step: 30 Learning rate: 0.005 #####
2025-06-11 12:00:37.011 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 1.0858, Val Loss: 0.8113
2025-06-11 12:00:37.013 INFO: Epoch 31, Train Loss: 1.0858, Val Loss: 0.8113
train_e/atom_mae: 0.003368
2025-06-11 12:00:37.016 INFO: train_e/atom_mae: 0.003368
train_e/atom_rmse: 0.004170
2025-06-11 12:00:37.017 INFO: train_e/atom_rmse: 0.004170
train_f_mae: 0.024767
2025-06-11 12:00:37.023 INFO: train_f_mae: 0.024767
train_f_rmse: 0.031964
2025-06-11 12:00:37.024 INFO: train_f_rmse: 0.031964
val_e/atom_mae: 0.003024
2025-06-11 12:00:37.028 INFO: val_e/atom_mae: 0.003024
val_e/atom_rmse: 0.003041
2025-06-11 12:00:37.030 INFO: val_e/atom_rmse: 0.003041
val_f_mae: 0.021563
2025-06-11 12:00:37.031 INFO: val_f_mae: 0.021563
val_f_rmse: 0.027878
2025-06-11 12:00:37.032 INFO: val_f_rmse: 0.027878
##### Step: 31 Learning rate: 0.005 #####
2025-06-11 12:02:51.732 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.8425, Val Loss: 1.3912
2025-06-11 12:02:51.734 INFO: Epoch 32, Train Loss: 0.8425, Val Loss: 1.3912
train_e/atom_mae: 0.002138
2025-06-11 12:02:51.736 INFO: train_e/atom_mae: 0.002138
train_e/atom_rmse: 0.002521
2025-06-11 12:02:51.737 INFO: train_e/atom_rmse: 0.002521
train_f_mae: 0.022029
2025-06-11 12:02:51.743 INFO: train_f_mae: 0.022029
train_f_rmse: 0.028620
2025-06-11 12:02:51.744 INFO: train_f_rmse: 0.028620
val_e/atom_mae: 0.003753
2025-06-11 12:02:51.749 INFO: val_e/atom_mae: 0.003753
val_e/atom_rmse: 0.003775
2025-06-11 12:02:51.750 INFO: val_e/atom_rmse: 0.003775
val_f_mae: 0.028190
2025-06-11 12:02:51.751 INFO: val_f_mae: 0.028190
val_f_rmse: 0.036587
2025-06-11 12:02:51.752 INFO: val_f_rmse: 0.036587
##### Step: 32 Learning rate: 0.005 #####
2025-06-11 12:05:06.509 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 1.0199, Val Loss: 0.7906
2025-06-11 12:05:06.510 INFO: Epoch 33, Train Loss: 1.0199, Val Loss: 0.7906
train_e/atom_mae: 0.003078
2025-06-11 12:05:06.513 INFO: train_e/atom_mae: 0.003078
train_e/atom_rmse: 0.003727
2025-06-11 12:05:06.514 INFO: train_e/atom_rmse: 0.003727
train_f_mae: 0.023990
2025-06-11 12:05:06.520 INFO: train_f_mae: 0.023990
train_f_rmse: 0.031124
2025-06-11 12:05:06.521 INFO: train_f_rmse: 0.031124
val_e/atom_mae: 0.000558
2025-06-11 12:05:06.525 INFO: val_e/atom_mae: 0.000558
val_e/atom_rmse: 0.000615
2025-06-11 12:05:06.527 INFO: val_e/atom_rmse: 0.000615
val_f_mae: 0.021761
2025-06-11 12:05:06.528 INFO: val_f_mae: 0.021761
val_f_rmse: 0.028092
2025-06-11 12:05:06.529 INFO: val_f_rmse: 0.028092
##### Step: 33 Learning rate: 0.005 #####
2025-06-11 12:07:21.229 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.8408, Val Loss: 0.7265
2025-06-11 12:07:21.232 INFO: Epoch 34, Train Loss: 0.8408, Val Loss: 0.7265
train_e/atom_mae: 0.002474
2025-06-11 12:07:21.234 INFO: train_e/atom_mae: 0.002474
train_e/atom_rmse: 0.003037
2025-06-11 12:07:21.235 INFO: train_e/atom_rmse: 0.003037
train_f_mae: 0.021873
2025-06-11 12:07:21.241 INFO: train_f_mae: 0.021873
train_f_rmse: 0.028404
2025-06-11 12:07:21.242 INFO: train_f_rmse: 0.028404
val_e/atom_mae: 0.000705
2025-06-11 12:07:21.247 INFO: val_e/atom_mae: 0.000705
val_e/atom_rmse: 0.000754
2025-06-11 12:07:21.248 INFO: val_e/atom_rmse: 0.000754
val_f_mae: 0.020724
2025-06-11 12:07:21.249 INFO: val_f_mae: 0.020724
val_f_rmse: 0.026916
2025-06-11 12:07:21.250 INFO: val_f_rmse: 0.026916
##### Step: 34 Learning rate: 0.005 #####
2025-06-11 12:09:35.908 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.8374, Val Loss: 0.8629
2025-06-11 12:09:35.910 INFO: Epoch 35, Train Loss: 0.8374, Val Loss: 0.8629
train_e/atom_mae: 0.003606
2025-06-11 12:09:35.913 INFO: train_e/atom_mae: 0.003606
train_e/atom_rmse: 0.004245
2025-06-11 12:09:35.914 INFO: train_e/atom_rmse: 0.004245
train_f_mae: 0.021401
2025-06-11 12:09:35.920 INFO: train_f_mae: 0.021401
train_f_rmse: 0.027767
2025-06-11 12:09:35.921 INFO: train_f_rmse: 0.027767
val_e/atom_mae: 0.004276
2025-06-11 12:09:35.925 INFO: val_e/atom_mae: 0.004276
val_e/atom_rmse: 0.004286
2025-06-11 12:09:35.927 INFO: val_e/atom_rmse: 0.004286
val_f_mae: 0.021784
2025-06-11 12:09:35.928 INFO: val_f_mae: 0.021784
val_f_rmse: 0.028199
2025-06-11 12:09:35.929 INFO: val_f_rmse: 0.028199
##### Step: 35 Learning rate: 0.005 #####
2025-06-11 12:11:50.797 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.8513, Val Loss: 0.8427
2025-06-11 12:11:50.798 INFO: Epoch 36, Train Loss: 0.8513, Val Loss: 0.8427
train_e/atom_mae: 0.003916
2025-06-11 12:11:50.801 INFO: train_e/atom_mae: 0.003916
train_e/atom_rmse: 0.004679
2025-06-11 12:11:50.802 INFO: train_e/atom_rmse: 0.004679
train_f_mae: 0.021395
2025-06-11 12:11:50.808 INFO: train_f_mae: 0.021395
train_f_rmse: 0.027759
2025-06-11 12:11:50.809 INFO: train_f_rmse: 0.027759
val_e/atom_mae: 0.005112
2025-06-11 12:11:50.814 INFO: val_e/atom_mae: 0.005112
val_e/atom_rmse: 0.005139
2025-06-11 12:11:50.815 INFO: val_e/atom_rmse: 0.005139
val_f_mae: 0.020948
2025-06-11 12:11:50.816 INFO: val_f_mae: 0.020948
val_f_rmse: 0.027302
2025-06-11 12:11:50.817 INFO: val_f_rmse: 0.027302
##### Step: 36 Learning rate: 0.005 #####
2025-06-11 12:14:05.542 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.9898, Val Loss: 0.8551
2025-06-11 12:14:05.545 INFO: Epoch 37, Train Loss: 0.9898, Val Loss: 0.8551
train_e/atom_mae: 0.003172
2025-06-11 12:14:05.547 INFO: train_e/atom_mae: 0.003172
train_e/atom_rmse: 0.003840
2025-06-11 12:14:05.548 INFO: train_e/atom_rmse: 0.003840
train_f_mae: 0.023692
2025-06-11 12:14:05.554 INFO: train_f_mae: 0.023692
train_f_rmse: 0.030586
2025-06-11 12:14:05.555 INFO: train_f_rmse: 0.030586
val_e/atom_mae: 0.003501
2025-06-11 12:14:05.560 INFO: val_e/atom_mae: 0.003501
val_e/atom_rmse: 0.003512
2025-06-11 12:14:05.561 INFO: val_e/atom_rmse: 0.003512
val_f_mae: 0.021907
2025-06-11 12:14:05.562 INFO: val_f_mae: 0.021907
val_f_rmse: 0.028453
2025-06-11 12:14:05.563 INFO: val_f_rmse: 0.028453
##### Step: 37 Learning rate: 0.005 #####
2025-06-11 12:16:20.414 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.8133, Val Loss: 0.7448
2025-06-11 12:16:20.416 INFO: Epoch 38, Train Loss: 0.8133, Val Loss: 0.7448
train_e/atom_mae: 0.002344
2025-06-11 12:16:20.419 INFO: train_e/atom_mae: 0.002344
train_e/atom_rmse: 0.002879
2025-06-11 12:16:20.420 INFO: train_e/atom_rmse: 0.002879
train_f_mae: 0.021623
2025-06-11 12:16:20.426 INFO: train_f_mae: 0.021623
train_f_rmse: 0.027978
2025-06-11 12:16:20.427 INFO: train_f_rmse: 0.027978
val_e/atom_mae: 0.000937
2025-06-11 12:16:20.432 INFO: val_e/atom_mae: 0.000937
val_e/atom_rmse: 0.000975
2025-06-11 12:16:20.433 INFO: val_e/atom_rmse: 0.000975
val_f_mae: 0.020881
2025-06-11 12:16:20.434 INFO: val_f_mae: 0.020881
val_f_rmse: 0.027226
2025-06-11 12:16:20.435 INFO: val_f_rmse: 0.027226
##### Step: 38 Learning rate: 0.005 #####
2025-06-11 12:18:35.209 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.8172, Val Loss: 0.7040
2025-06-11 12:18:35.212 INFO: Epoch 39, Train Loss: 0.8172, Val Loss: 0.7040
train_e/atom_mae: 0.001714
2025-06-11 12:18:35.214 INFO: train_e/atom_mae: 0.001714
train_e/atom_rmse: 0.002134
2025-06-11 12:18:35.215 INFO: train_e/atom_rmse: 0.002134
train_f_mae: 0.021897
2025-06-11 12:18:35.221 INFO: train_f_mae: 0.021897
train_f_rmse: 0.028291
2025-06-11 12:18:35.222 INFO: train_f_rmse: 0.028291
val_e/atom_mae: 0.003617
2025-06-11 12:18:35.227 INFO: val_e/atom_mae: 0.003617
val_e/atom_rmse: 0.003631
2025-06-11 12:18:35.228 INFO: val_e/atom_rmse: 0.003631
val_f_mae: 0.019682
2025-06-11 12:18:35.229 INFO: val_f_mae: 0.019682
val_f_rmse: 0.025600
2025-06-11 12:18:35.230 INFO: val_f_rmse: 0.025600
##### Step: 39 Learning rate: 0.005 #####
2025-06-11 12:20:49.970 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.9832, Val Loss: 0.8838
2025-06-11 12:20:49.972 INFO: Epoch 40, Train Loss: 0.9832, Val Loss: 0.8838
train_e/atom_mae: 0.006288
2025-06-11 12:20:49.974 INFO: train_e/atom_mae: 0.006288
train_e/atom_rmse: 0.007248
2025-06-11 12:20:49.975 INFO: train_e/atom_rmse: 0.007248
train_f_mae: 0.021731
2025-06-11 12:20:49.981 INFO: train_f_mae: 0.021731
train_f_rmse: 0.028099
2025-06-11 12:20:49.982 INFO: train_f_rmse: 0.028099
val_e/atom_mae: 0.007133
2025-06-11 12:20:49.987 INFO: val_e/atom_mae: 0.007133
val_e/atom_rmse: 0.007139
2025-06-11 12:20:49.988 INFO: val_e/atom_rmse: 0.007139
val_f_mae: 0.020252
2025-06-11 12:20:49.989 INFO: val_f_mae: 0.020252
val_f_rmse: 0.026380
2025-06-11 12:20:49.990 INFO: val_f_rmse: 0.026380
2025-06-11 12:20:50.027 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-11 12:23:04.745 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 1.2628, Val Loss: 0.6919
2025-06-11 12:23:04.746 INFO: Epoch 1, Train Loss: 1.2628, Val Loss: 0.6919
train_e/atom_mae: 0.006603
2025-06-11 12:23:04.749 INFO: train_e/atom_mae: 0.006603
train_e/atom_rmse: 0.010347
2025-06-11 12:23:04.750 INFO: train_e/atom_rmse: 0.010347
train_f_mae: 0.022319
2025-06-11 12:23:04.756 INFO: train_f_mae: 0.022319
train_f_rmse: 0.029463
2025-06-11 12:23:04.757 INFO: train_f_rmse: 0.029463
val_e/atom_mae: 0.002440
2025-06-11 12:23:04.762 INFO: val_e/atom_mae: 0.002440
val_e/atom_rmse: 0.002459
2025-06-11 12:23:04.762 INFO: val_e/atom_rmse: 0.002459
val_f_mae: 0.019903
2025-06-11 12:23:04.764 INFO: val_f_mae: 0.019903
val_f_rmse: 0.025877
2025-06-11 12:23:04.765 INFO: val_f_rmse: 0.025877
##### Step: 1 Learning rate: 0.004 #####
2025-06-11 12:25:19.374 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 0.8533, Val Loss: 0.6897
2025-06-11 12:25:19.376 INFO: Epoch 2, Train Loss: 0.8533, Val Loss: 0.6897
train_e/atom_mae: 0.002208
2025-06-11 12:25:19.379 INFO: train_e/atom_mae: 0.002208
train_e/atom_rmse: 0.002659
2025-06-11 12:25:19.380 INFO: train_e/atom_rmse: 0.002659
train_f_mae: 0.022181
2025-06-11 12:25:19.386 INFO: train_f_mae: 0.022181
train_f_rmse: 0.028762
2025-06-11 12:25:19.387 INFO: train_f_rmse: 0.028762
val_e/atom_mae: 0.002222
2025-06-11 12:25:19.392 INFO: val_e/atom_mae: 0.002222
val_e/atom_rmse: 0.002248
2025-06-11 12:25:19.393 INFO: val_e/atom_rmse: 0.002248
val_f_mae: 0.019881
2025-06-11 12:25:19.394 INFO: val_f_mae: 0.019881
val_f_rmse: 0.025905
2025-06-11 12:25:19.395 INFO: val_f_rmse: 0.025905
##### Step: 2 Learning rate: 0.006 #####
2025-06-11 12:27:34.017 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 1.0415, Val Loss: 0.8852
2025-06-11 12:27:34.021 INFO: Epoch 3, Train Loss: 1.0415, Val Loss: 0.8852
train_e/atom_mae: 0.003004
2025-06-11 12:27:34.023 INFO: train_e/atom_mae: 0.003004
train_e/atom_rmse: 0.003767
2025-06-11 12:27:34.024 INFO: train_e/atom_rmse: 0.003767
train_f_mae: 0.024328
2025-06-11 12:27:34.030 INFO: train_f_mae: 0.024328
train_f_rmse: 0.031452
2025-06-11 12:27:34.031 INFO: train_f_rmse: 0.031452
val_e/atom_mae: 0.004117
2025-06-11 12:27:34.036 INFO: val_e/atom_mae: 0.004117
val_e/atom_rmse: 0.004127
2025-06-11 12:27:34.037 INFO: val_e/atom_rmse: 0.004127
val_f_mae: 0.022179
2025-06-11 12:27:34.038 INFO: val_f_mae: 0.022179
val_f_rmse: 0.028677
2025-06-11 12:27:34.039 INFO: val_f_rmse: 0.028677
##### Step: 3 Learning rate: 0.008 #####
2025-06-11 12:29:48.656 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 1.0932, Val Loss: 1.0335
2025-06-11 12:29:48.658 INFO: Epoch 4, Train Loss: 1.0932, Val Loss: 1.0335
train_e/atom_mae: 0.004031
2025-06-11 12:29:48.660 INFO: train_e/atom_mae: 0.004031
train_e/atom_rmse: 0.004919
2025-06-11 12:29:48.661 INFO: train_e/atom_rmse: 0.004919
train_f_mae: 0.024540
2025-06-11 12:29:48.667 INFO: train_f_mae: 0.024540
train_f_rmse: 0.031686
2025-06-11 12:29:48.668 INFO: train_f_rmse: 0.031686
val_e/atom_mae: 0.001644
2025-06-11 12:29:48.673 INFO: val_e/atom_mae: 0.001644
val_e/atom_rmse: 0.001671
2025-06-11 12:29:48.674 INFO: val_e/atom_rmse: 0.001671
val_f_mae: 0.024848
2025-06-11 12:29:48.675 INFO: val_f_mae: 0.024848
val_f_rmse: 0.031987
2025-06-11 12:29:48.676 INFO: val_f_rmse: 0.031987
##### Step: 4 Learning rate: 0.01 #####
2025-06-11 12:32:03.369 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 1.0725, Val Loss: 0.8828
2025-06-11 12:32:03.372 INFO: Epoch 5, Train Loss: 1.0725, Val Loss: 0.8828
train_e/atom_mae: 0.004575
2025-06-11 12:32:03.375 INFO: train_e/atom_mae: 0.004575
train_e/atom_rmse: 0.005600
2025-06-11 12:32:03.375 INFO: train_e/atom_rmse: 0.005600
train_f_mae: 0.023948
2025-06-11 12:32:03.382 INFO: train_f_mae: 0.023948
train_f_rmse: 0.030934
2025-06-11 12:32:03.383 INFO: train_f_rmse: 0.030934
val_e/atom_mae: 0.003113
2025-06-11 12:32:03.387 INFO: val_e/atom_mae: 0.003113
val_e/atom_rmse: 0.003143
2025-06-11 12:32:03.389 INFO: val_e/atom_rmse: 0.003143
val_f_mae: 0.022463
2025-06-11 12:32:03.390 INFO: val_f_mae: 0.022463
val_f_rmse: 0.029093
2025-06-11 12:32:03.391 INFO: val_f_rmse: 0.029093
##### Step: 5 Learning rate: 0.01 #####
2025-06-11 12:34:18.125 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 1.0003, Val Loss: 1.1460
2025-06-11 12:34:18.126 INFO: Epoch 6, Train Loss: 1.0003, Val Loss: 1.1460
train_e/atom_mae: 0.003796
2025-06-11 12:34:18.128 INFO: train_e/atom_mae: 0.003796
train_e/atom_rmse: 0.004636
2025-06-11 12:34:18.129 INFO: train_e/atom_rmse: 0.004636
train_f_mae: 0.023407
2025-06-11 12:34:18.135 INFO: train_f_mae: 0.023407
train_f_rmse: 0.030349
2025-06-11 12:34:18.136 INFO: train_f_rmse: 0.030349
val_e/atom_mae: 0.003584
2025-06-11 12:34:18.141 INFO: val_e/atom_mae: 0.003584
val_e/atom_rmse: 0.003635
2025-06-11 12:34:18.142 INFO: val_e/atom_rmse: 0.003635
val_f_mae: 0.025856
2025-06-11 12:34:18.143 INFO: val_f_mae: 0.025856
val_f_rmse: 0.033125
2025-06-11 12:34:18.144 INFO: val_f_rmse: 0.033125
##### Step: 6 Learning rate: 0.01 #####
2025-06-11 12:36:32.774 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 1.0977, Val Loss: 0.8629
2025-06-11 12:36:32.776 INFO: Epoch 7, Train Loss: 1.0977, Val Loss: 0.8629
train_e/atom_mae: 0.003828
2025-06-11 12:36:32.779 INFO: train_e/atom_mae: 0.003828
train_e/atom_rmse: 0.004747
2025-06-11 12:36:32.780 INFO: train_e/atom_rmse: 0.004747
train_f_mae: 0.024648
2025-06-11 12:36:32.786 INFO: train_f_mae: 0.024648
train_f_rmse: 0.031853
2025-06-11 12:36:32.787 INFO: train_f_rmse: 0.031853
val_e/atom_mae: 0.006674
2025-06-11 12:36:32.792 INFO: val_e/atom_mae: 0.006674
val_e/atom_rmse: 0.006679
2025-06-11 12:36:32.793 INFO: val_e/atom_rmse: 0.006679
val_f_mae: 0.020338
2025-06-11 12:36:32.794 INFO: val_f_mae: 0.020338
val_f_rmse: 0.026427
2025-06-11 12:36:32.795 INFO: val_f_rmse: 0.026427
##### Step: 7 Learning rate: 0.01 #####
2025-06-11 12:38:47.310 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 1.5139, Val Loss: 1.1735
2025-06-11 12:38:47.311 INFO: Epoch 8, Train Loss: 1.5139, Val Loss: 1.1735
train_e/atom_mae: 0.006710
2025-06-11 12:38:47.314 INFO: train_e/atom_mae: 0.006710
train_e/atom_rmse: 0.007683
2025-06-11 12:38:47.315 INFO: train_e/atom_rmse: 0.007683
train_f_mae: 0.028039
2025-06-11 12:38:47.321 INFO: train_f_mae: 0.028039
train_f_rmse: 0.036004
2025-06-11 12:38:47.322 INFO: train_f_rmse: 0.036004
val_e/atom_mae: 0.008560
2025-06-11 12:38:47.327 INFO: val_e/atom_mae: 0.008560
val_e/atom_rmse: 0.008565
2025-06-11 12:38:47.328 INFO: val_e/atom_rmse: 0.008565
val_f_mae: 0.023224
2025-06-11 12:38:47.329 INFO: val_f_mae: 0.023224
val_f_rmse: 0.030052
2025-06-11 12:38:47.330 INFO: val_f_rmse: 0.030052
##### Step: 8 Learning rate: 0.01 #####
2025-06-11 12:41:01.908 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 1.1154, Val Loss: 1.6389
2025-06-11 12:41:01.910 INFO: Epoch 9, Train Loss: 1.1154, Val Loss: 1.6389
train_e/atom_mae: 0.004773
2025-06-11 12:41:01.913 INFO: train_e/atom_mae: 0.004773
train_e/atom_rmse: 0.005664
2025-06-11 12:41:01.914 INFO: train_e/atom_rmse: 0.005664
train_f_mae: 0.024475
2025-06-11 12:41:01.920 INFO: train_f_mae: 0.024475
train_f_rmse: 0.031578
2025-06-11 12:41:01.921 INFO: train_f_rmse: 0.031578
val_e/atom_mae: 0.001313
2025-06-11 12:41:01.925 INFO: val_e/atom_mae: 0.001313
val_e/atom_rmse: 0.001339
2025-06-11 12:41:01.927 INFO: val_e/atom_rmse: 0.001339
val_f_mae: 0.032411
2025-06-11 12:41:01.928 INFO: val_f_mae: 0.032411
val_f_rmse: 0.040401
2025-06-11 12:41:01.929 INFO: val_f_rmse: 0.040401
##### Step: 9 Learning rate: 0.01 #####
2025-06-11 12:43:16.516 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 1.2477, Val Loss: 1.0575
2025-06-11 12:43:16.519 INFO: Epoch 10, Train Loss: 1.2477, Val Loss: 1.0575
train_e/atom_mae: 0.005472
2025-06-11 12:43:16.521 INFO: train_e/atom_mae: 0.005472
train_e/atom_rmse: 0.006316
2025-06-11 12:43:16.522 INFO: train_e/atom_rmse: 0.006316
train_f_mae: 0.025752
2025-06-11 12:43:16.528 INFO: train_f_mae: 0.025752
train_f_rmse: 0.033176
2025-06-11 12:43:16.529 INFO: train_f_rmse: 0.033176
val_e/atom_mae: 0.004620
2025-06-11 12:43:16.534 INFO: val_e/atom_mae: 0.004620
val_e/atom_rmse: 0.004637
2025-06-11 12:43:16.535 INFO: val_e/atom_rmse: 0.004637
val_f_mae: 0.024292
2025-06-11 12:43:16.536 INFO: val_f_mae: 0.024292
val_f_rmse: 0.031276
2025-06-11 12:43:16.537 INFO: val_f_rmse: 0.031276
##### Step: 10 Learning rate: 0.01 #####
2025-06-11 12:45:31.211 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 1.2548, Val Loss: 1.0056
2025-06-11 12:45:31.214 INFO: Epoch 11, Train Loss: 1.2548, Val Loss: 1.0056
train_e/atom_mae: 0.005457
2025-06-11 12:45:31.216 INFO: train_e/atom_mae: 0.005457
train_e/atom_rmse: 0.006335
2025-06-11 12:45:31.217 INFO: train_e/atom_rmse: 0.006335
train_f_mae: 0.025778
2025-06-11 12:45:31.223 INFO: train_f_mae: 0.025778
train_f_rmse: 0.033269
2025-06-11 12:45:31.224 INFO: train_f_rmse: 0.033269
val_e/atom_mae: 0.006705
2025-06-11 12:45:31.229 INFO: val_e/atom_mae: 0.006705
val_e/atom_rmse: 0.006721
2025-06-11 12:45:31.230 INFO: val_e/atom_rmse: 0.006721
val_f_mae: 0.022169
2025-06-11 12:45:31.231 INFO: val_f_mae: 0.022169
val_f_rmse: 0.028967
2025-06-11 12:45:31.232 INFO: val_f_rmse: 0.028967
##### Step: 11 Learning rate: 0.01 #####
2025-06-11 12:47:45.784 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 1.3793, Val Loss: 1.0823
2025-06-11 12:47:45.785 INFO: Epoch 12, Train Loss: 1.3793, Val Loss: 1.0823
train_e/atom_mae: 0.006495
2025-06-11 12:47:45.787 INFO: train_e/atom_mae: 0.006495
train_e/atom_rmse: 0.007798
2025-06-11 12:47:45.788 INFO: train_e/atom_rmse: 0.007798
train_f_mae: 0.026346
2025-06-11 12:47:45.794 INFO: train_f_mae: 0.026346
train_f_rmse: 0.033987
2025-06-11 12:47:45.795 INFO: train_f_rmse: 0.033987
val_e/atom_mae: 0.000881
2025-06-11 12:47:45.800 INFO: val_e/atom_mae: 0.000881
val_e/atom_rmse: 0.000931
2025-06-11 12:47:45.801 INFO: val_e/atom_rmse: 0.000931
val_f_mae: 0.025797
2025-06-11 12:47:45.802 INFO: val_f_mae: 0.025797
val_f_rmse: 0.032850
2025-06-11 12:47:45.803 INFO: val_f_rmse: 0.032850
##### Step: 12 Learning rate: 0.01 #####
2025-06-11 12:50:00.518 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 1.0905, Val Loss: 1.1558
2025-06-11 12:50:00.519 INFO: Epoch 13, Train Loss: 1.0905, Val Loss: 1.1558
train_e/atom_mae: 0.005651
2025-06-11 12:50:00.522 INFO: train_e/atom_mae: 0.005651
train_e/atom_rmse: 0.006718
2025-06-11 12:50:00.523 INFO: train_e/atom_rmse: 0.006718
train_f_mae: 0.023489
2025-06-11 12:50:00.529 INFO: train_f_mae: 0.023489
train_f_rmse: 0.030399
2025-06-11 12:50:00.530 INFO: train_f_rmse: 0.030399
val_e/atom_mae: 0.001881
2025-06-11 12:50:00.535 INFO: val_e/atom_mae: 0.001881
val_e/atom_rmse: 0.001913
2025-06-11 12:50:00.536 INFO: val_e/atom_rmse: 0.001913
val_f_mae: 0.026418
2025-06-11 12:50:00.537 INFO: val_f_mae: 0.026418
val_f_rmse: 0.033799
2025-06-11 12:50:00.538 INFO: val_f_rmse: 0.033799
##### Step: 13 Learning rate: 0.01 #####
2025-06-11 12:52:15.148 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 0.9256, Val Loss: 0.7417
2025-06-11 12:52:15.151 INFO: Epoch 14, Train Loss: 0.9256, Val Loss: 0.7417
train_e/atom_mae: 0.003627
2025-06-11 12:52:15.153 INFO: train_e/atom_mae: 0.003627
train_e/atom_rmse: 0.004482
2025-06-11 12:52:15.154 INFO: train_e/atom_rmse: 0.004482
train_f_mae: 0.022560
2025-06-11 12:52:15.160 INFO: train_f_mae: 0.022560
train_f_rmse: 0.029181
2025-06-11 12:52:15.161 INFO: train_f_rmse: 0.029181
val_e/atom_mae: 0.001124
2025-06-11 12:52:15.166 INFO: val_e/atom_mae: 0.001124
val_e/atom_rmse: 0.001164
2025-06-11 12:52:15.167 INFO: val_e/atom_rmse: 0.001164
val_f_mae: 0.020912
2025-06-11 12:52:15.168 INFO: val_f_mae: 0.020912
val_f_rmse: 0.027143
2025-06-11 12:52:15.169 INFO: val_f_rmse: 0.027143
##### Step: 14 Learning rate: 0.01 #####
2025-06-11 12:54:29.588 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 0.9429, Val Loss: 1.4400
2025-06-11 12:54:29.590 INFO: Epoch 15, Train Loss: 0.9429, Val Loss: 1.4400
train_e/atom_mae: 0.002300
2025-06-11 12:54:29.593 INFO: train_e/atom_mae: 0.002300
train_e/atom_rmse: 0.002881
2025-06-11 12:54:29.594 INFO: train_e/atom_rmse: 0.002881
train_f_mae: 0.023313
2025-06-11 12:54:29.600 INFO: train_f_mae: 0.023313
train_f_rmse: 0.030205
2025-06-11 12:54:29.601 INFO: train_f_rmse: 0.030205
val_e/atom_mae: 0.008283
2025-06-11 12:54:29.605 INFO: val_e/atom_mae: 0.008283
val_e/atom_rmse: 0.008297
2025-06-11 12:54:29.607 INFO: val_e/atom_rmse: 0.008297
val_f_mae: 0.026699
2025-06-11 12:54:29.608 INFO: val_f_mae: 0.026699
val_f_rmse: 0.034441
2025-06-11 12:54:29.609 INFO: val_f_rmse: 0.034441
##### Step: 15 Learning rate: 0.01 #####
2025-06-11 12:56:44.087 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 1.3516, Val Loss: 0.9230
2025-06-11 12:56:44.089 INFO: Epoch 16, Train Loss: 1.3516, Val Loss: 0.9230
train_e/atom_mae: 0.004704
2025-06-11 12:56:44.092 INFO: train_e/atom_mae: 0.004704
train_e/atom_rmse: 0.005877
2025-06-11 12:56:44.093 INFO: train_e/atom_rmse: 0.005877
train_f_mae: 0.026922
2025-06-11 12:56:44.099 INFO: train_f_mae: 0.026922
train_f_rmse: 0.034990
2025-06-11 12:56:44.100 INFO: train_f_rmse: 0.034990
val_e/atom_mae: 0.003752
2025-06-11 12:56:44.104 INFO: val_e/atom_mae: 0.003752
val_e/atom_rmse: 0.003780
2025-06-11 12:56:44.106 INFO: val_e/atom_rmse: 0.003780
val_f_mae: 0.022752
2025-06-11 12:56:44.107 INFO: val_f_mae: 0.022752
val_f_rmse: 0.029501
2025-06-11 12:56:44.108 INFO: val_f_rmse: 0.029501
##### Step: 16 Learning rate: 0.01 #####
2025-06-11 12:58:58.586 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 1.0331, Val Loss: 0.7205
2025-06-11 12:58:58.587 INFO: Epoch 17, Train Loss: 1.0331, Val Loss: 0.7205
train_e/atom_mae: 0.004393
2025-06-11 12:58:58.589 INFO: train_e/atom_mae: 0.004393
train_e/atom_rmse: 0.005350
2025-06-11 12:58:58.590 INFO: train_e/atom_rmse: 0.005350
train_f_mae: 0.023595
2025-06-11 12:58:58.596 INFO: train_f_mae: 0.023595
train_f_rmse: 0.030457
2025-06-11 12:58:58.597 INFO: train_f_rmse: 0.030457
val_e/atom_mae: 0.001394
2025-06-11 12:58:58.602 INFO: val_e/atom_mae: 0.001394
val_e/atom_rmse: 0.001423
2025-06-11 12:58:58.603 INFO: val_e/atom_rmse: 0.001423
val_f_mae: 0.020559
2025-06-11 12:58:58.604 INFO: val_f_mae: 0.020559
val_f_rmse: 0.026704
2025-06-11 12:58:58.605 INFO: val_f_rmse: 0.026704
##### Step: 17 Learning rate: 0.01 #####
2025-06-11 13:01:13.097 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 1.3785, Val Loss: 1.4324
2025-06-11 13:01:13.098 INFO: Epoch 18, Train Loss: 1.3785, Val Loss: 1.4324
train_e/atom_mae: 0.005702
2025-06-11 13:01:13.101 INFO: train_e/atom_mae: 0.005702
train_e/atom_rmse: 0.006637
2025-06-11 13:01:13.101 INFO: train_e/atom_rmse: 0.006637
train_f_mae: 0.026956
2025-06-11 13:01:13.107 INFO: train_f_mae: 0.026956
train_f_rmse: 0.034873
2025-06-11 13:01:13.108 INFO: train_f_rmse: 0.034873
val_e/atom_mae: 0.005788
2025-06-11 13:01:13.113 INFO: val_e/atom_mae: 0.005788
val_e/atom_rmse: 0.005795
2025-06-11 13:01:13.114 INFO: val_e/atom_rmse: 0.005795
val_f_mae: 0.027956
2025-06-11 13:01:13.116 INFO: val_f_mae: 0.027956
val_f_rmse: 0.036174
2025-06-11 13:01:13.117 INFO: val_f_rmse: 0.036174
##### Step: 18 Learning rate: 0.01 #####
2025-06-11 13:03:27.496 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 1.3995, Val Loss: 1.8758
2025-06-11 13:03:27.498 INFO: Epoch 19, Train Loss: 1.3995, Val Loss: 1.8758
train_e/atom_mae: 0.006364
2025-06-11 13:03:27.501 INFO: train_e/atom_mae: 0.006364
train_e/atom_rmse: 0.007230
2025-06-11 13:03:27.502 INFO: train_e/atom_rmse: 0.007230
train_f_mae: 0.027035
2025-06-11 13:03:27.508 INFO: train_f_mae: 0.027035
train_f_rmse: 0.034739
2025-06-11 13:03:27.509 INFO: train_f_rmse: 0.034739
val_e/atom_mae: 0.003106
2025-06-11 13:03:27.514 INFO: val_e/atom_mae: 0.003106
val_e/atom_rmse: 0.003121
2025-06-11 13:03:27.515 INFO: val_e/atom_rmse: 0.003121
val_f_mae: 0.034451
2025-06-11 13:03:27.516 INFO: val_f_mae: 0.034451
val_f_rmse: 0.042893
2025-06-11 13:03:27.517 INFO: val_f_rmse: 0.042893
##### Step: 19 Learning rate: 0.01 #####
2025-06-11 13:05:41.963 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 1.2169, Val Loss: 1.1878
2025-06-11 13:05:41.965 INFO: Epoch 20, Train Loss: 1.2169, Val Loss: 1.1878
train_e/atom_mae: 0.005851
2025-06-11 13:05:41.968 INFO: train_e/atom_mae: 0.005851
train_e/atom_rmse: 0.006684
2025-06-11 13:05:41.969 INFO: train_e/atom_rmse: 0.006684
train_f_mae: 0.025225
2025-06-11 13:05:41.975 INFO: train_f_mae: 0.025225
train_f_rmse: 0.032438
2025-06-11 13:05:41.976 INFO: train_f_rmse: 0.032438
val_e/atom_mae: 0.008895
2025-06-11 13:05:41.980 INFO: val_e/atom_mae: 0.008895
val_e/atom_rmse: 0.008903
2025-06-11 13:05:41.982 INFO: val_e/atom_rmse: 0.008903
val_f_mae: 0.023074
2025-06-11 13:05:41.983 INFO: val_f_mae: 0.023074
val_f_rmse: 0.029927
2025-06-11 13:05:41.984 INFO: val_f_rmse: 0.029927
##### Step: 20 Learning rate: 0.005 #####
2025-06-11 13:07:56.464 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 1.0258, Val Loss: 0.8615
2025-06-11 13:07:56.465 INFO: Epoch 21, Train Loss: 1.0258, Val Loss: 0.8615
train_e/atom_mae: 0.004992
2025-06-11 13:07:56.468 INFO: train_e/atom_mae: 0.004992
train_e/atom_rmse: 0.005815
2025-06-11 13:07:56.469 INFO: train_e/atom_rmse: 0.005815
train_f_mae: 0.023297
2025-06-11 13:07:56.475 INFO: train_f_mae: 0.023297
train_f_rmse: 0.030019
2025-06-11 13:07:56.476 INFO: train_f_rmse: 0.030019
val_e/atom_mae: 0.001443
2025-06-11 13:07:56.481 INFO: val_e/atom_mae: 0.001443
val_e/atom_rmse: 0.001508
2025-06-11 13:07:56.482 INFO: val_e/atom_rmse: 0.001508
val_f_mae: 0.022759
2025-06-11 13:07:56.483 INFO: val_f_mae: 0.022759
val_f_rmse: 0.029208
2025-06-11 13:07:56.484 INFO: val_f_rmse: 0.029208
##### Step: 21 Learning rate: 0.005 #####
2025-06-11 13:10:11.086 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.7968, Val Loss: 0.9032
2025-06-11 13:10:11.087 INFO: Epoch 22, Train Loss: 0.7968, Val Loss: 0.9032
train_e/atom_mae: 0.001809
2025-06-11 13:10:11.089 INFO: train_e/atom_mae: 0.001809
train_e/atom_rmse: 0.002503
2025-06-11 13:10:11.090 INFO: train_e/atom_rmse: 0.002503
train_f_mae: 0.021472
2025-06-11 13:10:11.096 INFO: train_f_mae: 0.021472
train_f_rmse: 0.027816
2025-06-11 13:10:11.097 INFO: train_f_rmse: 0.027816
val_e/atom_mae: 0.004617
2025-06-11 13:10:11.102 INFO: val_e/atom_mae: 0.004617
val_e/atom_rmse: 0.004632
2025-06-11 13:10:11.103 INFO: val_e/atom_rmse: 0.004632
val_f_mae: 0.022174
2025-06-11 13:10:11.104 INFO: val_f_mae: 0.022174
val_f_rmse: 0.028707
2025-06-11 13:10:11.105 INFO: val_f_rmse: 0.028707
##### Step: 22 Learning rate: 0.005 #####
2025-06-11 13:12:25.733 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.7279, Val Loss: 0.7932
2025-06-11 13:12:25.735 INFO: Epoch 23, Train Loss: 0.7279, Val Loss: 0.7932
train_e/atom_mae: 0.002105
2025-06-11 13:12:25.738 INFO: train_e/atom_mae: 0.002105
train_e/atom_rmse: 0.002622
2025-06-11 13:12:25.739 INFO: train_e/atom_rmse: 0.002622
train_f_mae: 0.020443
2025-06-11 13:12:25.745 INFO: train_f_mae: 0.020443
train_f_rmse: 0.026505
2025-06-11 13:12:25.746 INFO: train_f_rmse: 0.026505
val_e/atom_mae: 0.004426
2025-06-11 13:12:25.750 INFO: val_e/atom_mae: 0.004426
val_e/atom_rmse: 0.004440
2025-06-11 13:12:25.752 INFO: val_e/atom_rmse: 0.004440
val_f_mae: 0.020833
2025-06-11 13:12:25.753 INFO: val_f_mae: 0.020833
val_f_rmse: 0.026843
2025-06-11 13:12:25.754 INFO: val_f_rmse: 0.026843
##### Step: 23 Learning rate: 0.005 #####
2025-06-11 13:14:40.379 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.7320, Val Loss: 0.8456
2025-06-11 13:14:40.380 INFO: Epoch 24, Train Loss: 0.7320, Val Loss: 0.8456
train_e/atom_mae: 0.002934
2025-06-11 13:14:40.382 INFO: train_e/atom_mae: 0.002934
train_e/atom_rmse: 0.003554
2025-06-11 13:14:40.383 INFO: train_e/atom_rmse: 0.003554
train_f_mae: 0.020166
2025-06-11 13:14:40.389 INFO: train_f_mae: 0.020166
train_f_rmse: 0.026181
2025-06-11 13:14:40.390 INFO: train_f_rmse: 0.026181
val_e/atom_mae: 0.004561
2025-06-11 13:14:40.395 INFO: val_e/atom_mae: 0.004561
val_e/atom_rmse: 0.004587
2025-06-11 13:14:40.396 INFO: val_e/atom_rmse: 0.004587
val_f_mae: 0.021318
2025-06-11 13:14:40.397 INFO: val_f_mae: 0.021318
val_f_rmse: 0.027713
2025-06-11 13:14:40.398 INFO: val_f_rmse: 0.027713
##### Step: 24 Learning rate: 0.005 #####
2025-06-11 13:16:55.173 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.8683, Val Loss: 0.8405
2025-06-11 13:16:55.176 INFO: Epoch 25, Train Loss: 0.8683, Val Loss: 0.8405
train_e/atom_mae: 0.003136
2025-06-11 13:16:55.178 INFO: train_e/atom_mae: 0.003136
train_e/atom_rmse: 0.003848
2025-06-11 13:16:55.179 INFO: train_e/atom_rmse: 0.003848
train_f_mae: 0.022022
2025-06-11 13:16:55.185 INFO: train_f_mae: 0.022022
train_f_rmse: 0.028526
2025-06-11 13:16:55.186 INFO: train_f_rmse: 0.028526
val_e/atom_mae: 0.003337
2025-06-11 13:16:55.191 INFO: val_e/atom_mae: 0.003337
val_e/atom_rmse: 0.003346
2025-06-11 13:16:55.192 INFO: val_e/atom_rmse: 0.003346
val_f_mae: 0.022199
2025-06-11 13:16:55.193 INFO: val_f_mae: 0.022199
val_f_rmse: 0.028271
2025-06-11 13:16:55.194 INFO: val_f_rmse: 0.028271
##### Step: 25 Learning rate: 0.005 #####
2025-06-11 13:19:09.887 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.8941, Val Loss: 0.8142
2025-06-11 13:19:09.888 INFO: Epoch 26, Train Loss: 0.8941, Val Loss: 0.8142
train_e/atom_mae: 0.003359
2025-06-11 13:19:09.890 INFO: train_e/atom_mae: 0.003359
train_e/atom_rmse: 0.004187
2025-06-11 13:19:09.891 INFO: train_e/atom_rmse: 0.004187
train_f_mae: 0.022361
2025-06-11 13:19:09.897 INFO: train_f_mae: 0.022361
train_f_rmse: 0.028801
2025-06-11 13:19:09.898 INFO: train_f_rmse: 0.028801
val_e/atom_mae: 0.003095
2025-06-11 13:19:09.903 INFO: val_e/atom_mae: 0.003095
val_e/atom_rmse: 0.003107
2025-06-11 13:19:09.904 INFO: val_e/atom_rmse: 0.003107
val_f_mae: 0.021591
2025-06-11 13:19:09.906 INFO: val_f_mae: 0.021591
val_f_rmse: 0.027903
2025-06-11 13:19:09.906 INFO: val_f_rmse: 0.027903
##### Step: 26 Learning rate: 0.005 #####
2025-06-11 13:21:24.619 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.9716, Val Loss: 1.1416
2025-06-11 13:21:24.620 INFO: Epoch 27, Train Loss: 0.9716, Val Loss: 1.1416
train_e/atom_mae: 0.004628
2025-06-11 13:21:24.623 INFO: train_e/atom_mae: 0.004628
train_e/atom_rmse: 0.005478
2025-06-11 13:21:24.624 INFO: train_e/atom_rmse: 0.005478
train_f_mae: 0.022837
2025-06-11 13:21:24.630 INFO: train_f_mae: 0.022837
train_f_rmse: 0.029342
2025-06-11 13:21:24.631 INFO: train_f_rmse: 0.029342
val_e/atom_mae: 0.005518
2025-06-11 13:21:24.635 INFO: val_e/atom_mae: 0.005518
val_e/atom_rmse: 0.005525
2025-06-11 13:21:24.637 INFO: val_e/atom_rmse: 0.005525
val_f_mae: 0.025208
2025-06-11 13:21:24.638 INFO: val_f_mae: 0.025208
val_f_rmse: 0.032079
2025-06-11 13:21:24.639 INFO: val_f_rmse: 0.032079
##### Step: 27 Learning rate: 0.005 #####
2025-06-11 13:23:39.329 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.7816, Val Loss: 0.7931
2025-06-11 13:23:39.331 INFO: Epoch 28, Train Loss: 0.7816, Val Loss: 0.7931
train_e/atom_mae: 0.003395
2025-06-11 13:23:39.334 INFO: train_e/atom_mae: 0.003395
train_e/atom_rmse: 0.003993
2025-06-11 13:23:39.335 INFO: train_e/atom_rmse: 0.003993
train_f_mae: 0.020813
2025-06-11 13:23:39.341 INFO: train_f_mae: 0.020813
train_f_rmse: 0.026885
2025-06-11 13:23:39.342 INFO: train_f_rmse: 0.026885
val_e/atom_mae: 0.005481
2025-06-11 13:23:39.346 INFO: val_e/atom_mae: 0.005481
val_e/atom_rmse: 0.005501
2025-06-11 13:23:39.348 INFO: val_e/atom_rmse: 0.005501
val_f_mae: 0.020014
2025-06-11 13:23:39.349 INFO: val_f_mae: 0.020014
val_f_rmse: 0.026106
2025-06-11 13:23:39.350 INFO: val_f_rmse: 0.026106
##### Step: 28 Learning rate: 0.005 #####
2025-06-11 13:25:54.141 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.7728, Val Loss: 0.7953
2025-06-11 13:25:54.143 INFO: Epoch 29, Train Loss: 0.7728, Val Loss: 0.7953
train_e/atom_mae: 0.004333
2025-06-11 13:25:54.146 INFO: train_e/atom_mae: 0.004333
train_e/atom_rmse: 0.005298
2025-06-11 13:25:54.147 INFO: train_e/atom_rmse: 0.005298
train_f_mae: 0.020000
2025-06-11 13:25:54.153 INFO: train_f_mae: 0.020000
train_f_rmse: 0.025871
2025-06-11 13:25:54.154 INFO: train_f_rmse: 0.025871
val_e/atom_mae: 0.001920
2025-06-11 13:25:54.158 INFO: val_e/atom_mae: 0.001920
val_e/atom_rmse: 0.001938
2025-06-11 13:25:54.160 INFO: val_e/atom_rmse: 0.001938
val_f_mae: 0.021639
2025-06-11 13:25:54.161 INFO: val_f_mae: 0.021639
val_f_rmse: 0.027955
2025-06-11 13:25:54.162 INFO: val_f_rmse: 0.027955
##### Step: 29 Learning rate: 0.005 #####
2025-06-11 13:28:08.875 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.6887, Val Loss: 0.6828
2025-06-11 13:28:08.877 INFO: Epoch 30, Train Loss: 0.6887, Val Loss: 0.6828
train_e/atom_mae: 0.002511
2025-06-11 13:28:08.879 INFO: train_e/atom_mae: 0.002511
train_e/atom_rmse: 0.002973
2025-06-11 13:28:08.880 INFO: train_e/atom_rmse: 0.002973
train_f_mae: 0.019791
2025-06-11 13:28:08.886 INFO: train_f_mae: 0.019791
train_f_rmse: 0.025615
2025-06-11 13:28:08.887 INFO: train_f_rmse: 0.025615
val_e/atom_mae: 0.002254
2025-06-11 13:28:08.892 INFO: val_e/atom_mae: 0.002254
val_e/atom_rmse: 0.002266
2025-06-11 13:28:08.893 INFO: val_e/atom_rmse: 0.002266
val_f_mae: 0.020029
2025-06-11 13:28:08.894 INFO: val_f_mae: 0.020029
val_f_rmse: 0.025765
2025-06-11 13:28:08.895 INFO: val_f_rmse: 0.025765
##### Step: 30 Learning rate: 0.005 #####
2025-06-11 13:30:23.681 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.6963, Val Loss: 0.6196
2025-06-11 13:30:23.683 INFO: Epoch 31, Train Loss: 0.6963, Val Loss: 0.6196
train_e/atom_mae: 0.001543
2025-06-11 13:30:23.686 INFO: train_e/atom_mae: 0.001543
train_e/atom_rmse: 0.001833
2025-06-11 13:30:23.687 INFO: train_e/atom_rmse: 0.001833
train_f_mae: 0.020204
2025-06-11 13:30:23.693 INFO: train_f_mae: 0.020204
train_f_rmse: 0.026152
2025-06-11 13:30:23.694 INFO: train_f_rmse: 0.026152
val_e/atom_mae: 0.002003
2025-06-11 13:30:23.698 INFO: val_e/atom_mae: 0.002003
val_e/atom_rmse: 0.002024
2025-06-11 13:30:23.699 INFO: val_e/atom_rmse: 0.002024
val_f_mae: 0.018997
2025-06-11 13:30:23.701 INFO: val_f_mae: 0.018997
val_f_rmse: 0.024587
2025-06-11 13:30:23.702 INFO: val_f_rmse: 0.024587
##### Step: 31 Learning rate: 0.005 #####
2025-06-11 13:32:38.389 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.6942, Val Loss: 0.7344
2025-06-11 13:32:38.391 INFO: Epoch 32, Train Loss: 0.6942, Val Loss: 0.7344
train_e/atom_mae: 0.001432
2025-06-11 13:32:38.393 INFO: train_e/atom_mae: 0.001432
train_e/atom_rmse: 0.001768
2025-06-11 13:32:38.394 INFO: train_e/atom_rmse: 0.001768
train_f_mae: 0.020169
2025-06-11 13:32:38.401 INFO: train_f_mae: 0.020169
train_f_rmse: 0.026129
2025-06-11 13:32:38.401 INFO: train_f_rmse: 0.026129
val_e/atom_mae: 0.000348
2025-06-11 13:32:38.406 INFO: val_e/atom_mae: 0.000348
val_e/atom_rmse: 0.000408
2025-06-11 13:32:38.407 INFO: val_e/atom_rmse: 0.000408
val_f_mae: 0.021146
2025-06-11 13:32:38.409 INFO: val_f_mae: 0.021146
val_f_rmse: 0.027088
2025-06-11 13:32:38.409 INFO: val_f_rmse: 0.027088
##### Step: 32 Learning rate: 0.005 #####
2025-06-11 13:34:53.006 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.9277, Val Loss: 0.7030
2025-06-11 13:34:53.007 INFO: Epoch 33, Train Loss: 0.9277, Val Loss: 0.7030
train_e/atom_mae: 0.004543
2025-06-11 13:34:53.010 INFO: train_e/atom_mae: 0.004543
train_e/atom_rmse: 0.005265
2025-06-11 13:34:53.011 INFO: train_e/atom_rmse: 0.005265
train_f_mae: 0.022324
2025-06-11 13:34:53.017 INFO: train_f_mae: 0.022324
train_f_rmse: 0.028732
2025-06-11 13:34:53.018 INFO: train_f_rmse: 0.028732
val_e/atom_mae: 0.005517
2025-06-11 13:34:53.022 INFO: val_e/atom_mae: 0.005517
val_e/atom_rmse: 0.005524
2025-06-11 13:34:53.023 INFO: val_e/atom_rmse: 0.005524
val_f_mae: 0.018709
2025-06-11 13:34:53.025 INFO: val_f_mae: 0.018709
val_f_rmse: 0.024301
2025-06-11 13:34:53.026 INFO: val_f_rmse: 0.024301
##### Step: 33 Learning rate: 0.005 #####
2025-06-11 13:37:07.760 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.9650, Val Loss: 0.9356
2025-06-11 13:37:07.762 INFO: Epoch 34, Train Loss: 0.9650, Val Loss: 0.9356
train_e/atom_mae: 0.004905
2025-06-11 13:37:07.765 INFO: train_e/atom_mae: 0.004905
train_e/atom_rmse: 0.005736
2025-06-11 13:37:07.766 INFO: train_e/atom_rmse: 0.005736
train_f_mae: 0.022622
2025-06-11 13:37:07.772 INFO: train_f_mae: 0.022622
train_f_rmse: 0.029046
2025-06-11 13:37:07.773 INFO: train_f_rmse: 0.029046
val_e/atom_mae: 0.006616
2025-06-11 13:37:07.777 INFO: val_e/atom_mae: 0.006616
val_e/atom_rmse: 0.006623
2025-06-11 13:37:07.779 INFO: val_e/atom_rmse: 0.006623
val_f_mae: 0.021516
2025-06-11 13:37:07.780 INFO: val_f_mae: 0.021516
val_f_rmse: 0.027820
2025-06-11 13:37:07.781 INFO: val_f_rmse: 0.027820
##### Step: 34 Learning rate: 0.005 #####
2025-06-11 13:39:22.546 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.7355, Val Loss: 0.7077
2025-06-11 13:39:22.547 INFO: Epoch 35, Train Loss: 0.7355, Val Loss: 0.7077
train_e/atom_mae: 0.003106
2025-06-11 13:39:22.550 INFO: train_e/atom_mae: 0.003106
train_e/atom_rmse: 0.003762
2025-06-11 13:39:22.550 INFO: train_e/atom_rmse: 0.003762
train_f_mae: 0.020255
2025-06-11 13:39:22.557 INFO: train_f_mae: 0.020255
train_f_rmse: 0.026141
2025-06-11 13:39:22.558 INFO: train_f_rmse: 0.026141
val_e/atom_mae: 0.004068
2025-06-11 13:39:22.562 INFO: val_e/atom_mae: 0.004068
val_e/atom_rmse: 0.004077
2025-06-11 13:39:22.563 INFO: val_e/atom_rmse: 0.004077
val_f_mae: 0.019683
2025-06-11 13:39:22.565 INFO: val_f_mae: 0.019683
val_f_rmse: 0.025424
2025-06-11 13:39:22.566 INFO: val_f_rmse: 0.025424
##### Step: 35 Learning rate: 0.005 #####
2025-06-11 13:41:37.305 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.7316, Val Loss: 0.6730
2025-06-11 13:41:37.306 INFO: Epoch 36, Train Loss: 0.7316, Val Loss: 0.6730
train_e/atom_mae: 0.002600
2025-06-11 13:41:37.308 INFO: train_e/atom_mae: 0.002600
train_e/atom_rmse: 0.003426
2025-06-11 13:41:37.309 INFO: train_e/atom_rmse: 0.003426
train_f_mae: 0.020302
2025-06-11 13:41:37.315 INFO: train_f_mae: 0.020302
train_f_rmse: 0.026235
2025-06-11 13:41:37.316 INFO: train_f_rmse: 0.026235
val_e/atom_mae: 0.001749
2025-06-11 13:41:37.321 INFO: val_e/atom_mae: 0.001749
val_e/atom_rmse: 0.001775
2025-06-11 13:41:37.322 INFO: val_e/atom_rmse: 0.001775
val_f_mae: 0.019938
2025-06-11 13:41:37.323 INFO: val_f_mae: 0.019938
val_f_rmse: 0.025718
2025-06-11 13:41:37.324 INFO: val_f_rmse: 0.025718
##### Step: 36 Learning rate: 0.005 #####
2025-06-11 13:43:51.975 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.7233, Val Loss: 0.7560
2025-06-11 13:43:51.976 INFO: Epoch 37, Train Loss: 0.7233, Val Loss: 0.7560
train_e/atom_mae: 0.001806
2025-06-11 13:43:51.978 INFO: train_e/atom_mae: 0.001806
train_e/atom_rmse: 0.002422
2025-06-11 13:43:51.979 INFO: train_e/atom_rmse: 0.002422
train_f_mae: 0.020486
2025-06-11 13:43:51.985 INFO: train_f_mae: 0.020486
train_f_rmse: 0.026489
2025-06-11 13:43:51.986 INFO: train_f_rmse: 0.026489
val_e/atom_mae: 0.003792
2025-06-11 13:43:51.991 INFO: val_e/atom_mae: 0.003792
val_e/atom_rmse: 0.003804
2025-06-11 13:43:51.992 INFO: val_e/atom_rmse: 0.003804
val_f_mae: 0.020666
2025-06-11 13:43:51.993 INFO: val_f_mae: 0.020666
val_f_rmse: 0.026508
2025-06-11 13:43:51.994 INFO: val_f_rmse: 0.026508
##### Step: 37 Learning rate: 0.005 #####
2025-06-11 13:46:06.630 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.7591, Val Loss: 0.6208
2025-06-11 13:46:06.631 INFO: Epoch 38, Train Loss: 0.7591, Val Loss: 0.6208
train_e/atom_mae: 0.002987
2025-06-11 13:46:06.634 INFO: train_e/atom_mae: 0.002987
train_e/atom_rmse: 0.003958
2025-06-11 13:46:06.634 INFO: train_e/atom_rmse: 0.003958
train_f_mae: 0.020515
2025-06-11 13:46:06.641 INFO: train_f_mae: 0.020515
train_f_rmse: 0.026482
2025-06-11 13:46:06.642 INFO: train_f_rmse: 0.026482
val_e/atom_mae: 0.001155
2025-06-11 13:46:06.646 INFO: val_e/atom_mae: 0.001155
val_e/atom_rmse: 0.001180
2025-06-11 13:46:06.647 INFO: val_e/atom_rmse: 0.001180
val_f_mae: 0.019114
2025-06-11 13:46:06.649 INFO: val_f_mae: 0.019114
val_f_rmse: 0.024812
2025-06-11 13:46:06.650 INFO: val_f_rmse: 0.024812
##### Step: 38 Learning rate: 0.005 #####
2025-06-11 13:48:21.312 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.7250, Val Loss: 0.6061
2025-06-11 13:48:21.314 INFO: Epoch 39, Train Loss: 0.7250, Val Loss: 0.6061
train_e/atom_mae: 0.002838
2025-06-11 13:48:21.317 INFO: train_e/atom_mae: 0.002838
train_e/atom_rmse: 0.003812
2025-06-11 13:48:21.318 INFO: train_e/atom_rmse: 0.003812
train_f_mae: 0.020026
2025-06-11 13:48:21.324 INFO: train_f_mae: 0.020026
train_f_rmse: 0.025911
2025-06-11 13:48:21.325 INFO: train_f_rmse: 0.025911
val_e/atom_mae: 0.001223
2025-06-11 13:48:21.329 INFO: val_e/atom_mae: 0.001223
val_e/atom_rmse: 0.001278
2025-06-11 13:48:21.331 INFO: val_e/atom_rmse: 0.001278
val_f_mae: 0.018896
2025-06-11 13:48:21.332 INFO: val_f_mae: 0.018896
val_f_rmse: 0.024496
2025-06-11 13:48:21.333 INFO: val_f_rmse: 0.024496
##### Step: 39 Learning rate: 0.005 #####
2025-06-11 13:50:35.949 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.6694, Val Loss: 0.6847
2025-06-11 13:50:35.953 INFO: Epoch 40, Train Loss: 0.6694, Val Loss: 0.6847
train_e/atom_mae: 0.002868
2025-06-11 13:50:35.956 INFO: train_e/atom_mae: 0.002868
train_e/atom_rmse: 0.003546
2025-06-11 13:50:35.957 INFO: train_e/atom_rmse: 0.003546
train_f_mae: 0.019270
2025-06-11 13:50:35.963 INFO: train_f_mae: 0.019270
train_f_rmse: 0.024961
2025-06-11 13:50:35.964 INFO: train_f_rmse: 0.024961
val_e/atom_mae: 0.004245
2025-06-11 13:50:35.968 INFO: val_e/atom_mae: 0.004245
val_e/atom_rmse: 0.004255
2025-06-11 13:50:35.970 INFO: val_e/atom_rmse: 0.004255
val_f_mae: 0.019314
2025-06-11 13:50:35.971 INFO: val_f_mae: 0.019314
val_f_rmse: 0.024860
2025-06-11 13:50:35.972 INFO: val_f_rmse: 0.024860
2025-06-11 13:50:36.038 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-11 13:52:50.782 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 0.9227, Val Loss: 0.5920
2025-06-11 13:52:50.785 INFO: Epoch 1, Train Loss: 0.9227, Val Loss: 0.5920
train_e/atom_mae: 0.004805
2025-06-11 13:52:50.787 INFO: train_e/atom_mae: 0.004805
train_e/atom_rmse: 0.007408
2025-06-11 13:52:50.788 INFO: train_e/atom_rmse: 0.007408
train_f_mae: 0.020475
2025-06-11 13:52:50.794 INFO: train_f_mae: 0.020475
train_f_rmse: 0.026841
2025-06-11 13:52:50.795 INFO: train_f_rmse: 0.026841
val_e/atom_mae: 0.002554
2025-06-11 13:52:50.800 INFO: val_e/atom_mae: 0.002554
val_e/atom_rmse: 0.002578
2025-06-11 13:52:50.801 INFO: val_e/atom_rmse: 0.002578
val_f_mae: 0.018385
2025-06-11 13:52:50.802 INFO: val_f_mae: 0.018385
val_f_rmse: 0.023822
2025-06-11 13:52:50.803 INFO: val_f_rmse: 0.023822
##### Step: 1 Learning rate: 0.004 #####
2025-06-11 13:55:05.699 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 0.6391, Val Loss: 0.7260
2025-06-11 13:55:05.702 INFO: Epoch 2, Train Loss: 0.6391, Val Loss: 0.7260
train_e/atom_mae: 0.001386
2025-06-11 13:55:05.704 INFO: train_e/atom_mae: 0.001386
train_e/atom_rmse: 0.001756
2025-06-11 13:55:05.705 INFO: train_e/atom_rmse: 0.001756
train_f_mae: 0.019334
2025-06-11 13:55:05.711 INFO: train_f_mae: 0.019334
train_f_rmse: 0.025055
2025-06-11 13:55:05.712 INFO: train_f_rmse: 0.025055
val_e/atom_mae: 0.001343
2025-06-11 13:55:05.717 INFO: val_e/atom_mae: 0.001343
val_e/atom_rmse: 0.001396
2025-06-11 13:55:05.718 INFO: val_e/atom_rmse: 0.001396
val_f_mae: 0.020726
2025-06-11 13:55:05.719 INFO: val_f_mae: 0.020726
val_f_rmse: 0.026810
2025-06-11 13:55:05.720 INFO: val_f_rmse: 0.026810
##### Step: 2 Learning rate: 0.006 #####
2025-06-11 13:57:20.425 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 0.6853, Val Loss: 0.7435
2025-06-11 13:57:20.427 INFO: Epoch 3, Train Loss: 0.6853, Val Loss: 0.7435
train_e/atom_mae: 0.002521
2025-06-11 13:57:20.429 INFO: train_e/atom_mae: 0.002521
train_e/atom_rmse: 0.003172
2025-06-11 13:57:20.430 INFO: train_e/atom_rmse: 0.003172
train_f_mae: 0.019695
2025-06-11 13:57:20.436 INFO: train_f_mae: 0.019695
train_f_rmse: 0.025461
2025-06-11 13:57:20.437 INFO: train_f_rmse: 0.025461
val_e/atom_mae: 0.005666
2025-06-11 13:57:20.442 INFO: val_e/atom_mae: 0.005666
val_e/atom_rmse: 0.005673
2025-06-11 13:57:20.443 INFO: val_e/atom_rmse: 0.005673
val_f_mae: 0.019162
2025-06-11 13:57:20.444 INFO: val_f_mae: 0.019162
val_f_rmse: 0.024997
2025-06-11 13:57:20.445 INFO: val_f_rmse: 0.024997
##### Step: 3 Learning rate: 0.008 #####
2025-06-11 13:59:35.107 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 0.9046, Val Loss: 1.1212
2025-06-11 13:59:35.110 INFO: Epoch 4, Train Loss: 0.9046, Val Loss: 1.1212
train_e/atom_mae: 0.003819
2025-06-11 13:59:35.112 INFO: train_e/atom_mae: 0.003819
train_e/atom_rmse: 0.004607
2025-06-11 13:59:35.113 INFO: train_e/atom_rmse: 0.004607
train_f_mae: 0.022229
2025-06-11 13:59:35.119 INFO: train_f_mae: 0.022229
train_f_rmse: 0.028747
2025-06-11 13:59:35.120 INFO: train_f_rmse: 0.028747
val_e/atom_mae: 0.008078
2025-06-11 13:59:35.125 INFO: val_e/atom_mae: 0.008078
val_e/atom_rmse: 0.008082
2025-06-11 13:59:35.126 INFO: val_e/atom_rmse: 0.008082
val_f_mae: 0.023192
2025-06-11 13:59:35.127 INFO: val_f_mae: 0.023192
val_f_rmse: 0.029673
2025-06-11 13:59:35.128 INFO: val_f_rmse: 0.029673
##### Step: 4 Learning rate: 0.01 #####
2025-06-11 14:01:49.658 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 1.3018, Val Loss: 0.9671
2025-06-11 14:01:49.661 INFO: Epoch 5, Train Loss: 1.3018, Val Loss: 0.9671
train_e/atom_mae: 0.006525
2025-06-11 14:01:49.663 INFO: train_e/atom_mae: 0.006525
train_e/atom_rmse: 0.007410
2025-06-11 14:01:49.664 INFO: train_e/atom_rmse: 0.007410
train_f_mae: 0.025921
2025-06-11 14:01:49.670 INFO: train_f_mae: 0.025921
train_f_rmse: 0.033156
2025-06-11 14:01:49.671 INFO: train_f_rmse: 0.033156
val_e/atom_mae: 0.009691
2025-06-11 14:01:49.676 INFO: val_e/atom_mae: 0.009691
val_e/atom_rmse: 0.009694
2025-06-11 14:01:49.677 INFO: val_e/atom_rmse: 0.009694
val_f_mae: 0.019258
2025-06-11 14:01:49.678 INFO: val_f_mae: 0.019258
val_f_rmse: 0.024913
2025-06-11 14:01:49.679 INFO: val_f_rmse: 0.024913
##### Step: 5 Learning rate: 0.01 #####
2025-06-11 14:04:04.252 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 1.1792, Val Loss: 0.9635
2025-06-11 14:04:04.255 INFO: Epoch 6, Train Loss: 1.1792, Val Loss: 0.9635
train_e/atom_mae: 0.006245
2025-06-11 14:04:04.257 INFO: train_e/atom_mae: 0.006245
train_e/atom_rmse: 0.007120
2025-06-11 14:04:04.258 INFO: train_e/atom_rmse: 0.007120
train_f_mae: 0.024541
2025-06-11 14:04:04.264 INFO: train_f_mae: 0.024541
train_f_rmse: 0.031501
2025-06-11 14:04:04.265 INFO: train_f_rmse: 0.031501
val_e/atom_mae: 0.001532
2025-06-11 14:04:04.270 INFO: val_e/atom_mae: 0.001532
val_e/atom_rmse: 0.001552
2025-06-11 14:04:04.271 INFO: val_e/atom_rmse: 0.001552
val_f_mae: 0.024379
2025-06-11 14:04:04.272 INFO: val_f_mae: 0.024379
val_f_rmse: 0.030898
2025-06-11 14:04:04.273 INFO: val_f_rmse: 0.030898
##### Step: 6 Learning rate: 0.01 #####
2025-06-11 14:06:18.754 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 1.2119, Val Loss: 0.9329
2025-06-11 14:06:18.757 INFO: Epoch 7, Train Loss: 1.2119, Val Loss: 0.9329
train_e/atom_mae: 0.004323
2025-06-11 14:06:18.759 INFO: train_e/atom_mae: 0.004323
train_e/atom_rmse: 0.005529
2025-06-11 14:06:18.760 INFO: train_e/atom_rmse: 0.005529
train_f_mae: 0.025753
2025-06-11 14:06:18.766 INFO: train_f_mae: 0.025753
train_f_rmse: 0.033154
2025-06-11 14:06:18.767 INFO: train_f_rmse: 0.033154
val_e/atom_mae: 0.004087
2025-06-11 14:06:18.772 INFO: val_e/atom_mae: 0.004087
val_e/atom_rmse: 0.004104
2025-06-11 14:06:18.773 INFO: val_e/atom_rmse: 0.004104
val_f_mae: 0.023205
2025-06-11 14:06:18.774 INFO: val_f_mae: 0.023205
val_f_rmse: 0.029509
2025-06-11 14:06:18.775 INFO: val_f_rmse: 0.029509
##### Step: 7 Learning rate: 0.01 #####
2025-06-11 14:08:33.256 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 0.8842, Val Loss: 0.7897
2025-06-11 14:08:33.257 INFO: Epoch 8, Train Loss: 0.8842, Val Loss: 0.7897
train_e/atom_mae: 0.003288
2025-06-11 14:08:33.260 INFO: train_e/atom_mae: 0.003288
train_e/atom_rmse: 0.004126
2025-06-11 14:08:33.261 INFO: train_e/atom_rmse: 0.004126
train_f_mae: 0.022050
2025-06-11 14:08:33.267 INFO: train_f_mae: 0.022050
train_f_rmse: 0.028662
2025-06-11 14:08:33.268 INFO: train_f_rmse: 0.028662
val_e/atom_mae: 0.002091
2025-06-11 14:08:33.272 INFO: val_e/atom_mae: 0.002091
val_e/atom_rmse: 0.002152
2025-06-11 14:08:33.274 INFO: val_e/atom_rmse: 0.002152
val_f_mae: 0.021512
2025-06-11 14:08:33.275 INFO: val_f_mae: 0.021512
val_f_rmse: 0.027797
2025-06-11 14:08:33.276 INFO: val_f_rmse: 0.027797
##### Step: 8 Learning rate: 0.01 #####
2025-06-11 14:10:47.900 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 0.9099, Val Loss: 0.9239
2025-06-11 14:10:47.902 INFO: Epoch 9, Train Loss: 0.9099, Val Loss: 0.9239
train_e/atom_mae: 0.003912
2025-06-11 14:10:47.904 INFO: train_e/atom_mae: 0.003912
train_e/atom_rmse: 0.004612
2025-06-11 14:10:47.905 INFO: train_e/atom_rmse: 0.004612
train_f_mae: 0.022220
2025-06-11 14:10:47.911 INFO: train_f_mae: 0.022220
train_f_rmse: 0.028836
2025-06-11 14:10:47.912 INFO: train_f_rmse: 0.028836
val_e/atom_mae: 0.006200
2025-06-11 14:10:47.917 INFO: val_e/atom_mae: 0.006200
val_e/atom_rmse: 0.006208
2025-06-11 14:10:47.918 INFO: val_e/atom_rmse: 0.006208
val_f_mae: 0.021652
2025-06-11 14:10:47.920 INFO: val_f_mae: 0.021652
val_f_rmse: 0.027960
2025-06-11 14:10:47.920 INFO: val_f_rmse: 0.027960
##### Step: 9 Learning rate: 0.01 #####
2025-06-11 14:13:02.384 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 0.8457, Val Loss: 0.9019
2025-06-11 14:13:02.387 INFO: Epoch 10, Train Loss: 0.8457, Val Loss: 0.9019
train_e/atom_mae: 0.003887
2025-06-11 14:13:02.389 INFO: train_e/atom_mae: 0.003887
train_e/atom_rmse: 0.004598
2025-06-11 14:13:02.390 INFO: train_e/atom_rmse: 0.004598
train_f_mae: 0.021337
2025-06-11 14:13:02.396 INFO: train_f_mae: 0.021337
train_f_rmse: 0.027709
2025-06-11 14:13:02.397 INFO: train_f_rmse: 0.027709
val_e/atom_mae: 0.003254
2025-06-11 14:13:02.402 INFO: val_e/atom_mae: 0.003254
val_e/atom_rmse: 0.003266
2025-06-11 14:13:02.403 INFO: val_e/atom_rmse: 0.003266
val_f_mae: 0.022889
2025-06-11 14:13:02.404 INFO: val_f_mae: 0.022889
val_f_rmse: 0.029369
2025-06-11 14:13:02.405 INFO: val_f_rmse: 0.029369
##### Step: 10 Learning rate: 0.01 #####
2025-06-11 14:15:16.928 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 1.0578, Val Loss: 1.0827
2025-06-11 14:15:16.929 INFO: Epoch 11, Train Loss: 1.0578, Val Loss: 1.0827
train_e/atom_mae: 0.004109
2025-06-11 14:15:16.931 INFO: train_e/atom_mae: 0.004109
train_e/atom_rmse: 0.005042
2025-06-11 14:15:16.932 INFO: train_e/atom_rmse: 0.005042
train_f_mae: 0.023886
2025-06-11 14:15:16.938 INFO: train_f_mae: 0.023886
train_f_rmse: 0.031050
2025-06-11 14:15:16.939 INFO: train_f_rmse: 0.031050
val_e/atom_mae: 0.000481
2025-06-11 14:15:16.944 INFO: val_e/atom_mae: 0.000481
val_e/atom_rmse: 0.000608
2025-06-11 14:15:16.945 INFO: val_e/atom_rmse: 0.000608
val_f_mae: 0.025870
2025-06-11 14:15:16.946 INFO: val_f_mae: 0.025870
val_f_rmse: 0.032884
2025-06-11 14:15:16.947 INFO: val_f_rmse: 0.032884
##### Step: 11 Learning rate: 0.01 #####
2025-06-11 14:17:31.405 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 1.2008, Val Loss: 1.2436
2025-06-11 14:17:31.407 INFO: Epoch 12, Train Loss: 1.2008, Val Loss: 1.2436
train_e/atom_mae: 0.004921
2025-06-11 14:17:31.409 INFO: train_e/atom_mae: 0.004921
train_e/atom_rmse: 0.006047
2025-06-11 14:17:31.410 INFO: train_e/atom_rmse: 0.006047
train_f_mae: 0.025360
2025-06-11 14:17:31.416 INFO: train_f_mae: 0.025360
train_f_rmse: 0.032649
2025-06-11 14:17:31.417 INFO: train_f_rmse: 0.032649
val_e/atom_mae: 0.007729
2025-06-11 14:17:31.422 INFO: val_e/atom_mae: 0.007729
val_e/atom_rmse: 0.007735
2025-06-11 14:17:31.423 INFO: val_e/atom_rmse: 0.007735
val_f_mae: 0.025348
2025-06-11 14:17:31.424 INFO: val_f_mae: 0.025348
val_f_rmse: 0.031985
2025-06-11 14:17:31.425 INFO: val_f_rmse: 0.031985
##### Step: 12 Learning rate: 0.01 #####
2025-06-11 14:19:45.941 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 0.9855, Val Loss: 1.1941
2025-06-11 14:19:45.943 INFO: Epoch 13, Train Loss: 0.9855, Val Loss: 1.1941
train_e/atom_mae: 0.004688
2025-06-11 14:19:45.945 INFO: train_e/atom_mae: 0.004688
train_e/atom_rmse: 0.005486
2025-06-11 14:19:45.946 INFO: train_e/atom_rmse: 0.005486
train_f_mae: 0.023010
2025-06-11 14:19:45.952 INFO: train_f_mae: 0.023010
train_f_rmse: 0.029573
2025-06-11 14:19:45.953 INFO: train_f_rmse: 0.029573
val_e/atom_mae: 0.004638
2025-06-11 14:19:45.958 INFO: val_e/atom_mae: 0.004638
val_e/atom_rmse: 0.004646
2025-06-11 14:19:45.959 INFO: val_e/atom_rmse: 0.004646
val_f_mae: 0.026420
2025-06-11 14:19:45.960 INFO: val_f_mae: 0.026420
val_f_rmse: 0.033384
2025-06-11 14:19:45.961 INFO: val_f_rmse: 0.033384
##### Step: 13 Learning rate: 0.01 #####
2025-06-11 14:22:00.410 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 1.0107, Val Loss: 0.8259
2025-06-11 14:22:00.413 INFO: Epoch 14, Train Loss: 1.0107, Val Loss: 0.8259
train_e/atom_mae: 0.005158
2025-06-11 14:22:00.415 INFO: train_e/atom_mae: 0.005158
train_e/atom_rmse: 0.005908
2025-06-11 14:22:00.416 INFO: train_e/atom_rmse: 0.005908
train_f_mae: 0.023098
2025-06-11 14:22:00.422 INFO: train_f_mae: 0.023098
train_f_rmse: 0.029698
2025-06-11 14:22:00.423 INFO: train_f_rmse: 0.029698
val_e/atom_mae: 0.006781
2025-06-11 14:22:00.428 INFO: val_e/atom_mae: 0.006781
val_e/atom_rmse: 0.006788
2025-06-11 14:22:00.429 INFO: val_e/atom_rmse: 0.006788
val_f_mae: 0.019615
2025-06-11 14:22:00.430 INFO: val_f_mae: 0.019615
val_f_rmse: 0.025614
2025-06-11 14:22:00.431 INFO: val_f_rmse: 0.025614
##### Step: 14 Learning rate: 0.01 #####
2025-06-11 14:24:14.866 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 0.9360, Val Loss: 0.9732
2025-06-11 14:24:14.868 INFO: Epoch 15, Train Loss: 0.9360, Val Loss: 0.9732
train_e/atom_mae: 0.004529
2025-06-11 14:24:14.871 INFO: train_e/atom_mae: 0.004529
train_e/atom_rmse: 0.005267
2025-06-11 14:24:14.872 INFO: train_e/atom_rmse: 0.005267
train_f_mae: 0.022404
2025-06-11 14:24:14.878 INFO: train_f_mae: 0.022404
train_f_rmse: 0.028874
2025-06-11 14:24:14.879 INFO: train_f_rmse: 0.028874
val_e/atom_mae: 0.004955
2025-06-11 14:24:14.883 INFO: val_e/atom_mae: 0.004955
val_e/atom_rmse: 0.004968
2025-06-11 14:24:14.885 INFO: val_e/atom_rmse: 0.004968
val_f_mae: 0.023324
2025-06-11 14:24:14.886 INFO: val_f_mae: 0.023324
val_f_rmse: 0.029702
2025-06-11 14:24:14.887 INFO: val_f_rmse: 0.029702
##### Step: 15 Learning rate: 0.01 #####
2025-06-11 14:26:29.342 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 1.0056, Val Loss: 0.7383
2025-06-11 14:26:29.344 INFO: Epoch 16, Train Loss: 1.0056, Val Loss: 0.7383
train_e/atom_mae: 0.005838
2025-06-11 14:26:29.346 INFO: train_e/atom_mae: 0.005838
train_e/atom_rmse: 0.006882
2025-06-11 14:26:29.347 INFO: train_e/atom_rmse: 0.006882
train_f_mae: 0.022318
2025-06-11 14:26:29.353 INFO: train_f_mae: 0.022318
train_f_rmse: 0.028828
2025-06-11 14:26:29.354 INFO: train_f_rmse: 0.028828
val_e/atom_mae: 0.005935
2025-06-11 14:26:29.358 INFO: val_e/atom_mae: 0.005935
val_e/atom_rmse: 0.005939
2025-06-11 14:26:29.360 INFO: val_e/atom_rmse: 0.005939
val_f_mae: 0.018995
2025-06-11 14:26:29.361 INFO: val_f_mae: 0.018995
val_f_rmse: 0.024664
2025-06-11 14:26:29.362 INFO: val_f_rmse: 0.024664
##### Step: 16 Learning rate: 0.01 #####
2025-06-11 14:28:43.880 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 0.7714, Val Loss: 0.6850
2025-06-11 14:28:43.883 INFO: Epoch 17, Train Loss: 0.7714, Val Loss: 0.6850
train_e/atom_mae: 0.003229
2025-06-11 14:28:43.885 INFO: train_e/atom_mae: 0.003229
train_e/atom_rmse: 0.003750
2025-06-11 14:28:43.886 INFO: train_e/atom_rmse: 0.003750
train_f_mae: 0.020754
2025-06-11 14:28:43.892 INFO: train_f_mae: 0.020754
train_f_rmse: 0.026824
2025-06-11 14:28:43.893 INFO: train_f_rmse: 0.026824
val_e/atom_mae: 0.003254
2025-06-11 14:28:43.898 INFO: val_e/atom_mae: 0.003254
val_e/atom_rmse: 0.003270
2025-06-11 14:28:43.899 INFO: val_e/atom_rmse: 0.003270
val_f_mae: 0.019526
2025-06-11 14:28:43.900 INFO: val_f_mae: 0.019526
val_f_rmse: 0.025408
2025-06-11 14:28:43.901 INFO: val_f_rmse: 0.025408
##### Step: 17 Learning rate: 0.01 #####
2025-06-11 14:30:58.456 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 0.7360, Val Loss: 0.7411
2025-06-11 14:30:58.457 INFO: Epoch 18, Train Loss: 0.7360, Val Loss: 0.7411
train_e/atom_mae: 0.002777
2025-06-11 14:30:58.459 INFO: train_e/atom_mae: 0.002777
train_e/atom_rmse: 0.003314
2025-06-11 14:30:58.460 INFO: train_e/atom_rmse: 0.003314
train_f_mae: 0.020402
2025-06-11 14:30:58.466 INFO: train_f_mae: 0.020402
train_f_rmse: 0.026372
2025-06-11 14:30:58.467 INFO: train_f_rmse: 0.026372
val_e/atom_mae: 0.003876
2025-06-11 14:30:58.472 INFO: val_e/atom_mae: 0.003876
val_e/atom_rmse: 0.003885
2025-06-11 14:30:58.473 INFO: val_e/atom_rmse: 0.003885
val_f_mae: 0.020304
2025-06-11 14:30:58.474 INFO: val_f_mae: 0.020304
val_f_rmse: 0.026182
2025-06-11 14:30:58.475 INFO: val_f_rmse: 0.026182
##### Step: 18 Learning rate: 0.01 #####
2025-06-11 14:33:13.095 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 0.7173, Val Loss: 1.3066
2025-06-11 14:33:13.096 INFO: Epoch 19, Train Loss: 0.7173, Val Loss: 1.3066
train_e/atom_mae: 0.002267
2025-06-11 14:33:13.099 INFO: train_e/atom_mae: 0.002267
train_e/atom_rmse: 0.002912
2025-06-11 14:33:13.100 INFO: train_e/atom_rmse: 0.002912
train_f_mae: 0.020226
2025-06-11 14:33:13.106 INFO: train_f_mae: 0.020226
train_f_rmse: 0.026193
2025-06-11 14:33:13.107 INFO: train_f_rmse: 0.026193
val_e/atom_mae: 0.000484
2025-06-11 14:33:13.112 INFO: val_e/atom_mae: 0.000484
val_e/atom_rmse: 0.000557
2025-06-11 14:33:13.113 INFO: val_e/atom_rmse: 0.000557
val_f_mae: 0.028892
2025-06-11 14:33:13.114 INFO: val_f_mae: 0.028892
val_f_rmse: 0.036131
2025-06-11 14:33:13.115 INFO: val_f_rmse: 0.036131
##### Step: 19 Learning rate: 0.01 #####
2025-06-11 14:35:27.697 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 0.9441, Val Loss: 0.8045
2025-06-11 14:35:27.698 INFO: Epoch 20, Train Loss: 0.9441, Val Loss: 0.8045
train_e/atom_mae: 0.004338
2025-06-11 14:35:27.701 INFO: train_e/atom_mae: 0.004338
train_e/atom_rmse: 0.005294
2025-06-11 14:35:27.702 INFO: train_e/atom_rmse: 0.005294
train_f_mae: 0.022485
2025-06-11 14:35:27.708 INFO: train_f_mae: 0.022485
train_f_rmse: 0.028997
2025-06-11 14:35:27.709 INFO: train_f_rmse: 0.028997
val_e/atom_mae: 0.003723
2025-06-11 14:35:27.713 INFO: val_e/atom_mae: 0.003723
val_e/atom_rmse: 0.003732
2025-06-11 14:35:27.715 INFO: val_e/atom_rmse: 0.003732
val_f_mae: 0.021287
2025-06-11 14:35:27.716 INFO: val_f_mae: 0.021287
val_f_rmse: 0.027443
2025-06-11 14:35:27.717 INFO: val_f_rmse: 0.027443
##### Step: 20 Learning rate: 0.005 #####
2025-06-11 14:37:42.199 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 0.7171, Val Loss: 0.8443
2025-06-11 14:37:42.200 INFO: Epoch 21, Train Loss: 0.7171, Val Loss: 0.8443
train_e/atom_mae: 0.003582
2025-06-11 14:37:42.202 INFO: train_e/atom_mae: 0.003582
train_e/atom_rmse: 0.004339
2025-06-11 14:37:42.203 INFO: train_e/atom_rmse: 0.004339
train_f_mae: 0.019713
2025-06-11 14:37:42.209 INFO: train_f_mae: 0.019713
train_f_rmse: 0.025450
2025-06-11 14:37:42.210 INFO: train_f_rmse: 0.025450
val_e/atom_mae: 0.007404
2025-06-11 14:37:42.215 INFO: val_e/atom_mae: 0.007404
val_e/atom_rmse: 0.007419
2025-06-11 14:37:42.217 INFO: val_e/atom_rmse: 0.007419
val_f_mae: 0.019713
2025-06-11 14:37:42.218 INFO: val_f_mae: 0.019713
val_f_rmse: 0.025325
2025-06-11 14:37:42.219 INFO: val_f_rmse: 0.025325
##### Step: 21 Learning rate: 0.005 #####
2025-06-11 14:39:56.706 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.6714, Val Loss: 0.6183
2025-06-11 14:39:56.708 INFO: Epoch 22, Train Loss: 0.6714, Val Loss: 0.6183
train_e/atom_mae: 0.003480
2025-06-11 14:39:56.710 INFO: train_e/atom_mae: 0.003480
train_e/atom_rmse: 0.004021
2025-06-11 14:39:56.711 INFO: train_e/atom_rmse: 0.004021
train_f_mae: 0.019125
2025-06-11 14:39:56.718 INFO: train_f_mae: 0.019125
train_f_rmse: 0.024735
2025-06-11 14:39:56.718 INFO: train_f_rmse: 0.024735
val_e/atom_mae: 0.003562
2025-06-11 14:39:56.723 INFO: val_e/atom_mae: 0.003562
val_e/atom_rmse: 0.003579
2025-06-11 14:39:56.724 INFO: val_e/atom_rmse: 0.003579
val_f_mae: 0.018457
2025-06-11 14:39:56.726 INFO: val_f_mae: 0.018457
val_f_rmse: 0.023897
2025-06-11 14:39:56.726 INFO: val_f_rmse: 0.023897
##### Step: 22 Learning rate: 0.005 #####
2025-06-11 14:42:11.336 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.7295, Val Loss: 0.8266
2025-06-11 14:42:11.339 INFO: Epoch 23, Train Loss: 0.7295, Val Loss: 0.8266
train_e/atom_mae: 0.001258
2025-06-11 14:42:11.342 INFO: train_e/atom_mae: 0.001258
train_e/atom_rmse: 0.001595
2025-06-11 14:42:11.343 INFO: train_e/atom_rmse: 0.001595
train_f_mae: 0.020682
2025-06-11 14:42:11.349 INFO: train_f_mae: 0.020682
train_f_rmse: 0.026835
2025-06-11 14:42:11.350 INFO: train_f_rmse: 0.026835
val_e/atom_mae: 0.000333
2025-06-11 14:42:11.355 INFO: val_e/atom_mae: 0.000333
val_e/atom_rmse: 0.000412
2025-06-11 14:42:11.356 INFO: val_e/atom_rmse: 0.000412
val_f_mae: 0.022246
2025-06-11 14:42:11.357 INFO: val_f_mae: 0.022246
val_f_rmse: 0.028739
2025-06-11 14:42:11.358 INFO: val_f_rmse: 0.028739
##### Step: 23 Learning rate: 0.005 #####
2025-06-11 14:44:25.832 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.6814, Val Loss: 0.6509
2025-06-11 14:44:25.833 INFO: Epoch 24, Train Loss: 0.6814, Val Loss: 0.6509
train_e/atom_mae: 0.000937
2025-06-11 14:44:25.835 INFO: train_e/atom_mae: 0.000937
train_e/atom_rmse: 0.001244
2025-06-11 14:44:25.836 INFO: train_e/atom_rmse: 0.001244
train_f_mae: 0.020047
2025-06-11 14:44:25.842 INFO: train_f_mae: 0.020047
train_f_rmse: 0.025994
2025-06-11 14:44:25.843 INFO: train_f_rmse: 0.025994
val_e/atom_mae: 0.000666
2025-06-11 14:44:25.848 INFO: val_e/atom_mae: 0.000666
val_e/atom_rmse: 0.000726
2025-06-11 14:44:25.849 INFO: val_e/atom_rmse: 0.000726
val_f_mae: 0.019776
2025-06-11 14:44:25.851 INFO: val_f_mae: 0.019776
val_f_rmse: 0.025474
2025-06-11 14:44:25.851 INFO: val_f_rmse: 0.025474
##### Step: 24 Learning rate: 0.005 #####
2025-06-11 14:46:40.329 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.6560, Val Loss: 0.6800
2025-06-11 14:46:40.330 INFO: Epoch 25, Train Loss: 0.6560, Val Loss: 0.6800
train_e/atom_mae: 0.001352
2025-06-11 14:46:40.332 INFO: train_e/atom_mae: 0.001352
train_e/atom_rmse: 0.001613
2025-06-11 14:46:40.333 INFO: train_e/atom_rmse: 0.001613
train_f_mae: 0.019692
2025-06-11 14:46:40.339 INFO: train_f_mae: 0.019692
train_f_rmse: 0.025424
2025-06-11 14:46:40.340 INFO: train_f_rmse: 0.025424
val_e/atom_mae: 0.000223
2025-06-11 14:46:40.345 INFO: val_e/atom_mae: 0.000223
val_e/atom_rmse: 0.000276
2025-06-11 14:46:40.346 INFO: val_e/atom_rmse: 0.000276
val_f_mae: 0.020075
2025-06-11 14:46:40.347 INFO: val_f_mae: 0.020075
val_f_rmse: 0.026071
2025-06-11 14:46:40.348 INFO: val_f_rmse: 0.026071
##### Step: 25 Learning rate: 0.005 #####
2025-06-11 14:48:54.755 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.6236, Val Loss: 0.7029
2025-06-11 14:48:54.757 INFO: Epoch 26, Train Loss: 0.6236, Val Loss: 0.7029
train_e/atom_mae: 0.001465
2025-06-11 14:48:54.760 INFO: train_e/atom_mae: 0.001465
train_e/atom_rmse: 0.001940
2025-06-11 14:48:54.761 INFO: train_e/atom_rmse: 0.001940
train_f_mae: 0.019058
2025-06-11 14:48:54.767 INFO: train_f_mae: 0.019058
train_f_rmse: 0.024693
2025-06-11 14:48:54.768 INFO: train_f_rmse: 0.024693
val_e/atom_mae: 0.000366
2025-06-11 14:48:54.772 INFO: val_e/atom_mae: 0.000366
val_e/atom_rmse: 0.000411
2025-06-11 14:48:54.774 INFO: val_e/atom_rmse: 0.000411
val_f_mae: 0.020445
2025-06-11 14:48:54.775 INFO: val_f_mae: 0.020445
val_f_rmse: 0.026501
2025-06-11 14:48:54.776 INFO: val_f_rmse: 0.026501
##### Step: 26 Learning rate: 0.005 #####
2025-06-11 14:51:09.132 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.6280, Val Loss: 0.5889
2025-06-11 14:51:09.135 INFO: Epoch 27, Train Loss: 0.6280, Val Loss: 0.5889
train_e/atom_mae: 0.001823
2025-06-11 14:51:09.137 INFO: train_e/atom_mae: 0.001823
train_e/atom_rmse: 0.002188
2025-06-11 14:51:09.138 INFO: train_e/atom_rmse: 0.002188
train_f_mae: 0.019084
2025-06-11 14:51:09.144 INFO: train_f_mae: 0.019084
train_f_rmse: 0.024706
2025-06-11 14:51:09.145 INFO: train_f_rmse: 0.024706
val_e/atom_mae: 0.000263
2025-06-11 14:51:09.150 INFO: val_e/atom_mae: 0.000263
val_e/atom_rmse: 0.000330
2025-06-11 14:51:09.151 INFO: val_e/atom_rmse: 0.000330
val_f_mae: 0.018775
2025-06-11 14:51:09.152 INFO: val_f_mae: 0.018775
val_f_rmse: 0.024260
2025-06-11 14:51:09.153 INFO: val_f_rmse: 0.024260
##### Step: 27 Learning rate: 0.005 #####
2025-06-11 14:53:24.110 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.8284, Val Loss: 1.0824
2025-06-11 14:53:24.112 INFO: Epoch 28, Train Loss: 0.8284, Val Loss: 1.0824
train_e/atom_mae: 0.003823
2025-06-11 14:53:24.114 INFO: train_e/atom_mae: 0.003823
train_e/atom_rmse: 0.004763
2025-06-11 14:53:24.115 INFO: train_e/atom_rmse: 0.004763
train_f_mae: 0.021174
2025-06-11 14:53:24.121 INFO: train_f_mae: 0.021174
train_f_rmse: 0.027290
2025-06-11 14:53:24.122 INFO: train_f_rmse: 0.027290
val_e/atom_mae: 0.002923
2025-06-11 14:53:24.127 INFO: val_e/atom_mae: 0.002923
val_e/atom_rmse: 0.002946
2025-06-11 14:53:24.128 INFO: val_e/atom_rmse: 0.002946
val_f_mae: 0.025560
2025-06-11 14:53:24.129 INFO: val_f_mae: 0.025560
val_f_rmse: 0.032409
2025-06-11 14:53:24.130 INFO: val_f_rmse: 0.032409
##### Step: 28 Learning rate: 0.005 #####
2025-06-11 14:55:38.649 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.7520, Val Loss: 0.7359
2025-06-11 14:55:38.651 INFO: Epoch 29, Train Loss: 0.7520, Val Loss: 0.7359
train_e/atom_mae: 0.003861
2025-06-11 14:55:38.654 INFO: train_e/atom_mae: 0.003861
train_e/atom_rmse: 0.004404
2025-06-11 14:55:38.654 INFO: train_e/atom_rmse: 0.004404
train_f_mae: 0.020273
2025-06-11 14:55:38.661 INFO: train_f_mae: 0.020273
train_f_rmse: 0.026086
2025-06-11 14:55:38.662 INFO: train_f_rmse: 0.026086
val_e/atom_mae: 0.001507
2025-06-11 14:55:38.666 INFO: val_e/atom_mae: 0.001507
val_e/atom_rmse: 0.001550
2025-06-11 14:55:38.667 INFO: val_e/atom_rmse: 0.001550
val_f_mae: 0.021002
2025-06-11 14:55:38.669 INFO: val_f_mae: 0.021002
val_f_rmse: 0.026964
2025-06-11 14:55:38.670 INFO: val_f_rmse: 0.026964
##### Step: 29 Learning rate: 0.005 #####
2025-06-11 14:57:53.173 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.6967, Val Loss: 0.7808
2025-06-11 14:57:53.175 INFO: Epoch 30, Train Loss: 0.6967, Val Loss: 0.7808
train_e/atom_mae: 0.003471
2025-06-11 14:57:53.178 INFO: train_e/atom_mae: 0.003471
train_e/atom_rmse: 0.004149
2025-06-11 14:57:53.178 INFO: train_e/atom_rmse: 0.004149
train_f_mae: 0.019544
2025-06-11 14:57:53.185 INFO: train_f_mae: 0.019544
train_f_rmse: 0.025164
2025-06-11 14:57:53.185 INFO: train_f_rmse: 0.025164
val_e/atom_mae: 0.001058
2025-06-11 14:57:53.190 INFO: val_e/atom_mae: 0.001058
val_e/atom_rmse: 0.001141
2025-06-11 14:57:53.191 INFO: val_e/atom_rmse: 0.001141
val_f_mae: 0.021873
2025-06-11 14:57:53.193 INFO: val_f_mae: 0.021873
val_f_rmse: 0.027856
2025-06-11 14:57:53.193 INFO: val_f_rmse: 0.027856
##### Step: 30 Learning rate: 0.005 #####
2025-06-11 15:00:07.678 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.7004, Val Loss: 0.7530
2025-06-11 15:00:07.681 INFO: Epoch 31, Train Loss: 0.7004, Val Loss: 0.7530
train_e/atom_mae: 0.004440
2025-06-11 15:00:07.683 INFO: train_e/atom_mae: 0.004440
train_e/atom_rmse: 0.005221
2025-06-11 15:00:07.684 INFO: train_e/atom_rmse: 0.005221
train_f_mae: 0.018963
2025-06-11 15:00:07.690 INFO: train_f_mae: 0.018963
train_f_rmse: 0.024493
2025-06-11 15:00:07.691 INFO: train_f_rmse: 0.024493
val_e/atom_mae: 0.005669
2025-06-11 15:00:07.696 INFO: val_e/atom_mae: 0.005669
val_e/atom_rmse: 0.005674
2025-06-11 15:00:07.697 INFO: val_e/atom_rmse: 0.005674
val_f_mae: 0.019426
2025-06-11 15:00:07.698 INFO: val_f_mae: 0.019426
val_f_rmse: 0.025186
2025-06-11 15:00:07.699 INFO: val_f_rmse: 0.025186
##### Step: 31 Learning rate: 0.005 #####
2025-06-11 15:02:22.162 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.6064, Val Loss: 0.6096
2025-06-11 15:02:22.164 INFO: Epoch 32, Train Loss: 0.6064, Val Loss: 0.6096
train_e/atom_mae: 0.001393
2025-06-11 15:02:22.167 INFO: train_e/atom_mae: 0.001393
train_e/atom_rmse: 0.001966
2025-06-11 15:02:22.167 INFO: train_e/atom_rmse: 0.001966
train_f_mae: 0.018826
2025-06-11 15:02:22.174 INFO: train_f_mae: 0.018826
train_f_rmse: 0.024334
2025-06-11 15:02:22.175 INFO: train_f_rmse: 0.024334
val_e/atom_mae: 0.002012
2025-06-11 15:02:22.179 INFO: val_e/atom_mae: 0.002012
val_e/atom_rmse: 0.002080
2025-06-11 15:02:22.180 INFO: val_e/atom_rmse: 0.002080
val_f_mae: 0.018798
2025-06-11 15:02:22.182 INFO: val_f_mae: 0.018798
val_f_rmse: 0.024364
2025-06-11 15:02:22.182 INFO: val_f_rmse: 0.024364
##### Step: 32 Learning rate: 0.005 #####
2025-06-11 15:04:36.748 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.6661, Val Loss: 0.6405
2025-06-11 15:04:36.749 INFO: Epoch 33, Train Loss: 0.6661, Val Loss: 0.6405
train_e/atom_mae: 0.002666
2025-06-11 15:04:36.751 INFO: train_e/atom_mae: 0.002666
train_e/atom_rmse: 0.003452
2025-06-11 15:04:36.752 INFO: train_e/atom_rmse: 0.003452
train_f_mae: 0.019305
2025-06-11 15:04:36.758 INFO: train_f_mae: 0.019305
train_f_rmse: 0.024945
2025-06-11 15:04:36.759 INFO: train_f_rmse: 0.024945
val_e/atom_mae: 0.004965
2025-06-11 15:04:36.764 INFO: val_e/atom_mae: 0.004965
val_e/atom_rmse: 0.004987
2025-06-11 15:04:36.765 INFO: val_e/atom_rmse: 0.004987
val_f_mae: 0.018056
2025-06-11 15:04:36.766 INFO: val_f_mae: 0.018056
val_f_rmse: 0.023427
2025-06-11 15:04:36.767 INFO: val_f_rmse: 0.023427
##### Step: 33 Learning rate: 0.005 #####
2025-06-11 15:06:51.361 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.6187, Val Loss: 0.6520
2025-06-11 15:06:51.362 INFO: Epoch 34, Train Loss: 0.6187, Val Loss: 0.6520
train_e/atom_mae: 0.002772
2025-06-11 15:06:51.364 INFO: train_e/atom_mae: 0.002772
train_e/atom_rmse: 0.003373
2025-06-11 15:06:51.365 INFO: train_e/atom_rmse: 0.003373
train_f_mae: 0.018560
2025-06-11 15:06:51.371 INFO: train_f_mae: 0.018560
train_f_rmse: 0.024015
2025-06-11 15:06:51.372 INFO: train_f_rmse: 0.024015
val_e/atom_mae: 0.000338
2025-06-11 15:06:51.377 INFO: val_e/atom_mae: 0.000338
val_e/atom_rmse: 0.000401
2025-06-11 15:06:51.378 INFO: val_e/atom_rmse: 0.000401
val_f_mae: 0.019690
2025-06-11 15:06:51.379 INFO: val_f_mae: 0.019690
val_f_rmse: 0.025522
2025-06-11 15:06:51.380 INFO: val_f_rmse: 0.025522
##### Step: 34 Learning rate: 0.005 #####
2025-06-11 15:09:05.943 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.6466, Val Loss: 0.5756
2025-06-11 15:09:05.944 INFO: Epoch 35, Train Loss: 0.6466, Val Loss: 0.5756
train_e/atom_mae: 0.002941
2025-06-11 15:09:05.946 INFO: train_e/atom_mae: 0.002941
train_e/atom_rmse: 0.003580
2025-06-11 15:09:05.947 INFO: train_e/atom_rmse: 0.003580
train_f_mae: 0.018941
2025-06-11 15:09:05.954 INFO: train_f_mae: 0.018941
train_f_rmse: 0.024481
2025-06-11 15:09:05.954 INFO: train_f_rmse: 0.024481
val_e/atom_mae: 0.002746
2025-06-11 15:09:05.959 INFO: val_e/atom_mae: 0.002746
val_e/atom_rmse: 0.002759
2025-06-11 15:09:05.960 INFO: val_e/atom_rmse: 0.002759
val_f_mae: 0.018073
2025-06-11 15:09:05.962 INFO: val_f_mae: 0.018073
val_f_rmse: 0.023399
2025-06-11 15:09:05.963 INFO: val_f_rmse: 0.023399
##### Step: 35 Learning rate: 0.005 #####
2025-06-11 15:11:20.590 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.5870, Val Loss: 0.5524
2025-06-11 15:11:20.593 INFO: Epoch 36, Train Loss: 0.5870, Val Loss: 0.5524
train_e/atom_mae: 0.001845
2025-06-11 15:11:20.596 INFO: train_e/atom_mae: 0.001845
train_e/atom_rmse: 0.002150
2025-06-11 15:11:20.597 INFO: train_e/atom_rmse: 0.002150
train_f_mae: 0.018480
2025-06-11 15:11:20.603 INFO: train_f_mae: 0.018480
train_f_rmse: 0.023873
2025-06-11 15:11:20.604 INFO: train_f_rmse: 0.023873
val_e/atom_mae: 0.001610
2025-06-11 15:11:20.609 INFO: val_e/atom_mae: 0.001610
val_e/atom_rmse: 0.001639
2025-06-11 15:11:20.610 INFO: val_e/atom_rmse: 0.001639
val_f_mae: 0.017929
2025-06-11 15:11:20.611 INFO: val_f_mae: 0.017929
val_f_rmse: 0.023292
2025-06-11 15:11:20.612 INFO: val_f_rmse: 0.023292
##### Step: 36 Learning rate: 0.005 #####
2025-06-11 15:13:35.152 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.6149, Val Loss: 0.5655
2025-06-11 15:13:35.155 INFO: Epoch 37, Train Loss: 0.6149, Val Loss: 0.5655
train_e/atom_mae: 0.002215
2025-06-11 15:13:35.157 INFO: train_e/atom_mae: 0.002215
train_e/atom_rmse: 0.002751
2025-06-11 15:13:35.158 INFO: train_e/atom_rmse: 0.002751
train_f_mae: 0.018780
2025-06-11 15:13:35.164 INFO: train_f_mae: 0.018780
train_f_rmse: 0.024228
2025-06-11 15:13:35.165 INFO: train_f_rmse: 0.024228
val_e/atom_mae: 0.001912
2025-06-11 15:13:35.170 INFO: val_e/atom_mae: 0.001912
val_e/atom_rmse: 0.001940
2025-06-11 15:13:35.171 INFO: val_e/atom_rmse: 0.001940
val_f_mae: 0.018135
2025-06-11 15:13:35.172 INFO: val_f_mae: 0.018135
val_f_rmse: 0.023486
2025-06-11 15:13:35.173 INFO: val_f_rmse: 0.023486
##### Step: 37 Learning rate: 0.005 #####
2025-06-11 15:15:49.794 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.6151, Val Loss: 0.6559
2025-06-11 15:15:49.795 INFO: Epoch 38, Train Loss: 0.6151, Val Loss: 0.6559
train_e/atom_mae: 0.002430
2025-06-11 15:15:49.797 INFO: train_e/atom_mae: 0.002430
train_e/atom_rmse: 0.002817
2025-06-11 15:15:49.798 INFO: train_e/atom_rmse: 0.002817
train_f_mae: 0.018709
2025-06-11 15:15:49.804 INFO: train_f_mae: 0.018709
train_f_rmse: 0.024205
2025-06-11 15:15:49.805 INFO: train_f_rmse: 0.024205
val_e/atom_mae: 0.001873
2025-06-11 15:15:49.810 INFO: val_e/atom_mae: 0.001873
val_e/atom_rmse: 0.001930
2025-06-11 15:15:49.811 INFO: val_e/atom_rmse: 0.001930
val_f_mae: 0.019477
2025-06-11 15:15:49.813 INFO: val_f_mae: 0.019477
val_f_rmse: 0.025341
2025-06-11 15:15:49.813 INFO: val_f_rmse: 0.025341
##### Step: 38 Learning rate: 0.005 #####
2025-06-11 15:18:04.383 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.6037, Val Loss: 0.5558
2025-06-11 15:18:04.384 INFO: Epoch 39, Train Loss: 0.6037, Val Loss: 0.5558
train_e/atom_mae: 0.001466
2025-06-11 15:18:04.386 INFO: train_e/atom_mae: 0.001466
train_e/atom_rmse: 0.001728
2025-06-11 15:18:04.387 INFO: train_e/atom_rmse: 0.001728
train_f_mae: 0.018818
2025-06-11 15:18:04.393 INFO: train_f_mae: 0.018818
train_f_rmse: 0.024345
2025-06-11 15:18:04.394 INFO: train_f_rmse: 0.024345
val_e/atom_mae: 0.001622
2025-06-11 15:18:04.399 INFO: val_e/atom_mae: 0.001622
val_e/atom_rmse: 0.001644
2025-06-11 15:18:04.400 INFO: val_e/atom_rmse: 0.001644
val_f_mae: 0.018091
2025-06-11 15:18:04.401 INFO: val_f_mae: 0.018091
val_f_rmse: 0.023362
2025-06-11 15:18:04.402 INFO: val_f_rmse: 0.023362
##### Step: 39 Learning rate: 0.005 #####
2025-06-11 15:20:18.960 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.6628, Val Loss: 0.7910
2025-06-11 15:20:18.961 INFO: Epoch 40, Train Loss: 0.6628, Val Loss: 0.7910
train_e/atom_mae: 0.002753
2025-06-11 15:20:18.963 INFO: train_e/atom_mae: 0.002753
train_e/atom_rmse: 0.003367
2025-06-11 15:20:18.964 INFO: train_e/atom_rmse: 0.003367
train_f_mae: 0.019273
2025-06-11 15:20:18.970 INFO: train_f_mae: 0.019273
train_f_rmse: 0.024921
2025-06-11 15:20:18.971 INFO: train_f_rmse: 0.024921
val_e/atom_mae: 0.001691
2025-06-11 15:20:18.976 INFO: val_e/atom_mae: 0.001691
val_e/atom_rmse: 0.001715
2025-06-11 15:20:18.977 INFO: val_e/atom_rmse: 0.001715
val_f_mae: 0.021754
2025-06-11 15:20:18.979 INFO: val_f_mae: 0.021754
val_f_rmse: 0.027931
2025-06-11 15:20:18.980 INFO: val_f_rmse: 0.027931
2025-06-11 15:20:19.035 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-06-11 15:22:33.693 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 0.7765, Val Loss: 0.5780
2025-06-11 15:22:33.696 INFO: Epoch 1, Train Loss: 0.7765, Val Loss: 0.5780
train_e/atom_mae: 0.004668
2025-06-11 15:22:33.698 INFO: train_e/atom_mae: 0.004668
train_e/atom_rmse: 0.005974
2025-06-11 15:22:33.699 INFO: train_e/atom_rmse: 0.005974
train_f_mae: 0.019115
2025-06-11 15:22:33.705 INFO: train_f_mae: 0.019115
train_f_rmse: 0.025397
2025-06-11 15:22:33.706 INFO: train_f_rmse: 0.025397
val_e/atom_mae: 0.001384
2025-06-11 15:22:33.711 INFO: val_e/atom_mae: 0.001384
val_e/atom_rmse: 0.001415
2025-06-11 15:22:33.712 INFO: val_e/atom_rmse: 0.001415
val_f_mae: 0.018454
2025-06-11 15:22:33.713 INFO: val_f_mae: 0.018454
val_f_rmse: 0.023887
2025-06-11 15:22:33.714 INFO: val_f_rmse: 0.023887
##### Step: 1 Learning rate: 0.004 #####
2025-06-11 15:24:48.499 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 0.6142, Val Loss: 0.5697
2025-06-11 15:24:48.500 INFO: Epoch 2, Train Loss: 0.6142, Val Loss: 0.5697
train_e/atom_mae: 0.004204
2025-06-11 15:24:48.503 INFO: train_e/atom_mae: 0.004204
train_e/atom_rmse: 0.004753
2025-06-11 15:24:48.503 INFO: train_e/atom_rmse: 0.004753
train_f_mae: 0.017805
2025-06-11 15:24:48.510 INFO: train_f_mae: 0.017805
train_f_rmse: 0.023043
2025-06-11 15:24:48.511 INFO: train_f_rmse: 0.023043
val_e/atom_mae: 0.003127
2025-06-11 15:24:48.515 INFO: val_e/atom_mae: 0.003127
val_e/atom_rmse: 0.003143
2025-06-11 15:24:48.517 INFO: val_e/atom_rmse: 0.003143
val_f_mae: 0.017898
2025-06-11 15:24:48.518 INFO: val_f_mae: 0.017898
val_f_rmse: 0.023092
2025-06-11 15:24:48.519 INFO: val_f_rmse: 0.023092
##### Step: 2 Learning rate: 0.006 #####
2025-06-11 15:27:03.131 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 0.6484, Val Loss: 0.6288
2025-06-11 15:27:03.133 INFO: Epoch 3, Train Loss: 0.6484, Val Loss: 0.6288
train_e/atom_mae: 0.001493
2025-06-11 15:27:03.135 INFO: train_e/atom_mae: 0.001493
train_e/atom_rmse: 0.001854
2025-06-11 15:27:03.136 INFO: train_e/atom_rmse: 0.001854
train_f_mae: 0.019534
2025-06-11 15:27:03.142 INFO: train_f_mae: 0.019534
train_f_rmse: 0.025213
2025-06-11 15:27:03.143 INFO: train_f_rmse: 0.025213
val_e/atom_mae: 0.003591
2025-06-11 15:27:03.147 INFO: val_e/atom_mae: 0.003591
val_e/atom_rmse: 0.003610
2025-06-11 15:27:03.149 INFO: val_e/atom_rmse: 0.003610
val_f_mae: 0.018689
2025-06-11 15:27:03.150 INFO: val_f_mae: 0.018689
val_f_rmse: 0.024099
2025-06-11 15:27:03.151 INFO: val_f_rmse: 0.024099
##### Step: 3 Learning rate: 0.008 #####
2025-06-11 15:29:17.559 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 0.6774, Val Loss: 0.6921
2025-06-11 15:29:17.561 INFO: Epoch 4, Train Loss: 0.6774, Val Loss: 0.6921
train_e/atom_mae: 0.001997
2025-06-11 15:29:17.563 INFO: train_e/atom_mae: 0.001997
train_e/atom_rmse: 0.002601
2025-06-11 15:29:17.564 INFO: train_e/atom_rmse: 0.002601
train_f_mae: 0.019793
2025-06-11 15:29:17.570 INFO: train_f_mae: 0.019793
train_f_rmse: 0.025543
2025-06-11 15:29:17.571 INFO: train_f_rmse: 0.025543
val_e/atom_mae: 0.003118
2025-06-11 15:29:17.576 INFO: val_e/atom_mae: 0.003118
val_e/atom_rmse: 0.003167
2025-06-11 15:29:17.577 INFO: val_e/atom_rmse: 0.003167
val_f_mae: 0.019671
2025-06-11 15:29:17.578 INFO: val_f_mae: 0.019671
val_f_rmse: 0.025596
2025-06-11 15:29:17.579 INFO: val_f_rmse: 0.025596
##### Step: 4 Learning rate: 0.01 #####
2025-06-11 15:31:31.994 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 0.6675, Val Loss: 0.6808
2025-06-11 15:31:31.996 INFO: Epoch 5, Train Loss: 0.6675, Val Loss: 0.6808
train_e/atom_mae: 0.001860
2025-06-11 15:31:31.999 INFO: train_e/atom_mae: 0.001860
train_e/atom_rmse: 0.002554
2025-06-11 15:31:32.000 INFO: train_e/atom_rmse: 0.002554
train_f_mae: 0.019592
2025-06-11 15:31:32.006 INFO: train_f_mae: 0.019592
train_f_rmse: 0.025366
2025-06-11 15:31:32.007 INFO: train_f_rmse: 0.025366
val_e/atom_mae: 0.003534
2025-06-11 15:31:32.012 INFO: val_e/atom_mae: 0.003534
val_e/atom_rmse: 0.003569
2025-06-11 15:31:32.013 INFO: val_e/atom_rmse: 0.003569
val_f_mae: 0.019306
2025-06-11 15:31:32.014 INFO: val_f_mae: 0.019306
val_f_rmse: 0.025176
2025-06-11 15:31:32.015 INFO: val_f_rmse: 0.025176
##### Step: 5 Learning rate: 0.01 #####
2025-06-11 15:33:46.472 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 0.7150, Val Loss: 1.0363
2025-06-11 15:33:46.473 INFO: Epoch 6, Train Loss: 0.7150, Val Loss: 1.0363
train_e/atom_mae: 0.002343
2025-06-11 15:33:46.476 INFO: train_e/atom_mae: 0.002343
train_e/atom_rmse: 0.003037
2025-06-11 15:33:46.477 INFO: train_e/atom_rmse: 0.003037
train_f_mae: 0.020202
2025-06-11 15:33:46.483 INFO: train_f_mae: 0.020202
train_f_rmse: 0.026096
2025-06-11 15:33:46.484 INFO: train_f_rmse: 0.026096
val_e/atom_mae: 0.002722
2025-06-11 15:33:46.488 INFO: val_e/atom_mae: 0.002722
val_e/atom_rmse: 0.002734
2025-06-11 15:33:46.490 INFO: val_e/atom_rmse: 0.002734
val_f_mae: 0.025081
2025-06-11 15:33:46.491 INFO: val_f_mae: 0.025081
val_f_rmse: 0.031760
2025-06-11 15:33:46.492 INFO: val_f_rmse: 0.031760
##### Step: 6 Learning rate: 0.01 #####
2025-06-11 15:36:00.956 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 0.8329, Val Loss: 0.7158
2025-06-11 15:36:00.957 INFO: Epoch 7, Train Loss: 0.8329, Val Loss: 0.7158
train_e/atom_mae: 0.003612
2025-06-11 15:36:00.960 INFO: train_e/atom_mae: 0.003612
train_e/atom_rmse: 0.004293
2025-06-11 15:36:00.961 INFO: train_e/atom_rmse: 0.004293
train_f_mae: 0.021239
2025-06-11 15:36:00.967 INFO: train_f_mae: 0.021239
train_f_rmse: 0.027658
2025-06-11 15:36:00.968 INFO: train_f_rmse: 0.027658
val_e/atom_mae: 0.005041
2025-06-11 15:36:00.972 INFO: val_e/atom_mae: 0.005041
val_e/atom_rmse: 0.005048
2025-06-11 15:36:00.974 INFO: val_e/atom_rmse: 0.005048
val_f_mae: 0.019326
2025-06-11 15:36:00.975 INFO: val_f_mae: 0.019326
val_f_rmse: 0.024937
2025-06-11 15:36:00.976 INFO: val_f_rmse: 0.024937
##### Step: 7 Learning rate: 0.01 #####
2025-06-11 15:38:15.508 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 1.4028, Val Loss: 1.3138
2025-06-11 15:38:15.510 INFO: Epoch 8, Train Loss: 1.4028, Val Loss: 1.3138
train_e/atom_mae: 0.006550
2025-06-11 15:38:15.512 INFO: train_e/atom_mae: 0.006550
train_e/atom_rmse: 0.007595
2025-06-11 15:38:15.513 INFO: train_e/atom_rmse: 0.007595
train_f_mae: 0.026874
2025-06-11 15:38:15.519 INFO: train_f_mae: 0.026874
train_f_rmse: 0.034498
2025-06-11 15:38:15.520 INFO: train_f_rmse: 0.034498
val_e/atom_mae: 0.012103
2025-06-11 15:38:15.525 INFO: val_e/atom_mae: 0.012103
val_e/atom_rmse: 0.012106
2025-06-11 15:38:15.526 INFO: val_e/atom_rmse: 0.012106
val_f_mae: 0.021655
2025-06-11 15:38:15.528 INFO: val_f_mae: 0.021655
val_f_rmse: 0.027813
2025-06-11 15:38:15.528 INFO: val_f_rmse: 0.027813
##### Step: 8 Learning rate: 0.01 #####
2025-06-11 15:40:29.970 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 1.3429, Val Loss: 1.0912
2025-06-11 15:40:29.971 INFO: Epoch 9, Train Loss: 1.3429, Val Loss: 1.0912
train_e/atom_mae: 0.005986
2025-06-11 15:40:29.974 INFO: train_e/atom_mae: 0.005986
train_e/atom_rmse: 0.007128
2025-06-11 15:40:29.975 INFO: train_e/atom_rmse: 0.007128
train_f_mae: 0.026487
2025-06-11 15:40:29.981 INFO: train_f_mae: 0.026487
train_f_rmse: 0.033995
2025-06-11 15:40:29.982 INFO: train_f_rmse: 0.033995
val_e/atom_mae: 0.001070
2025-06-11 15:40:29.986 INFO: val_e/atom_mae: 0.001070
val_e/atom_rmse: 0.001187
2025-06-11 15:40:29.988 INFO: val_e/atom_rmse: 0.001187
val_f_mae: 0.025795
2025-06-11 15:40:29.989 INFO: val_f_mae: 0.025795
val_f_rmse: 0.032954
2025-06-11 15:40:29.990 INFO: val_f_rmse: 0.032954
##### Step: 9 Learning rate: 0.01 #####
2025-06-11 15:42:44.439 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 0.9359, Val Loss: 0.8936
2025-06-11 15:42:44.440 INFO: Epoch 10, Train Loss: 0.9359, Val Loss: 0.8936
train_e/atom_mae: 0.004413
2025-06-11 15:42:44.443 INFO: train_e/atom_mae: 0.004413
train_e/atom_rmse: 0.005571
2025-06-11 15:42:44.443 INFO: train_e/atom_rmse: 0.005571
train_f_mae: 0.022227
2025-06-11 15:42:44.450 INFO: train_f_mae: 0.022227
train_f_rmse: 0.028661
2025-06-11 15:42:44.451 INFO: train_f_rmse: 0.028661
val_e/atom_mae: 0.001750
2025-06-11 15:42:44.455 INFO: val_e/atom_mae: 0.001750
val_e/atom_rmse: 0.001780
2025-06-11 15:42:44.456 INFO: val_e/atom_rmse: 0.001780
val_f_mae: 0.023291
2025-06-11 15:42:44.458 INFO: val_f_mae: 0.023291
val_f_rmse: 0.029698
2025-06-11 15:42:44.458 INFO: val_f_rmse: 0.029698
##### Step: 10 Learning rate: 0.01 #####
2025-06-11 15:44:58.967 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 0.9059, Val Loss: 0.6888
2025-06-11 15:44:58.969 INFO: Epoch 11, Train Loss: 0.9059, Val Loss: 0.6888
train_e/atom_mae: 0.004994
2025-06-11 15:44:58.971 INFO: train_e/atom_mae: 0.004994
train_e/atom_rmse: 0.005805
2025-06-11 15:44:58.972 INFO: train_e/atom_rmse: 0.005805
train_f_mae: 0.021725
2025-06-11 15:44:58.978 INFO: train_f_mae: 0.021725
train_f_rmse: 0.027958
2025-06-11 15:44:58.979 INFO: train_f_rmse: 0.027958
val_e/atom_mae: 0.003926
2025-06-11 15:44:58.984 INFO: val_e/atom_mae: 0.003926
val_e/atom_rmse: 0.003945
2025-06-11 15:44:58.985 INFO: val_e/atom_rmse: 0.003945
val_f_mae: 0.019421
2025-06-11 15:44:58.986 INFO: val_f_mae: 0.019421
val_f_rmse: 0.025129
2025-06-11 15:44:58.987 INFO: val_f_rmse: 0.025129
##### Step: 11 Learning rate: 0.01 #####
2025-06-11 15:47:13.480 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 1.0543, Val Loss: 0.7833
2025-06-11 15:47:13.481 INFO: Epoch 12, Train Loss: 1.0543, Val Loss: 0.7833
train_e/atom_mae: 0.005575
2025-06-11 15:47:13.484 INFO: train_e/atom_mae: 0.005575
train_e/atom_rmse: 0.006636
2025-06-11 15:47:13.485 INFO: train_e/atom_rmse: 0.006636
train_f_mae: 0.023233
2025-06-11 15:47:13.491 INFO: train_f_mae: 0.023233
train_f_rmse: 0.029867
2025-06-11 15:47:13.492 INFO: train_f_rmse: 0.029867
val_e/atom_mae: 0.000914
2025-06-11 15:47:13.496 INFO: val_e/atom_mae: 0.000914
val_e/atom_rmse: 0.000959
2025-06-11 15:47:13.498 INFO: val_e/atom_rmse: 0.000959
val_f_mae: 0.021350
2025-06-11 15:47:13.499 INFO: val_f_mae: 0.021350
val_f_rmse: 0.027927
2025-06-11 15:47:13.500 INFO: val_f_rmse: 0.027927
##### Step: 12 Learning rate: 0.01 #####
2025-06-11 15:49:28.004 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 0.7919, Val Loss: 0.9600
2025-06-11 15:49:28.005 INFO: Epoch 13, Train Loss: 0.7919, Val Loss: 0.9600
train_e/atom_mae: 0.002428
2025-06-11 15:49:28.007 INFO: train_e/atom_mae: 0.002428
train_e/atom_rmse: 0.003019
2025-06-11 15:49:28.008 INFO: train_e/atom_rmse: 0.003019
train_f_mae: 0.021259
2025-06-11 15:49:28.014 INFO: train_f_mae: 0.021259
train_f_rmse: 0.027538
2025-06-11 15:49:28.015 INFO: train_f_rmse: 0.027538
val_e/atom_mae: 0.004491
2025-06-11 15:49:28.020 INFO: val_e/atom_mae: 0.004491
val_e/atom_rmse: 0.004507
2025-06-11 15:49:28.021 INFO: val_e/atom_rmse: 0.004507
val_f_mae: 0.023366
2025-06-11 15:49:28.022 INFO: val_f_mae: 0.023366
val_f_rmse: 0.029751
2025-06-11 15:49:28.023 INFO: val_f_rmse: 0.029751
##### Step: 13 Learning rate: 0.01 #####
2025-06-11 15:51:42.521 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 0.7887, Val Loss: 0.9493
2025-06-11 15:51:42.522 INFO: Epoch 14, Train Loss: 0.7887, Val Loss: 0.9493
train_e/atom_mae: 0.003062
2025-06-11 15:51:42.525 INFO: train_e/atom_mae: 0.003062
train_e/atom_rmse: 0.003759
2025-06-11 15:51:42.526 INFO: train_e/atom_rmse: 0.003759
train_f_mae: 0.021038
2025-06-11 15:51:42.532 INFO: train_f_mae: 0.021038
train_f_rmse: 0.027140
2025-06-11 15:51:42.533 INFO: train_f_rmse: 0.027140
val_e/atom_mae: 0.002998
2025-06-11 15:51:42.537 INFO: val_e/atom_mae: 0.002998
val_e/atom_rmse: 0.003013
2025-06-11 15:51:42.539 INFO: val_e/atom_rmse: 0.003013
val_f_mae: 0.023727
2025-06-11 15:51:42.540 INFO: val_f_mae: 0.023727
val_f_rmse: 0.030263
2025-06-11 15:51:42.541 INFO: val_f_rmse: 0.030263
##### Step: 14 Learning rate: 0.01 #####
2025-06-11 15:53:57.009 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 1.0265, Val Loss: 1.0372
2025-06-11 15:53:57.011 INFO: Epoch 15, Train Loss: 1.0265, Val Loss: 1.0372
train_e/atom_mae: 0.005125
2025-06-11 15:53:57.014 INFO: train_e/atom_mae: 0.005125
train_e/atom_rmse: 0.005874
2025-06-11 15:53:57.014 INFO: train_e/atom_rmse: 0.005874
train_f_mae: 0.023327
2025-06-11 15:53:57.021 INFO: train_f_mae: 0.023327
train_f_rmse: 0.029989
2025-06-11 15:53:57.021 INFO: train_f_rmse: 0.029989
val_e/atom_mae: 0.010469
2025-06-11 15:53:57.026 INFO: val_e/atom_mae: 0.010469
val_e/atom_rmse: 0.010475
2025-06-11 15:53:57.027 INFO: val_e/atom_rmse: 0.010475
val_f_mae: 0.019394
2025-06-11 15:53:57.029 INFO: val_f_mae: 0.019394
val_f_rmse: 0.025153
2025-06-11 15:53:57.029 INFO: val_f_rmse: 0.025153
##### Step: 15 Learning rate: 0.01 #####
2025-06-11 15:56:11.572 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 0.9775, Val Loss: 0.8377
2025-06-11 15:56:11.574 INFO: Epoch 16, Train Loss: 0.9775, Val Loss: 0.8377
train_e/atom_mae: 0.005337
2025-06-11 15:56:11.576 INFO: train_e/atom_mae: 0.005337
train_e/atom_rmse: 0.006084
2025-06-11 15:56:11.577 INFO: train_e/atom_rmse: 0.006084
train_f_mae: 0.022581
2025-06-11 15:56:11.583 INFO: train_f_mae: 0.022581
train_f_rmse: 0.029001
2025-06-11 15:56:11.584 INFO: train_f_rmse: 0.029001
val_e/atom_mae: 0.005465
2025-06-11 15:56:11.589 INFO: val_e/atom_mae: 0.005465
val_e/atom_rmse: 0.005472
2025-06-11 15:56:11.590 INFO: val_e/atom_rmse: 0.005472
val_f_mae: 0.020809
2025-06-11 15:56:11.592 INFO: val_f_mae: 0.020809
val_f_rmse: 0.026968
2025-06-11 15:56:11.592 INFO: val_f_rmse: 0.026968
##### Step: 16 Learning rate: 0.01 #####
2025-06-11 15:58:26.073 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 0.8240, Val Loss: 0.8686
2025-06-11 15:58:26.075 INFO: Epoch 17, Train Loss: 0.8240, Val Loss: 0.8686
train_e/atom_mae: 0.004131
2025-06-11 15:58:26.077 INFO: train_e/atom_mae: 0.004131
train_e/atom_rmse: 0.004820
2025-06-11 15:58:26.078 INFO: train_e/atom_rmse: 0.004820
train_f_mae: 0.021140
2025-06-11 15:58:26.084 INFO: train_f_mae: 0.021140
train_f_rmse: 0.027174
2025-06-11 15:58:26.085 INFO: train_f_rmse: 0.027174
val_e/atom_mae: 0.001976
2025-06-11 15:58:26.090 INFO: val_e/atom_mae: 0.001976
val_e/atom_rmse: 0.002006
2025-06-11 15:58:26.091 INFO: val_e/atom_rmse: 0.002006
val_f_mae: 0.022761
2025-06-11 15:58:26.092 INFO: val_f_mae: 0.022761
val_f_rmse: 0.029219
2025-06-11 15:58:26.093 INFO: val_f_rmse: 0.029219
##### Step: 17 Learning rate: 0.01 #####
2025-06-11 16:00:40.517 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 0.9457, Val Loss: 1.2521
2025-06-11 16:00:40.518 INFO: Epoch 18, Train Loss: 0.9457, Val Loss: 1.2521
train_e/atom_mae: 0.005200
2025-06-11 16:00:40.520 INFO: train_e/atom_mae: 0.005200
train_e/atom_rmse: 0.005937
2025-06-11 16:00:40.521 INFO: train_e/atom_rmse: 0.005937
train_f_mae: 0.022239
2025-06-11 16:00:40.527 INFO: train_f_mae: 0.022239
train_f_rmse: 0.028561
2025-06-11 16:00:40.528 INFO: train_f_rmse: 0.028561
val_e/atom_mae: 0.002620
2025-06-11 16:00:40.533 INFO: val_e/atom_mae: 0.002620
val_e/atom_rmse: 0.002688
2025-06-11 16:00:40.534 INFO: val_e/atom_rmse: 0.002688
val_f_mae: 0.027486
2025-06-11 16:00:40.536 INFO: val_f_mae: 0.027486
val_f_rmse: 0.035006
2025-06-11 16:00:40.536 INFO: val_f_rmse: 0.035006
##### Step: 18 Learning rate: 0.01 #####
2025-06-11 16:02:54.954 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 0.8867, Val Loss: 0.7316
2025-06-11 16:02:54.955 INFO: Epoch 19, Train Loss: 0.8867, Val Loss: 0.7316
train_e/atom_mae: 0.003326
2025-06-11 16:02:54.958 INFO: train_e/atom_mae: 0.003326
train_e/atom_rmse: 0.004275
2025-06-11 16:02:54.959 INFO: train_e/atom_rmse: 0.004275
train_f_mae: 0.022218
2025-06-11 16:02:54.965 INFO: train_f_mae: 0.022218
train_f_rmse: 0.028624
2025-06-11 16:02:54.966 INFO: train_f_rmse: 0.028624
val_e/atom_mae: 0.002647
2025-06-11 16:02:54.970 INFO: val_e/atom_mae: 0.002647
val_e/atom_rmse: 0.002662
2025-06-11 16:02:54.972 INFO: val_e/atom_rmse: 0.002662
val_f_mae: 0.020503
2025-06-11 16:02:54.973 INFO: val_f_mae: 0.020503
val_f_rmse: 0.026561
2025-06-11 16:02:54.974 INFO: val_f_rmse: 0.026561
##### Step: 19 Learning rate: 0.01 #####
2025-06-11 16:05:09.463 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 1.0511, Val Loss: 0.7877
2025-06-11 16:05:09.464 INFO: Epoch 20, Train Loss: 1.0511, Val Loss: 0.7877
train_e/atom_mae: 0.006960
2025-06-11 16:05:09.467 INFO: train_e/atom_mae: 0.006960
train_e/atom_rmse: 0.008760
2025-06-11 16:05:09.468 INFO: train_e/atom_rmse: 0.008760
train_f_mae: 0.021514
2025-06-11 16:05:09.474 INFO: train_f_mae: 0.021514
train_f_rmse: 0.027717
2025-06-11 16:05:09.475 INFO: train_f_rmse: 0.027717
val_e/atom_mae: 0.003325
2025-06-11 16:05:09.480 INFO: val_e/atom_mae: 0.003325
val_e/atom_rmse: 0.003347
2025-06-11 16:05:09.481 INFO: val_e/atom_rmse: 0.003347
val_f_mae: 0.021219
2025-06-11 16:05:09.482 INFO: val_f_mae: 0.021219
val_f_rmse: 0.027320
2025-06-11 16:05:09.483 INFO: val_f_rmse: 0.027320
##### Step: 20 Learning rate: 0.005 #####
2025-06-11 16:07:23.947 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 0.6078, Val Loss: 0.6337
2025-06-11 16:07:23.948 INFO: Epoch 21, Train Loss: 0.6078, Val Loss: 0.6337
train_e/atom_mae: 0.002345
2025-06-11 16:07:23.950 INFO: train_e/atom_mae: 0.002345
train_e/atom_rmse: 0.002890
2025-06-11 16:07:23.951 INFO: train_e/atom_rmse: 0.002890
train_f_mae: 0.018553
2025-06-11 16:07:23.957 INFO: train_f_mae: 0.018553
train_f_rmse: 0.024021
2025-06-11 16:07:23.958 INFO: train_f_rmse: 0.024021
val_e/atom_mae: 0.000331
2025-06-11 16:07:23.963 INFO: val_e/atom_mae: 0.000331
val_e/atom_rmse: 0.000427
2025-06-11 16:07:23.964 INFO: val_e/atom_rmse: 0.000427
val_f_mae: 0.019296
2025-06-11 16:07:23.965 INFO: val_f_mae: 0.019296
val_f_rmse: 0.025160
2025-06-11 16:07:23.966 INFO: val_f_rmse: 0.025160
##### Step: 21 Learning rate: 0.005 #####
2025-06-11 16:09:38.433 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 0.6230, Val Loss: 0.5427
2025-06-11 16:09:38.434 INFO: Epoch 22, Train Loss: 0.6230, Val Loss: 0.5427
train_e/atom_mae: 0.001717
2025-06-11 16:09:38.437 INFO: train_e/atom_mae: 0.001717
train_e/atom_rmse: 0.002009
2025-06-11 16:09:38.438 INFO: train_e/atom_rmse: 0.002009
train_f_mae: 0.019043
2025-06-11 16:09:38.444 INFO: train_f_mae: 0.019043
train_f_rmse: 0.024660
2025-06-11 16:09:38.445 INFO: train_f_rmse: 0.024660
val_e/atom_mae: 0.000265
2025-06-11 16:09:38.449 INFO: val_e/atom_mae: 0.000265
val_e/atom_rmse: 0.000340
2025-06-11 16:09:38.451 INFO: val_e/atom_rmse: 0.000340
val_f_mae: 0.018016
2025-06-11 16:09:38.452 INFO: val_f_mae: 0.018016
val_f_rmse: 0.023287
2025-06-11 16:09:38.453 INFO: val_f_rmse: 0.023287
##### Step: 22 Learning rate: 0.005 #####
2025-06-11 16:11:53.065 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 0.5794, Val Loss: 0.5351
2025-06-11 16:11:53.068 INFO: Epoch 23, Train Loss: 0.5794, Val Loss: 0.5351
train_e/atom_mae: 0.001133
2025-06-11 16:11:53.070 INFO: train_e/atom_mae: 0.001133
train_e/atom_rmse: 0.001334
2025-06-11 16:11:53.071 INFO: train_e/atom_rmse: 0.001334
train_f_mae: 0.018515
2025-06-11 16:11:53.077 INFO: train_f_mae: 0.018515
train_f_rmse: 0.023934
2025-06-11 16:11:53.078 INFO: train_f_rmse: 0.023934
val_e/atom_mae: 0.000580
2025-06-11 16:11:53.083 INFO: val_e/atom_mae: 0.000580
val_e/atom_rmse: 0.000632
2025-06-11 16:11:53.084 INFO: val_e/atom_rmse: 0.000632
val_f_mae: 0.017825
2025-06-11 16:11:53.085 INFO: val_f_mae: 0.017825
val_f_rmse: 0.023100
2025-06-11 16:11:53.086 INFO: val_f_rmse: 0.023100
##### Step: 23 Learning rate: 0.005 #####
2025-06-11 16:14:07.536 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 0.6618, Val Loss: 0.6385
2025-06-11 16:14:07.538 INFO: Epoch 24, Train Loss: 0.6618, Val Loss: 0.6385
train_e/atom_mae: 0.002612
2025-06-11 16:14:07.540 INFO: train_e/atom_mae: 0.002612
train_e/atom_rmse: 0.003206
2025-06-11 16:14:07.541 INFO: train_e/atom_rmse: 0.003206
train_f_mae: 0.019343
2025-06-11 16:14:07.547 INFO: train_f_mae: 0.019343
train_f_rmse: 0.024978
2025-06-11 16:14:07.548 INFO: train_f_rmse: 0.024978
val_e/atom_mae: 0.002727
2025-06-11 16:14:07.553 INFO: val_e/atom_mae: 0.002727
val_e/atom_rmse: 0.002738
2025-06-11 16:14:07.554 INFO: val_e/atom_rmse: 0.002738
val_f_mae: 0.019109
2025-06-11 16:14:07.556 INFO: val_f_mae: 0.019109
val_f_rmse: 0.024715
2025-06-11 16:14:07.556 INFO: val_f_rmse: 0.024715
##### Step: 24 Learning rate: 0.005 #####
2025-06-11 16:16:22.011 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 0.7091, Val Loss: 0.7955
2025-06-11 16:16:22.012 INFO: Epoch 25, Train Loss: 0.7091, Val Loss: 0.7955
train_e/atom_mae: 0.003538
2025-06-11 16:16:22.015 INFO: train_e/atom_mae: 0.003538
train_e/atom_rmse: 0.004108
2025-06-11 16:16:22.016 INFO: train_e/atom_rmse: 0.004108
train_f_mae: 0.019793
2025-06-11 16:16:22.022 INFO: train_f_mae: 0.019793
train_f_rmse: 0.025434
2025-06-11 16:16:22.023 INFO: train_f_rmse: 0.025434
val_e/atom_mae: 0.006538
2025-06-11 16:16:22.027 INFO: val_e/atom_mae: 0.006538
val_e/atom_rmse: 0.006552
2025-06-11 16:16:22.029 INFO: val_e/atom_rmse: 0.006552
val_f_mae: 0.019541
2025-06-11 16:16:22.030 INFO: val_f_mae: 0.019541
val_f_rmse: 0.025245
2025-06-11 16:16:22.031 INFO: val_f_rmse: 0.025245
##### Step: 25 Learning rate: 0.005 #####
2025-06-11 16:18:36.564 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 0.6942, Val Loss: 0.6286
2025-06-11 16:18:36.565 INFO: Epoch 26, Train Loss: 0.6942, Val Loss: 0.6286
train_e/atom_mae: 0.003458
2025-06-11 16:18:36.567 INFO: train_e/atom_mae: 0.003458
train_e/atom_rmse: 0.004100
2025-06-11 16:18:36.568 INFO: train_e/atom_rmse: 0.004100
train_f_mae: 0.019492
2025-06-11 16:18:36.574 INFO: train_f_mae: 0.019492
train_f_rmse: 0.025144
2025-06-11 16:18:36.575 INFO: train_f_rmse: 0.025144
val_e/atom_mae: 0.000519
2025-06-11 16:18:36.580 INFO: val_e/atom_mae: 0.000519
val_e/atom_rmse: 0.000601
2025-06-11 16:18:36.581 INFO: val_e/atom_rmse: 0.000601
val_f_mae: 0.019594
2025-06-11 16:18:36.582 INFO: val_f_mae: 0.019594
val_f_rmse: 0.025046
2025-06-11 16:18:36.583 INFO: val_f_rmse: 0.025046
##### Step: 26 Learning rate: 0.005 #####
2025-06-11 16:20:50.998 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 0.6340, Val Loss: 0.5621
2025-06-11 16:20:51.001 INFO: Epoch 27, Train Loss: 0.6340, Val Loss: 0.5621
train_e/atom_mae: 0.002342
2025-06-11 16:20:51.003 INFO: train_e/atom_mae: 0.002342
train_e/atom_rmse: 0.002916
2025-06-11 16:20:51.004 INFO: train_e/atom_rmse: 0.002916
train_f_mae: 0.019023
2025-06-11 16:20:51.010 INFO: train_f_mae: 0.019023
train_f_rmse: 0.024550
2025-06-11 16:20:51.011 INFO: train_f_rmse: 0.024550
val_e/atom_mae: 0.000355
2025-06-11 16:20:51.016 INFO: val_e/atom_mae: 0.000355
val_e/atom_rmse: 0.000421
2025-06-11 16:20:51.017 INFO: val_e/atom_rmse: 0.000421
val_f_mae: 0.018460
2025-06-11 16:20:51.018 INFO: val_f_mae: 0.018460
val_f_rmse: 0.023694
2025-06-11 16:20:51.019 INFO: val_f_rmse: 0.023694
##### Step: 27 Learning rate: 0.005 #####
2025-06-11 16:23:05.685 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 0.5725, Val Loss: 0.5106
2025-06-11 16:23:05.687 INFO: Epoch 28, Train Loss: 0.5725, Val Loss: 0.5106
train_e/atom_mae: 0.001778
2025-06-11 16:23:05.689 INFO: train_e/atom_mae: 0.001778
train_e/atom_rmse: 0.002167
2025-06-11 16:23:05.690 INFO: train_e/atom_rmse: 0.002167
train_f_mae: 0.018250
2025-06-11 16:23:05.696 INFO: train_f_mae: 0.018250
train_f_rmse: 0.023562
2025-06-11 16:23:05.697 INFO: train_f_rmse: 0.023562
val_e/atom_mae: 0.000246
2025-06-11 16:23:05.702 INFO: val_e/atom_mae: 0.000246
val_e/atom_rmse: 0.000311
2025-06-11 16:23:05.703 INFO: val_e/atom_rmse: 0.000311
val_f_mae: 0.017440
2025-06-11 16:23:05.705 INFO: val_f_mae: 0.017440
val_f_rmse: 0.022588
2025-06-11 16:23:05.705 INFO: val_f_rmse: 0.022588
##### Step: 28 Learning rate: 0.005 #####
2025-06-11 16:25:20.414 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 0.5756, Val Loss: 0.6193
2025-06-11 16:25:20.416 INFO: Epoch 29, Train Loss: 0.5756, Val Loss: 0.6193
train_e/atom_mae: 0.001482
2025-06-11 16:25:20.419 INFO: train_e/atom_mae: 0.001482
train_e/atom_rmse: 0.001925
2025-06-11 16:25:20.420 INFO: train_e/atom_rmse: 0.001925
train_f_mae: 0.018319
2025-06-11 16:25:20.426 INFO: train_f_mae: 0.018319
train_f_rmse: 0.023705
2025-06-11 16:25:20.427 INFO: train_f_rmse: 0.023705
val_e/atom_mae: 0.002493
2025-06-11 16:25:20.431 INFO: val_e/atom_mae: 0.002493
val_e/atom_rmse: 0.002505
2025-06-11 16:25:20.433 INFO: val_e/atom_rmse: 0.002505
val_f_mae: 0.019019
2025-06-11 16:25:20.434 INFO: val_f_mae: 0.019019
val_f_rmse: 0.024416
2025-06-11 16:25:20.435 INFO: val_f_rmse: 0.024416
##### Step: 29 Learning rate: 0.005 #####
2025-06-11 16:27:35.196 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 0.6101, Val Loss: 0.7061
2025-06-11 16:27:35.197 INFO: Epoch 30, Train Loss: 0.6101, Val Loss: 0.7061
train_e/atom_mae: 0.002041
2025-06-11 16:27:35.199 INFO: train_e/atom_mae: 0.002041
train_e/atom_rmse: 0.002690
2025-06-11 16:27:35.200 INFO: train_e/atom_rmse: 0.002690
train_f_mae: 0.018662
2025-06-11 16:27:35.206 INFO: train_f_mae: 0.018662
train_f_rmse: 0.024154
2025-06-11 16:27:35.207 INFO: train_f_rmse: 0.024154
val_e/atom_mae: 0.006338
2025-06-11 16:27:35.212 INFO: val_e/atom_mae: 0.006338
val_e/atom_rmse: 0.006342
2025-06-11 16:27:35.213 INFO: val_e/atom_rmse: 0.006342
val_f_mae: 0.018277
2025-06-11 16:27:35.214 INFO: val_f_mae: 0.018277
val_f_rmse: 0.023619
2025-06-11 16:27:35.215 INFO: val_f_rmse: 0.023619
##### Step: 30 Learning rate: 0.005 #####
2025-06-11 16:29:49.993 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 0.6651, Val Loss: 0.5190
2025-06-11 16:29:49.994 INFO: Epoch 31, Train Loss: 0.6651, Val Loss: 0.5190
train_e/atom_mae: 0.003377
2025-06-11 16:29:49.996 INFO: train_e/atom_mae: 0.003377
train_e/atom_rmse: 0.004553
2025-06-11 16:29:49.997 INFO: train_e/atom_rmse: 0.004553
train_f_mae: 0.018793
2025-06-11 16:29:50.003 INFO: train_f_mae: 0.018793
train_f_rmse: 0.024262
2025-06-11 16:29:50.004 INFO: train_f_rmse: 0.024262
val_e/atom_mae: 0.000592
2025-06-11 16:29:50.009 INFO: val_e/atom_mae: 0.000592
val_e/atom_rmse: 0.000653
2025-06-11 16:29:50.010 INFO: val_e/atom_rmse: 0.000653
val_f_mae: 0.017667
2025-06-11 16:29:50.012 INFO: val_f_mae: 0.017667
val_f_rmse: 0.022748
2025-06-11 16:29:50.012 INFO: val_f_rmse: 0.022748
##### Step: 31 Learning rate: 0.005 #####
2025-06-11 16:32:04.825 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 0.6080, Val Loss: 0.6299
2025-06-11 16:32:04.827 INFO: Epoch 32, Train Loss: 0.6080, Val Loss: 0.6299
train_e/atom_mae: 0.003288
2025-06-11 16:32:04.829 INFO: train_e/atom_mae: 0.003288
train_e/atom_rmse: 0.004036
2025-06-11 16:32:04.830 INFO: train_e/atom_rmse: 0.004036
train_f_mae: 0.018124
2025-06-11 16:32:04.836 INFO: train_f_mae: 0.018124
train_f_rmse: 0.023408
2025-06-11 16:32:04.837 INFO: train_f_rmse: 0.023408
val_e/atom_mae: 0.005180
2025-06-11 16:32:04.842 INFO: val_e/atom_mae: 0.005180
val_e/atom_rmse: 0.005186
2025-06-11 16:32:04.843 INFO: val_e/atom_rmse: 0.005186
val_f_mae: 0.017757
2025-06-11 16:32:04.844 INFO: val_f_mae: 0.017757
val_f_rmse: 0.023037
2025-06-11 16:32:04.845 INFO: val_f_rmse: 0.023037
##### Step: 32 Learning rate: 0.005 #####
2025-06-11 16:34:19.522 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 0.5643, Val Loss: 0.5723
2025-06-11 16:34:19.524 INFO: Epoch 33, Train Loss: 0.5643, Val Loss: 0.5723
train_e/atom_mae: 0.002281
2025-06-11 16:34:19.526 INFO: train_e/atom_mae: 0.002281
train_e/atom_rmse: 0.002819
2025-06-11 16:34:19.527 INFO: train_e/atom_rmse: 0.002819
train_f_mae: 0.017896
2025-06-11 16:34:19.533 INFO: train_f_mae: 0.017896
train_f_rmse: 0.023130
2025-06-11 16:34:19.534 INFO: train_f_rmse: 0.023130
val_e/atom_mae: 0.002677
2025-06-11 16:34:19.539 INFO: val_e/atom_mae: 0.002677
val_e/atom_rmse: 0.002694
2025-06-11 16:34:19.540 INFO: val_e/atom_rmse: 0.002694
val_f_mae: 0.018093
2025-06-11 16:34:19.542 INFO: val_f_mae: 0.018093
val_f_rmse: 0.023358
2025-06-11 16:34:19.542 INFO: val_f_rmse: 0.023358
##### Step: 33 Learning rate: 0.005 #####
2025-06-11 16:36:34.277 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 0.5937, Val Loss: 0.7474
2025-06-11 16:36:34.278 INFO: Epoch 34, Train Loss: 0.5937, Val Loss: 0.7474
train_e/atom_mae: 0.002274
2025-06-11 16:36:34.281 INFO: train_e/atom_mae: 0.002274
train_e/atom_rmse: 0.002672
2025-06-11 16:36:34.282 INFO: train_e/atom_rmse: 0.002672
train_f_mae: 0.018451
2025-06-11 16:36:34.288 INFO: train_f_mae: 0.018451
train_f_rmse: 0.023819
2025-06-11 16:36:34.289 INFO: train_f_rmse: 0.023819
val_e/atom_mae: 0.001838
2025-06-11 16:36:34.293 INFO: val_e/atom_mae: 0.001838
val_e/atom_rmse: 0.001857
2025-06-11 16:36:34.294 INFO: val_e/atom_rmse: 0.001857
val_f_mae: 0.021287
2025-06-11 16:36:34.296 INFO: val_f_mae: 0.021287
val_f_rmse: 0.027105
2025-06-11 16:36:34.297 INFO: val_f_rmse: 0.027105
##### Step: 34 Learning rate: 0.005 #####
2025-06-11 16:38:49.045 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 0.5666, Val Loss: 0.5381
2025-06-11 16:38:49.046 INFO: Epoch 35, Train Loss: 0.5666, Val Loss: 0.5381
train_e/atom_mae: 0.001588
2025-06-11 16:38:49.049 INFO: train_e/atom_mae: 0.001588
train_e/atom_rmse: 0.001914
2025-06-11 16:38:49.050 INFO: train_e/atom_rmse: 0.001914
train_f_mae: 0.018204
2025-06-11 16:38:49.056 INFO: train_f_mae: 0.018204
train_f_rmse: 0.023519
2025-06-11 16:38:49.057 INFO: train_f_rmse: 0.023519
val_e/atom_mae: 0.001129
2025-06-11 16:38:49.061 INFO: val_e/atom_mae: 0.001129
val_e/atom_rmse: 0.001167
2025-06-11 16:38:49.063 INFO: val_e/atom_rmse: 0.001167
val_f_mae: 0.017829
2025-06-11 16:38:49.064 INFO: val_f_mae: 0.017829
val_f_rmse: 0.023089
2025-06-11 16:38:49.065 INFO: val_f_rmse: 0.023089
##### Step: 35 Learning rate: 0.005 #####
2025-06-11 16:41:03.833 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 0.6570, Val Loss: 1.0090
2025-06-11 16:41:03.834 INFO: Epoch 36, Train Loss: 0.6570, Val Loss: 1.0090
train_e/atom_mae: 0.003603
2025-06-11 16:41:03.837 INFO: train_e/atom_mae: 0.003603
train_e/atom_rmse: 0.004395
2025-06-11 16:41:03.837 INFO: train_e/atom_rmse: 0.004395
train_f_mae: 0.018789
2025-06-11 16:41:03.844 INFO: train_f_mae: 0.018789
train_f_rmse: 0.024202
2025-06-11 16:41:03.844 INFO: train_f_rmse: 0.024202
val_e/atom_mae: 0.009992
2025-06-11 16:41:03.849 INFO: val_e/atom_mae: 0.009992
val_e/atom_rmse: 0.009996
2025-06-11 16:41:03.850 INFO: val_e/atom_rmse: 0.009996
val_f_mae: 0.019514
2025-06-11 16:41:03.852 INFO: val_f_mae: 0.019514
val_f_rmse: 0.025311
2025-06-11 16:41:03.852 INFO: val_f_rmse: 0.025311
##### Step: 36 Learning rate: 0.005 #####
2025-06-11 16:43:18.591 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 0.6035, Val Loss: 0.7428
2025-06-11 16:43:18.592 INFO: Epoch 37, Train Loss: 0.6035, Val Loss: 0.7428
train_e/atom_mae: 0.002561
2025-06-11 16:43:18.595 INFO: train_e/atom_mae: 0.002561
train_e/atom_rmse: 0.003462
2025-06-11 16:43:18.595 INFO: train_e/atom_rmse: 0.003462
train_f_mae: 0.018340
2025-06-11 16:43:18.602 INFO: train_f_mae: 0.018340
train_f_rmse: 0.023649
2025-06-11 16:43:18.602 INFO: train_f_rmse: 0.023649
val_e/atom_mae: 0.006863
2025-06-11 16:43:18.607 INFO: val_e/atom_mae: 0.006863
val_e/atom_rmse: 0.006869
2025-06-11 16:43:18.609 INFO: val_e/atom_rmse: 0.006869
val_f_mae: 0.018471
2025-06-11 16:43:18.610 INFO: val_f_mae: 0.018471
val_f_rmse: 0.023851
2025-06-11 16:43:18.611 INFO: val_f_rmse: 0.023851
##### Step: 37 Learning rate: 0.005 #####
2025-06-11 16:45:33.347 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 0.7732, Val Loss: 0.6471
2025-06-11 16:45:33.350 INFO: Epoch 38, Train Loss: 0.7732, Val Loss: 0.6471
train_e/atom_mae: 0.003925
2025-06-11 16:45:33.352 INFO: train_e/atom_mae: 0.003925
train_e/atom_rmse: 0.004503
2025-06-11 16:45:33.353 INFO: train_e/atom_rmse: 0.004503
train_f_mae: 0.020590
2025-06-11 16:45:33.359 INFO: train_f_mae: 0.020590
train_f_rmse: 0.026429
2025-06-11 16:45:33.360 INFO: train_f_rmse: 0.026429
val_e/atom_mae: 0.003803
2025-06-11 16:45:33.365 INFO: val_e/atom_mae: 0.003803
val_e/atom_rmse: 0.003811
2025-06-11 16:45:33.366 INFO: val_e/atom_rmse: 0.003811
val_f_mae: 0.018894
2025-06-11 16:45:33.368 INFO: val_f_mae: 0.018894
val_f_rmse: 0.024363
2025-06-11 16:45:33.369 INFO: val_f_rmse: 0.024363
##### Step: 38 Learning rate: 0.005 #####
2025-06-11 16:47:48.588 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 0.6291, Val Loss: 0.5617
2025-06-11 16:47:48.589 INFO: Epoch 39, Train Loss: 0.6291, Val Loss: 0.5617
train_e/atom_mae: 0.002650
2025-06-11 16:47:48.591 INFO: train_e/atom_mae: 0.002650
train_e/atom_rmse: 0.003093
2025-06-11 16:47:48.592 INFO: train_e/atom_rmse: 0.003093
train_f_mae: 0.018930
2025-06-11 16:47:48.598 INFO: train_f_mae: 0.018930
train_f_rmse: 0.024368
2025-06-11 16:47:48.599 INFO: train_f_rmse: 0.024368
val_e/atom_mae: 0.000390
2025-06-11 16:47:48.604 INFO: val_e/atom_mae: 0.000390
val_e/atom_rmse: 0.000448
2025-06-11 16:47:48.605 INFO: val_e/atom_rmse: 0.000448
val_f_mae: 0.018406
2025-06-11 16:47:48.606 INFO: val_f_mae: 0.018406
val_f_rmse: 0.023685
2025-06-11 16:47:48.607 INFO: val_f_rmse: 0.023685
##### Step: 39 Learning rate: 0.005 #####
2025-06-11 16:50:03.538 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 0.6757, Val Loss: 0.8164
2025-06-11 16:50:03.539 INFO: Epoch 40, Train Loss: 0.6757, Val Loss: 0.8164
train_e/atom_mae: 0.002988
2025-06-11 16:50:03.542 INFO: train_e/atom_mae: 0.002988
train_e/atom_rmse: 0.003728
2025-06-11 16:50:03.543 INFO: train_e/atom_rmse: 0.003728
train_f_mae: 0.019375
2025-06-11 16:50:03.549 INFO: train_f_mae: 0.019375
train_f_rmse: 0.024989
2025-06-11 16:50:03.550 INFO: train_f_rmse: 0.024989
val_e/atom_mae: 0.003745
2025-06-11 16:50:03.554 INFO: val_e/atom_mae: 0.003745
val_e/atom_rmse: 0.003767
2025-06-11 16:50:03.556 INFO: val_e/atom_rmse: 0.003767
val_f_mae: 0.021542
2025-06-11 16:50:03.557 INFO: val_f_mae: 0.021542
val_f_rmse: 0.027643
2025-06-11 16:50:03.558 INFO: val_f_rmse: 0.027643
2025-06-11 16:50:03.660 INFO: Second train loop:
2025-06-11 16:50:03.661 INFO: training
##### Step: 40 Learning rate: 0.0025 #####
2025-06-11 16:52:18.388 INFO: ##### Step: 40 Learning rate: 0.0025 #####
Epoch 1, Train Loss: 0.7469, Val Loss: 0.5410
2025-06-11 16:52:18.390 INFO: Epoch 1, Train Loss: 0.7469, Val Loss: 0.5410
train_e/atom_mae: 0.001721
2025-06-11 16:52:18.393 INFO: train_e/atom_mae: 0.001721
train_e/atom_rmse: 0.002228
2025-06-11 16:52:18.393 INFO: train_e/atom_rmse: 0.002228
train_f_mae: 0.018451
2025-06-11 16:52:18.399 INFO: train_f_mae: 0.018451
train_f_rmse: 0.023747
2025-06-11 16:52:18.400 INFO: train_f_rmse: 0.023747
val_e/atom_mae: 0.000710
2025-06-11 16:52:18.405 INFO: val_e/atom_mae: 0.000710
val_e/atom_rmse: 0.000776
2025-06-11 16:52:18.406 INFO: val_e/atom_rmse: 0.000776
val_f_mae: 0.017519
2025-06-11 16:52:18.408 INFO: val_f_mae: 0.017519
val_f_rmse: 0.022776
2025-06-11 16:52:18.408 INFO: val_f_rmse: 0.022776
##### Step: 41 Learning rate: 0.0025 #####
2025-06-11 16:54:33.100 INFO: ##### Step: 41 Learning rate: 0.0025 #####
Epoch 2, Train Loss: 0.6170, Val Loss: 0.7161
2025-06-11 16:54:33.102 INFO: Epoch 2, Train Loss: 0.6170, Val Loss: 0.7161
train_e/atom_mae: 0.001257
2025-06-11 16:54:33.104 INFO: train_e/atom_mae: 0.001257
train_e/atom_rmse: 0.001511
2025-06-11 16:54:33.105 INFO: train_e/atom_rmse: 0.001511
train_f_mae: 0.017907
2025-06-11 16:54:33.111 INFO: train_f_mae: 0.017907
train_f_rmse: 0.023085
2025-06-11 16:54:33.112 INFO: train_f_rmse: 0.023085
val_e/atom_mae: 0.002230
2025-06-11 16:54:33.117 INFO: val_e/atom_mae: 0.002230
val_e/atom_rmse: 0.002241
2025-06-11 16:54:33.118 INFO: val_e/atom_rmse: 0.002241
val_f_mae: 0.017916
2025-06-11 16:54:33.119 INFO: val_f_mae: 0.017916
val_f_rmse: 0.023044
2025-06-11 16:54:33.120 INFO: val_f_rmse: 0.023044
##### Step: 42 Learning rate: 0.0025 #####
2025-06-11 16:56:47.951 INFO: ##### Step: 42 Learning rate: 0.0025 #####
Epoch 3, Train Loss: 0.5793, Val Loss: 0.6463
2025-06-11 16:56:47.952 INFO: Epoch 3, Train Loss: 0.5793, Val Loss: 0.6463
train_e/atom_mae: 0.001040
2025-06-11 16:56:47.955 INFO: train_e/atom_mae: 0.001040
train_e/atom_rmse: 0.001249
2025-06-11 16:56:47.956 INFO: train_e/atom_rmse: 0.001249
train_f_mae: 0.017712
2025-06-11 16:56:47.962 INFO: train_f_mae: 0.017712
train_f_rmse: 0.022842
2025-06-11 16:56:47.963 INFO: train_f_rmse: 0.022842
val_e/atom_mae: 0.001135
2025-06-11 16:56:47.967 INFO: val_e/atom_mae: 0.001135
val_e/atom_rmse: 0.001161
2025-06-11 16:56:47.969 INFO: val_e/atom_rmse: 0.001161
val_f_mae: 0.018889
2025-06-11 16:56:47.970 INFO: val_f_mae: 0.018889
val_f_rmse: 0.024426
2025-06-11 16:56:47.971 INFO: val_f_rmse: 0.024426
##### Step: 43 Learning rate: 0.0025 #####
2025-06-11 16:59:02.726 INFO: ##### Step: 43 Learning rate: 0.0025 #####
Epoch 4, Train Loss: 0.5628, Val Loss: 0.5445
2025-06-11 16:59:02.729 INFO: Epoch 4, Train Loss: 0.5628, Val Loss: 0.5445
train_e/atom_mae: 0.000910
2025-06-11 16:59:02.731 INFO: train_e/atom_mae: 0.000910
train_e/atom_rmse: 0.001091
2025-06-11 16:59:02.732 INFO: train_e/atom_rmse: 0.001091
train_f_mae: 0.017669
2025-06-11 16:59:02.738 INFO: train_f_mae: 0.017669
train_f_rmse: 0.022779
2025-06-11 16:59:02.739 INFO: train_f_rmse: 0.022779
val_e/atom_mae: 0.001167
2025-06-11 16:59:02.744 INFO: val_e/atom_mae: 0.001167
val_e/atom_rmse: 0.001193
2025-06-11 16:59:02.745 INFO: val_e/atom_rmse: 0.001193
val_f_mae: 0.017147
2025-06-11 16:59:02.746 INFO: val_f_mae: 0.017147
val_f_rmse: 0.022181
2025-06-11 16:59:02.747 INFO: val_f_rmse: 0.022181
##### Step: 44 Learning rate: 0.0025 #####
2025-06-11 17:01:17.551 INFO: ##### Step: 44 Learning rate: 0.0025 #####
Epoch 5, Train Loss: 0.6108, Val Loss: 0.7029
2025-06-11 17:01:17.553 INFO: Epoch 5, Train Loss: 0.6108, Val Loss: 0.7029
train_e/atom_mae: 0.001057
2025-06-11 17:01:17.555 INFO: train_e/atom_mae: 0.001057
train_e/atom_rmse: 0.001294
2025-06-11 17:01:17.556 INFO: train_e/atom_rmse: 0.001294
train_f_mae: 0.018168
2025-06-11 17:01:17.562 INFO: train_f_mae: 0.018168
train_f_rmse: 0.023431
2025-06-11 17:01:17.563 INFO: train_f_rmse: 0.023431
val_e/atom_mae: 0.001096
2025-06-11 17:01:17.568 INFO: val_e/atom_mae: 0.001096
val_e/atom_rmse: 0.001119
2025-06-11 17:01:17.569 INFO: val_e/atom_rmse: 0.001119
val_f_mae: 0.019959
2025-06-11 17:01:17.570 INFO: val_f_mae: 0.019959
val_f_rmse: 0.025627
2025-06-11 17:01:17.571 INFO: val_f_rmse: 0.025627
##### Step: 45 Learning rate: 0.0025 #####
2025-06-11 17:03:32.344 INFO: ##### Step: 45 Learning rate: 0.0025 #####
Epoch 6, Train Loss: 0.6831, Val Loss: 0.7305
2025-06-11 17:03:32.346 INFO: Epoch 6, Train Loss: 0.6831, Val Loss: 0.7305
train_e/atom_mae: 0.001699
2025-06-11 17:03:32.349 INFO: train_e/atom_mae: 0.001699
train_e/atom_rmse: 0.002063
2025-06-11 17:03:32.350 INFO: train_e/atom_rmse: 0.002063
train_f_mae: 0.017769
2025-06-11 17:03:32.356 INFO: train_f_mae: 0.017769
train_f_rmse: 0.022939
2025-06-11 17:03:32.357 INFO: train_f_rmse: 0.022939
val_e/atom_mae: 0.002572
2025-06-11 17:03:32.361 INFO: val_e/atom_mae: 0.002572
val_e/atom_rmse: 0.002584
2025-06-11 17:03:32.363 INFO: val_e/atom_rmse: 0.002584
val_f_mae: 0.017020
2025-06-11 17:03:32.364 INFO: val_f_mae: 0.017020
val_f_rmse: 0.022006
2025-06-11 17:03:32.365 INFO: val_f_rmse: 0.022006
##### Step: 46 Learning rate: 0.0025 #####
2025-06-11 17:05:47.084 INFO: ##### Step: 46 Learning rate: 0.0025 #####
Epoch 7, Train Loss: 0.6174, Val Loss: 0.4981
2025-06-11 17:05:47.085 INFO: Epoch 7, Train Loss: 0.6174, Val Loss: 0.4981
train_e/atom_mae: 0.001271
2025-06-11 17:05:47.088 INFO: train_e/atom_mae: 0.001271
train_e/atom_rmse: 0.001605
2025-06-11 17:05:47.089 INFO: train_e/atom_rmse: 0.001605
train_f_mae: 0.017745
2025-06-11 17:05:47.095 INFO: train_f_mae: 0.017745
train_f_rmse: 0.022855
2025-06-11 17:05:47.096 INFO: train_f_rmse: 0.022855
val_e/atom_mae: 0.000251
2025-06-11 17:05:47.100 INFO: val_e/atom_mae: 0.000251
val_e/atom_rmse: 0.000306
2025-06-11 17:05:47.102 INFO: val_e/atom_rmse: 0.000306
val_f_mae: 0.017207
2025-06-11 17:05:47.103 INFO: val_f_mae: 0.017207
val_f_rmse: 0.022241
2025-06-11 17:05:47.104 INFO: val_f_rmse: 0.022241
##### Step: 47 Learning rate: 0.0025 #####
2025-06-11 17:08:01.878 INFO: ##### Step: 47 Learning rate: 0.0025 #####
Epoch 8, Train Loss: 0.6800, Val Loss: 0.9142
2025-06-11 17:08:01.879 INFO: Epoch 8, Train Loss: 0.6800, Val Loss: 0.9142
train_e/atom_mae: 0.001476
2025-06-11 17:08:01.882 INFO: train_e/atom_mae: 0.001476
train_e/atom_rmse: 0.001706
2025-06-11 17:08:01.883 INFO: train_e/atom_rmse: 0.001706
train_f_mae: 0.018571
2025-06-11 17:08:01.889 INFO: train_f_mae: 0.018571
train_f_rmse: 0.023930
2025-06-11 17:08:01.890 INFO: train_f_rmse: 0.023930
val_e/atom_mae: 0.003320
2025-06-11 17:08:01.894 INFO: val_e/atom_mae: 0.003320
val_e/atom_rmse: 0.003326
2025-06-11 17:08:01.896 INFO: val_e/atom_rmse: 0.003326
val_f_mae: 0.017426
2025-06-11 17:08:01.897 INFO: val_f_mae: 0.017426
val_f_rmse: 0.022502
2025-06-11 17:08:01.898 INFO: val_f_rmse: 0.022502
##### Step: 48 Learning rate: 0.0025 #####
2025-06-11 17:10:16.638 INFO: ##### Step: 48 Learning rate: 0.0025 #####
Epoch 9, Train Loss: 0.7645, Val Loss: 0.5401
2025-06-11 17:10:16.641 INFO: Epoch 9, Train Loss: 0.7645, Val Loss: 0.5401
train_e/atom_mae: 0.002240
2025-06-11 17:10:16.643 INFO: train_e/atom_mae: 0.002240
train_e/atom_rmse: 0.002655
2025-06-11 17:10:16.644 INFO: train_e/atom_rmse: 0.002655
train_f_mae: 0.017408
2025-06-11 17:10:16.650 INFO: train_f_mae: 0.017408
train_f_rmse: 0.022463
2025-06-11 17:10:16.651 INFO: train_f_rmse: 0.022463
val_e/atom_mae: 0.001015
2025-06-11 17:10:16.656 INFO: val_e/atom_mae: 0.001015
val_e/atom_rmse: 0.001039
2025-06-11 17:10:16.657 INFO: val_e/atom_rmse: 0.001039
val_f_mae: 0.017304
2025-06-11 17:10:16.658 INFO: val_f_mae: 0.017304
val_f_rmse: 0.022368
2025-06-11 17:10:16.659 INFO: val_f_rmse: 0.022368
##### Step: 49 Learning rate: 0.0025 #####
2025-06-11 17:12:31.383 INFO: ##### Step: 49 Learning rate: 0.0025 #####
Epoch 10, Train Loss: 0.6221, Val Loss: 0.5167
2025-06-11 17:12:31.384 INFO: Epoch 10, Train Loss: 0.6221, Val Loss: 0.5167
train_e/atom_mae: 0.001496
2025-06-11 17:12:31.387 INFO: train_e/atom_mae: 0.001496
train_e/atom_rmse: 0.001720
2025-06-11 17:12:31.388 INFO: train_e/atom_rmse: 0.001720
train_f_mae: 0.017556
2025-06-11 17:12:31.394 INFO: train_f_mae: 0.017556
train_f_rmse: 0.022650
2025-06-11 17:12:31.395 INFO: train_f_rmse: 0.022650
val_e/atom_mae: 0.000859
2025-06-11 17:12:31.399 INFO: val_e/atom_mae: 0.000859
val_e/atom_rmse: 0.000890
2025-06-11 17:12:31.401 INFO: val_e/atom_rmse: 0.000890
val_f_mae: 0.017027
2025-06-11 17:12:31.402 INFO: val_f_mae: 0.017027
val_f_rmse: 0.022079
2025-06-11 17:12:31.403 INFO: val_f_rmse: 0.022079
##### Step: 50 Learning rate: 0.0025 #####
2025-06-11 17:14:45.951 INFO: ##### Step: 50 Learning rate: 0.0025 #####
Epoch 11, Train Loss: 0.5758, Val Loss: 0.6755
2025-06-11 17:14:45.952 INFO: Epoch 11, Train Loss: 0.5758, Val Loss: 0.6755
train_e/atom_mae: 0.001165
2025-06-11 17:14:45.954 INFO: train_e/atom_mae: 0.001165
train_e/atom_rmse: 0.001356
2025-06-11 17:14:45.955 INFO: train_e/atom_rmse: 0.001356
train_f_mae: 0.017450
2025-06-11 17:14:45.961 INFO: train_f_mae: 0.017450
train_f_rmse: 0.022540
2025-06-11 17:14:45.962 INFO: train_f_rmse: 0.022540
val_e/atom_mae: 0.002058
2025-06-11 17:14:45.967 INFO: val_e/atom_mae: 0.002058
val_e/atom_rmse: 0.002073
2025-06-11 17:14:45.968 INFO: val_e/atom_rmse: 0.002073
val_f_mae: 0.017647
2025-06-11 17:14:45.969 INFO: val_f_mae: 0.017647
val_f_rmse: 0.022741
2025-06-11 17:14:45.970 INFO: val_f_rmse: 0.022741
##### Step: 51 Learning rate: 0.0025 #####
2025-06-11 17:17:00.561 INFO: ##### Step: 51 Learning rate: 0.0025 #####
Epoch 12, Train Loss: 0.6595, Val Loss: 0.7083
2025-06-11 17:17:00.563 INFO: Epoch 12, Train Loss: 0.6595, Val Loss: 0.7083
train_e/atom_mae: 0.001624
2025-06-11 17:17:00.565 INFO: train_e/atom_mae: 0.001624
train_e/atom_rmse: 0.001899
2025-06-11 17:17:00.566 INFO: train_e/atom_rmse: 0.001899
train_f_mae: 0.017780
2025-06-11 17:17:00.572 INFO: train_f_mae: 0.017780
train_f_rmse: 0.022946
2025-06-11 17:17:00.573 INFO: train_f_rmse: 0.022946
val_e/atom_mae: 0.002301
2025-06-11 17:17:00.578 INFO: val_e/atom_mae: 0.002301
val_e/atom_rmse: 0.002320
2025-06-11 17:17:00.579 INFO: val_e/atom_rmse: 0.002320
val_f_mae: 0.017498
2025-06-11 17:17:00.580 INFO: val_f_mae: 0.017498
val_f_rmse: 0.022579
2025-06-11 17:17:00.581 INFO: val_f_rmse: 0.022579
##### Step: 52 Learning rate: 0.0025 #####
2025-06-11 17:19:15.193 INFO: ##### Step: 52 Learning rate: 0.0025 #####
Epoch 13, Train Loss: 0.7939, Val Loss: 1.5088
2025-06-11 17:19:15.195 INFO: Epoch 13, Train Loss: 0.7939, Val Loss: 1.5088
train_e/atom_mae: 0.002434
2025-06-11 17:19:15.197 INFO: train_e/atom_mae: 0.002434
train_e/atom_rmse: 0.002751
2025-06-11 17:19:15.198 INFO: train_e/atom_rmse: 0.002751
train_f_mae: 0.017603
2025-06-11 17:19:15.204 INFO: train_f_mae: 0.017603
train_f_rmse: 0.022695
2025-06-11 17:19:15.205 INFO: train_f_rmse: 0.022695
val_e/atom_mae: 0.005168
2025-06-11 17:19:15.210 INFO: val_e/atom_mae: 0.005168
val_e/atom_rmse: 0.005177
2025-06-11 17:19:15.211 INFO: val_e/atom_rmse: 0.005177
val_f_mae: 0.017639
2025-06-11 17:19:15.213 INFO: val_f_mae: 0.017639
val_f_rmse: 0.022820
2025-06-11 17:19:15.213 INFO: val_f_rmse: 0.022820
##### Step: 53 Learning rate: 0.0025 #####
2025-06-11 17:21:29.829 INFO: ##### Step: 53 Learning rate: 0.0025 #####
Epoch 14, Train Loss: 1.0301, Val Loss: 0.5058
2025-06-11 17:21:29.831 INFO: Epoch 14, Train Loss: 1.0301, Val Loss: 0.5058
train_e/atom_mae: 0.003198
2025-06-11 17:21:29.834 INFO: train_e/atom_mae: 0.003198
train_e/atom_rmse: 0.003677
2025-06-11 17:21:29.835 INFO: train_e/atom_rmse: 0.003677
train_f_mae: 0.017917
2025-06-11 17:21:29.841 INFO: train_f_mae: 0.017917
train_f_rmse: 0.023058
2025-06-11 17:21:29.842 INFO: train_f_rmse: 0.023058
val_e/atom_mae: 0.000607
2025-06-11 17:21:29.846 INFO: val_e/atom_mae: 0.000607
val_e/atom_rmse: 0.000640
2025-06-11 17:21:29.848 INFO: val_e/atom_rmse: 0.000640
val_f_mae: 0.017175
2025-06-11 17:21:29.849 INFO: val_f_mae: 0.017175
val_f_rmse: 0.022151
2025-06-11 17:21:29.850 INFO: val_f_rmse: 0.022151
##### Step: 54 Learning rate: 0.0025 #####
2025-06-11 17:23:44.501 INFO: ##### Step: 54 Learning rate: 0.0025 #####
Epoch 15, Train Loss: 0.6327, Val Loss: 0.5193
2025-06-11 17:23:44.502 INFO: Epoch 15, Train Loss: 0.6327, Val Loss: 0.5193
train_e/atom_mae: 0.001127
2025-06-11 17:23:44.505 INFO: train_e/atom_mae: 0.001127
train_e/atom_rmse: 0.001336
2025-06-11 17:23:44.506 INFO: train_e/atom_rmse: 0.001336
train_f_mae: 0.018501
2025-06-11 17:23:44.512 INFO: train_f_mae: 0.018501
train_f_rmse: 0.023809
2025-06-11 17:23:44.513 INFO: train_f_rmse: 0.023809
val_e/atom_mae: 0.000442
2025-06-11 17:23:44.517 INFO: val_e/atom_mae: 0.000442
val_e/atom_rmse: 0.000545
2025-06-11 17:23:44.519 INFO: val_e/atom_rmse: 0.000545
val_f_mae: 0.017412
2025-06-11 17:23:44.520 INFO: val_f_mae: 0.017412
val_f_rmse: 0.022547
2025-06-11 17:23:44.521 INFO: val_f_rmse: 0.022547
##### Step: 55 Learning rate: 0.0025 #####
2025-06-11 17:25:59.178 INFO: ##### Step: 55 Learning rate: 0.0025 #####
Epoch 16, Train Loss: 0.6045, Val Loss: 0.4954
2025-06-11 17:25:59.181 INFO: Epoch 16, Train Loss: 0.6045, Val Loss: 0.4954
train_e/atom_mae: 0.001214
2025-06-11 17:25:59.183 INFO: train_e/atom_mae: 0.001214
train_e/atom_rmse: 0.001523
2025-06-11 17:25:59.184 INFO: train_e/atom_rmse: 0.001523
train_f_mae: 0.017687
2025-06-11 17:25:59.190 INFO: train_f_mae: 0.017687
train_f_rmse: 0.022783
2025-06-11 17:25:59.191 INFO: train_f_rmse: 0.022783
val_e/atom_mae: 0.000710
2025-06-11 17:25:59.196 INFO: val_e/atom_mae: 0.000710
val_e/atom_rmse: 0.000759
2025-06-11 17:25:59.197 INFO: val_e/atom_rmse: 0.000759
val_f_mae: 0.016883
2025-06-11 17:25:59.198 INFO: val_f_mae: 0.016883
val_f_rmse: 0.021775
2025-06-11 17:25:59.199 INFO: val_f_rmse: 0.021775
##### Step: 56 Learning rate: 0.0025 #####
2025-06-11 17:28:13.916 INFO: ##### Step: 56 Learning rate: 0.0025 #####
Epoch 17, Train Loss: 0.6005, Val Loss: 0.5423
2025-06-11 17:28:13.918 INFO: Epoch 17, Train Loss: 0.6005, Val Loss: 0.5423
train_e/atom_mae: 0.001199
2025-06-11 17:28:13.920 INFO: train_e/atom_mae: 0.001199
train_e/atom_rmse: 0.001356
2025-06-11 17:28:13.921 INFO: train_e/atom_rmse: 0.001356
train_f_mae: 0.017903
2025-06-11 17:28:13.927 INFO: train_f_mae: 0.017903
train_f_rmse: 0.023081
2025-06-11 17:28:13.928 INFO: train_f_rmse: 0.023081
val_e/atom_mae: 0.000419
2025-06-11 17:28:13.933 INFO: val_e/atom_mae: 0.000419
val_e/atom_rmse: 0.000521
2025-06-11 17:28:13.934 INFO: val_e/atom_rmse: 0.000521
val_f_mae: 0.017898
2025-06-11 17:28:13.935 INFO: val_f_mae: 0.017898
val_f_rmse: 0.023071
2025-06-11 17:28:13.936 INFO: val_f_rmse: 0.023071
##### Step: 57 Learning rate: 0.0025 #####
2025-06-11 17:30:28.637 INFO: ##### Step: 57 Learning rate: 0.0025 #####
Epoch 18, Train Loss: 0.5999, Val Loss: 0.5592
2025-06-11 17:30:28.638 INFO: Epoch 18, Train Loss: 0.5999, Val Loss: 0.5592
train_e/atom_mae: 0.001455
2025-06-11 17:30:28.641 INFO: train_e/atom_mae: 0.001455
train_e/atom_rmse: 0.001682
2025-06-11 17:30:28.642 INFO: train_e/atom_rmse: 0.001682
train_f_mae: 0.017251
2025-06-11 17:30:28.648 INFO: train_f_mae: 0.017251
train_f_rmse: 0.022261
2025-06-11 17:30:28.649 INFO: train_f_rmse: 0.022261
val_e/atom_mae: 0.001216
2025-06-11 17:30:28.653 INFO: val_e/atom_mae: 0.001216
val_e/atom_rmse: 0.001251
2025-06-11 17:30:28.655 INFO: val_e/atom_rmse: 0.001251
val_f_mae: 0.017313
2025-06-11 17:30:28.656 INFO: val_f_mae: 0.017313
val_f_rmse: 0.022395
2025-06-11 17:30:28.657 INFO: val_f_rmse: 0.022395
##### Step: 58 Learning rate: 0.0025 #####
2025-06-11 17:32:43.283 INFO: ##### Step: 58 Learning rate: 0.0025 #####
Epoch 19, Train Loss: 0.7252, Val Loss: 0.5064
2025-06-11 17:32:43.285 INFO: Epoch 19, Train Loss: 0.7252, Val Loss: 0.5064
train_e/atom_mae: 0.002043
2025-06-11 17:32:43.287 INFO: train_e/atom_mae: 0.002043
train_e/atom_rmse: 0.002373
2025-06-11 17:32:43.288 INFO: train_e/atom_rmse: 0.002373
train_f_mae: 0.017644
2025-06-11 17:32:43.294 INFO: train_f_mae: 0.017644
train_f_rmse: 0.022751
2025-06-11 17:32:43.295 INFO: train_f_rmse: 0.022751
val_e/atom_mae: 0.000484
2025-06-11 17:32:43.300 INFO: val_e/atom_mae: 0.000484
val_e/atom_rmse: 0.000570
2025-06-11 17:32:43.301 INFO: val_e/atom_rmse: 0.000570
val_f_mae: 0.017154
2025-06-11 17:32:43.302 INFO: val_f_mae: 0.017154
val_f_rmse: 0.022235
2025-06-11 17:32:43.303 INFO: val_f_rmse: 0.022235
##### Step: 59 Learning rate: 0.0025 #####
2025-06-11 17:34:58.106 INFO: ##### Step: 59 Learning rate: 0.0025 #####
Epoch 20, Train Loss: 0.5577, Val Loss: 0.5583
2025-06-11 17:34:58.109 INFO: Epoch 20, Train Loss: 0.5577, Val Loss: 0.5583
train_e/atom_mae: 0.000748
2025-06-11 17:34:58.111 INFO: train_e/atom_mae: 0.000748
train_e/atom_rmse: 0.000937
2025-06-11 17:34:58.112 INFO: train_e/atom_rmse: 0.000937
train_f_mae: 0.017740
2025-06-11 17:34:58.118 INFO: train_f_mae: 0.017740
train_f_rmse: 0.022922
2025-06-11 17:34:58.119 INFO: train_f_rmse: 0.022922
val_e/atom_mae: 0.000945
2025-06-11 17:34:58.124 INFO: val_e/atom_mae: 0.000945
val_e/atom_rmse: 0.001002
2025-06-11 17:34:58.125 INFO: val_e/atom_rmse: 0.001002
val_f_mae: 0.017628
2025-06-11 17:34:58.126 INFO: val_f_mae: 0.017628
val_f_rmse: 0.022832
2025-06-11 17:34:58.127 INFO: val_f_rmse: 0.022832
##### Step: 60 Learning rate: 0.00125 #####
2025-06-11 17:37:13.014 INFO: ##### Step: 60 Learning rate: 0.00125 #####
Epoch 21, Train Loss: 0.5132, Val Loss: 0.5210
2025-06-11 17:37:13.016 INFO: Epoch 21, Train Loss: 0.5132, Val Loss: 0.5210
train_e/atom_mae: 0.000755
2025-06-11 17:37:13.019 INFO: train_e/atom_mae: 0.000755
train_e/atom_rmse: 0.000932
2025-06-11 17:37:13.020 INFO: train_e/atom_rmse: 0.000932
train_f_mae: 0.017005
2025-06-11 17:37:13.026 INFO: train_f_mae: 0.017005
train_f_rmse: 0.021937
2025-06-11 17:37:13.027 INFO: train_f_rmse: 0.021937
val_e/atom_mae: 0.000571
2025-06-11 17:37:13.031 INFO: val_e/atom_mae: 0.000571
val_e/atom_rmse: 0.000615
2025-06-11 17:37:13.033 INFO: val_e/atom_rmse: 0.000615
val_f_mae: 0.017348
2025-06-11 17:37:13.034 INFO: val_f_mae: 0.017348
val_f_rmse: 0.022518
2025-06-11 17:37:13.035 INFO: val_f_rmse: 0.022518
##### Step: 61 Learning rate: 0.00125 #####
2025-06-11 17:39:27.700 INFO: ##### Step: 61 Learning rate: 0.00125 #####
Epoch 22, Train Loss: 0.5323, Val Loss: 0.4826
2025-06-11 17:39:27.701 INFO: Epoch 22, Train Loss: 0.5323, Val Loss: 0.4826
train_e/atom_mae: 0.000719
2025-06-11 17:39:27.703 INFO: train_e/atom_mae: 0.000719
train_e/atom_rmse: 0.000860
2025-06-11 17:39:27.704 INFO: train_e/atom_rmse: 0.000860
train_f_mae: 0.017449
2025-06-11 17:39:27.711 INFO: train_f_mae: 0.017449
train_f_rmse: 0.022473
2025-06-11 17:39:27.712 INFO: train_f_rmse: 0.022473
val_e/atom_mae: 0.000200
2025-06-11 17:39:27.716 INFO: val_e/atom_mae: 0.000200
val_e/atom_rmse: 0.000263
2025-06-11 17:39:27.718 INFO: val_e/atom_rmse: 0.000263
val_f_mae: 0.016899
2025-06-11 17:39:27.719 INFO: val_f_mae: 0.016899
val_f_rmse: 0.021910
2025-06-11 17:39:27.720 INFO: val_f_rmse: 0.021910
##### Step: 62 Learning rate: 0.00125 #####
2025-06-11 17:41:42.371 INFO: ##### Step: 62 Learning rate: 0.00125 #####
Epoch 23, Train Loss: 0.5706, Val Loss: 0.5735
2025-06-11 17:41:42.372 INFO: Epoch 23, Train Loss: 0.5706, Val Loss: 0.5735
train_e/atom_mae: 0.001306
2025-06-11 17:41:42.375 INFO: train_e/atom_mae: 0.001306
train_e/atom_rmse: 0.001525
2025-06-11 17:41:42.376 INFO: train_e/atom_rmse: 0.001525
train_f_mae: 0.017072
2025-06-11 17:41:42.382 INFO: train_f_mae: 0.017072
train_f_rmse: 0.022021
2025-06-11 17:41:42.383 INFO: train_f_rmse: 0.022021
val_e/atom_mae: 0.000334
2025-06-11 17:41:42.387 INFO: val_e/atom_mae: 0.000334
val_e/atom_rmse: 0.000399
2025-06-11 17:41:42.388 INFO: val_e/atom_rmse: 0.000399
val_f_mae: 0.018354
2025-06-11 17:41:42.390 INFO: val_f_mae: 0.018354
val_f_rmse: 0.023825
2025-06-11 17:41:42.391 INFO: val_f_rmse: 0.023825
##### Step: 63 Learning rate: 0.00125 #####
2025-06-11 17:43:57.035 INFO: ##### Step: 63 Learning rate: 0.00125 #####
Epoch 24, Train Loss: 0.5546, Val Loss: 0.8101
2025-06-11 17:43:57.037 INFO: Epoch 24, Train Loss: 0.5546, Val Loss: 0.8101
train_e/atom_mae: 0.001222
2025-06-11 17:43:57.039 INFO: train_e/atom_mae: 0.001222
train_e/atom_rmse: 0.001437
2025-06-11 17:43:57.040 INFO: train_e/atom_rmse: 0.001437
train_f_mae: 0.016954
2025-06-11 17:43:57.046 INFO: train_f_mae: 0.016954
train_f_rmse: 0.021874
2025-06-11 17:43:57.047 INFO: train_f_rmse: 0.021874
val_e/atom_mae: 0.002862
2025-06-11 17:43:57.052 INFO: val_e/atom_mae: 0.002862
val_e/atom_rmse: 0.002874
2025-06-11 17:43:57.053 INFO: val_e/atom_rmse: 0.002874
val_f_mae: 0.017402
2025-06-11 17:43:57.054 INFO: val_f_mae: 0.017402
val_f_rmse: 0.022484
2025-06-11 17:43:57.055 INFO: val_f_rmse: 0.022484
##### Step: 64 Learning rate: 0.00125 #####
2025-06-11 17:46:11.787 INFO: ##### Step: 64 Learning rate: 0.00125 #####
Epoch 25, Train Loss: 0.6443, Val Loss: 0.6357
2025-06-11 17:46:11.789 INFO: Epoch 25, Train Loss: 0.6443, Val Loss: 0.6357
train_e/atom_mae: 0.001938
2025-06-11 17:46:11.791 INFO: train_e/atom_mae: 0.001938
train_e/atom_rmse: 0.002164
2025-06-11 17:46:11.792 INFO: train_e/atom_rmse: 0.002164
train_f_mae: 0.016829
2025-06-11 17:46:11.798 INFO: train_f_mae: 0.016829
train_f_rmse: 0.021719
2025-06-11 17:46:11.799 INFO: train_f_rmse: 0.021719
val_e/atom_mae: 0.002038
2025-06-11 17:46:11.803 INFO: val_e/atom_mae: 0.002038
val_e/atom_rmse: 0.002049
2025-06-11 17:46:11.805 INFO: val_e/atom_rmse: 0.002049
val_f_mae: 0.016945
2025-06-11 17:46:11.806 INFO: val_f_mae: 0.016945
val_f_rmse: 0.021931
2025-06-11 17:46:11.807 INFO: val_f_rmse: 0.021931
##### Step: 65 Learning rate: 0.00125 #####
2025-06-11 17:48:26.538 INFO: ##### Step: 65 Learning rate: 0.00125 #####
Epoch 26, Train Loss: 0.5419, Val Loss: 0.6204
2025-06-11 17:48:26.539 INFO: Epoch 26, Train Loss: 0.5419, Val Loss: 0.6204
train_e/atom_mae: 0.001063
2025-06-11 17:48:26.542 INFO: train_e/atom_mae: 0.001063
train_e/atom_rmse: 0.001297
2025-06-11 17:48:26.542 INFO: train_e/atom_rmse: 0.001297
train_f_mae: 0.016979
2025-06-11 17:48:26.548 INFO: train_f_mae: 0.016979
train_f_rmse: 0.021908
2025-06-11 17:48:26.549 INFO: train_f_rmse: 0.021908
val_e/atom_mae: 0.001884
2025-06-11 17:48:26.554 INFO: val_e/atom_mae: 0.001884
val_e/atom_rmse: 0.001896
2025-06-11 17:48:26.555 INFO: val_e/atom_rmse: 0.001896
val_f_mae: 0.017066
2025-06-11 17:48:26.556 INFO: val_f_mae: 0.017066
val_f_rmse: 0.022087
2025-06-11 17:48:26.557 INFO: val_f_rmse: 0.022087
##### Step: 66 Learning rate: 0.00125 #####
2025-06-11 17:50:41.285 INFO: ##### Step: 66 Learning rate: 0.00125 #####
Epoch 27, Train Loss: 0.5432, Val Loss: 0.4767
2025-06-11 17:50:41.287 INFO: Epoch 27, Train Loss: 0.5432, Val Loss: 0.4767
train_e/atom_mae: 0.001236
2025-06-11 17:50:41.289 INFO: train_e/atom_mae: 0.001236
train_e/atom_rmse: 0.001419
2025-06-11 17:50:41.290 INFO: train_e/atom_rmse: 0.001419
train_f_mae: 0.016774
2025-06-11 17:50:41.296 INFO: train_f_mae: 0.016774
train_f_rmse: 0.021654
2025-06-11 17:50:41.297 INFO: train_f_rmse: 0.021654
val_e/atom_mae: 0.000569
2025-06-11 17:50:41.302 INFO: val_e/atom_mae: 0.000569
val_e/atom_rmse: 0.000629
2025-06-11 17:50:41.303 INFO: val_e/atom_rmse: 0.000629
val_f_mae: 0.016628
2025-06-11 17:50:41.304 INFO: val_f_mae: 0.016628
val_f_rmse: 0.021497
2025-06-11 17:50:41.305 INFO: val_f_rmse: 0.021497
##### Step: 67 Learning rate: 0.00125 #####
2025-06-11 17:52:56.092 INFO: ##### Step: 67 Learning rate: 0.00125 #####
Epoch 28, Train Loss: 0.4992, Val Loss: 0.4844
2025-06-11 17:52:56.094 INFO: Epoch 28, Train Loss: 0.4992, Val Loss: 0.4844
train_e/atom_mae: 0.000672
2025-06-11 17:52:56.097 INFO: train_e/atom_mae: 0.000672
train_e/atom_rmse: 0.000813
2025-06-11 17:52:56.098 INFO: train_e/atom_rmse: 0.000813
train_f_mae: 0.016894
2025-06-11 17:52:56.104 INFO: train_f_mae: 0.016894
train_f_rmse: 0.021791
2025-06-11 17:52:56.105 INFO: train_f_rmse: 0.021791
val_e/atom_mae: 0.000388
2025-06-11 17:52:56.109 INFO: val_e/atom_mae: 0.000388
val_e/atom_rmse: 0.000459
2025-06-11 17:52:56.111 INFO: val_e/atom_rmse: 0.000459
val_f_mae: 0.016871
2025-06-11 17:52:56.112 INFO: val_f_mae: 0.016871
val_f_rmse: 0.021833
2025-06-11 17:52:56.113 INFO: val_f_rmse: 0.021833
##### Step: 68 Learning rate: 0.00125 #####
2025-06-11 17:55:10.789 INFO: ##### Step: 68 Learning rate: 0.00125 #####
Epoch 29, Train Loss: 0.5165, Val Loss: 0.4687
2025-06-11 17:55:10.790 INFO: Epoch 29, Train Loss: 0.5165, Val Loss: 0.4687
train_e/atom_mae: 0.000897
2025-06-11 17:55:10.792 INFO: train_e/atom_mae: 0.000897
train_e/atom_rmse: 0.001062
2025-06-11 17:55:10.793 INFO: train_e/atom_rmse: 0.001062
train_f_mae: 0.016889
2025-06-11 17:55:10.799 INFO: train_f_mae: 0.016889
train_f_rmse: 0.021792
2025-06-11 17:55:10.800 INFO: train_f_rmse: 0.021792
val_e/atom_mae: 0.000231
2025-06-11 17:55:10.805 INFO: val_e/atom_mae: 0.000231
val_e/atom_rmse: 0.000269
2025-06-11 17:55:10.806 INFO: val_e/atom_rmse: 0.000269
val_f_mae: 0.016717
2025-06-11 17:55:10.807 INFO: val_f_mae: 0.016717
val_f_rmse: 0.021588
2025-06-11 17:55:10.808 INFO: val_f_rmse: 0.021588
##### Step: 69 Learning rate: 0.00125 #####
2025-06-11 17:57:25.541 INFO: ##### Step: 69 Learning rate: 0.00125 #####
Epoch 30, Train Loss: 0.5163, Val Loss: 0.5111
2025-06-11 17:57:25.544 INFO: Epoch 30, Train Loss: 0.5163, Val Loss: 0.5111
train_e/atom_mae: 0.000739
2025-06-11 17:57:25.547 INFO: train_e/atom_mae: 0.000739
train_e/atom_rmse: 0.000877
2025-06-11 17:57:25.548 INFO: train_e/atom_rmse: 0.000877
train_f_mae: 0.017142
2025-06-11 17:57:25.554 INFO: train_f_mae: 0.017142
train_f_rmse: 0.022090
2025-06-11 17:57:25.555 INFO: train_f_rmse: 0.022090
val_e/atom_mae: 0.001041
2025-06-11 17:57:25.559 INFO: val_e/atom_mae: 0.001041
val_e/atom_rmse: 0.001062
2025-06-11 17:57:25.560 INFO: val_e/atom_rmse: 0.001062
val_f_mae: 0.016726
2025-06-11 17:57:25.562 INFO: val_f_mae: 0.016726
val_f_rmse: 0.021668
2025-06-11 17:57:25.562 INFO: val_f_rmse: 0.021668
##### Step: 70 Learning rate: 0.00125 #####
2025-06-11 17:59:40.338 INFO: ##### Step: 70 Learning rate: 0.00125 #####
Epoch 31, Train Loss: 0.4927, Val Loss: 0.5004
2025-06-11 17:59:40.340 INFO: Epoch 31, Train Loss: 0.4927, Val Loss: 0.5004
train_e/atom_mae: 0.000606
2025-06-11 17:59:40.343 INFO: train_e/atom_mae: 0.000606
train_e/atom_rmse: 0.000774
2025-06-11 17:59:40.343 INFO: train_e/atom_rmse: 0.000774
train_f_mae: 0.016819
2025-06-11 17:59:40.349 INFO: train_f_mae: 0.016819
train_f_rmse: 0.021693
2025-06-11 17:59:40.350 INFO: train_f_rmse: 0.021693
val_e/atom_mae: 0.000324
2025-06-11 17:59:40.355 INFO: val_e/atom_mae: 0.000324
val_e/atom_rmse: 0.000376
2025-06-11 17:59:40.356 INFO: val_e/atom_rmse: 0.000376
val_f_mae: 0.017271
2025-06-11 17:59:40.358 INFO: val_f_mae: 0.017271
val_f_rmse: 0.022253
2025-06-11 17:59:40.359 INFO: val_f_rmse: 0.022253
##### Step: 71 Learning rate: 0.00125 #####
2025-06-11 18:01:55.128 INFO: ##### Step: 71 Learning rate: 0.00125 #####
Epoch 32, Train Loss: 0.5154, Val Loss: 0.4901
2025-06-11 18:01:55.130 INFO: Epoch 32, Train Loss: 0.5154, Val Loss: 0.4901
train_e/atom_mae: 0.000867
2025-06-11 18:01:55.133 INFO: train_e/atom_mae: 0.000867
train_e/atom_rmse: 0.001045
2025-06-11 18:01:55.134 INFO: train_e/atom_rmse: 0.001045
train_f_mae: 0.016880
2025-06-11 18:01:55.140 INFO: train_f_mae: 0.016880
train_f_rmse: 0.021799
2025-06-11 18:01:55.141 INFO: train_f_rmse: 0.021799
val_e/atom_mae: 0.000301
2025-06-11 18:01:55.145 INFO: val_e/atom_mae: 0.000301
val_e/atom_rmse: 0.000370
2025-06-11 18:01:55.147 INFO: val_e/atom_rmse: 0.000370
val_f_mae: 0.017062
2025-06-11 18:01:55.148 INFO: val_f_mae: 0.017062
val_f_rmse: 0.022024
2025-06-11 18:01:55.149 INFO: val_f_rmse: 0.022024
##### Step: 72 Learning rate: 0.00125 #####
2025-06-11 18:04:09.822 INFO: ##### Step: 72 Learning rate: 0.00125 #####
Epoch 33, Train Loss: 0.5036, Val Loss: 0.4888
2025-06-11 18:04:09.823 INFO: Epoch 33, Train Loss: 0.5036, Val Loss: 0.4888
train_e/atom_mae: 0.000797
2025-06-11 18:04:09.826 INFO: train_e/atom_mae: 0.000797
train_e/atom_rmse: 0.000942
2025-06-11 18:04:09.827 INFO: train_e/atom_rmse: 0.000942
train_f_mae: 0.016843
2025-06-11 18:04:09.833 INFO: train_f_mae: 0.016843
train_f_rmse: 0.021701
2025-06-11 18:04:09.834 INFO: train_f_rmse: 0.021701
val_e/atom_mae: 0.000309
2025-06-11 18:04:09.838 INFO: val_e/atom_mae: 0.000309
val_e/atom_rmse: 0.000367
2025-06-11 18:04:09.840 INFO: val_e/atom_rmse: 0.000367
val_f_mae: 0.017017
2025-06-11 18:04:09.841 INFO: val_f_mae: 0.017017
val_f_rmse: 0.021995
2025-06-11 18:04:09.842 INFO: val_f_rmse: 0.021995
##### Step: 73 Learning rate: 0.00125 #####
2025-06-11 18:06:24.486 INFO: ##### Step: 73 Learning rate: 0.00125 #####
Epoch 34, Train Loss: 0.5031, Val Loss: 0.4653
2025-06-11 18:06:24.487 INFO: Epoch 34, Train Loss: 0.5031, Val Loss: 0.4653
train_e/atom_mae: 0.000702
2025-06-11 18:06:24.489 INFO: train_e/atom_mae: 0.000702
train_e/atom_rmse: 0.000892
2025-06-11 18:06:24.490 INFO: train_e/atom_rmse: 0.000892
train_f_mae: 0.016871
2025-06-11 18:06:24.496 INFO: train_f_mae: 0.016871
train_f_rmse: 0.021766
2025-06-11 18:06:24.497 INFO: train_f_rmse: 0.021766
val_e/atom_mae: 0.000240
2025-06-11 18:06:24.502 INFO: val_e/atom_mae: 0.000240
val_e/atom_rmse: 0.000293
2025-06-11 18:06:24.503 INFO: val_e/atom_rmse: 0.000293
val_f_mae: 0.016636
2025-06-11 18:06:24.504 INFO: val_f_mae: 0.016636
val_f_rmse: 0.021497
2025-06-11 18:06:24.505 INFO: val_f_rmse: 0.021497
##### Step: 74 Learning rate: 0.00125 #####
2025-06-11 18:08:39.350 INFO: ##### Step: 74 Learning rate: 0.00125 #####
Epoch 35, Train Loss: 0.5112, Val Loss: 0.5089
2025-06-11 18:08:39.352 INFO: Epoch 35, Train Loss: 0.5112, Val Loss: 0.5089
train_e/atom_mae: 0.000802
2025-06-11 18:08:39.354 INFO: train_e/atom_mae: 0.000802
train_e/atom_rmse: 0.001007
2025-06-11 18:08:39.355 INFO: train_e/atom_rmse: 0.001007
train_f_mae: 0.016873
2025-06-11 18:08:39.361 INFO: train_f_mae: 0.016873
train_f_rmse: 0.021767
2025-06-11 18:08:39.362 INFO: train_f_rmse: 0.021767
val_e/atom_mae: 0.001025
2025-06-11 18:08:39.367 INFO: val_e/atom_mae: 0.001025
val_e/atom_rmse: 0.001044
2025-06-11 18:08:39.368 INFO: val_e/atom_rmse: 0.001044
val_f_mae: 0.016768
2025-06-11 18:08:39.369 INFO: val_f_mae: 0.016768
val_f_rmse: 0.021649
2025-06-11 18:08:39.370 INFO: val_f_rmse: 0.021649
##### Step: 75 Learning rate: 0.00125 #####
2025-06-11 18:10:54.065 INFO: ##### Step: 75 Learning rate: 0.00125 #####
Epoch 36, Train Loss: 0.6107, Val Loss: 0.5830
2025-06-11 18:10:54.067 INFO: Epoch 36, Train Loss: 0.6107, Val Loss: 0.5830
train_e/atom_mae: 0.001642
2025-06-11 18:10:54.069 INFO: train_e/atom_mae: 0.001642
train_e/atom_rmse: 0.001961
2025-06-11 18:10:54.070 INFO: train_e/atom_rmse: 0.001961
train_f_mae: 0.016778
2025-06-11 18:10:54.076 INFO: train_f_mae: 0.016778
train_f_rmse: 0.021655
2025-06-11 18:10:54.077 INFO: train_f_rmse: 0.021655
val_e/atom_mae: 0.001769
2025-06-11 18:10:54.082 INFO: val_e/atom_mae: 0.001769
val_e/atom_rmse: 0.001783
2025-06-11 18:10:54.083 INFO: val_e/atom_rmse: 0.001783
val_f_mae: 0.016688
2025-06-11 18:10:54.084 INFO: val_f_mae: 0.016688
val_f_rmse: 0.021584
2025-06-11 18:10:54.085 INFO: val_f_rmse: 0.021584
##### Step: 76 Learning rate: 0.00125 #####
2025-06-11 18:13:08.825 INFO: ##### Step: 76 Learning rate: 0.00125 #####
Epoch 37, Train Loss: 0.5776, Val Loss: 0.4924
2025-06-11 18:13:08.826 INFO: Epoch 37, Train Loss: 0.5776, Val Loss: 0.4924
train_e/atom_mae: 0.001427
2025-06-11 18:13:08.828 INFO: train_e/atom_mae: 0.001427
train_e/atom_rmse: 0.001706
2025-06-11 18:13:08.829 INFO: train_e/atom_rmse: 0.001706
train_f_mae: 0.016803
2025-06-11 18:13:08.836 INFO: train_f_mae: 0.016803
train_f_rmse: 0.021687
2025-06-11 18:13:08.837 INFO: train_f_rmse: 0.021687
val_e/atom_mae: 0.000456
2025-06-11 18:13:08.841 INFO: val_e/atom_mae: 0.000456
val_e/atom_rmse: 0.000507
2025-06-11 18:13:08.842 INFO: val_e/atom_rmse: 0.000507
val_f_mae: 0.017097
2025-06-11 18:13:08.844 INFO: val_f_mae: 0.017097
val_f_rmse: 0.021976
2025-06-11 18:13:08.844 INFO: val_f_rmse: 0.021976
##### Step: 77 Learning rate: 0.00125 #####
2025-06-11 18:15:23.554 INFO: ##### Step: 77 Learning rate: 0.00125 #####
Epoch 38, Train Loss: 0.5429, Val Loss: 0.5198
2025-06-11 18:15:23.555 INFO: Epoch 38, Train Loss: 0.5429, Val Loss: 0.5198
train_e/atom_mae: 0.001127
2025-06-11 18:15:23.558 INFO: train_e/atom_mae: 0.001127
train_e/atom_rmse: 0.001352
2025-06-11 18:15:23.559 INFO: train_e/atom_rmse: 0.001352
train_f_mae: 0.016913
2025-06-11 18:15:23.565 INFO: train_f_mae: 0.016913
train_f_rmse: 0.021806
2025-06-11 18:15:23.566 INFO: train_f_rmse: 0.021806
val_e/atom_mae: 0.001071
2025-06-11 18:15:23.570 INFO: val_e/atom_mae: 0.001071
val_e/atom_rmse: 0.001107
2025-06-11 18:15:23.572 INFO: val_e/atom_rmse: 0.001107
val_f_mae: 0.016825
2025-06-11 18:15:23.573 INFO: val_f_mae: 0.016825
val_f_rmse: 0.021786
2025-06-11 18:15:23.574 INFO: val_f_rmse: 0.021786
##### Step: 78 Learning rate: 0.00125 #####
2025-06-11 18:17:38.271 INFO: ##### Step: 78 Learning rate: 0.00125 #####
Epoch 39, Train Loss: 0.4891, Val Loss: 0.4736
2025-06-11 18:17:38.273 INFO: Epoch 39, Train Loss: 0.4891, Val Loss: 0.4736
train_e/atom_mae: 0.000694
2025-06-11 18:17:38.276 INFO: train_e/atom_mae: 0.000694
train_e/atom_rmse: 0.000845
2025-06-11 18:17:38.277 INFO: train_e/atom_rmse: 0.000845
train_f_mae: 0.016674
2025-06-11 18:17:38.283 INFO: train_f_mae: 0.016674
train_f_rmse: 0.021512
2025-06-11 18:17:38.284 INFO: train_f_rmse: 0.021512
val_e/atom_mae: 0.000316
2025-06-11 18:17:38.288 INFO: val_e/atom_mae: 0.000316
val_e/atom_rmse: 0.000373
2025-06-11 18:17:38.290 INFO: val_e/atom_rmse: 0.000373
val_f_mae: 0.016766
2025-06-11 18:17:38.291 INFO: val_f_mae: 0.016766
val_f_rmse: 0.021644
2025-06-11 18:17:38.292 INFO: val_f_rmse: 0.021644
##### Step: 79 Learning rate: 0.00125 #####
2025-06-11 18:19:52.990 INFO: ##### Step: 79 Learning rate: 0.00125 #####
Epoch 40, Train Loss: 0.5021, Val Loss: 0.5469
2025-06-11 18:19:52.992 INFO: Epoch 40, Train Loss: 0.5021, Val Loss: 0.5469
train_e/atom_mae: 0.000790
2025-06-11 18:19:52.995 INFO: train_e/atom_mae: 0.000790
train_e/atom_rmse: 0.000964
2025-06-11 18:19:52.996 INFO: train_e/atom_rmse: 0.000964
train_f_mae: 0.016753
2025-06-11 18:19:53.002 INFO: train_f_mae: 0.016753
train_f_rmse: 0.021628
2025-06-11 18:19:53.003 INFO: train_f_rmse: 0.021628
val_e/atom_mae: 0.000894
2025-06-11 18:19:53.007 INFO: val_e/atom_mae: 0.000894
val_e/atom_rmse: 0.000928
2025-06-11 18:19:53.009 INFO: val_e/atom_rmse: 0.000928
val_f_mae: 0.017583
2025-06-11 18:19:53.010 INFO: val_f_mae: 0.017583
val_f_rmse: 0.022698
2025-06-11 18:19:53.011 INFO: val_f_rmse: 0.022698
##### Step: 80 Learning rate: 0.000625 #####
2025-06-11 18:22:07.726 INFO: ##### Step: 80 Learning rate: 0.000625 #####
Epoch 41, Train Loss: 0.4763, Val Loss: 0.4877
2025-06-11 18:22:07.728 INFO: Epoch 41, Train Loss: 0.4763, Val Loss: 0.4877
train_e/atom_mae: 0.000432
2025-06-11 18:22:07.730 INFO: train_e/atom_mae: 0.000432
train_e/atom_rmse: 0.000528
2025-06-11 18:22:07.731 INFO: train_e/atom_rmse: 0.000528
train_f_mae: 0.016742
2025-06-11 18:22:07.737 INFO: train_f_mae: 0.016742
train_f_rmse: 0.021589
2025-06-11 18:22:07.738 INFO: train_f_rmse: 0.021589
val_e/atom_mae: 0.000415
2025-06-11 18:22:07.743 INFO: val_e/atom_mae: 0.000415
val_e/atom_rmse: 0.000459
2025-06-11 18:22:07.744 INFO: val_e/atom_rmse: 0.000459
val_f_mae: 0.016970
2025-06-11 18:22:07.746 INFO: val_f_mae: 0.016970
val_f_rmse: 0.021907
2025-06-11 18:22:07.746 INFO: val_f_rmse: 0.021907
##### Step: 81 Learning rate: 0.000625 #####
2025-06-11 18:24:22.332 INFO: ##### Step: 81 Learning rate: 0.000625 #####
Epoch 42, Train Loss: 0.4845, Val Loss: 0.4555
2025-06-11 18:24:22.333 INFO: Epoch 42, Train Loss: 0.4845, Val Loss: 0.4555
train_e/atom_mae: 0.000558
2025-06-11 18:24:22.335 INFO: train_e/atom_mae: 0.000558
train_e/atom_rmse: 0.000708
2025-06-11 18:24:22.336 INFO: train_e/atom_rmse: 0.000708
train_f_mae: 0.016732
2025-06-11 18:24:22.342 INFO: train_f_mae: 0.016732
train_f_rmse: 0.021588
2025-06-11 18:24:22.343 INFO: train_f_rmse: 0.021588
val_e/atom_mae: 0.000176
2025-06-11 18:24:22.348 INFO: val_e/atom_mae: 0.000176
val_e/atom_rmse: 0.000222
2025-06-11 18:24:22.349 INFO: val_e/atom_rmse: 0.000222
val_f_mae: 0.016486
2025-06-11 18:24:22.350 INFO: val_f_mae: 0.016486
val_f_rmse: 0.021301
2025-06-11 18:24:22.351 INFO: val_f_rmse: 0.021301
##### Step: 82 Learning rate: 0.000625 #####
2025-06-11 18:26:37.157 INFO: ##### Step: 82 Learning rate: 0.000625 #####
Epoch 43, Train Loss: 0.4810, Val Loss: 0.4630
2025-06-11 18:26:37.158 INFO: Epoch 43, Train Loss: 0.4810, Val Loss: 0.4630
train_e/atom_mae: 0.000626
2025-06-11 18:26:37.160 INFO: train_e/atom_mae: 0.000626
train_e/atom_rmse: 0.000798
2025-06-11 18:26:37.161 INFO: train_e/atom_rmse: 0.000798
train_f_mae: 0.016584
2025-06-11 18:26:37.167 INFO: train_f_mae: 0.016584
train_f_rmse: 0.021390
2025-06-11 18:26:37.168 INFO: train_f_rmse: 0.021390
val_e/atom_mae: 0.000240
2025-06-11 18:26:37.173 INFO: val_e/atom_mae: 0.000240
val_e/atom_rmse: 0.000304
2025-06-11 18:26:37.174 INFO: val_e/atom_rmse: 0.000304
val_f_mae: 0.016600
2025-06-11 18:26:37.176 INFO: val_f_mae: 0.016600
val_f_rmse: 0.021437
2025-06-11 18:26:37.176 INFO: val_f_rmse: 0.021437
##### Step: 83 Learning rate: 0.000625 #####
2025-06-11 18:28:51.914 INFO: ##### Step: 83 Learning rate: 0.000625 #####
Epoch 44, Train Loss: 0.4691, Val Loss: 0.4754
2025-06-11 18:28:51.916 INFO: Epoch 44, Train Loss: 0.4691, Val Loss: 0.4754
train_e/atom_mae: 0.000470
2025-06-11 18:28:51.919 INFO: train_e/atom_mae: 0.000470
train_e/atom_rmse: 0.000598
2025-06-11 18:28:51.920 INFO: train_e/atom_rmse: 0.000598
train_f_mae: 0.016548
2025-06-11 18:28:51.926 INFO: train_f_mae: 0.016548
train_f_rmse: 0.021351
2025-06-11 18:28:51.927 INFO: train_f_rmse: 0.021351
val_e/atom_mae: 0.000527
2025-06-11 18:28:51.931 INFO: val_e/atom_mae: 0.000527
val_e/atom_rmse: 0.000563
2025-06-11 18:28:51.933 INFO: val_e/atom_rmse: 0.000563
val_f_mae: 0.016672
2025-06-11 18:28:51.934 INFO: val_f_mae: 0.016672
val_f_rmse: 0.021535
2025-06-11 18:28:51.935 INFO: val_f_rmse: 0.021535
##### Step: 84 Learning rate: 0.000625 #####
2025-06-11 18:31:06.679 INFO: ##### Step: 84 Learning rate: 0.000625 #####
Epoch 45, Train Loss: 0.5063, Val Loss: 0.5329
2025-06-11 18:31:06.680 INFO: Epoch 45, Train Loss: 0.5063, Val Loss: 0.5329
train_e/atom_mae: 0.001027
2025-06-11 18:31:06.682 INFO: train_e/atom_mae: 0.001027
train_e/atom_rmse: 0.001175
2025-06-11 18:31:06.683 INFO: train_e/atom_rmse: 0.001175
train_f_mae: 0.016553
2025-06-11 18:31:06.689 INFO: train_f_mae: 0.016553
train_f_rmse: 0.021340
2025-06-11 18:31:06.690 INFO: train_f_rmse: 0.021340
val_e/atom_mae: 0.001439
2025-06-11 18:31:06.695 INFO: val_e/atom_mae: 0.001439
val_e/atom_rmse: 0.001455
2025-06-11 18:31:06.696 INFO: val_e/atom_rmse: 0.001455
val_f_mae: 0.016524
2025-06-11 18:31:06.697 INFO: val_f_mae: 0.016524
val_f_rmse: 0.021327
2025-06-11 18:31:06.698 INFO: val_f_rmse: 0.021327
##### Step: 85 Learning rate: 0.000625 #####
2025-06-11 18:33:21.332 INFO: ##### Step: 85 Learning rate: 0.000625 #####
Epoch 46, Train Loss: 0.4936, Val Loss: 0.4817
2025-06-11 18:33:21.333 INFO: Epoch 46, Train Loss: 0.4936, Val Loss: 0.4817
train_e/atom_mae: 0.000911
2025-06-11 18:33:21.336 INFO: train_e/atom_mae: 0.000911
train_e/atom_rmse: 0.001088
2025-06-11 18:33:21.337 INFO: train_e/atom_rmse: 0.001088
train_f_mae: 0.016439
2025-06-11 18:33:21.343 INFO: train_f_mae: 0.016439
train_f_rmse: 0.021214
2025-06-11 18:33:21.344 INFO: train_f_rmse: 0.021214
val_e/atom_mae: 0.000522
2025-06-11 18:33:21.348 INFO: val_e/atom_mae: 0.000522
val_e/atom_rmse: 0.000560
2025-06-11 18:33:21.350 INFO: val_e/atom_rmse: 0.000560
val_f_mae: 0.016792
2025-06-11 18:33:21.351 INFO: val_f_mae: 0.016792
val_f_rmse: 0.021682
2025-06-11 18:33:21.352 INFO: val_f_rmse: 0.021682
##### Step: 86 Learning rate: 0.000625 #####
2025-06-11 18:35:36.064 INFO: ##### Step: 86 Learning rate: 0.000625 #####
Epoch 47, Train Loss: 0.5021, Val Loss: 0.4711
2025-06-11 18:35:36.066 INFO: Epoch 47, Train Loss: 0.5021, Val Loss: 0.4711
train_e/atom_mae: 0.000775
2025-06-11 18:35:36.069 INFO: train_e/atom_mae: 0.000775
train_e/atom_rmse: 0.000978
2025-06-11 18:35:36.070 INFO: train_e/atom_rmse: 0.000978
train_f_mae: 0.016753
2025-06-11 18:35:36.076 INFO: train_f_mae: 0.016753
train_f_rmse: 0.021606
2025-06-11 18:35:36.077 INFO: train_f_rmse: 0.021606
val_e/atom_mae: 0.000683
2025-06-11 18:35:36.081 INFO: val_e/atom_mae: 0.000683
val_e/atom_rmse: 0.000713
2025-06-11 18:35:36.083 INFO: val_e/atom_rmse: 0.000713
val_f_mae: 0.016464
2025-06-11 18:35:36.084 INFO: val_f_mae: 0.016464
val_f_rmse: 0.021269
2025-06-11 18:35:36.085 INFO: val_f_rmse: 0.021269
##### Step: 87 Learning rate: 0.000625 #####
2025-06-11 18:37:50.742 INFO: ##### Step: 87 Learning rate: 0.000625 #####
Epoch 48, Train Loss: 0.4726, Val Loss: 0.4923
2025-06-11 18:37:50.743 INFO: Epoch 48, Train Loss: 0.4726, Val Loss: 0.4923
train_e/atom_mae: 0.000354
2025-06-11 18:37:50.746 INFO: train_e/atom_mae: 0.000354
train_e/atom_rmse: 0.000451
2025-06-11 18:37:50.747 INFO: train_e/atom_rmse: 0.000451
train_f_mae: 0.016719
2025-06-11 18:37:50.753 INFO: train_f_mae: 0.016719
train_f_rmse: 0.021566
2025-06-11 18:37:50.754 INFO: train_f_rmse: 0.021566
val_e/atom_mae: 0.000651
2025-06-11 18:37:50.758 INFO: val_e/atom_mae: 0.000651
val_e/atom_rmse: 0.000686
2025-06-11 18:37:50.759 INFO: val_e/atom_rmse: 0.000686
val_f_mae: 0.016912
2025-06-11 18:37:50.761 INFO: val_f_mae: 0.016912
val_f_rmse: 0.021794
2025-06-11 18:37:50.762 INFO: val_f_rmse: 0.021794
##### Step: 88 Learning rate: 0.000625 #####
2025-06-11 18:40:05.507 INFO: ##### Step: 88 Learning rate: 0.000625 #####
Epoch 49, Train Loss: 0.4653, Val Loss: 0.4836
2025-06-11 18:40:05.509 INFO: Epoch 49, Train Loss: 0.4653, Val Loss: 0.4836
train_e/atom_mae: 0.000508
2025-06-11 18:40:05.511 INFO: train_e/atom_mae: 0.000508
train_e/atom_rmse: 0.000619
2025-06-11 18:40:05.512 INFO: train_e/atom_rmse: 0.000619
train_f_mae: 0.016465
2025-06-11 18:40:05.518 INFO: train_f_mae: 0.016465
train_f_rmse: 0.021241
2025-06-11 18:40:05.519 INFO: train_f_rmse: 0.021241
val_e/atom_mae: 0.000789
2025-06-11 18:40:05.524 INFO: val_e/atom_mae: 0.000789
val_e/atom_rmse: 0.000822
2025-06-11 18:40:05.525 INFO: val_e/atom_rmse: 0.000822
val_f_mae: 0.016581
2025-06-11 18:40:05.527 INFO: val_f_mae: 0.016581
val_f_rmse: 0.021417
2025-06-11 18:40:05.527 INFO: val_f_rmse: 0.021417
##### Step: 89 Learning rate: 0.000625 #####
2025-06-11 18:42:20.200 INFO: ##### Step: 89 Learning rate: 0.000625 #####
Epoch 50, Train Loss: 0.5393, Val Loss: 0.7041
2025-06-11 18:42:20.203 INFO: Epoch 50, Train Loss: 0.5393, Val Loss: 0.7041
train_e/atom_mae: 0.001344
2025-06-11 18:42:20.205 INFO: train_e/atom_mae: 0.001344
train_e/atom_rmse: 0.001566
2025-06-11 18:42:20.206 INFO: train_e/atom_rmse: 0.001566
train_f_mae: 0.016420
2025-06-11 18:42:20.212 INFO: train_f_mae: 0.016420
train_f_rmse: 0.021189
2025-06-11 18:42:20.213 INFO: train_f_rmse: 0.021189
val_e/atom_mae: 0.002600
2025-06-11 18:42:20.218 INFO: val_e/atom_mae: 0.002600
val_e/atom_rmse: 0.002609
2025-06-11 18:42:20.219 INFO: val_e/atom_rmse: 0.002609
val_f_mae: 0.016478
2025-06-11 18:42:20.221 INFO: val_f_mae: 0.016478
val_f_rmse: 0.021286
2025-06-11 18:42:20.221 INFO: val_f_rmse: 0.021286
##### Step: 90 Learning rate: 0.000625 #####
2025-06-11 18:44:34.896 INFO: ##### Step: 90 Learning rate: 0.000625 #####
Epoch 51, Train Loss: 0.5443, Val Loss: 0.5143
2025-06-11 18:44:34.898 INFO: Epoch 51, Train Loss: 0.5443, Val Loss: 0.5143
train_e/atom_mae: 0.001437
2025-06-11 18:44:34.901 INFO: train_e/atom_mae: 0.001437
train_e/atom_rmse: 0.001621
2025-06-11 18:44:34.901 INFO: train_e/atom_rmse: 0.001621
train_f_mae: 0.016400
2025-06-11 18:44:34.908 INFO: train_f_mae: 0.016400
train_f_rmse: 0.021151
2025-06-11 18:44:34.909 INFO: train_f_rmse: 0.021151
val_e/atom_mae: 0.001187
2025-06-11 18:44:34.913 INFO: val_e/atom_mae: 0.001187
val_e/atom_rmse: 0.001208
2025-06-11 18:44:34.914 INFO: val_e/atom_rmse: 0.001208
val_f_mae: 0.016589
2025-06-11 18:44:34.916 INFO: val_f_mae: 0.016589
val_f_rmse: 0.021460
2025-06-11 18:44:34.917 INFO: val_f_rmse: 0.021460
##### Step: 91 Learning rate: 0.000625 #####
2025-06-11 18:46:49.417 INFO: ##### Step: 91 Learning rate: 0.000625 #####
Epoch 52, Train Loss: 0.5588, Val Loss: 0.5645
2025-06-11 18:46:49.420 INFO: Epoch 52, Train Loss: 0.5588, Val Loss: 0.5645
train_e/atom_mae: 0.001444
2025-06-11 18:46:49.422 INFO: train_e/atom_mae: 0.001444
train_e/atom_rmse: 0.001698
2025-06-11 18:46:49.423 INFO: train_e/atom_rmse: 0.001698
train_f_mae: 0.016490
2025-06-11 18:46:49.429 INFO: train_f_mae: 0.016490
train_f_rmse: 0.021272
2025-06-11 18:46:49.430 INFO: train_f_rmse: 0.021272
val_e/atom_mae: 0.001644
2025-06-11 18:46:49.435 INFO: val_e/atom_mae: 0.001644
val_e/atom_rmse: 0.001659
2025-06-11 18:46:49.436 INFO: val_e/atom_rmse: 0.001659
val_f_mae: 0.016682
2025-06-11 18:46:49.437 INFO: val_f_mae: 0.016682
val_f_rmse: 0.021519
2025-06-11 18:46:49.438 INFO: val_f_rmse: 0.021519
##### Step: 92 Learning rate: 0.000625 #####
2025-06-11 18:49:03.981 INFO: ##### Step: 92 Learning rate: 0.000625 #####
Epoch 53, Train Loss: 0.5249, Val Loss: 0.4607
2025-06-11 18:49:03.984 INFO: Epoch 53, Train Loss: 0.5249, Val Loss: 0.4607
train_e/atom_mae: 0.001177
2025-06-11 18:49:03.986 INFO: train_e/atom_mae: 0.001177
train_e/atom_rmse: 0.001457
2025-06-11 18:49:03.987 INFO: train_e/atom_rmse: 0.001457
train_f_mae: 0.016381
2025-06-11 18:49:03.993 INFO: train_f_mae: 0.016381
train_f_rmse: 0.021134
2025-06-11 18:49:03.994 INFO: train_f_rmse: 0.021134
val_e/atom_mae: 0.000302
2025-06-11 18:49:03.998 INFO: val_e/atom_mae: 0.000302
val_e/atom_rmse: 0.000364
2025-06-11 18:49:04.000 INFO: val_e/atom_rmse: 0.000364
val_f_mae: 0.016528
2025-06-11 18:49:04.001 INFO: val_f_mae: 0.016528
val_f_rmse: 0.021351
2025-06-11 18:49:04.002 INFO: val_f_rmse: 0.021351
##### Step: 93 Learning rate: 0.000625 #####
2025-06-11 18:51:18.615 INFO: ##### Step: 93 Learning rate: 0.000625 #####
Epoch 54, Train Loss: 0.4922, Val Loss: 0.5142
2025-06-11 18:51:18.617 INFO: Epoch 54, Train Loss: 0.4922, Val Loss: 0.5142
train_e/atom_mae: 0.000738
2025-06-11 18:51:18.619 INFO: train_e/atom_mae: 0.000738
train_e/atom_rmse: 0.000881
2025-06-11 18:51:18.620 INFO: train_e/atom_rmse: 0.000881
train_f_mae: 0.016700
2025-06-11 18:51:18.626 INFO: train_f_mae: 0.016700
train_f_rmse: 0.021532
2025-06-11 18:51:18.627 INFO: train_f_rmse: 0.021532
val_e/atom_mae: 0.001273
2025-06-11 18:51:18.631 INFO: val_e/atom_mae: 0.001273
val_e/atom_rmse: 0.001295
2025-06-11 18:51:18.633 INFO: val_e/atom_rmse: 0.001295
val_f_mae: 0.016461
2025-06-11 18:51:18.634 INFO: val_f_mae: 0.016461
val_f_rmse: 0.021270
2025-06-11 18:51:18.635 INFO: val_f_rmse: 0.021270
##### Step: 94 Learning rate: 0.000625 #####
2025-06-11 18:53:33.234 INFO: ##### Step: 94 Learning rate: 0.000625 #####
Epoch 55, Train Loss: 0.4763, Val Loss: 0.4908
2025-06-11 18:53:33.236 INFO: Epoch 55, Train Loss: 0.4763, Val Loss: 0.4908
train_e/atom_mae: 0.000696
2025-06-11 18:53:33.238 INFO: train_e/atom_mae: 0.000696
train_e/atom_rmse: 0.000851
2025-06-11 18:53:33.239 INFO: train_e/atom_rmse: 0.000851
train_f_mae: 0.016439
2025-06-11 18:53:33.245 INFO: train_f_mae: 0.016439
train_f_rmse: 0.021203
2025-06-11 18:53:33.246 INFO: train_f_rmse: 0.021203
val_e/atom_mae: 0.000998
2025-06-11 18:53:33.251 INFO: val_e/atom_mae: 0.000998
val_e/atom_rmse: 0.001029
2025-06-11 18:53:33.252 INFO: val_e/atom_rmse: 0.001029
val_f_mae: 0.016421
2025-06-11 18:53:33.254 INFO: val_f_mae: 0.016421
val_f_rmse: 0.021254
2025-06-11 18:53:33.255 INFO: val_f_rmse: 0.021254
##### Step: 95 Learning rate: 0.000625 #####
2025-06-11 18:55:47.800 INFO: ##### Step: 95 Learning rate: 0.000625 #####
Epoch 56, Train Loss: 0.4625, Val Loss: 0.4867
2025-06-11 18:55:47.802 INFO: Epoch 56, Train Loss: 0.4625, Val Loss: 0.4867
train_e/atom_mae: 0.000500
2025-06-11 18:55:47.804 INFO: train_e/atom_mae: 0.000500
train_e/atom_rmse: 0.000603
2025-06-11 18:55:47.805 INFO: train_e/atom_rmse: 0.000603
train_f_mae: 0.016432
2025-06-11 18:55:47.811 INFO: train_f_mae: 0.016432
train_f_rmse: 0.021191
2025-06-11 18:55:47.812 INFO: train_f_rmse: 0.021191
val_e/atom_mae: 0.000517
2025-06-11 18:55:47.817 INFO: val_e/atom_mae: 0.000517
val_e/atom_rmse: 0.000572
2025-06-11 18:55:47.818 INFO: val_e/atom_rmse: 0.000572
val_f_mae: 0.016860
2025-06-11 18:55:47.819 INFO: val_f_mae: 0.016860
val_f_rmse: 0.021786
2025-06-11 18:55:47.820 INFO: val_f_rmse: 0.021786
##### Step: 96 Learning rate: 0.000625 #####
2025-06-11 18:58:02.400 INFO: ##### Step: 96 Learning rate: 0.000625 #####
Epoch 57, Train Loss: 0.4649, Val Loss: 0.4583
2025-06-11 18:58:02.402 INFO: Epoch 57, Train Loss: 0.4649, Val Loss: 0.4583
train_e/atom_mae: 0.000559
2025-06-11 18:58:02.404 INFO: train_e/atom_mae: 0.000559
train_e/atom_rmse: 0.000679
2025-06-11 18:58:02.405 INFO: train_e/atom_rmse: 0.000679
train_f_mae: 0.016405
2025-06-11 18:58:02.411 INFO: train_f_mae: 0.016405
train_f_rmse: 0.021164
2025-06-11 18:58:02.412 INFO: train_f_rmse: 0.021164
val_e/atom_mae: 0.000408
2025-06-11 18:58:02.417 INFO: val_e/atom_mae: 0.000408
val_e/atom_rmse: 0.000453
2025-06-11 18:58:02.418 INFO: val_e/atom_rmse: 0.000453
val_f_mae: 0.016436
2025-06-11 18:58:02.419 INFO: val_f_mae: 0.016436
val_f_rmse: 0.021231
2025-06-11 18:58:02.420 INFO: val_f_rmse: 0.021231
##### Step: 97 Learning rate: 0.000625 #####
2025-06-11 19:00:16.999 INFO: ##### Step: 97 Learning rate: 0.000625 #####
Epoch 58, Train Loss: 0.4615, Val Loss: 0.4550
2025-06-11 19:00:17.001 INFO: Epoch 58, Train Loss: 0.4615, Val Loss: 0.4550
train_e/atom_mae: 0.000453
2025-06-11 19:00:17.003 INFO: train_e/atom_mae: 0.000453
train_e/atom_rmse: 0.000549
2025-06-11 19:00:17.004 INFO: train_e/atom_rmse: 0.000549
train_f_mae: 0.016453
2025-06-11 19:00:17.010 INFO: train_f_mae: 0.016453
train_f_rmse: 0.021222
2025-06-11 19:00:17.011 INFO: train_f_rmse: 0.021222
val_e/atom_mae: 0.000282
2025-06-11 19:00:17.016 INFO: val_e/atom_mae: 0.000282
val_e/atom_rmse: 0.000330
2025-06-11 19:00:17.017 INFO: val_e/atom_rmse: 0.000330
val_f_mae: 0.016426
2025-06-11 19:00:17.018 INFO: val_f_mae: 0.016426
val_f_rmse: 0.021238
2025-06-11 19:00:17.019 INFO: val_f_rmse: 0.021238
##### Step: 98 Learning rate: 0.000625 #####
2025-06-11 19:02:31.707 INFO: ##### Step: 98 Learning rate: 0.000625 #####
Epoch 59, Train Loss: 0.4748, Val Loss: 0.5198
2025-06-11 19:02:31.709 INFO: Epoch 59, Train Loss: 0.4748, Val Loss: 0.5198
train_e/atom_mae: 0.000769
2025-06-11 19:02:31.712 INFO: train_e/atom_mae: 0.000769
train_e/atom_rmse: 0.000898
2025-06-11 19:02:31.712 INFO: train_e/atom_rmse: 0.000898
train_f_mae: 0.016348
2025-06-11 19:02:31.719 INFO: train_f_mae: 0.016348
train_f_rmse: 0.021096
2025-06-11 19:02:31.720 INFO: train_f_rmse: 0.021096
val_e/atom_mae: 0.001294
2025-06-11 19:02:31.724 INFO: val_e/atom_mae: 0.001294
val_e/atom_rmse: 0.001310
2025-06-11 19:02:31.725 INFO: val_e/atom_rmse: 0.001310
val_f_mae: 0.016563
2025-06-11 19:02:31.727 INFO: val_f_mae: 0.016563
val_f_rmse: 0.021367
2025-06-11 19:02:31.728 INFO: val_f_rmse: 0.021367
##### Step: 99 Learning rate: 0.000625 #####
2025-06-11 19:04:46.374 INFO: ##### Step: 99 Learning rate: 0.000625 #####
Epoch 60, Train Loss: 0.4689, Val Loss: 0.4526
2025-06-11 19:04:46.375 INFO: Epoch 60, Train Loss: 0.4689, Val Loss: 0.4526
train_e/atom_mae: 0.000623
2025-06-11 19:04:46.378 INFO: train_e/atom_mae: 0.000623
train_e/atom_rmse: 0.000732
2025-06-11 19:04:46.378 INFO: train_e/atom_rmse: 0.000732
train_f_mae: 0.016431
2025-06-11 19:04:46.385 INFO: train_f_mae: 0.016431
train_f_rmse: 0.021192
2025-06-11 19:04:46.386 INFO: train_f_rmse: 0.021192
val_e/atom_mae: 0.000212
2025-06-11 19:04:46.390 INFO: val_e/atom_mae: 0.000212
val_e/atom_rmse: 0.000250
2025-06-11 19:04:46.392 INFO: val_e/atom_rmse: 0.000250
val_f_mae: 0.016420
2025-06-11 19:04:46.393 INFO: val_f_mae: 0.016420
val_f_rmse: 0.021221
2025-06-11 19:04:46.394 INFO: val_f_rmse: 0.021221
##### Step: 100 Learning rate: 0.0003125 #####
2025-06-11 19:07:15.686 INFO: ##### Step: 100 Learning rate: 0.0003125 #####
Epoch 61, Train Loss: 0.4601, Val Loss: 0.4691
2025-06-11 19:07:15.687 INFO: Epoch 61, Train Loss: 0.4601, Val Loss: 0.4691
train_e/atom_mae: 0.000618
2025-06-11 19:07:15.689 INFO: train_e/atom_mae: 0.000618
train_e/atom_rmse: 0.000720
2025-06-11 19:07:15.690 INFO: train_e/atom_rmse: 0.000720
train_f_mae: 0.016281
2025-06-11 19:07:15.696 INFO: train_f_mae: 0.016281
train_f_rmse: 0.021000
2025-06-11 19:07:15.697 INFO: train_f_rmse: 0.021000
val_e/atom_mae: 0.000738
2025-06-11 19:07:15.702 INFO: val_e/atom_mae: 0.000738
val_e/atom_rmse: 0.000770
2025-06-11 19:07:15.703 INFO: val_e/atom_rmse: 0.000770
val_f_mae: 0.016363
2025-06-11 19:07:15.704 INFO: val_f_mae: 0.016363
val_f_rmse: 0.021148
2025-06-11 19:07:15.705 INFO: val_f_rmse: 0.021148
##### Step: 101 Learning rate: 0.0003125 #####
2025-06-11 19:09:30.263 INFO: ##### Step: 101 Learning rate: 0.0003125 #####
Epoch 62, Train Loss: 0.4650, Val Loss: 0.4504
2025-06-11 19:09:30.264 INFO: Epoch 62, Train Loss: 0.4650, Val Loss: 0.4504
train_e/atom_mae: 0.000637
2025-06-11 19:09:30.266 INFO: train_e/atom_mae: 0.000637
train_e/atom_rmse: 0.000804
2025-06-11 19:09:30.267 INFO: train_e/atom_rmse: 0.000804
train_f_mae: 0.016277
2025-06-11 19:09:30.273 INFO: train_f_mae: 0.016277
train_f_rmse: 0.021003
2025-06-11 19:09:30.274 INFO: train_f_rmse: 0.021003
val_e/atom_mae: 0.000383
2025-06-11 19:09:30.279 INFO: val_e/atom_mae: 0.000383
val_e/atom_rmse: 0.000439
2025-06-11 19:09:30.280 INFO: val_e/atom_rmse: 0.000439
val_f_mae: 0.016298
2025-06-11 19:09:30.281 INFO: val_f_mae: 0.016298
val_f_rmse: 0.021055
2025-06-11 19:09:30.282 INFO: val_f_rmse: 0.021055
##### Step: 102 Learning rate: 0.0003125 #####
2025-06-11 19:11:44.794 INFO: ##### Step: 102 Learning rate: 0.0003125 #####
Epoch 63, Train Loss: 0.4563, Val Loss: 0.4511
2025-06-11 19:11:44.795 INFO: Epoch 63, Train Loss: 0.4563, Val Loss: 0.4511
train_e/atom_mae: 0.000502
2025-06-11 19:11:44.797 INFO: train_e/atom_mae: 0.000502
train_e/atom_rmse: 0.000612
2025-06-11 19:11:44.798 INFO: train_e/atom_rmse: 0.000612
train_f_mae: 0.016303
2025-06-11 19:11:44.804 INFO: train_f_mae: 0.016303
train_f_rmse: 0.021035
2025-06-11 19:11:44.805 INFO: train_f_rmse: 0.021035
val_e/atom_mae: 0.000249
2025-06-11 19:11:44.810 INFO: val_e/atom_mae: 0.000249
val_e/atom_rmse: 0.000298
2025-06-11 19:11:44.811 INFO: val_e/atom_rmse: 0.000298
val_f_mae: 0.016376
2025-06-11 19:11:44.812 INFO: val_f_mae: 0.016376
val_f_rmse: 0.021161
2025-06-11 19:11:44.813 INFO: val_f_rmse: 0.021161
##### Step: 103 Learning rate: 0.0003125 #####
2025-06-11 19:13:59.190 INFO: ##### Step: 103 Learning rate: 0.0003125 #####
Epoch 64, Train Loss: 0.4532, Val Loss: 0.4480
2025-06-11 19:13:59.191 INFO: Epoch 64, Train Loss: 0.4532, Val Loss: 0.4480
train_e/atom_mae: 0.000416
2025-06-11 19:13:59.193 INFO: train_e/atom_mae: 0.000416
train_e/atom_rmse: 0.000517
2025-06-11 19:13:59.194 INFO: train_e/atom_rmse: 0.000517
train_f_mae: 0.016323
2025-06-11 19:13:59.200 INFO: train_f_mae: 0.016323
train_f_rmse: 0.021056
2025-06-11 19:13:59.201 INFO: train_f_rmse: 0.021056
val_e/atom_mae: 0.000163
2025-06-11 19:13:59.206 INFO: val_e/atom_mae: 0.000163
val_e/atom_rmse: 0.000207
2025-06-11 19:13:59.207 INFO: val_e/atom_rmse: 0.000207
val_f_mae: 0.016355
2025-06-11 19:13:59.208 INFO: val_f_mae: 0.016355
val_f_rmse: 0.021129
2025-06-11 19:13:59.209 INFO: val_f_rmse: 0.021129
##### Step: 104 Learning rate: 0.0003125 #####
2025-06-11 19:16:13.764 INFO: ##### Step: 104 Learning rate: 0.0003125 #####
Epoch 65, Train Loss: 0.4476, Val Loss: 0.4578
2025-06-11 19:16:13.766 INFO: Epoch 65, Train Loss: 0.4476, Val Loss: 0.4578
train_e/atom_mae: 0.000368
2025-06-11 19:16:13.768 INFO: train_e/atom_mae: 0.000368
train_e/atom_rmse: 0.000451
2025-06-11 19:16:13.769 INFO: train_e/atom_rmse: 0.000451
train_f_mae: 0.016256
2025-06-11 19:16:13.775 INFO: train_f_mae: 0.016256
train_f_rmse: 0.020978
2025-06-11 19:16:13.776 INFO: train_f_rmse: 0.020978
val_e/atom_mae: 0.000464
2025-06-11 19:16:13.780 INFO: val_e/atom_mae: 0.000464
val_e/atom_rmse: 0.000512
2025-06-11 19:16:13.782 INFO: val_e/atom_rmse: 0.000512
val_f_mae: 0.016376
2025-06-11 19:16:13.783 INFO: val_f_mae: 0.016376
val_f_rmse: 0.021169
2025-06-11 19:16:13.784 INFO: val_f_rmse: 0.021169
##### Step: 105 Learning rate: 0.0003125 #####
2025-06-11 19:18:28.216 INFO: ##### Step: 105 Learning rate: 0.0003125 #####
Epoch 66, Train Loss: 0.4502, Val Loss: 0.4631
2025-06-11 19:18:28.218 INFO: Epoch 66, Train Loss: 0.4502, Val Loss: 0.4631
train_e/atom_mae: 0.000409
2025-06-11 19:18:28.221 INFO: train_e/atom_mae: 0.000409
train_e/atom_rmse: 0.000499
2025-06-11 19:18:28.222 INFO: train_e/atom_rmse: 0.000499
train_f_mae: 0.016288
2025-06-11 19:18:28.228 INFO: train_f_mae: 0.016288
train_f_rmse: 0.021001
2025-06-11 19:18:28.228 INFO: train_f_rmse: 0.021001
val_e/atom_mae: 0.000325
2025-06-11 19:18:28.233 INFO: val_e/atom_mae: 0.000325
val_e/atom_rmse: 0.000382
2025-06-11 19:18:28.234 INFO: val_e/atom_rmse: 0.000382
val_f_mae: 0.016528
2025-06-11 19:18:28.236 INFO: val_f_mae: 0.016528
val_f_rmse: 0.021395
2025-06-11 19:18:28.236 INFO: val_f_rmse: 0.021395
##### Step: 106 Learning rate: 0.0003125 #####
2025-06-11 19:20:42.736 INFO: ##### Step: 106 Learning rate: 0.0003125 #####
Epoch 67, Train Loss: 0.4505, Val Loss: 0.4539
2025-06-11 19:20:42.737 INFO: Epoch 67, Train Loss: 0.4505, Val Loss: 0.4539
train_e/atom_mae: 0.000432
2025-06-11 19:20:42.740 INFO: train_e/atom_mae: 0.000432
train_e/atom_rmse: 0.000531
2025-06-11 19:20:42.741 INFO: train_e/atom_rmse: 0.000531
train_f_mae: 0.016264
2025-06-11 19:20:42.747 INFO: train_f_mae: 0.016264
train_f_rmse: 0.020979
2025-06-11 19:20:42.748 INFO: train_f_rmse: 0.020979
val_e/atom_mae: 0.000213
2025-06-11 19:20:42.752 INFO: val_e/atom_mae: 0.000213
val_e/atom_rmse: 0.000267
2025-06-11 19:20:42.754 INFO: val_e/atom_rmse: 0.000267
val_f_mae: 0.016441
2025-06-11 19:20:42.755 INFO: val_f_mae: 0.016441
val_f_rmse: 0.021243
2025-06-11 19:20:42.756 INFO: val_f_rmse: 0.021243
##### Step: 107 Learning rate: 0.0003125 #####
2025-06-11 19:22:57.158 INFO: ##### Step: 107 Learning rate: 0.0003125 #####
Epoch 68, Train Loss: 0.4572, Val Loss: 0.4659
2025-06-11 19:22:57.161 INFO: Epoch 68, Train Loss: 0.4572, Val Loss: 0.4659
train_e/atom_mae: 0.000470
2025-06-11 19:22:57.163 INFO: train_e/atom_mae: 0.000470
train_e/atom_rmse: 0.000576
2025-06-11 19:22:57.164 INFO: train_e/atom_rmse: 0.000576
train_f_mae: 0.016347
2025-06-11 19:22:57.170 INFO: train_f_mae: 0.016347
train_f_rmse: 0.021093
2025-06-11 19:22:57.171 INFO: train_f_rmse: 0.021093
val_e/atom_mae: 0.000178
2025-06-11 19:22:57.175 INFO: val_e/atom_mae: 0.000178
val_e/atom_rmse: 0.000216
2025-06-11 19:22:57.177 INFO: val_e/atom_rmse: 0.000216
val_f_mae: 0.016648
2025-06-11 19:22:57.178 INFO: val_f_mae: 0.016648
val_f_rmse: 0.021545
2025-06-11 19:22:57.179 INFO: val_f_rmse: 0.021545
##### Step: 108 Learning rate: 0.0003125 #####
2025-06-11 19:25:11.637 INFO: ##### Step: 108 Learning rate: 0.0003125 #####
Epoch 69, Train Loss: 0.4593, Val Loss: 0.4766
2025-06-11 19:25:11.639 INFO: Epoch 69, Train Loss: 0.4593, Val Loss: 0.4766
train_e/atom_mae: 0.000596
2025-06-11 19:25:11.641 INFO: train_e/atom_mae: 0.000596
train_e/atom_rmse: 0.000740
2025-06-11 19:25:11.642 INFO: train_e/atom_rmse: 0.000740
train_f_mae: 0.016245
2025-06-11 19:25:11.648 INFO: train_f_mae: 0.016245
train_f_rmse: 0.020956
2025-06-11 19:25:11.649 INFO: train_f_rmse: 0.020956
val_e/atom_mae: 0.000891
2025-06-11 19:25:11.654 INFO: val_e/atom_mae: 0.000891
val_e/atom_rmse: 0.000914
2025-06-11 19:25:11.655 INFO: val_e/atom_rmse: 0.000914
val_f_mae: 0.016354
2025-06-11 19:25:11.656 INFO: val_f_mae: 0.016354
val_f_rmse: 0.021114
2025-06-11 19:25:11.657 INFO: val_f_rmse: 0.021114
##### Step: 109 Learning rate: 0.0003125 #####
2025-06-11 19:27:26.052 INFO: ##### Step: 109 Learning rate: 0.0003125 #####
Epoch 70, Train Loss: 0.4477, Val Loss: 0.4798
2025-06-11 19:27:26.055 INFO: Epoch 70, Train Loss: 0.4477, Val Loss: 0.4798
train_e/atom_mae: 0.000397
2025-06-11 19:27:26.057 INFO: train_e/atom_mae: 0.000397
train_e/atom_rmse: 0.000487
2025-06-11 19:27:26.058 INFO: train_e/atom_rmse: 0.000487
train_f_mae: 0.016242
2025-06-11 19:27:26.064 INFO: train_f_mae: 0.016242
train_f_rmse: 0.020951
2025-06-11 19:27:26.065 INFO: train_f_rmse: 0.020951
val_e/atom_mae: 0.000804
2025-06-11 19:27:26.070 INFO: val_e/atom_mae: 0.000804
val_e/atom_rmse: 0.000832
2025-06-11 19:27:26.071 INFO: val_e/atom_rmse: 0.000832
val_f_mae: 0.016488
2025-06-11 19:27:26.072 INFO: val_f_mae: 0.016488
val_f_rmse: 0.021314
2025-06-11 19:27:26.073 INFO: val_f_rmse: 0.021314
##### Step: 110 Learning rate: 0.0003125 #####
2025-06-11 19:29:41.831 INFO: ##### Step: 110 Learning rate: 0.0003125 #####
Epoch 71, Train Loss: 0.4569, Val Loss: 0.4664
2025-06-11 19:29:41.832 INFO: Epoch 71, Train Loss: 0.4569, Val Loss: 0.4664
train_e/atom_mae: 0.000441
2025-06-11 19:29:41.834 INFO: train_e/atom_mae: 0.000441
train_e/atom_rmse: 0.000552
2025-06-11 19:29:41.835 INFO: train_e/atom_rmse: 0.000552
train_f_mae: 0.016366
2025-06-11 19:29:41.841 INFO: train_f_mae: 0.016366
train_f_rmse: 0.021110
2025-06-11 19:29:41.842 INFO: train_f_rmse: 0.021110
val_e/atom_mae: 0.000528
2025-06-11 19:29:41.847 INFO: val_e/atom_mae: 0.000528
val_e/atom_rmse: 0.000569
2025-06-11 19:29:41.848 INFO: val_e/atom_rmse: 0.000569
val_f_mae: 0.016540
2025-06-11 19:29:41.849 INFO: val_f_mae: 0.016540
val_f_rmse: 0.021317
2025-06-11 19:29:41.850 INFO: val_f_rmse: 0.021317
##### Step: 111 Learning rate: 0.0003125 #####
2025-06-11 19:31:56.296 INFO: ##### Step: 111 Learning rate: 0.0003125 #####
Epoch 72, Train Loss: 0.4486, Val Loss: 0.4476
2025-06-11 19:31:56.297 INFO: Epoch 72, Train Loss: 0.4486, Val Loss: 0.4476
train_e/atom_mae: 0.000405
2025-06-11 19:31:56.300 INFO: train_e/atom_mae: 0.000405
train_e/atom_rmse: 0.000495
2025-06-11 19:31:56.300 INFO: train_e/atom_rmse: 0.000495
train_f_mae: 0.016258
2025-06-11 19:31:56.307 INFO: train_f_mae: 0.016258
train_f_rmse: 0.020966
2025-06-11 19:31:56.307 INFO: train_f_rmse: 0.020966
val_e/atom_mae: 0.000164
2025-06-11 19:31:56.312 INFO: val_e/atom_mae: 0.000164
val_e/atom_rmse: 0.000212
2025-06-11 19:31:56.313 INFO: val_e/atom_rmse: 0.000212
val_f_mae: 0.016344
2025-06-11 19:31:56.315 INFO: val_f_mae: 0.016344
val_f_rmse: 0.021117
2025-06-11 19:31:56.315 INFO: val_f_rmse: 0.021117
##### Step: 112 Learning rate: 0.0003125 #####
2025-06-11 19:34:10.750 INFO: ##### Step: 112 Learning rate: 0.0003125 #####
Epoch 73, Train Loss: 0.4495, Val Loss: 0.4549
2025-06-11 19:34:10.752 INFO: Epoch 73, Train Loss: 0.4495, Val Loss: 0.4549
train_e/atom_mae: 0.000338
2025-06-11 19:34:10.754 INFO: train_e/atom_mae: 0.000338
train_e/atom_rmse: 0.000424
2025-06-11 19:34:10.755 INFO: train_e/atom_rmse: 0.000424
train_f_mae: 0.016323
2025-06-11 19:34:10.761 INFO: train_f_mae: 0.016323
train_f_rmse: 0.021044
2025-06-11 19:34:10.762 INFO: train_f_rmse: 0.021044
val_e/atom_mae: 0.000384
2025-06-11 19:34:10.767 INFO: val_e/atom_mae: 0.000384
val_e/atom_rmse: 0.000432
2025-06-11 19:34:10.768 INFO: val_e/atom_rmse: 0.000432
val_f_mae: 0.016392
2025-06-11 19:34:10.769 INFO: val_f_mae: 0.016392
val_f_rmse: 0.021166
2025-06-11 19:34:10.770 INFO: val_f_rmse: 0.021166
##### Step: 113 Learning rate: 0.0003125 #####
2025-06-11 19:36:25.232 INFO: ##### Step: 113 Learning rate: 0.0003125 #####
Epoch 74, Train Loss: 0.4598, Val Loss: 0.4529
2025-06-11 19:36:25.233 INFO: Epoch 74, Train Loss: 0.4598, Val Loss: 0.4529
train_e/atom_mae: 0.000608
2025-06-11 19:36:25.236 INFO: train_e/atom_mae: 0.000608
train_e/atom_rmse: 0.000724
2025-06-11 19:36:25.236 INFO: train_e/atom_rmse: 0.000724
train_f_mae: 0.016278
2025-06-11 19:36:25.243 INFO: train_f_mae: 0.016278
train_f_rmse: 0.020987
2025-06-11 19:36:25.243 INFO: train_f_rmse: 0.020987
val_e/atom_mae: 0.000314
2025-06-11 19:36:25.248 INFO: val_e/atom_mae: 0.000314
val_e/atom_rmse: 0.000366
2025-06-11 19:36:25.250 INFO: val_e/atom_rmse: 0.000366
val_f_mae: 0.016391
2025-06-11 19:36:25.251 INFO: val_f_mae: 0.016391
val_f_rmse: 0.021166
2025-06-11 19:36:25.252 INFO: val_f_rmse: 0.021166
##### Step: 114 Learning rate: 0.0003125 #####
2025-06-11 19:38:39.706 INFO: ##### Step: 114 Learning rate: 0.0003125 #####
Epoch 75, Train Loss: 0.4542, Val Loss: 0.4574
2025-06-11 19:38:39.708 INFO: Epoch 75, Train Loss: 0.4542, Val Loss: 0.4574
train_e/atom_mae: 0.000457
2025-06-11 19:38:39.710 INFO: train_e/atom_mae: 0.000457
train_e/atom_rmse: 0.000569
2025-06-11 19:38:39.711 INFO: train_e/atom_rmse: 0.000569
train_f_mae: 0.016306
2025-06-11 19:38:39.717 INFO: train_f_mae: 0.016306
train_f_rmse: 0.021030
2025-06-11 19:38:39.718 INFO: train_f_rmse: 0.021030
val_e/atom_mae: 0.000371
2025-06-11 19:38:39.723 INFO: val_e/atom_mae: 0.000371
val_e/atom_rmse: 0.000422
2025-06-11 19:38:39.724 INFO: val_e/atom_rmse: 0.000422
val_f_mae: 0.016441
2025-06-11 19:38:39.725 INFO: val_f_mae: 0.016441
val_f_rmse: 0.021232
2025-06-11 19:38:39.726 INFO: val_f_rmse: 0.021232
##### Step: 115 Learning rate: 0.0003125 #####
2025-06-11 19:40:54.145 INFO: ##### Step: 115 Learning rate: 0.0003125 #####
Epoch 76, Train Loss: 0.4561, Val Loss: 0.4676
2025-06-11 19:40:54.148 INFO: Epoch 76, Train Loss: 0.4561, Val Loss: 0.4676
train_e/atom_mae: 0.000566
2025-06-11 19:40:54.150 INFO: train_e/atom_mae: 0.000566
train_e/atom_rmse: 0.000693
2025-06-11 19:40:54.151 INFO: train_e/atom_rmse: 0.000693
train_f_mae: 0.016232
2025-06-11 19:40:54.157 INFO: train_f_mae: 0.016232
train_f_rmse: 0.020939
2025-06-11 19:40:54.158 INFO: train_f_rmse: 0.020939
val_e/atom_mae: 0.000622
2025-06-11 19:40:54.163 INFO: val_e/atom_mae: 0.000622
val_e/atom_rmse: 0.000653
2025-06-11 19:40:54.164 INFO: val_e/atom_rmse: 0.000653
val_f_mae: 0.016454
2025-06-11 19:40:54.165 INFO: val_f_mae: 0.016454
val_f_rmse: 0.021258
2025-06-11 19:40:54.166 INFO: val_f_rmse: 0.021258
##### Step: 116 Learning rate: 0.0003125 #####
2025-06-11 19:43:08.668 INFO: ##### Step: 116 Learning rate: 0.0003125 #####
Epoch 77, Train Loss: 0.4483, Val Loss: 0.4704
2025-06-11 19:43:08.670 INFO: Epoch 77, Train Loss: 0.4483, Val Loss: 0.4704
train_e/atom_mae: 0.000424
2025-06-11 19:43:08.673 INFO: train_e/atom_mae: 0.000424
train_e/atom_rmse: 0.000525
2025-06-11 19:43:08.673 INFO: train_e/atom_rmse: 0.000525
train_f_mae: 0.016230
2025-06-11 19:43:08.680 INFO: train_f_mae: 0.016230
train_f_rmse: 0.020932
2025-06-11 19:43:08.680 INFO: train_f_rmse: 0.020932
val_e/atom_mae: 0.000795
2025-06-11 19:43:08.685 INFO: val_e/atom_mae: 0.000795
val_e/atom_rmse: 0.000820
2025-06-11 19:43:08.686 INFO: val_e/atom_rmse: 0.000820
val_f_mae: 0.016340
2025-06-11 19:43:08.688 INFO: val_f_mae: 0.016340
val_f_rmse: 0.021109
2025-06-11 19:43:08.688 INFO: val_f_rmse: 0.021109
##### Step: 117 Learning rate: 0.0003125 #####
2025-06-11 19:45:23.189 INFO: ##### Step: 117 Learning rate: 0.0003125 #####
Epoch 78, Train Loss: 0.4576, Val Loss: 0.4565
2025-06-11 19:45:23.191 INFO: Epoch 78, Train Loss: 0.4576, Val Loss: 0.4565
train_e/atom_mae: 0.000631
2025-06-11 19:45:23.194 INFO: train_e/atom_mae: 0.000631
train_e/atom_rmse: 0.000741
2025-06-11 19:45:23.194 INFO: train_e/atom_rmse: 0.000741
train_f_mae: 0.016213
2025-06-11 19:45:23.201 INFO: train_f_mae: 0.016213
train_f_rmse: 0.020913
2025-06-11 19:45:23.202 INFO: train_f_rmse: 0.020913
val_e/atom_mae: 0.000276
2025-06-11 19:45:23.206 INFO: val_e/atom_mae: 0.000276
val_e/atom_rmse: 0.000335
2025-06-11 19:45:23.207 INFO: val_e/atom_rmse: 0.000335
val_f_mae: 0.016471
2025-06-11 19:45:23.209 INFO: val_f_mae: 0.016471
val_f_rmse: 0.021270
2025-06-11 19:45:23.209 INFO: val_f_rmse: 0.021270
##### Step: 118 Learning rate: 0.0003125 #####
2025-06-11 19:47:37.748 INFO: ##### Step: 118 Learning rate: 0.0003125 #####
Epoch 79, Train Loss: 0.4540, Val Loss: 0.4556
2025-06-11 19:47:37.750 INFO: Epoch 79, Train Loss: 0.4540, Val Loss: 0.4556
train_e/atom_mae: 0.000470
2025-06-11 19:47:37.752 INFO: train_e/atom_mae: 0.000470
train_e/atom_rmse: 0.000557
2025-06-11 19:47:37.753 INFO: train_e/atom_rmse: 0.000557
train_f_mae: 0.016314
2025-06-11 19:47:37.759 INFO: train_f_mae: 0.016314
train_f_rmse: 0.021036
2025-06-11 19:47:37.760 INFO: train_f_rmse: 0.021036
val_e/atom_mae: 0.000227
2025-06-11 19:47:37.765 INFO: val_e/atom_mae: 0.000227
val_e/atom_rmse: 0.000267
2025-06-11 19:47:37.766 INFO: val_e/atom_rmse: 0.000267
val_f_mae: 0.016492
2025-06-11 19:47:37.767 INFO: val_f_mae: 0.016492
val_f_rmse: 0.021283
2025-06-11 19:47:37.768 INFO: val_f_rmse: 0.021283
##### Step: 119 Learning rate: 0.0003125 #####
2025-06-11 19:49:52.254 INFO: ##### Step: 119 Learning rate: 0.0003125 #####
Epoch 80, Train Loss: 0.4448, Val Loss: 0.4618
2025-06-11 19:49:52.256 INFO: Epoch 80, Train Loss: 0.4448, Val Loss: 0.4618
train_e/atom_mae: 0.000344
2025-06-11 19:49:52.258 INFO: train_e/atom_mae: 0.000344
train_e/atom_rmse: 0.000439
2025-06-11 19:49:52.259 INFO: train_e/atom_rmse: 0.000439
train_f_mae: 0.016221
2025-06-11 19:49:52.265 INFO: train_f_mae: 0.016221
train_f_rmse: 0.020920
2025-06-11 19:49:52.266 INFO: train_f_rmse: 0.020920
val_e/atom_mae: 0.000233
2025-06-11 19:49:52.271 INFO: val_e/atom_mae: 0.000233
val_e/atom_rmse: 0.000271
2025-06-11 19:49:52.272 INFO: val_e/atom_rmse: 0.000271
val_f_mae: 0.016610
2025-06-11 19:49:52.274 INFO: val_f_mae: 0.016610
val_f_rmse: 0.021427
2025-06-11 19:49:52.274 INFO: val_f_rmse: 0.021427
##### Step: 120 Learning rate: 0.00015625 #####
2025-06-11 19:52:06.794 INFO: ##### Step: 120 Learning rate: 0.00015625 #####
Epoch 81, Train Loss: 0.4457, Val Loss: 0.4532
2025-06-11 19:52:06.797 INFO: Epoch 81, Train Loss: 0.4457, Val Loss: 0.4532
train_e/atom_mae: 0.000369
2025-06-11 19:52:06.799 INFO: train_e/atom_mae: 0.000369
train_e/atom_rmse: 0.000458
2025-06-11 19:52:06.800 INFO: train_e/atom_rmse: 0.000458
train_f_mae: 0.016223
2025-06-11 19:52:06.806 INFO: train_f_mae: 0.016223
train_f_rmse: 0.020928
2025-06-11 19:52:06.807 INFO: train_f_rmse: 0.020928
val_e/atom_mae: 0.000385
2025-06-11 19:52:06.811 INFO: val_e/atom_mae: 0.000385
val_e/atom_rmse: 0.000440
2025-06-11 19:52:06.813 INFO: val_e/atom_rmse: 0.000440
val_f_mae: 0.016349
2025-06-11 19:52:06.814 INFO: val_f_mae: 0.016349
val_f_rmse: 0.021120
2025-06-11 19:52:06.815 INFO: val_f_rmse: 0.021120
##### Step: 121 Learning rate: 0.00015625 #####
2025-06-11 19:54:21.213 INFO: ##### Step: 121 Learning rate: 0.00015625 #####
Epoch 82, Train Loss: 0.4416, Val Loss: 0.4649
2025-06-11 19:54:21.215 INFO: Epoch 82, Train Loss: 0.4416, Val Loss: 0.4649
train_e/atom_mae: 0.000327
2025-06-11 19:54:21.218 INFO: train_e/atom_mae: 0.000327
train_e/atom_rmse: 0.000404
2025-06-11 19:54:21.219 INFO: train_e/atom_rmse: 0.000404
train_f_mae: 0.016184
2025-06-11 19:54:21.225 INFO: train_f_mae: 0.016184
train_f_rmse: 0.020871
2025-06-11 19:54:21.226 INFO: train_f_rmse: 0.020871
val_e/atom_mae: 0.000697
2025-06-11 19:54:21.230 INFO: val_e/atom_mae: 0.000697
val_e/atom_rmse: 0.000737
2025-06-11 19:54:21.232 INFO: val_e/atom_rmse: 0.000737
val_f_mae: 0.016341
2025-06-11 19:54:21.233 INFO: val_f_mae: 0.016341
val_f_rmse: 0.021093
2025-06-11 19:54:21.234 INFO: val_f_rmse: 0.021093
##### Step: 122 Learning rate: 0.00015625 #####
2025-06-11 19:56:35.633 INFO: ##### Step: 122 Learning rate: 0.00015625 #####
Epoch 83, Train Loss: 0.4422, Val Loss: 0.4467
2025-06-11 19:56:35.636 INFO: Epoch 83, Train Loss: 0.4422, Val Loss: 0.4467
train_e/atom_mae: 0.000409
2025-06-11 19:56:35.638 INFO: train_e/atom_mae: 0.000409
train_e/atom_rmse: 0.000503
2025-06-11 19:56:35.639 INFO: train_e/atom_rmse: 0.000503
train_f_mae: 0.016133
2025-06-11 19:56:35.645 INFO: train_f_mae: 0.016133
train_f_rmse: 0.020805
2025-06-11 19:56:35.646 INFO: train_f_rmse: 0.020805
val_e/atom_mae: 0.000278
2025-06-11 19:56:35.651 INFO: val_e/atom_mae: 0.000278
val_e/atom_rmse: 0.000333
2025-06-11 19:56:35.652 INFO: val_e/atom_rmse: 0.000333
val_f_mae: 0.016285
2025-06-11 19:56:35.653 INFO: val_f_mae: 0.016285
val_f_rmse: 0.021039
2025-06-11 19:56:35.654 INFO: val_f_rmse: 0.021039
##### Step: 123 Learning rate: 0.00015625 #####
2025-06-11 19:58:50.149 INFO: ##### Step: 123 Learning rate: 0.00015625 #####
Epoch 84, Train Loss: 0.4441, Val Loss: 0.4478
2025-06-11 19:58:50.152 INFO: Epoch 84, Train Loss: 0.4441, Val Loss: 0.4478
train_e/atom_mae: 0.000381
2025-06-11 19:58:50.154 INFO: train_e/atom_mae: 0.000381
train_e/atom_rmse: 0.000491
2025-06-11 19:58:50.155 INFO: train_e/atom_rmse: 0.000491
train_f_mae: 0.016172
2025-06-11 19:58:50.161 INFO: train_f_mae: 0.016172
train_f_rmse: 0.020862
2025-06-11 19:58:50.162 INFO: train_f_rmse: 0.020862
val_e/atom_mae: 0.000324
2025-06-11 19:58:50.167 INFO: val_e/atom_mae: 0.000324
val_e/atom_rmse: 0.000373
2025-06-11 19:58:50.168 INFO: val_e/atom_rmse: 0.000373
val_f_mae: 0.016286
2025-06-11 19:58:50.169 INFO: val_f_mae: 0.016286
val_f_rmse: 0.021040
2025-06-11 19:58:50.170 INFO: val_f_rmse: 0.021040
##### Step: 124 Learning rate: 0.00015625 #####
2025-06-11 20:01:04.509 INFO: ##### Step: 124 Learning rate: 0.00015625 #####
Epoch 85, Train Loss: 0.4394, Val Loss: 0.4440
2025-06-11 20:01:04.512 INFO: Epoch 85, Train Loss: 0.4394, Val Loss: 0.4440
train_e/atom_mae: 0.000276
2025-06-11 20:01:04.514 INFO: train_e/atom_mae: 0.000276
train_e/atom_rmse: 0.000352
2025-06-11 20:01:04.515 INFO: train_e/atom_rmse: 0.000352
train_f_mae: 0.016171
2025-06-11 20:01:04.521 INFO: train_f_mae: 0.016171
train_f_rmse: 0.020853
2025-06-11 20:01:04.522 INFO: train_f_rmse: 0.020853
val_e/atom_mae: 0.000166
2025-06-11 20:01:04.527 INFO: val_e/atom_mae: 0.000166
val_e/atom_rmse: 0.000205
2025-06-11 20:01:04.528 INFO: val_e/atom_rmse: 0.000205
val_f_mae: 0.016301
2025-06-11 20:01:04.529 INFO: val_f_mae: 0.016301
val_f_rmse: 0.021034
2025-06-11 20:01:04.530 INFO: val_f_rmse: 0.021034
##### Step: 125 Learning rate: 0.00015625 #####
2025-06-11 20:03:18.953 INFO: ##### Step: 125 Learning rate: 0.00015625 #####
Epoch 86, Train Loss: 0.4500, Val Loss: 0.4430
2025-06-11 20:03:18.955 INFO: Epoch 86, Train Loss: 0.4500, Val Loss: 0.4430
train_e/atom_mae: 0.000504
2025-06-11 20:03:18.957 INFO: train_e/atom_mae: 0.000504
train_e/atom_rmse: 0.000634
2025-06-11 20:03:18.958 INFO: train_e/atom_rmse: 0.000634
train_f_mae: 0.016173
2025-06-11 20:03:18.964 INFO: train_f_mae: 0.016173
train_f_rmse: 0.020860
2025-06-11 20:03:18.965 INFO: train_f_rmse: 0.020860
val_e/atom_mae: 0.000300
2025-06-11 20:03:18.970 INFO: val_e/atom_mae: 0.000300
val_e/atom_rmse: 0.000352
2025-06-11 20:03:18.971 INFO: val_e/atom_rmse: 0.000352
val_f_mae: 0.016243
2025-06-11 20:03:18.973 INFO: val_f_mae: 0.016243
val_f_rmse: 0.020939
2025-06-11 20:03:18.973 INFO: val_f_rmse: 0.020939
##### Step: 126 Learning rate: 0.00015625 #####
2025-06-11 20:05:33.451 INFO: ##### Step: 126 Learning rate: 0.00015625 #####
Epoch 87, Train Loss: 0.4412, Val Loss: 0.4698
2025-06-11 20:05:33.453 INFO: Epoch 87, Train Loss: 0.4412, Val Loss: 0.4698
train_e/atom_mae: 0.000361
2025-06-11 20:05:33.456 INFO: train_e/atom_mae: 0.000361
train_e/atom_rmse: 0.000444
2025-06-11 20:05:33.456 INFO: train_e/atom_rmse: 0.000444
train_f_mae: 0.016145
2025-06-11 20:05:33.463 INFO: train_f_mae: 0.016145
train_f_rmse: 0.020831
2025-06-11 20:05:33.464 INFO: train_f_rmse: 0.020831
val_e/atom_mae: 0.000811
2025-06-11 20:05:33.468 INFO: val_e/atom_mae: 0.000811
val_e/atom_rmse: 0.000834
2025-06-11 20:05:33.470 INFO: val_e/atom_rmse: 0.000834
val_f_mae: 0.016336
2025-06-11 20:05:33.471 INFO: val_f_mae: 0.016336
val_f_rmse: 0.021075
2025-06-11 20:05:33.472 INFO: val_f_rmse: 0.021075
##### Step: 127 Learning rate: 0.00015625 #####
2025-06-11 20:07:47.779 INFO: ##### Step: 127 Learning rate: 0.00015625 #####
Epoch 88, Train Loss: 0.4397, Val Loss: 0.4550
2025-06-11 20:07:47.782 INFO: Epoch 88, Train Loss: 0.4397, Val Loss: 0.4550
train_e/atom_mae: 0.000333
2025-06-11 20:07:47.785 INFO: train_e/atom_mae: 0.000333
train_e/atom_rmse: 0.000411
2025-06-11 20:07:47.786 INFO: train_e/atom_rmse: 0.000411
train_f_mae: 0.016142
2025-06-11 20:07:47.792 INFO: train_f_mae: 0.016142
train_f_rmse: 0.020820
2025-06-11 20:07:47.793 INFO: train_f_rmse: 0.020820
val_e/atom_mae: 0.000236
2025-06-11 20:07:47.797 INFO: val_e/atom_mae: 0.000236
val_e/atom_rmse: 0.000273
2025-06-11 20:07:47.799 INFO: val_e/atom_rmse: 0.000273
val_f_mae: 0.016471
2025-06-11 20:07:47.800 INFO: val_f_mae: 0.016471
val_f_rmse: 0.021266
2025-06-11 20:07:47.801 INFO: val_f_rmse: 0.021266
##### Step: 128 Learning rate: 0.00015625 #####
2025-06-11 20:10:02.252 INFO: ##### Step: 128 Learning rate: 0.00015625 #####
Epoch 89, Train Loss: 0.4421, Val Loss: 0.4541
2025-06-11 20:10:02.253 INFO: Epoch 89, Train Loss: 0.4421, Val Loss: 0.4541
train_e/atom_mae: 0.000410
2025-06-11 20:10:02.255 INFO: train_e/atom_mae: 0.000410
train_e/atom_rmse: 0.000513
2025-06-11 20:10:02.256 INFO: train_e/atom_rmse: 0.000513
train_f_mae: 0.016121
2025-06-11 20:10:02.262 INFO: train_f_mae: 0.016121
train_f_rmse: 0.020795
2025-06-11 20:10:02.263 INFO: train_f_rmse: 0.020795
val_e/atom_mae: 0.000494
2025-06-11 20:10:02.268 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000544
2025-06-11 20:10:02.269 INFO: val_e/atom_rmse: 0.000544
val_f_mae: 0.016294
2025-06-11 20:10:02.270 INFO: val_f_mae: 0.016294
val_f_rmse: 0.021053
2025-06-11 20:10:02.271 INFO: val_f_rmse: 0.021053
##### Step: 129 Learning rate: 0.00015625 #####
2025-06-11 20:12:16.710 INFO: ##### Step: 129 Learning rate: 0.00015625 #####
Epoch 90, Train Loss: 0.4409, Val Loss: 0.4457
2025-06-11 20:12:16.712 INFO: Epoch 90, Train Loss: 0.4409, Val Loss: 0.4457
train_e/atom_mae: 0.000330
2025-06-11 20:12:16.714 INFO: train_e/atom_mae: 0.000330
train_e/atom_rmse: 0.000415
2025-06-11 20:12:16.715 INFO: train_e/atom_rmse: 0.000415
train_f_mae: 0.016164
2025-06-11 20:12:16.721 INFO: train_f_mae: 0.016164
train_f_rmse: 0.020846
2025-06-11 20:12:16.722 INFO: train_f_rmse: 0.020846
val_e/atom_mae: 0.000312
2025-06-11 20:12:16.727 INFO: val_e/atom_mae: 0.000312
val_e/atom_rmse: 0.000363
2025-06-11 20:12:16.728 INFO: val_e/atom_rmse: 0.000363
val_f_mae: 0.016292
2025-06-11 20:12:16.729 INFO: val_f_mae: 0.016292
val_f_rmse: 0.020997
2025-06-11 20:12:16.730 INFO: val_f_rmse: 0.020997
##### Step: 130 Learning rate: 0.00015625 #####
2025-06-11 20:14:31.603 INFO: ##### Step: 130 Learning rate: 0.00015625 #####
Epoch 91, Train Loss: 0.4417, Val Loss: 0.4592
2025-06-11 20:14:31.604 INFO: Epoch 91, Train Loss: 0.4417, Val Loss: 0.4592
train_e/atom_mae: 0.000361
2025-06-11 20:14:31.606 INFO: train_e/atom_mae: 0.000361
train_e/atom_rmse: 0.000458
2025-06-11 20:14:31.607 INFO: train_e/atom_rmse: 0.000458
train_f_mae: 0.016149
2025-06-11 20:14:31.613 INFO: train_f_mae: 0.016149
train_f_rmse: 0.020832
2025-06-11 20:14:31.614 INFO: train_f_rmse: 0.020832
val_e/atom_mae: 0.000439
2025-06-11 20:14:31.619 INFO: val_e/atom_mae: 0.000439
val_e/atom_rmse: 0.000492
2025-06-11 20:14:31.620 INFO: val_e/atom_rmse: 0.000492
val_f_mae: 0.016420
2025-06-11 20:14:31.622 INFO: val_f_mae: 0.016420
val_f_rmse: 0.021219
2025-06-11 20:14:31.622 INFO: val_f_rmse: 0.021219
##### Step: 131 Learning rate: 0.00015625 #####
2025-06-11 20:16:46.210 INFO: ##### Step: 131 Learning rate: 0.00015625 #####
Epoch 92, Train Loss: 0.4556, Val Loss: 0.4457
2025-06-11 20:16:46.211 INFO: Epoch 92, Train Loss: 0.4556, Val Loss: 0.4457
train_e/atom_mae: 0.000596
2025-06-11 20:16:46.214 INFO: train_e/atom_mae: 0.000596
train_e/atom_rmse: 0.000713
2025-06-11 20:16:46.214 INFO: train_e/atom_rmse: 0.000713
train_f_mae: 0.016211
2025-06-11 20:16:46.220 INFO: train_f_mae: 0.016211
train_f_rmse: 0.020901
2025-06-11 20:16:46.221 INFO: train_f_rmse: 0.020901
val_e/atom_mae: 0.000307
2025-06-11 20:16:46.226 INFO: val_e/atom_mae: 0.000307
val_e/atom_rmse: 0.000365
2025-06-11 20:16:46.227 INFO: val_e/atom_rmse: 0.000365
val_f_mae: 0.016249
2025-06-11 20:16:46.229 INFO: val_f_mae: 0.016249
val_f_rmse: 0.020994
2025-06-11 20:16:46.230 INFO: val_f_rmse: 0.020994
##### Step: 132 Learning rate: 0.00015625 #####
2025-06-11 20:19:00.765 INFO: ##### Step: 132 Learning rate: 0.00015625 #####
Epoch 93, Train Loss: 0.4433, Val Loss: 0.4463
2025-06-11 20:19:00.767 INFO: Epoch 93, Train Loss: 0.4433, Val Loss: 0.4463
train_e/atom_mae: 0.000471
2025-06-11 20:19:00.769 INFO: train_e/atom_mae: 0.000471
train_e/atom_rmse: 0.000563
2025-06-11 20:19:00.770 INFO: train_e/atom_rmse: 0.000563
train_f_mae: 0.016108
2025-06-11 20:19:00.776 INFO: train_f_mae: 0.016108
train_f_rmse: 0.020776
2025-06-11 20:19:00.777 INFO: train_f_rmse: 0.020776
val_e/atom_mae: 0.000252
2025-06-11 20:19:00.782 INFO: val_e/atom_mae: 0.000252
val_e/atom_rmse: 0.000308
2025-06-11 20:19:00.783 INFO: val_e/atom_rmse: 0.000308
val_f_mae: 0.016278
2025-06-11 20:19:00.784 INFO: val_f_mae: 0.016278
val_f_rmse: 0.021044
2025-06-11 20:19:00.785 INFO: val_f_rmse: 0.021044
##### Step: 133 Learning rate: 0.00015625 #####
2025-06-11 20:21:15.293 INFO: ##### Step: 133 Learning rate: 0.00015625 #####
Epoch 94, Train Loss: 0.4422, Val Loss: 0.4543
2025-06-11 20:21:15.295 INFO: Epoch 94, Train Loss: 0.4422, Val Loss: 0.4543
train_e/atom_mae: 0.000414
2025-06-11 20:21:15.297 INFO: train_e/atom_mae: 0.000414
train_e/atom_rmse: 0.000512
2025-06-11 20:21:15.298 INFO: train_e/atom_rmse: 0.000512
train_f_mae: 0.016120
2025-06-11 20:21:15.304 INFO: train_f_mae: 0.016120
train_f_rmse: 0.020798
2025-06-11 20:21:15.305 INFO: train_f_rmse: 0.020798
val_e/atom_mae: 0.000389
2025-06-11 20:21:15.310 INFO: val_e/atom_mae: 0.000389
val_e/atom_rmse: 0.000455
2025-06-11 20:21:15.311 INFO: val_e/atom_rmse: 0.000455
val_f_mae: 0.016353
2025-06-11 20:21:15.312 INFO: val_f_mae: 0.016353
val_f_rmse: 0.021135
2025-06-11 20:21:15.313 INFO: val_f_rmse: 0.021135
##### Step: 134 Learning rate: 0.00015625 #####
2025-06-11 20:23:29.849 INFO: ##### Step: 134 Learning rate: 0.00015625 #####
Epoch 95, Train Loss: 0.4415, Val Loss: 0.4521
2025-06-11 20:23:29.851 INFO: Epoch 95, Train Loss: 0.4415, Val Loss: 0.4521
train_e/atom_mae: 0.000265
2025-06-11 20:23:29.854 INFO: train_e/atom_mae: 0.000265
train_e/atom_rmse: 0.000327
2025-06-11 20:23:29.855 INFO: train_e/atom_rmse: 0.000327
train_f_mae: 0.016221
2025-06-11 20:23:29.861 INFO: train_f_mae: 0.016221
train_f_rmse: 0.020917
2025-06-11 20:23:29.862 INFO: train_f_rmse: 0.020917
val_e/atom_mae: 0.000169
2025-06-11 20:23:29.866 INFO: val_e/atom_mae: 0.000169
val_e/atom_rmse: 0.000219
2025-06-11 20:23:29.868 INFO: val_e/atom_rmse: 0.000219
val_f_mae: 0.016413
2025-06-11 20:23:29.869 INFO: val_f_mae: 0.016413
val_f_rmse: 0.021221
2025-06-11 20:23:29.870 INFO: val_f_rmse: 0.021221
##### Step: 135 Learning rate: 0.00015625 #####
2025-06-11 20:25:44.320 INFO: ##### Step: 135 Learning rate: 0.00015625 #####
Epoch 96, Train Loss: 0.4371, Val Loss: 0.4496
2025-06-11 20:25:44.322 INFO: Epoch 96, Train Loss: 0.4371, Val Loss: 0.4496
train_e/atom_mae: 0.000302
2025-06-11 20:25:44.324 INFO: train_e/atom_mae: 0.000302
train_e/atom_rmse: 0.000376
2025-06-11 20:25:44.325 INFO: train_e/atom_rmse: 0.000376
train_f_mae: 0.016109
2025-06-11 20:25:44.331 INFO: train_f_mae: 0.016109
train_f_rmse: 0.020781
2025-06-11 20:25:44.332 INFO: train_f_rmse: 0.020781
val_e/atom_mae: 0.000448
2025-06-11 20:25:44.337 INFO: val_e/atom_mae: 0.000448
val_e/atom_rmse: 0.000502
2025-06-11 20:25:44.338 INFO: val_e/atom_rmse: 0.000502
val_f_mae: 0.016255
2025-06-11 20:25:44.339 INFO: val_f_mae: 0.016255
val_f_rmse: 0.020984
2025-06-11 20:25:44.340 INFO: val_f_rmse: 0.020984
##### Step: 136 Learning rate: 0.00015625 #####
2025-06-11 20:27:58.759 INFO: ##### Step: 136 Learning rate: 0.00015625 #####
Epoch 97, Train Loss: 0.4417, Val Loss: 0.4484
2025-06-11 20:27:58.761 INFO: Epoch 97, Train Loss: 0.4417, Val Loss: 0.4484
train_e/atom_mae: 0.000384
2025-06-11 20:27:58.763 INFO: train_e/atom_mae: 0.000384
train_e/atom_rmse: 0.000481
2025-06-11 20:27:58.764 INFO: train_e/atom_rmse: 0.000481
train_f_mae: 0.016137
2025-06-11 20:27:58.770 INFO: train_f_mae: 0.016137
train_f_rmse: 0.020812
2025-06-11 20:27:58.771 INFO: train_f_rmse: 0.020812
val_e/atom_mae: 0.000273
2025-06-11 20:27:58.776 INFO: val_e/atom_mae: 0.000273
val_e/atom_rmse: 0.000329
2025-06-11 20:27:58.777 INFO: val_e/atom_rmse: 0.000329
val_f_mae: 0.016316
2025-06-11 20:27:58.779 INFO: val_f_mae: 0.016316
val_f_rmse: 0.021080
2025-06-11 20:27:58.779 INFO: val_f_rmse: 0.021080
##### Step: 137 Learning rate: 0.00015625 #####
2025-06-11 20:30:13.379 INFO: ##### Step: 137 Learning rate: 0.00015625 #####
Epoch 98, Train Loss: 0.4377, Val Loss: 0.4545
2025-06-11 20:30:13.381 INFO: Epoch 98, Train Loss: 0.4377, Val Loss: 0.4545
train_e/atom_mae: 0.000323
2025-06-11 20:30:13.383 INFO: train_e/atom_mae: 0.000323
train_e/atom_rmse: 0.000405
2025-06-11 20:30:13.384 INFO: train_e/atom_rmse: 0.000405
train_f_mae: 0.016111
2025-06-11 20:30:13.390 INFO: train_f_mae: 0.016111
train_f_rmse: 0.020775
2025-06-11 20:30:13.391 INFO: train_f_rmse: 0.020775
val_e/atom_mae: 0.000560
2025-06-11 20:30:13.396 INFO: val_e/atom_mae: 0.000560
val_e/atom_rmse: 0.000595
2025-06-11 20:30:13.397 INFO: val_e/atom_rmse: 0.000595
val_f_mae: 0.016269
2025-06-11 20:30:13.398 INFO: val_f_mae: 0.016269
val_f_rmse: 0.021009
2025-06-11 20:30:13.399 INFO: val_f_rmse: 0.021009
##### Step: 138 Learning rate: 0.00015625 #####
2025-06-11 20:32:27.982 INFO: ##### Step: 138 Learning rate: 0.00015625 #####
Epoch 99, Train Loss: 0.4410, Val Loss: 0.4477
2025-06-11 20:32:27.985 INFO: Epoch 99, Train Loss: 0.4410, Val Loss: 0.4477
train_e/atom_mae: 0.000387
2025-06-11 20:32:27.987 INFO: train_e/atom_mae: 0.000387
train_e/atom_rmse: 0.000481
2025-06-11 20:32:27.988 INFO: train_e/atom_rmse: 0.000481
train_f_mae: 0.016121
2025-06-11 20:32:27.994 INFO: train_f_mae: 0.016121
train_f_rmse: 0.020796
2025-06-11 20:32:27.995 INFO: train_f_rmse: 0.020796
val_e/atom_mae: 0.000281
2025-06-11 20:32:28.000 INFO: val_e/atom_mae: 0.000281
val_e/atom_rmse: 0.000338
2025-06-11 20:32:28.001 INFO: val_e/atom_rmse: 0.000338
val_f_mae: 0.016323
2025-06-11 20:32:28.002 INFO: val_f_mae: 0.016323
val_f_rmse: 0.021060
2025-06-11 20:32:28.003 INFO: val_f_rmse: 0.021060
##### Step: 139 Learning rate: 0.00015625 #####
2025-06-11 20:34:42.429 INFO: ##### Step: 139 Learning rate: 0.00015625 #####
Epoch 100, Train Loss: 0.4453, Val Loss: 0.4571
2025-06-11 20:34:42.432 INFO: Epoch 100, Train Loss: 0.4453, Val Loss: 0.4571
train_e/atom_mae: 0.000438
2025-06-11 20:34:42.434 INFO: train_e/atom_mae: 0.000438
train_e/atom_rmse: 0.000531
2025-06-11 20:34:42.435 INFO: train_e/atom_rmse: 0.000531
train_f_mae: 0.016165
2025-06-11 20:34:42.441 INFO: train_f_mae: 0.016165
train_f_rmse: 0.020854
2025-06-11 20:34:42.442 INFO: train_f_rmse: 0.020854
val_e/atom_mae: 0.000591
2025-06-11 20:34:42.447 INFO: val_e/atom_mae: 0.000591
val_e/atom_rmse: 0.000631
2025-06-11 20:34:42.448 INFO: val_e/atom_rmse: 0.000631
val_f_mae: 0.016269
2025-06-11 20:34:42.449 INFO: val_f_mae: 0.016269
val_f_rmse: 0.021034
2025-06-11 20:34:42.450 INFO: val_f_rmse: 0.021034
2025-06-11 20:34:42.699 INFO: Third train loop:
##### Step: 140 Learning rate: 7.8125e-05 #####
2025-06-11 20:36:57.271 INFO: ##### Step: 140 Learning rate: 7.8125e-05 #####
Epoch 1, Train Loss: 0.4731, Val Loss: 0.5073
2025-06-11 20:36:57.272 INFO: Epoch 1, Train Loss: 0.4731, Val Loss: 0.5073
train_e/atom_mae: 0.000275
2025-06-11 20:36:57.274 INFO: train_e/atom_mae: 0.000275
train_e/atom_rmse: 0.000341
2025-06-11 20:36:57.275 INFO: train_e/atom_rmse: 0.000341
train_f_mae: 0.016081
2025-06-11 20:36:57.281 INFO: train_f_mae: 0.016081
train_f_rmse: 0.020745
2025-06-11 20:36:57.282 INFO: train_f_rmse: 0.020745
val_e/atom_mae: 0.000398
2025-06-11 20:36:57.287 INFO: val_e/atom_mae: 0.000398
val_e/atom_rmse: 0.000445
2025-06-11 20:36:57.288 INFO: val_e/atom_rmse: 0.000445
val_f_mae: 0.016133
2025-06-11 20:36:57.289 INFO: val_f_mae: 0.016133
val_f_rmse: 0.020838
2025-06-11 20:36:57.290 INFO: val_f_rmse: 0.020838
##### Step: 141 Learning rate: 7.8125e-05 #####
2025-06-11 20:39:11.858 INFO: ##### Step: 141 Learning rate: 7.8125e-05 #####
Epoch 2, Train Loss: 0.5263, Val Loss: 0.4897
2025-06-11 20:39:11.860 INFO: Epoch 2, Train Loss: 0.5263, Val Loss: 0.4897
train_e/atom_mae: 0.000393
2025-06-11 20:39:11.863 INFO: train_e/atom_mae: 0.000393
train_e/atom_rmse: 0.000508
2025-06-11 20:39:11.864 INFO: train_e/atom_rmse: 0.000508
train_f_mae: 0.016098
2025-06-11 20:39:11.870 INFO: train_f_mae: 0.016098
train_f_rmse: 0.020766
2025-06-11 20:39:11.871 INFO: train_f_rmse: 0.020766
val_e/atom_mae: 0.000319
2025-06-11 20:39:11.876 INFO: val_e/atom_mae: 0.000319
val_e/atom_rmse: 0.000368
2025-06-11 20:39:11.877 INFO: val_e/atom_rmse: 0.000368
val_f_mae: 0.016246
2025-06-11 20:39:11.878 INFO: val_f_mae: 0.016246
val_f_rmse: 0.020969
2025-06-11 20:39:11.879 INFO: val_f_rmse: 0.020969
##### Step: 142 Learning rate: 7.8125e-05 #####
2025-06-11 20:41:27.190 INFO: ##### Step: 142 Learning rate: 7.8125e-05 #####
Epoch 3, Train Loss: 0.4761, Val Loss: 0.4652
2025-06-11 20:41:27.191 INFO: Epoch 3, Train Loss: 0.4761, Val Loss: 0.4652
train_e/atom_mae: 0.000281
2025-06-11 20:41:27.194 INFO: train_e/atom_mae: 0.000281
train_e/atom_rmse: 0.000353
2025-06-11 20:41:27.195 INFO: train_e/atom_rmse: 0.000353
train_f_mae: 0.016079
2025-06-11 20:41:27.201 INFO: train_f_mae: 0.016079
train_f_rmse: 0.020740
2025-06-11 20:41:27.202 INFO: train_f_rmse: 0.020740
val_e/atom_mae: 0.000226
2025-06-11 20:41:27.206 INFO: val_e/atom_mae: 0.000226
val_e/atom_rmse: 0.000274
2025-06-11 20:41:27.208 INFO: val_e/atom_rmse: 0.000274
val_f_mae: 0.016165
2025-06-11 20:41:27.209 INFO: val_f_mae: 0.016165
val_f_rmse: 0.020915
2025-06-11 20:41:27.210 INFO: val_f_rmse: 0.020915
##### Step: 143 Learning rate: 7.8125e-05 #####
2025-06-11 20:43:41.858 INFO: ##### Step: 143 Learning rate: 7.8125e-05 #####
Epoch 4, Train Loss: 0.5077, Val Loss: 0.5224
2025-06-11 20:43:41.860 INFO: Epoch 4, Train Loss: 0.5077, Val Loss: 0.5224
train_e/atom_mae: 0.000374
2025-06-11 20:43:41.862 INFO: train_e/atom_mae: 0.000374
train_e/atom_rmse: 0.000456
2025-06-11 20:43:41.863 INFO: train_e/atom_rmse: 0.000456
train_f_mae: 0.016094
2025-06-11 20:43:41.869 INFO: train_f_mae: 0.016094
train_f_rmse: 0.020760
2025-06-11 20:43:41.870 INFO: train_f_rmse: 0.020760
val_e/atom_mae: 0.000413
2025-06-11 20:43:41.875 INFO: val_e/atom_mae: 0.000413
val_e/atom_rmse: 0.000471
2025-06-11 20:43:41.876 INFO: val_e/atom_rmse: 0.000471
val_f_mae: 0.016241
2025-06-11 20:43:41.877 INFO: val_f_mae: 0.016241
val_f_rmse: 0.020987
2025-06-11 20:43:41.878 INFO: val_f_rmse: 0.020987
##### Step: 144 Learning rate: 7.8125e-05 #####
2025-06-11 20:45:56.382 INFO: ##### Step: 144 Learning rate: 7.8125e-05 #####
Epoch 5, Train Loss: 0.4795, Val Loss: 0.4557
2025-06-11 20:45:56.384 INFO: Epoch 5, Train Loss: 0.4795, Val Loss: 0.4557
train_e/atom_mae: 0.000288
2025-06-11 20:45:56.386 INFO: train_e/atom_mae: 0.000288
train_e/atom_rmse: 0.000360
2025-06-11 20:45:56.387 INFO: train_e/atom_rmse: 0.000360
train_f_mae: 0.016109
2025-06-11 20:45:56.393 INFO: train_f_mae: 0.016109
train_f_rmse: 0.020778
2025-06-11 20:45:56.394 INFO: train_f_rmse: 0.020778
val_e/atom_mae: 0.000152
2025-06-11 20:45:56.399 INFO: val_e/atom_mae: 0.000152
val_e/atom_rmse: 0.000197
2025-06-11 20:45:56.400 INFO: val_e/atom_rmse: 0.000197
val_f_mae: 0.016271
2025-06-11 20:45:56.401 INFO: val_f_mae: 0.016271
val_f_rmse: 0.021010
2025-06-11 20:45:56.402 INFO: val_f_rmse: 0.021010
##### Step: 145 Learning rate: 7.8125e-05 #####
2025-06-11 20:48:11.154 INFO: ##### Step: 145 Learning rate: 7.8125e-05 #####
Epoch 6, Train Loss: 0.5547, Val Loss: 0.4977
2025-06-11 20:48:11.156 INFO: Epoch 6, Train Loss: 0.5547, Val Loss: 0.4977
train_e/atom_mae: 0.000481
2025-06-11 20:48:11.158 INFO: train_e/atom_mae: 0.000481
train_e/atom_rmse: 0.000580
2025-06-11 20:48:11.159 INFO: train_e/atom_rmse: 0.000580
train_f_mae: 0.016091
2025-06-11 20:48:11.166 INFO: train_f_mae: 0.016091
train_f_rmse: 0.020757
2025-06-11 20:48:11.166 INFO: train_f_rmse: 0.020757
val_e/atom_mae: 0.000336
2025-06-11 20:48:11.171 INFO: val_e/atom_mae: 0.000336
val_e/atom_rmse: 0.000386
2025-06-11 20:48:11.172 INFO: val_e/atom_rmse: 0.000386
val_f_mae: 0.016286
2025-06-11 20:48:11.174 INFO: val_f_mae: 0.016286
val_f_rmse: 0.021041
2025-06-11 20:48:11.174 INFO: val_f_rmse: 0.021041
##### Step: 146 Learning rate: 7.8125e-05 #####
2025-06-11 20:50:25.661 INFO: ##### Step: 146 Learning rate: 7.8125e-05 #####
Epoch 7, Train Loss: 0.5055, Val Loss: 0.6257
2025-06-11 20:50:25.664 INFO: Epoch 7, Train Loss: 0.5055, Val Loss: 0.6257
train_e/atom_mae: 0.000364
2025-06-11 20:50:25.666 INFO: train_e/atom_mae: 0.000364
train_e/atom_rmse: 0.000448
2025-06-11 20:50:25.667 INFO: train_e/atom_rmse: 0.000448
train_f_mae: 0.016115
2025-06-11 20:50:25.673 INFO: train_f_mae: 0.016115
train_f_rmse: 0.020775
2025-06-11 20:50:25.674 INFO: train_f_rmse: 0.020775
val_e/atom_mae: 0.000682
2025-06-11 20:50:25.679 INFO: val_e/atom_mae: 0.000682
val_e/atom_rmse: 0.000714
2025-06-11 20:50:25.680 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.016196
2025-06-11 20:50:25.681 INFO: val_f_mae: 0.016196
val_f_rmse: 0.020927
2025-06-11 20:50:25.682 INFO: val_f_rmse: 0.020927
##### Step: 147 Learning rate: 7.8125e-05 #####
2025-06-11 20:52:40.419 INFO: ##### Step: 147 Learning rate: 7.8125e-05 #####
Epoch 8, Train Loss: 0.5338, Val Loss: 0.6846
2025-06-11 20:52:40.431 INFO: Epoch 8, Train Loss: 0.5338, Val Loss: 0.6846
train_e/atom_mae: 0.000419
2025-06-11 20:52:40.433 INFO: train_e/atom_mae: 0.000419
train_e/atom_rmse: 0.000516
2025-06-11 20:52:40.434 INFO: train_e/atom_rmse: 0.000516
train_f_mae: 0.016180
2025-06-11 20:52:40.440 INFO: train_f_mae: 0.016180
train_f_rmse: 0.020874
2025-06-11 20:52:40.441 INFO: train_f_rmse: 0.020874
val_e/atom_mae: 0.000759
2025-06-11 20:52:40.446 INFO: val_e/atom_mae: 0.000759
val_e/atom_rmse: 0.000796
2025-06-11 20:52:40.447 INFO: val_e/atom_rmse: 0.000796
val_f_mae: 0.016471
2025-06-11 20:52:40.449 INFO: val_f_mae: 0.016471
val_f_rmse: 0.021242
2025-06-11 20:52:40.449 INFO: val_f_rmse: 0.021242
##### Step: 148 Learning rate: 7.8125e-05 #####
2025-06-11 20:54:55.229 INFO: ##### Step: 148 Learning rate: 7.8125e-05 #####
Epoch 9, Train Loss: 0.6211, Val Loss: 0.5080
2025-06-11 20:54:55.231 INFO: Epoch 9, Train Loss: 0.6211, Val Loss: 0.5080
train_e/atom_mae: 0.000612
2025-06-11 20:54:55.234 INFO: train_e/atom_mae: 0.000612
train_e/atom_rmse: 0.000713
2025-06-11 20:54:55.234 INFO: train_e/atom_rmse: 0.000713
train_f_mae: 0.016149
2025-06-11 20:54:55.241 INFO: train_f_mae: 0.016149
train_f_rmse: 0.020827
2025-06-11 20:54:55.242 INFO: train_f_rmse: 0.020827
val_e/atom_mae: 0.000367
2025-06-11 20:54:55.246 INFO: val_e/atom_mae: 0.000367
val_e/atom_rmse: 0.000419
2025-06-11 20:54:55.247 INFO: val_e/atom_rmse: 0.000419
val_f_mae: 0.016286
2025-06-11 20:54:55.249 INFO: val_f_mae: 0.016286
val_f_rmse: 0.021057
2025-06-11 20:54:55.249 INFO: val_f_rmse: 0.021057
##### Step: 149 Learning rate: 7.8125e-05 #####
2025-06-11 20:57:10.055 INFO: ##### Step: 149 Learning rate: 7.8125e-05 #####
Epoch 10, Train Loss: 0.4732, Val Loss: 0.4612
2025-06-11 20:57:10.057 INFO: Epoch 10, Train Loss: 0.4732, Val Loss: 0.4612
train_e/atom_mae: 0.000269
2025-06-11 20:57:10.060 INFO: train_e/atom_mae: 0.000269
train_e/atom_rmse: 0.000342
2025-06-11 20:57:10.061 INFO: train_e/atom_rmse: 0.000342
train_f_mae: 0.016074
2025-06-11 20:57:10.067 INFO: train_f_mae: 0.016074
train_f_rmse: 0.020737
2025-06-11 20:57:10.068 INFO: train_f_rmse: 0.020737
val_e/atom_mae: 0.000178
2025-06-11 20:57:10.072 INFO: val_e/atom_mae: 0.000178
val_e/atom_rmse: 0.000215
2025-06-11 20:57:10.074 INFO: val_e/atom_rmse: 0.000215
val_f_mae: 0.016294
2025-06-11 20:57:10.075 INFO: val_f_mae: 0.016294
val_f_rmse: 0.021075
2025-06-11 20:57:10.076 INFO: val_f_rmse: 0.021075
##### Step: 150 Learning rate: 7.8125e-05 #####
2025-06-11 20:59:25.070 INFO: ##### Step: 150 Learning rate: 7.8125e-05 #####
Epoch 11, Train Loss: 0.4835, Val Loss: 0.4514
2025-06-11 20:59:25.072 INFO: Epoch 11, Train Loss: 0.4835, Val Loss: 0.4514
train_e/atom_mae: 0.000304
2025-06-11 20:59:25.074 INFO: train_e/atom_mae: 0.000304
train_e/atom_rmse: 0.000375
2025-06-11 20:59:25.075 INFO: train_e/atom_rmse: 0.000375
train_f_mae: 0.016103
2025-06-11 20:59:25.081 INFO: train_f_mae: 0.016103
train_f_rmse: 0.020777
2025-06-11 20:59:25.082 INFO: train_f_rmse: 0.020777
val_e/atom_mae: 0.000154
2025-06-11 20:59:25.087 INFO: val_e/atom_mae: 0.000154
val_e/atom_rmse: 0.000196
2025-06-11 20:59:25.088 INFO: val_e/atom_rmse: 0.000196
val_f_mae: 0.016179
2025-06-11 20:59:25.089 INFO: val_f_mae: 0.016179
val_f_rmse: 0.020913
2025-06-11 20:59:25.090 INFO: val_f_rmse: 0.020913
##### Step: 151 Learning rate: 7.8125e-05 #####
2025-06-11 21:01:39.943 INFO: ##### Step: 151 Learning rate: 7.8125e-05 #####
Epoch 12, Train Loss: 0.4808, Val Loss: 0.5543
2025-06-11 21:01:39.946 INFO: Epoch 12, Train Loss: 0.4808, Val Loss: 0.5543
train_e/atom_mae: 0.000296
2025-06-11 21:01:39.948 INFO: train_e/atom_mae: 0.000296
train_e/atom_rmse: 0.000369
2025-06-11 21:01:39.949 INFO: train_e/atom_rmse: 0.000369
train_f_mae: 0.016090
2025-06-11 21:01:39.955 INFO: train_f_mae: 0.016090
train_f_rmse: 0.020752
2025-06-11 21:01:39.956 INFO: train_f_rmse: 0.020752
val_e/atom_mae: 0.000505
2025-06-11 21:01:39.961 INFO: val_e/atom_mae: 0.000505
val_e/atom_rmse: 0.000544
2025-06-11 21:01:39.962 INFO: val_e/atom_rmse: 0.000544
val_f_mae: 0.016341
2025-06-11 21:01:39.963 INFO: val_f_mae: 0.016341
val_f_rmse: 0.021102
2025-06-11 21:01:39.964 INFO: val_f_rmse: 0.021102
##### Step: 152 Learning rate: 7.8125e-05 #####
2025-06-11 21:03:54.716 INFO: ##### Step: 152 Learning rate: 7.8125e-05 #####
Epoch 13, Train Loss: 0.5074, Val Loss: 0.4668
2025-06-11 21:03:54.718 INFO: Epoch 13, Train Loss: 0.5074, Val Loss: 0.4668
train_e/atom_mae: 0.000367
2025-06-11 21:03:54.721 INFO: train_e/atom_mae: 0.000367
train_e/atom_rmse: 0.000456
2025-06-11 21:03:54.722 INFO: train_e/atom_rmse: 0.000456
train_f_mae: 0.016086
2025-06-11 21:03:54.728 INFO: train_f_mae: 0.016086
train_f_rmse: 0.020749
2025-06-11 21:03:54.729 INFO: train_f_rmse: 0.020749
val_e/atom_mae: 0.000226
2025-06-11 21:03:54.733 INFO: val_e/atom_mae: 0.000226
val_e/atom_rmse: 0.000264
2025-06-11 21:03:54.735 INFO: val_e/atom_rmse: 0.000264
val_f_mae: 0.016268
2025-06-11 21:03:54.736 INFO: val_f_mae: 0.016268
val_f_rmse: 0.021002
2025-06-11 21:03:54.737 INFO: val_f_rmse: 0.021002
##### Step: 153 Learning rate: 7.8125e-05 #####
2025-06-11 21:06:09.407 INFO: ##### Step: 153 Learning rate: 7.8125e-05 #####
Epoch 14, Train Loss: 0.5164, Val Loss: 0.4513
2025-06-11 21:06:09.409 INFO: Epoch 14, Train Loss: 0.5164, Val Loss: 0.4513
train_e/atom_mae: 0.000394
2025-06-11 21:06:09.411 INFO: train_e/atom_mae: 0.000394
train_e/atom_rmse: 0.000483
2025-06-11 21:06:09.412 INFO: train_e/atom_rmse: 0.000483
train_f_mae: 0.016075
2025-06-11 21:06:09.418 INFO: train_f_mae: 0.016075
train_f_rmse: 0.020743
2025-06-11 21:06:09.419 INFO: train_f_rmse: 0.020743
val_e/atom_mae: 0.000167
2025-06-11 21:06:09.424 INFO: val_e/atom_mae: 0.000167
val_e/atom_rmse: 0.000205
2025-06-11 21:06:09.425 INFO: val_e/atom_rmse: 0.000205
val_f_mae: 0.016163
2025-06-11 21:06:09.427 INFO: val_f_mae: 0.016163
val_f_rmse: 0.020876
2025-06-11 21:06:09.427 INFO: val_f_rmse: 0.020876
##### Step: 154 Learning rate: 7.8125e-05 #####
2025-06-11 21:08:27.609 INFO: ##### Step: 154 Learning rate: 7.8125e-05 #####
Epoch 15, Train Loss: 0.4892, Val Loss: 0.4677
2025-06-11 21:08:27.610 INFO: Epoch 15, Train Loss: 0.4892, Val Loss: 0.4677
train_e/atom_mae: 0.000321
2025-06-11 21:08:27.613 INFO: train_e/atom_mae: 0.000321
train_e/atom_rmse: 0.000399
2025-06-11 21:08:27.614 INFO: train_e/atom_rmse: 0.000399
train_f_mae: 0.016088
2025-06-11 21:08:27.620 INFO: train_f_mae: 0.016088
train_f_rmse: 0.020752
2025-06-11 21:08:27.621 INFO: train_f_rmse: 0.020752
val_e/atom_mae: 0.000207
2025-06-11 21:08:27.625 INFO: val_e/atom_mae: 0.000207
val_e/atom_rmse: 0.000259
2025-06-11 21:08:27.627 INFO: val_e/atom_rmse: 0.000259
val_f_mae: 0.016285
2025-06-11 21:08:27.628 INFO: val_f_mae: 0.016285
val_f_rmse: 0.021049
2025-06-11 21:08:27.629 INFO: val_f_rmse: 0.021049
##### Step: 155 Learning rate: 7.8125e-05 #####
2025-06-11 21:10:42.271 INFO: ##### Step: 155 Learning rate: 7.8125e-05 #####
Epoch 16, Train Loss: 0.4862, Val Loss: 0.4880
2025-06-11 21:10:42.273 INFO: Epoch 16, Train Loss: 0.4862, Val Loss: 0.4880
train_e/atom_mae: 0.000289
2025-06-11 21:10:42.275 INFO: train_e/atom_mae: 0.000289
train_e/atom_rmse: 0.000377
2025-06-11 21:10:42.276 INFO: train_e/atom_rmse: 0.000377
train_f_mae: 0.016153
2025-06-11 21:10:42.282 INFO: train_f_mae: 0.016153
train_f_rmse: 0.020832
2025-06-11 21:10:42.283 INFO: train_f_rmse: 0.020832
val_e/atom_mae: 0.000300
2025-06-11 21:10:42.288 INFO: val_e/atom_mae: 0.000300
val_e/atom_rmse: 0.000357
2025-06-11 21:10:42.289 INFO: val_e/atom_rmse: 0.000357
val_f_mae: 0.016234
2025-06-11 21:10:42.291 INFO: val_f_mae: 0.016234
val_f_rmse: 0.021001
2025-06-11 21:10:42.291 INFO: val_f_rmse: 0.021001
##### Step: 156 Learning rate: 7.8125e-05 #####
2025-06-11 21:12:56.977 INFO: ##### Step: 156 Learning rate: 7.8125e-05 #####
Epoch 17, Train Loss: 0.4861, Val Loss: 0.4618
2025-06-11 21:12:56.980 INFO: Epoch 17, Train Loss: 0.4861, Val Loss: 0.4618
train_e/atom_mae: 0.000284
2025-06-11 21:12:56.982 INFO: train_e/atom_mae: 0.000284
train_e/atom_rmse: 0.000360
2025-06-11 21:12:56.983 INFO: train_e/atom_rmse: 0.000360
train_f_mae: 0.016234
2025-06-11 21:12:56.989 INFO: train_f_mae: 0.016234
train_f_rmse: 0.020936
2025-06-11 21:12:56.990 INFO: train_f_rmse: 0.020936
val_e/atom_mae: 0.000192
2025-06-11 21:12:56.995 INFO: val_e/atom_mae: 0.000192
val_e/atom_rmse: 0.000242
2025-06-11 21:12:56.996 INFO: val_e/atom_rmse: 0.000242
val_f_mae: 0.016252
2025-06-11 21:12:56.997 INFO: val_f_mae: 0.016252
val_f_rmse: 0.020982
2025-06-11 21:12:56.998 INFO: val_f_rmse: 0.020982
##### Step: 157 Learning rate: 7.8125e-05 #####
2025-06-11 21:15:11.709 INFO: ##### Step: 157 Learning rate: 7.8125e-05 #####
Epoch 18, Train Loss: 0.5032, Val Loss: 0.4589
2025-06-11 21:15:11.712 INFO: Epoch 18, Train Loss: 0.5032, Val Loss: 0.4589
train_e/atom_mae: 0.000354
2025-06-11 21:15:11.714 INFO: train_e/atom_mae: 0.000354
train_e/atom_rmse: 0.000440
2025-06-11 21:15:11.715 INFO: train_e/atom_rmse: 0.000440
train_f_mae: 0.016112
2025-06-11 21:15:11.721 INFO: train_f_mae: 0.016112
train_f_rmse: 0.020784
2025-06-11 21:15:11.722 INFO: train_f_rmse: 0.020784
val_e/atom_mae: 0.000180
2025-06-11 21:15:11.727 INFO: val_e/atom_mae: 0.000180
val_e/atom_rmse: 0.000220
2025-06-11 21:15:11.728 INFO: val_e/atom_rmse: 0.000220
val_f_mae: 0.016269
2025-06-11 21:15:11.729 INFO: val_f_mae: 0.016269
val_f_rmse: 0.021001
2025-06-11 21:15:11.730 INFO: val_f_rmse: 0.021001
##### Step: 158 Learning rate: 7.8125e-05 #####
2025-06-11 21:17:26.475 INFO: ##### Step: 158 Learning rate: 7.8125e-05 #####
Epoch 19, Train Loss: 0.5935, Val Loss: 0.5023
2025-06-11 21:17:26.477 INFO: Epoch 19, Train Loss: 0.5935, Val Loss: 0.5023
train_e/atom_mae: 0.000563
2025-06-11 21:17:26.479 INFO: train_e/atom_mae: 0.000563
train_e/atom_rmse: 0.000657
2025-06-11 21:17:26.480 INFO: train_e/atom_rmse: 0.000657
train_f_mae: 0.016153
2025-06-11 21:17:26.487 INFO: train_f_mae: 0.016153
train_f_rmse: 0.020838
2025-06-11 21:17:26.488 INFO: train_f_rmse: 0.020838
val_e/atom_mae: 0.000331
2025-06-11 21:17:26.492 INFO: val_e/atom_mae: 0.000331
val_e/atom_rmse: 0.000395
2025-06-11 21:17:26.493 INFO: val_e/atom_rmse: 0.000395
val_f_mae: 0.016302
2025-06-11 21:17:26.495 INFO: val_f_mae: 0.016302
val_f_rmse: 0.021090
2025-06-11 21:17:26.496 INFO: val_f_rmse: 0.021090
##### Step: 159 Learning rate: 7.8125e-05 #####
2025-06-11 21:19:41.199 INFO: ##### Step: 159 Learning rate: 7.8125e-05 #####
Epoch 20, Train Loss: 0.4724, Val Loss: 0.4585
2025-06-11 21:19:41.200 INFO: Epoch 20, Train Loss: 0.4724, Val Loss: 0.4585
train_e/atom_mae: 0.000262
2025-06-11 21:19:41.203 INFO: train_e/atom_mae: 0.000262
train_e/atom_rmse: 0.000329
2025-06-11 21:19:41.204 INFO: train_e/atom_rmse: 0.000329
train_f_mae: 0.016123
2025-06-11 21:19:41.210 INFO: train_f_mae: 0.016123
train_f_rmse: 0.020799
2025-06-11 21:19:41.211 INFO: train_f_rmse: 0.020799
val_e/atom_mae: 0.000169
2025-06-11 21:19:41.215 INFO: val_e/atom_mae: 0.000169
val_e/atom_rmse: 0.000211
2025-06-11 21:19:41.217 INFO: val_e/atom_rmse: 0.000211
val_f_mae: 0.016277
2025-06-11 21:19:41.218 INFO: val_f_mae: 0.016277
val_f_rmse: 0.021024
2025-06-11 21:19:41.219 INFO: val_f_rmse: 0.021024
##### Step: 160 Learning rate: 3.90625e-05 #####
2025-06-11 21:21:56.001 INFO: ##### Step: 160 Learning rate: 3.90625e-05 #####
Epoch 21, Train Loss: 0.4783, Val Loss: 0.5853
2025-06-11 21:21:56.002 INFO: Epoch 21, Train Loss: 0.4783, Val Loss: 0.5853
train_e/atom_mae: 0.000285
2025-06-11 21:21:56.005 INFO: train_e/atom_mae: 0.000285
train_e/atom_rmse: 0.000357
2025-06-11 21:21:56.006 INFO: train_e/atom_rmse: 0.000357
train_f_mae: 0.016101
2025-06-11 21:21:56.012 INFO: train_f_mae: 0.016101
train_f_rmse: 0.020770
2025-06-11 21:21:56.013 INFO: train_f_rmse: 0.020770
val_e/atom_mae: 0.000573
2025-06-11 21:21:56.017 INFO: val_e/atom_mae: 0.000573
val_e/atom_rmse: 0.000619
2025-06-11 21:21:56.019 INFO: val_e/atom_rmse: 0.000619
val_f_mae: 0.016298
2025-06-11 21:21:56.020 INFO: val_f_mae: 0.016298
val_f_rmse: 0.021067
2025-06-11 21:21:56.021 INFO: val_f_rmse: 0.021067
##### Step: 161 Learning rate: 3.90625e-05 #####
2025-06-11 21:24:10.740 INFO: ##### Step: 161 Learning rate: 3.90625e-05 #####
Epoch 22, Train Loss: 0.5450, Val Loss: 0.5295
2025-06-11 21:24:10.742 INFO: Epoch 22, Train Loss: 0.5450, Val Loss: 0.5295
train_e/atom_mae: 0.000479
2025-06-11 21:24:10.744 INFO: train_e/atom_mae: 0.000479
train_e/atom_rmse: 0.000559
2025-06-11 21:24:10.745 INFO: train_e/atom_rmse: 0.000559
train_f_mae: 0.016072
2025-06-11 21:24:10.751 INFO: train_f_mae: 0.016072
train_f_rmse: 0.020727
2025-06-11 21:24:10.752 INFO: train_f_rmse: 0.020727
val_e/atom_mae: 0.000436
2025-06-11 21:24:10.757 INFO: val_e/atom_mae: 0.000436
val_e/atom_rmse: 0.000486
2025-06-11 21:24:10.758 INFO: val_e/atom_rmse: 0.000486
val_f_mae: 0.016277
2025-06-11 21:24:10.760 INFO: val_f_mae: 0.016277
val_f_rmse: 0.021032
2025-06-11 21:24:10.760 INFO: val_f_rmse: 0.021032
##### Step: 162 Learning rate: 3.90625e-05 #####
2025-06-11 21:26:25.500 INFO: ##### Step: 162 Learning rate: 3.90625e-05 #####
Epoch 23, Train Loss: 0.5058, Val Loss: 0.4667
2025-06-11 21:26:25.501 INFO: Epoch 23, Train Loss: 0.5058, Val Loss: 0.4667
train_e/atom_mae: 0.000384
2025-06-11 21:26:25.503 INFO: train_e/atom_mae: 0.000384
train_e/atom_rmse: 0.000456
2025-06-11 21:26:25.504 INFO: train_e/atom_rmse: 0.000456
train_f_mae: 0.016060
2025-06-11 21:26:25.510 INFO: train_f_mae: 0.016060
train_f_rmse: 0.020714
2025-06-11 21:26:25.511 INFO: train_f_rmse: 0.020714
val_e/atom_mae: 0.000228
2025-06-11 21:26:25.516 INFO: val_e/atom_mae: 0.000228
val_e/atom_rmse: 0.000266
2025-06-11 21:26:25.517 INFO: val_e/atom_rmse: 0.000266
val_f_mae: 0.016249
2025-06-11 21:26:25.518 INFO: val_f_mae: 0.016249
val_f_rmse: 0.020993
2025-06-11 21:26:25.519 INFO: val_f_rmse: 0.020993
##### Step: 163 Learning rate: 3.90625e-05 #####
2025-06-11 21:28:40.212 INFO: ##### Step: 163 Learning rate: 3.90625e-05 #####
Epoch 24, Train Loss: 0.4708, Val Loss: 0.4716
2025-06-11 21:28:40.214 INFO: Epoch 24, Train Loss: 0.4708, Val Loss: 0.4716
train_e/atom_mae: 0.000272
2025-06-11 21:28:40.216 INFO: train_e/atom_mae: 0.000272
train_e/atom_rmse: 0.000336
2025-06-11 21:28:40.217 INFO: train_e/atom_rmse: 0.000336
train_f_mae: 0.016061
2025-06-11 21:28:40.223 INFO: train_f_mae: 0.016061
train_f_rmse: 0.020719
2025-06-11 21:28:40.224 INFO: train_f_rmse: 0.020719
val_e/atom_mae: 0.000221
2025-06-11 21:28:40.229 INFO: val_e/atom_mae: 0.000221
val_e/atom_rmse: 0.000278
2025-06-11 21:28:40.230 INFO: val_e/atom_rmse: 0.000278
val_f_mae: 0.016290
2025-06-11 21:28:40.231 INFO: val_f_mae: 0.016290
val_f_rmse: 0.021053
2025-06-11 21:28:40.232 INFO: val_f_rmse: 0.021053
##### Step: 164 Learning rate: 3.90625e-05 #####
2025-06-11 21:30:54.998 INFO: ##### Step: 164 Learning rate: 3.90625e-05 #####
Epoch 25, Train Loss: 0.4784, Val Loss: 0.5268
2025-06-11 21:30:55.000 INFO: Epoch 25, Train Loss: 0.4784, Val Loss: 0.5268
train_e/atom_mae: 0.000299
2025-06-11 21:30:55.003 INFO: train_e/atom_mae: 0.000299
train_e/atom_rmse: 0.000365
2025-06-11 21:30:55.004 INFO: train_e/atom_rmse: 0.000365
train_f_mae: 0.016066
2025-06-11 21:30:55.010 INFO: train_f_mae: 0.016066
train_f_rmse: 0.020721
2025-06-11 21:30:55.011 INFO: train_f_rmse: 0.020721
val_e/atom_mae: 0.000436
2025-06-11 21:30:55.015 INFO: val_e/atom_mae: 0.000436
val_e/atom_rmse: 0.000490
2025-06-11 21:30:55.017 INFO: val_e/atom_rmse: 0.000490
val_f_mae: 0.016193
2025-06-11 21:30:55.018 INFO: val_f_mae: 0.016193
val_f_rmse: 0.020933
2025-06-11 21:30:55.019 INFO: val_f_rmse: 0.020933
##### Step: 165 Learning rate: 3.90625e-05 #####
2025-06-11 21:33:09.781 INFO: ##### Step: 165 Learning rate: 3.90625e-05 #####
Epoch 26, Train Loss: 0.4610, Val Loss: 0.4590
2025-06-11 21:33:09.783 INFO: Epoch 26, Train Loss: 0.4610, Val Loss: 0.4590
train_e/atom_mae: 0.000236
2025-06-11 21:33:09.786 INFO: train_e/atom_mae: 0.000236
train_e/atom_rmse: 0.000294
2025-06-11 21:33:09.787 INFO: train_e/atom_rmse: 0.000294
train_f_mae: 0.016059
2025-06-11 21:33:09.793 INFO: train_f_mae: 0.016059
train_f_rmse: 0.020718
2025-06-11 21:33:09.794 INFO: train_f_rmse: 0.020718
val_e/atom_mae: 0.000195
2025-06-11 21:33:09.798 INFO: val_e/atom_mae: 0.000195
val_e/atom_rmse: 0.000231
2025-06-11 21:33:09.800 INFO: val_e/atom_rmse: 0.000231
val_f_mae: 0.016229
2025-06-11 21:33:09.801 INFO: val_f_mae: 0.016229
val_f_rmse: 0.020961
2025-06-11 21:33:09.802 INFO: val_f_rmse: 0.020961
##### Step: 166 Learning rate: 3.90625e-05 #####
2025-06-11 21:35:24.584 INFO: ##### Step: 166 Learning rate: 3.90625e-05 #####
Epoch 27, Train Loss: 0.4845, Val Loss: 0.4781
2025-06-11 21:35:24.587 INFO: Epoch 27, Train Loss: 0.4845, Val Loss: 0.4781
train_e/atom_mae: 0.000315
2025-06-11 21:35:24.589 INFO: train_e/atom_mae: 0.000315
train_e/atom_rmse: 0.000385
2025-06-11 21:35:24.590 INFO: train_e/atom_rmse: 0.000385
train_f_mae: 0.016076
2025-06-11 21:35:24.596 INFO: train_f_mae: 0.016076
train_f_rmse: 0.020735
2025-06-11 21:35:24.597 INFO: train_f_rmse: 0.020735
val_e/atom_mae: 0.000273
2025-06-11 21:35:24.602 INFO: val_e/atom_mae: 0.000273
val_e/atom_rmse: 0.000315
2025-06-11 21:35:24.603 INFO: val_e/atom_rmse: 0.000315
val_f_mae: 0.016260
2025-06-11 21:35:24.604 INFO: val_f_mae: 0.016260
val_f_rmse: 0.021014
2025-06-11 21:35:24.605 INFO: val_f_rmse: 0.021014
##### Step: 167 Learning rate: 3.90625e-05 #####
2025-06-11 21:37:39.330 INFO: ##### Step: 167 Learning rate: 3.90625e-05 #####
Epoch 28, Train Loss: 0.4720, Val Loss: 0.4840
2025-06-11 21:37:39.332 INFO: Epoch 28, Train Loss: 0.4720, Val Loss: 0.4840
train_e/atom_mae: 0.000275
2025-06-11 21:37:39.334 INFO: train_e/atom_mae: 0.000275
train_e/atom_rmse: 0.000339
2025-06-11 21:37:39.335 INFO: train_e/atom_rmse: 0.000339
train_f_mae: 0.016069
2025-06-11 21:37:39.341 INFO: train_f_mae: 0.016069
train_f_rmse: 0.020726
2025-06-11 21:37:39.342 INFO: train_f_rmse: 0.020726
val_e/atom_mae: 0.000293
2025-06-11 21:37:39.347 INFO: val_e/atom_mae: 0.000293
val_e/atom_rmse: 0.000341
2025-06-11 21:37:39.348 INFO: val_e/atom_rmse: 0.000341
val_f_mae: 0.016265
2025-06-11 21:37:39.349 INFO: val_f_mae: 0.016265
val_f_rmse: 0.021004
2025-06-11 21:37:39.350 INFO: val_f_rmse: 0.021004
##### Step: 168 Learning rate: 3.90625e-05 #####
2025-06-11 21:39:53.947 INFO: ##### Step: 168 Learning rate: 3.90625e-05 #####
Epoch 29, Train Loss: 0.4727, Val Loss: 0.5699
2025-06-11 21:39:53.949 INFO: Epoch 29, Train Loss: 0.4727, Val Loss: 0.5699
train_e/atom_mae: 0.000282
2025-06-11 21:39:53.952 INFO: train_e/atom_mae: 0.000282
train_e/atom_rmse: 0.000344
2025-06-11 21:39:53.953 INFO: train_e/atom_rmse: 0.000344
train_f_mae: 0.016060
2025-06-11 21:39:53.959 INFO: train_f_mae: 0.016060
train_f_rmse: 0.020717
2025-06-11 21:39:53.960 INFO: train_f_rmse: 0.020717
val_e/atom_mae: 0.000560
2025-06-11 21:39:53.964 INFO: val_e/atom_mae: 0.000560
val_e/atom_rmse: 0.000596
2025-06-11 21:39:53.966 INFO: val_e/atom_rmse: 0.000596
val_f_mae: 0.016225
2025-06-11 21:39:53.967 INFO: val_f_mae: 0.016225
val_f_rmse: 0.020950
2025-06-11 21:39:53.968 INFO: val_f_rmse: 0.020950
##### Step: 169 Learning rate: 3.90625e-05 #####
2025-06-11 21:42:08.418 INFO: ##### Step: 169 Learning rate: 3.90625e-05 #####
Epoch 30, Train Loss: 0.4667, Val Loss: 0.4787
2025-06-11 21:42:08.419 INFO: Epoch 30, Train Loss: 0.4667, Val Loss: 0.4787
train_e/atom_mae: 0.000250
2025-06-11 21:42:08.421 INFO: train_e/atom_mae: 0.000250
train_e/atom_rmse: 0.000316
2025-06-11 21:42:08.422 INFO: train_e/atom_rmse: 0.000316
train_f_mae: 0.016073
2025-06-11 21:42:08.428 INFO: train_f_mae: 0.016073
train_f_rmse: 0.020731
2025-06-11 21:42:08.429 INFO: train_f_rmse: 0.020731
val_e/atom_mae: 0.000288
2025-06-11 21:42:08.434 INFO: val_e/atom_mae: 0.000288
val_e/atom_rmse: 0.000338
2025-06-11 21:42:08.435 INFO: val_e/atom_rmse: 0.000338
val_f_mae: 0.016166
2025-06-11 21:42:08.436 INFO: val_f_mae: 0.016166
val_f_rmse: 0.020892
2025-06-11 21:42:08.437 INFO: val_f_rmse: 0.020892
##### Step: 170 Learning rate: 3.90625e-05 #####
2025-06-11 21:44:23.025 INFO: ##### Step: 170 Learning rate: 3.90625e-05 #####
Epoch 31, Train Loss: 0.4643, Val Loss: 0.4595
2025-06-11 21:44:23.026 INFO: Epoch 31, Train Loss: 0.4643, Val Loss: 0.4595
train_e/atom_mae: 0.000245
2025-06-11 21:44:23.029 INFO: train_e/atom_mae: 0.000245
train_e/atom_rmse: 0.000304
2025-06-11 21:44:23.029 INFO: train_e/atom_rmse: 0.000304
train_f_mae: 0.016084
2025-06-11 21:44:23.036 INFO: train_f_mae: 0.016084
train_f_rmse: 0.020744
2025-06-11 21:44:23.036 INFO: train_f_rmse: 0.020744
val_e/atom_mae: 0.000186
2025-06-11 21:44:23.041 INFO: val_e/atom_mae: 0.000186
val_e/atom_rmse: 0.000220
2025-06-11 21:44:23.042 INFO: val_e/atom_rmse: 0.000220
val_f_mae: 0.016283
2025-06-11 21:44:23.044 INFO: val_f_mae: 0.016283
val_f_rmse: 0.021015
2025-06-11 21:44:23.044 INFO: val_f_rmse: 0.021015
##### Step: 171 Learning rate: 3.90625e-05 #####
2025-06-11 21:46:37.528 INFO: ##### Step: 171 Learning rate: 3.90625e-05 #####
Epoch 32, Train Loss: 0.4697, Val Loss: 0.4793
2025-06-11 21:46:37.531 INFO: Epoch 32, Train Loss: 0.4697, Val Loss: 0.4793
train_e/atom_mae: 0.000263
2025-06-11 21:46:37.533 INFO: train_e/atom_mae: 0.000263
train_e/atom_rmse: 0.000328
2025-06-11 21:46:37.534 INFO: train_e/atom_rmse: 0.000328
train_f_mae: 0.016076
2025-06-11 21:46:37.540 INFO: train_f_mae: 0.016076
train_f_rmse: 0.020735
2025-06-11 21:46:37.541 INFO: train_f_rmse: 0.020735
val_e/atom_mae: 0.000284
2025-06-11 21:46:37.546 INFO: val_e/atom_mae: 0.000284
val_e/atom_rmse: 0.000337
2025-06-11 21:46:37.547 INFO: val_e/atom_rmse: 0.000337
val_f_mae: 0.016179
2025-06-11 21:46:37.548 INFO: val_f_mae: 0.016179
val_f_rmse: 0.020916
2025-06-11 21:46:37.549 INFO: val_f_rmse: 0.020916
##### Step: 172 Learning rate: 3.90625e-05 #####
2025-06-11 21:48:52.014 INFO: ##### Step: 172 Learning rate: 3.90625e-05 #####
Epoch 33, Train Loss: 0.4664, Val Loss: 0.5235
2025-06-11 21:48:52.016 INFO: Epoch 33, Train Loss: 0.4664, Val Loss: 0.5235
train_e/atom_mae: 0.000258
2025-06-11 21:48:52.018 INFO: train_e/atom_mae: 0.000258
train_e/atom_rmse: 0.000318
2025-06-11 21:48:52.019 INFO: train_e/atom_rmse: 0.000318
train_f_mae: 0.016058
2025-06-11 21:48:52.025 INFO: train_f_mae: 0.016058
train_f_rmse: 0.020716
2025-06-11 21:48:52.026 INFO: train_f_rmse: 0.020716
val_e/atom_mae: 0.000430
2025-06-11 21:48:52.031 INFO: val_e/atom_mae: 0.000430
val_e/atom_rmse: 0.000475
2025-06-11 21:48:52.032 INFO: val_e/atom_rmse: 0.000475
val_f_mae: 0.016237
2025-06-11 21:48:52.034 INFO: val_f_mae: 0.016237
val_f_rmse: 0.020983
2025-06-11 21:48:52.034 INFO: val_f_rmse: 0.020983
##### Step: 173 Learning rate: 3.90625e-05 #####
2025-06-11 21:51:06.470 INFO: ##### Step: 173 Learning rate: 3.90625e-05 #####
Epoch 34, Train Loss: 0.4620, Val Loss: 0.4897
2025-06-11 21:51:06.471 INFO: Epoch 34, Train Loss: 0.4620, Val Loss: 0.4897
train_e/atom_mae: 0.000239
2025-06-11 21:51:06.474 INFO: train_e/atom_mae: 0.000239
train_e/atom_rmse: 0.000299
2025-06-11 21:51:06.474 INFO: train_e/atom_rmse: 0.000299
train_f_mae: 0.016062
2025-06-11 21:51:06.481 INFO: train_f_mae: 0.016062
train_f_rmse: 0.020716
2025-06-11 21:51:06.481 INFO: train_f_rmse: 0.020716
val_e/atom_mae: 0.000324
2025-06-11 21:51:06.486 INFO: val_e/atom_mae: 0.000324
val_e/atom_rmse: 0.000378
2025-06-11 21:51:06.487 INFO: val_e/atom_rmse: 0.000378
val_f_mae: 0.016183
2025-06-11 21:51:06.489 INFO: val_f_mae: 0.016183
val_f_rmse: 0.020902
2025-06-11 21:51:06.489 INFO: val_f_rmse: 0.020902
##### Step: 174 Learning rate: 3.90625e-05 #####
2025-06-11 21:53:20.946 INFO: ##### Step: 174 Learning rate: 3.90625e-05 #####
Epoch 35, Train Loss: 0.4720, Val Loss: 0.4777
2025-06-11 21:53:20.947 INFO: Epoch 35, Train Loss: 0.4720, Val Loss: 0.4777
train_e/atom_mae: 0.000273
2025-06-11 21:53:20.949 INFO: train_e/atom_mae: 0.000273
train_e/atom_rmse: 0.000340
2025-06-11 21:53:20.950 INFO: train_e/atom_rmse: 0.000340
train_f_mae: 0.016063
2025-06-11 21:53:20.956 INFO: train_f_mae: 0.016063
train_f_rmse: 0.020721
2025-06-11 21:53:20.957 INFO: train_f_rmse: 0.020721
val_e/atom_mae: 0.000270
2025-06-11 21:53:20.962 INFO: val_e/atom_mae: 0.000270
val_e/atom_rmse: 0.000323
2025-06-11 21:53:20.963 INFO: val_e/atom_rmse: 0.000323
val_f_mae: 0.016212
2025-06-11 21:53:20.964 INFO: val_f_mae: 0.016212
val_f_rmse: 0.020959
2025-06-11 21:53:20.965 INFO: val_f_rmse: 0.020959
##### Step: 175 Learning rate: 3.90625e-05 #####
2025-06-11 21:55:35.426 INFO: ##### Step: 175 Learning rate: 3.90625e-05 #####
Epoch 36, Train Loss: 0.4768, Val Loss: 0.5322
2025-06-11 21:55:35.427 INFO: Epoch 36, Train Loss: 0.4768, Val Loss: 0.5322
train_e/atom_mae: 0.000290
2025-06-11 21:55:35.429 INFO: train_e/atom_mae: 0.000290
train_e/atom_rmse: 0.000358
2025-06-11 21:55:35.430 INFO: train_e/atom_rmse: 0.000358
train_f_mae: 0.016068
2025-06-11 21:55:35.436 INFO: train_f_mae: 0.016068
train_f_rmse: 0.020725
2025-06-11 21:55:35.437 INFO: train_f_rmse: 0.020725
val_e/atom_mae: 0.000457
2025-06-11 21:55:35.442 INFO: val_e/atom_mae: 0.000457
val_e/atom_rmse: 0.000509
2025-06-11 21:55:35.443 INFO: val_e/atom_rmse: 0.000509
val_f_mae: 0.016167
2025-06-11 21:55:35.445 INFO: val_f_mae: 0.016167
val_f_rmse: 0.020898
2025-06-11 21:55:35.446 INFO: val_f_rmse: 0.020898
##### Step: 176 Learning rate: 3.90625e-05 #####
2025-06-11 21:57:49.910 INFO: ##### Step: 176 Learning rate: 3.90625e-05 #####
Epoch 37, Train Loss: 0.4934, Val Loss: 0.6007
2025-06-11 21:57:49.911 INFO: Epoch 37, Train Loss: 0.4934, Val Loss: 0.6007
train_e/atom_mae: 0.000344
2025-06-11 21:57:49.913 INFO: train_e/atom_mae: 0.000344
train_e/atom_rmse: 0.000415
2025-06-11 21:57:49.914 INFO: train_e/atom_rmse: 0.000415
train_f_mae: 0.016077
2025-06-11 21:57:49.920 INFO: train_f_mae: 0.016077
train_f_rmse: 0.020736
2025-06-11 21:57:49.921 INFO: train_f_rmse: 0.020736
val_e/atom_mae: 0.000627
2025-06-11 21:57:49.926 INFO: val_e/atom_mae: 0.000627
val_e/atom_rmse: 0.000657
2025-06-11 21:57:49.927 INFO: val_e/atom_rmse: 0.000657
val_f_mae: 0.016257
2025-06-11 21:57:49.928 INFO: val_f_mae: 0.016257
val_f_rmse: 0.021009
2025-06-11 21:57:49.929 INFO: val_f_rmse: 0.021009
##### Step: 177 Learning rate: 3.90625e-05 #####
2025-06-11 22:00:04.435 INFO: ##### Step: 177 Learning rate: 3.90625e-05 #####
Epoch 38, Train Loss: 0.4724, Val Loss: 0.4594
2025-06-11 22:00:04.438 INFO: Epoch 38, Train Loss: 0.4724, Val Loss: 0.4594
train_e/atom_mae: 0.000277
2025-06-11 22:00:04.440 INFO: train_e/atom_mae: 0.000277
train_e/atom_rmse: 0.000343
2025-06-11 22:00:04.441 INFO: train_e/atom_rmse: 0.000343
train_f_mae: 0.016061
2025-06-11 22:00:04.447 INFO: train_f_mae: 0.016061
train_f_rmse: 0.020716
2025-06-11 22:00:04.448 INFO: train_f_rmse: 0.020716
val_e/atom_mae: 0.000177
2025-06-11 22:00:04.453 INFO: val_e/atom_mae: 0.000177
val_e/atom_rmse: 0.000222
2025-06-11 22:00:04.454 INFO: val_e/atom_rmse: 0.000222
val_f_mae: 0.016266
2025-06-11 22:00:04.455 INFO: val_f_mae: 0.016266
val_f_rmse: 0.021006
2025-06-11 22:00:04.456 INFO: val_f_rmse: 0.021006
##### Step: 178 Learning rate: 3.90625e-05 #####
2025-06-11 22:02:18.917 INFO: ##### Step: 178 Learning rate: 3.90625e-05 #####
Epoch 39, Train Loss: 0.4657, Val Loss: 0.4744
2025-06-11 22:02:18.920 INFO: Epoch 39, Train Loss: 0.4657, Val Loss: 0.4744
train_e/atom_mae: 0.000256
2025-06-11 22:02:18.922 INFO: train_e/atom_mae: 0.000256
train_e/atom_rmse: 0.000314
2025-06-11 22:02:18.923 INFO: train_e/atom_rmse: 0.000314
train_f_mae: 0.016063
2025-06-11 22:02:18.929 INFO: train_f_mae: 0.016063
train_f_rmse: 0.020720
2025-06-11 22:02:18.930 INFO: train_f_rmse: 0.020720
val_e/atom_mae: 0.000276
2025-06-11 22:02:18.935 INFO: val_e/atom_mae: 0.000276
val_e/atom_rmse: 0.000325
2025-06-11 22:02:18.936 INFO: val_e/atom_rmse: 0.000325
val_f_mae: 0.016151
2025-06-11 22:02:18.937 INFO: val_f_mae: 0.016151
val_f_rmse: 0.020865
2025-06-11 22:02:18.938 INFO: val_f_rmse: 0.020865
##### Step: 179 Learning rate: 3.90625e-05 #####
2025-06-11 22:04:33.511 INFO: ##### Step: 179 Learning rate: 3.90625e-05 #####
Epoch 40, Train Loss: 0.4698, Val Loss: 0.4547
2025-06-11 22:04:33.513 INFO: Epoch 40, Train Loss: 0.4698, Val Loss: 0.4547
train_e/atom_mae: 0.000269
2025-06-11 22:04:33.515 INFO: train_e/atom_mae: 0.000269
train_e/atom_rmse: 0.000334
2025-06-11 22:04:33.516 INFO: train_e/atom_rmse: 0.000334
train_f_mae: 0.016054
2025-06-11 22:04:33.522 INFO: train_f_mae: 0.016054
train_f_rmse: 0.020707
2025-06-11 22:04:33.523 INFO: train_f_rmse: 0.020707
val_e/atom_mae: 0.000179
2025-06-11 22:04:33.528 INFO: val_e/atom_mae: 0.000179
val_e/atom_rmse: 0.000224
2025-06-11 22:04:33.529 INFO: val_e/atom_rmse: 0.000224
val_f_mae: 0.016175
2025-06-11 22:04:33.530 INFO: val_f_mae: 0.016175
val_f_rmse: 0.020887
2025-06-11 22:04:33.531 INFO: val_f_rmse: 0.020887
##### Step: 180 Learning rate: 1.953125e-05 #####
2025-06-11 22:06:48.248 INFO: ##### Step: 180 Learning rate: 1.953125e-05 #####
Epoch 41, Train Loss: 0.4623, Val Loss: 0.4541
2025-06-11 22:06:48.250 INFO: Epoch 41, Train Loss: 0.4623, Val Loss: 0.4541
train_e/atom_mae: 0.000245
2025-06-11 22:06:48.253 INFO: train_e/atom_mae: 0.000245
train_e/atom_rmse: 0.000300
2025-06-11 22:06:48.253 INFO: train_e/atom_rmse: 0.000300
train_f_mae: 0.016056
2025-06-11 22:06:48.260 INFO: train_f_mae: 0.016056
train_f_rmse: 0.020713
2025-06-11 22:06:48.260 INFO: train_f_rmse: 0.020713
val_e/atom_mae: 0.000180
2025-06-11 22:06:48.265 INFO: val_e/atom_mae: 0.000180
val_e/atom_rmse: 0.000214
2025-06-11 22:06:48.266 INFO: val_e/atom_rmse: 0.000214
val_f_mae: 0.016192
2025-06-11 22:06:48.268 INFO: val_f_mae: 0.016192
val_f_rmse: 0.020910
2025-06-11 22:06:48.268 INFO: val_f_rmse: 0.020910
##### Step: 181 Learning rate: 1.953125e-05 #####
2025-06-11 22:09:03.057 INFO: ##### Step: 181 Learning rate: 1.953125e-05 #####
Epoch 42, Train Loss: 0.4718, Val Loss: 0.4623
2025-06-11 22:09:03.058 INFO: Epoch 42, Train Loss: 0.4718, Val Loss: 0.4623
train_e/atom_mae: 0.000275
2025-06-11 22:09:03.061 INFO: train_e/atom_mae: 0.000275
train_e/atom_rmse: 0.000342
2025-06-11 22:09:03.062 INFO: train_e/atom_rmse: 0.000342
train_f_mae: 0.016052
2025-06-11 22:09:03.068 INFO: train_f_mae: 0.016052
train_f_rmse: 0.020705
2025-06-11 22:09:03.069 INFO: train_f_rmse: 0.020705
val_e/atom_mae: 0.000199
2025-06-11 22:09:03.073 INFO: val_e/atom_mae: 0.000199
val_e/atom_rmse: 0.000252
2025-06-11 22:09:03.075 INFO: val_e/atom_rmse: 0.000252
val_f_mae: 0.016232
2025-06-11 22:09:03.076 INFO: val_f_mae: 0.016232
val_f_rmse: 0.020950
2025-06-11 22:09:03.077 INFO: val_f_rmse: 0.020950
##### Step: 182 Learning rate: 1.953125e-05 #####
2025-06-11 22:11:17.783 INFO: ##### Step: 182 Learning rate: 1.953125e-05 #####
Epoch 43, Train Loss: 0.4618, Val Loss: 0.4582
2025-06-11 22:11:17.786 INFO: Epoch 43, Train Loss: 0.4618, Val Loss: 0.4582
train_e/atom_mae: 0.000245
2025-06-11 22:11:17.788 INFO: train_e/atom_mae: 0.000245
train_e/atom_rmse: 0.000301
2025-06-11 22:11:17.789 INFO: train_e/atom_rmse: 0.000301
train_f_mae: 0.016045
2025-06-11 22:11:17.795 INFO: train_f_mae: 0.016045
train_f_rmse: 0.020697
2025-06-11 22:11:17.796 INFO: train_f_rmse: 0.020697
val_e/atom_mae: 0.000183
2025-06-11 22:11:17.801 INFO: val_e/atom_mae: 0.000183
val_e/atom_rmse: 0.000217
2025-06-11 22:11:17.802 INFO: val_e/atom_rmse: 0.000217
val_f_mae: 0.016259
2025-06-11 22:11:17.803 INFO: val_f_mae: 0.016259
val_f_rmse: 0.020993
2025-06-11 22:11:17.804 INFO: val_f_rmse: 0.020993
##### Step: 183 Learning rate: 1.953125e-05 #####
2025-06-11 22:13:32.509 INFO: ##### Step: 183 Learning rate: 1.953125e-05 #####
Epoch 44, Train Loss: 0.4559, Val Loss: 0.4567
2025-06-11 22:13:32.510 INFO: Epoch 44, Train Loss: 0.4559, Val Loss: 0.4567
train_e/atom_mae: 0.000215
2025-06-11 22:13:32.513 INFO: train_e/atom_mae: 0.000215
train_e/atom_rmse: 0.000270
2025-06-11 22:13:32.514 INFO: train_e/atom_rmse: 0.000270
train_f_mae: 0.016060
2025-06-11 22:13:32.520 INFO: train_f_mae: 0.016060
train_f_rmse: 0.020713
2025-06-11 22:13:32.521 INFO: train_f_rmse: 0.020713
val_e/atom_mae: 0.000155
2025-06-11 22:13:32.525 INFO: val_e/atom_mae: 0.000155
val_e/atom_rmse: 0.000194
2025-06-11 22:13:32.527 INFO: val_e/atom_rmse: 0.000194
val_f_mae: 0.016286
2025-06-11 22:13:32.528 INFO: val_f_mae: 0.016286
val_f_rmse: 0.021043
2025-06-11 22:13:32.529 INFO: val_f_rmse: 0.021043
##### Step: 184 Learning rate: 1.953125e-05 #####
2025-06-11 22:15:47.296 INFO: ##### Step: 184 Learning rate: 1.953125e-05 #####
Epoch 45, Train Loss: 0.4560, Val Loss: 0.4604
2025-06-11 22:15:47.299 INFO: Epoch 45, Train Loss: 0.4560, Val Loss: 0.4604
train_e/atom_mae: 0.000220
2025-06-11 22:15:47.301 INFO: train_e/atom_mae: 0.000220
train_e/atom_rmse: 0.000273
2025-06-11 22:15:47.302 INFO: train_e/atom_rmse: 0.000273
train_f_mae: 0.016049
2025-06-11 22:15:47.308 INFO: train_f_mae: 0.016049
train_f_rmse: 0.020704
2025-06-11 22:15:47.309 INFO: train_f_rmse: 0.020704
val_e/atom_mae: 0.000201
2025-06-11 22:15:47.314 INFO: val_e/atom_mae: 0.000201
val_e/atom_rmse: 0.000253
2025-06-11 22:15:47.315 INFO: val_e/atom_rmse: 0.000253
val_f_mae: 0.016173
2025-06-11 22:15:47.316 INFO: val_f_mae: 0.016173
val_f_rmse: 0.020898
2025-06-11 22:15:47.317 INFO: val_f_rmse: 0.020898
##### Step: 185 Learning rate: 1.953125e-05 #####
2025-06-11 22:18:02.078 INFO: ##### Step: 185 Learning rate: 1.953125e-05 #####
Epoch 46, Train Loss: 0.4814, Val Loss: 0.4540
2025-06-11 22:18:02.080 INFO: Epoch 46, Train Loss: 0.4814, Val Loss: 0.4540
train_e/atom_mae: 0.000302
2025-06-11 22:18:02.082 INFO: train_e/atom_mae: 0.000302
train_e/atom_rmse: 0.000378
2025-06-11 22:18:02.083 INFO: train_e/atom_rmse: 0.000378
train_f_mae: 0.016055
2025-06-11 22:18:02.089 INFO: train_f_mae: 0.016055
train_f_rmse: 0.020707
2025-06-11 22:18:02.090 INFO: train_f_rmse: 0.020707
val_e/atom_mae: 0.000174
2025-06-11 22:18:02.095 INFO: val_e/atom_mae: 0.000174
val_e/atom_rmse: 0.000216
2025-06-11 22:18:02.096 INFO: val_e/atom_rmse: 0.000216
val_f_mae: 0.016172
2025-06-11 22:18:02.097 INFO: val_f_mae: 0.016172
val_f_rmse: 0.020902
2025-06-11 22:18:02.098 INFO: val_f_rmse: 0.020902
##### Step: 186 Learning rate: 1.953125e-05 #####
2025-06-11 22:20:16.813 INFO: ##### Step: 186 Learning rate: 1.953125e-05 #####
Epoch 47, Train Loss: 0.4767, Val Loss: 0.4529
2025-06-11 22:20:16.814 INFO: Epoch 47, Train Loss: 0.4767, Val Loss: 0.4529
train_e/atom_mae: 0.000295
2025-06-11 22:20:16.817 INFO: train_e/atom_mae: 0.000295
train_e/atom_rmse: 0.000360
2025-06-11 22:20:16.818 INFO: train_e/atom_rmse: 0.000360
train_f_mae: 0.016054
2025-06-11 22:20:16.824 INFO: train_f_mae: 0.016054
train_f_rmse: 0.020707
2025-06-11 22:20:16.825 INFO: train_f_rmse: 0.020707
val_e/atom_mae: 0.000164
2025-06-11 22:20:16.830 INFO: val_e/atom_mae: 0.000164
val_e/atom_rmse: 0.000205
2025-06-11 22:20:16.831 INFO: val_e/atom_rmse: 0.000205
val_f_mae: 0.016186
2025-06-11 22:20:16.832 INFO: val_f_mae: 0.016186
val_f_rmse: 0.020913
2025-06-11 22:20:16.833 INFO: val_f_rmse: 0.020913
##### Step: 187 Learning rate: 1.953125e-05 #####
2025-06-11 22:22:31.565 INFO: ##### Step: 187 Learning rate: 1.953125e-05 #####
Epoch 48, Train Loss: 0.4629, Val Loss: 0.4627
2025-06-11 22:22:31.567 INFO: Epoch 48, Train Loss: 0.4629, Val Loss: 0.4627
train_e/atom_mae: 0.000244
2025-06-11 22:22:31.569 INFO: train_e/atom_mae: 0.000244
train_e/atom_rmse: 0.000306
2025-06-11 22:22:31.570 INFO: train_e/atom_rmse: 0.000306
train_f_mae: 0.016046
2025-06-11 22:22:31.576 INFO: train_f_mae: 0.016046
train_f_rmse: 0.020699
2025-06-11 22:22:31.577 INFO: train_f_rmse: 0.020699
val_e/atom_mae: 0.000198
2025-06-11 22:22:31.582 INFO: val_e/atom_mae: 0.000198
val_e/atom_rmse: 0.000252
2025-06-11 22:22:31.583 INFO: val_e/atom_rmse: 0.000252
val_f_mae: 0.016220
2025-06-11 22:22:31.584 INFO: val_f_mae: 0.016220
val_f_rmse: 0.020959
2025-06-11 22:22:31.585 INFO: val_f_rmse: 0.020959
##### Step: 188 Learning rate: 1.953125e-05 #####
2025-06-11 22:24:46.267 INFO: ##### Step: 188 Learning rate: 1.953125e-05 #####
Epoch 49, Train Loss: 0.5246, Val Loss: 0.4585
2025-06-11 22:24:46.269 INFO: Epoch 49, Train Loss: 0.5246, Val Loss: 0.4585
train_e/atom_mae: 0.000432
2025-06-11 22:24:46.271 INFO: train_e/atom_mae: 0.000432
train_e/atom_rmse: 0.000510
2025-06-11 22:24:46.272 INFO: train_e/atom_rmse: 0.000510
train_f_mae: 0.016050
2025-06-11 22:24:46.278 INFO: train_f_mae: 0.016050
train_f_rmse: 0.020702
2025-06-11 22:24:46.279 INFO: train_f_rmse: 0.020702
val_e/atom_mae: 0.000173
2025-06-11 22:24:46.284 INFO: val_e/atom_mae: 0.000173
val_e/atom_rmse: 0.000215
2025-06-11 22:24:46.285 INFO: val_e/atom_rmse: 0.000215
val_f_mae: 0.016272
2025-06-11 22:24:46.286 INFO: val_f_mae: 0.016272
val_f_rmse: 0.021010
2025-06-11 22:24:46.287 INFO: val_f_rmse: 0.021010
##### Step: 189 Learning rate: 1.953125e-05 #####
2025-06-11 22:27:01.045 INFO: ##### Step: 189 Learning rate: 1.953125e-05 #####
Epoch 50, Train Loss: 0.4653, Val Loss: 0.4552
2025-06-11 22:27:01.046 INFO: Epoch 50, Train Loss: 0.4653, Val Loss: 0.4552
train_e/atom_mae: 0.000256
2025-06-11 22:27:01.049 INFO: train_e/atom_mae: 0.000256
train_e/atom_rmse: 0.000317
2025-06-11 22:27:01.050 INFO: train_e/atom_rmse: 0.000317
train_f_mae: 0.016045
2025-06-11 22:27:01.056 INFO: train_f_mae: 0.016045
train_f_rmse: 0.020696
2025-06-11 22:27:01.057 INFO: train_f_rmse: 0.020696
val_e/atom_mae: 0.000190
2025-06-11 22:27:01.061 INFO: val_e/atom_mae: 0.000190
val_e/atom_rmse: 0.000228
2025-06-11 22:27:01.062 INFO: val_e/atom_rmse: 0.000228
val_f_mae: 0.016165
2025-06-11 22:27:01.064 INFO: val_f_mae: 0.016165
val_f_rmse: 0.020883
2025-06-11 22:27:01.065 INFO: val_f_rmse: 0.020883
##### Step: 190 Learning rate: 1.953125e-05 #####
2025-06-11 22:29:15.819 INFO: ##### Step: 190 Learning rate: 1.953125e-05 #####
Epoch 51, Train Loss: 0.4740, Val Loss: 0.4615
2025-06-11 22:29:15.820 INFO: Epoch 51, Train Loss: 0.4740, Val Loss: 0.4615
train_e/atom_mae: 0.000277
2025-06-11 22:29:15.822 INFO: train_e/atom_mae: 0.000277
train_e/atom_rmse: 0.000350
2025-06-11 22:29:15.823 INFO: train_e/atom_rmse: 0.000350
train_f_mae: 0.016050
2025-06-11 22:29:15.829 INFO: train_f_mae: 0.016050
train_f_rmse: 0.020706
2025-06-11 22:29:15.830 INFO: train_f_rmse: 0.020706
val_e/atom_mae: 0.000187
2025-06-11 22:29:15.835 INFO: val_e/atom_mae: 0.000187
val_e/atom_rmse: 0.000236
2025-06-11 22:29:15.836 INFO: val_e/atom_rmse: 0.000236
val_f_mae: 0.016243
2025-06-11 22:29:15.837 INFO: val_f_mae: 0.016243
val_f_rmse: 0.020998
2025-06-11 22:29:15.838 INFO: val_f_rmse: 0.020998
##### Step: 191 Learning rate: 1.953125e-05 #####
2025-06-11 22:31:30.641 INFO: ##### Step: 191 Learning rate: 1.953125e-05 #####
Epoch 52, Train Loss: 0.4585, Val Loss: 0.4580
2025-06-11 22:31:30.642 INFO: Epoch 52, Train Loss: 0.4585, Val Loss: 0.4580
train_e/atom_mae: 0.000231
2025-06-11 22:31:30.645 INFO: train_e/atom_mae: 0.000231
train_e/atom_rmse: 0.000284
2025-06-11 22:31:30.645 INFO: train_e/atom_rmse: 0.000284
train_f_mae: 0.016049
2025-06-11 22:31:30.651 INFO: train_f_mae: 0.016049
train_f_rmse: 0.020705
2025-06-11 22:31:30.652 INFO: train_f_rmse: 0.020705
val_e/atom_mae: 0.000170
2025-06-11 22:31:30.657 INFO: val_e/atom_mae: 0.000170
val_e/atom_rmse: 0.000208
2025-06-11 22:31:30.658 INFO: val_e/atom_rmse: 0.000208
val_f_mae: 0.016280
2025-06-11 22:31:30.660 INFO: val_f_mae: 0.016280
val_f_rmse: 0.021025
2025-06-11 22:31:30.660 INFO: val_f_rmse: 0.021025
##### Step: 192 Learning rate: 1.953125e-05 #####
2025-06-11 22:33:45.319 INFO: ##### Step: 192 Learning rate: 1.953125e-05 #####
Epoch 53, Train Loss: 0.4676, Val Loss: 0.4659
2025-06-11 22:33:45.320 INFO: Epoch 53, Train Loss: 0.4676, Val Loss: 0.4659
train_e/atom_mae: 0.000263
2025-06-11 22:33:45.322 INFO: train_e/atom_mae: 0.000263
train_e/atom_rmse: 0.000325
2025-06-11 22:33:45.323 INFO: train_e/atom_rmse: 0.000325
train_f_mae: 0.016052
2025-06-11 22:33:45.329 INFO: train_f_mae: 0.016052
train_f_rmse: 0.020705
2025-06-11 22:33:45.330 INFO: train_f_rmse: 0.020705
val_e/atom_mae: 0.000212
2025-06-11 22:33:45.335 INFO: val_e/atom_mae: 0.000212
val_e/atom_rmse: 0.000258
2025-06-11 22:33:45.336 INFO: val_e/atom_rmse: 0.000258
val_f_mae: 0.016253
2025-06-11 22:33:45.338 INFO: val_f_mae: 0.016253
val_f_rmse: 0.021010
2025-06-11 22:33:45.339 INFO: val_f_rmse: 0.021010
##### Step: 193 Learning rate: 1.953125e-05 #####
2025-06-11 22:36:00.057 INFO: ##### Step: 193 Learning rate: 1.953125e-05 #####
Epoch 54, Train Loss: 0.4687, Val Loss: 0.4605
2025-06-11 22:36:00.058 INFO: Epoch 54, Train Loss: 0.4687, Val Loss: 0.4605
train_e/atom_mae: 0.000269
2025-06-11 22:36:00.060 INFO: train_e/atom_mae: 0.000269
train_e/atom_rmse: 0.000330
2025-06-11 22:36:00.061 INFO: train_e/atom_rmse: 0.000330
train_f_mae: 0.016050
2025-06-11 22:36:00.067 INFO: train_f_mae: 0.016050
train_f_rmse: 0.020702
2025-06-11 22:36:00.068 INFO: train_f_rmse: 0.020702
val_e/atom_mae: 0.000200
2025-06-11 22:36:00.073 INFO: val_e/atom_mae: 0.000200
val_e/atom_rmse: 0.000237
2025-06-11 22:36:00.074 INFO: val_e/atom_rmse: 0.000237
val_f_mae: 0.016225
2025-06-11 22:36:00.076 INFO: val_f_mae: 0.016225
val_f_rmse: 0.020970
2025-06-11 22:36:00.076 INFO: val_f_rmse: 0.020970
##### Step: 194 Learning rate: 1.953125e-05 #####
2025-06-11 22:38:14.809 INFO: ##### Step: 194 Learning rate: 1.953125e-05 #####
Epoch 55, Train Loss: 0.4745, Val Loss: 0.4589
2025-06-11 22:38:14.811 INFO: Epoch 55, Train Loss: 0.4745, Val Loss: 0.4589
train_e/atom_mae: 0.000288
2025-06-11 22:38:14.814 INFO: train_e/atom_mae: 0.000288
train_e/atom_rmse: 0.000353
2025-06-11 22:38:14.815 INFO: train_e/atom_rmse: 0.000353
train_f_mae: 0.016049
2025-06-11 22:38:14.821 INFO: train_f_mae: 0.016049
train_f_rmse: 0.020700
2025-06-11 22:38:14.822 INFO: train_f_rmse: 0.020700
val_e/atom_mae: 0.000197
2025-06-11 22:38:14.826 INFO: val_e/atom_mae: 0.000197
val_e/atom_rmse: 0.000250
2025-06-11 22:38:14.828 INFO: val_e/atom_rmse: 0.000250
val_f_mae: 0.016166
2025-06-11 22:38:14.829 INFO: val_f_mae: 0.016166
val_f_rmse: 0.020876
2025-06-11 22:38:14.829 INFO: val_f_rmse: 0.020876
##### Step: 195 Learning rate: 1.953125e-05 #####
2025-06-11 22:40:29.830 INFO: ##### Step: 195 Learning rate: 1.953125e-05 #####
Epoch 56, Train Loss: 0.4629, Val Loss: 0.4761
2025-06-11 22:40:29.832 INFO: Epoch 56, Train Loss: 0.4629, Val Loss: 0.4761
train_e/atom_mae: 0.000245
2025-06-11 22:40:29.834 INFO: train_e/atom_mae: 0.000245
train_e/atom_rmse: 0.000306
2025-06-11 22:40:29.835 INFO: train_e/atom_rmse: 0.000306
train_f_mae: 0.016046
2025-06-11 22:40:29.841 INFO: train_f_mae: 0.016046
train_f_rmse: 0.020696
2025-06-11 22:40:29.842 INFO: train_f_rmse: 0.020696
val_e/atom_mae: 0.000278
2025-06-11 22:40:29.846 INFO: val_e/atom_mae: 0.000278
val_e/atom_rmse: 0.000325
2025-06-11 22:40:29.848 INFO: val_e/atom_rmse: 0.000325
val_f_mae: 0.016192
2025-06-11 22:40:29.849 INFO: val_f_mae: 0.016192
val_f_rmse: 0.020910
2025-06-11 22:40:29.850 INFO: val_f_rmse: 0.020910
##### Step: 196 Learning rate: 1.953125e-05 #####
2025-06-11 22:42:44.617 INFO: ##### Step: 196 Learning rate: 1.953125e-05 #####
Epoch 57, Train Loss: 0.4676, Val Loss: 0.4720
2025-06-11 22:42:44.619 INFO: Epoch 57, Train Loss: 0.4676, Val Loss: 0.4720
train_e/atom_mae: 0.000260
2025-06-11 22:42:44.621 INFO: train_e/atom_mae: 0.000260
train_e/atom_rmse: 0.000324
2025-06-11 22:42:44.622 INFO: train_e/atom_rmse: 0.000324
train_f_mae: 0.016056
2025-06-11 22:42:44.628 INFO: train_f_mae: 0.016056
train_f_rmse: 0.020709
2025-06-11 22:42:44.629 INFO: train_f_rmse: 0.020709
val_e/atom_mae: 0.000234
2025-06-11 22:42:44.634 INFO: val_e/atom_mae: 0.000234
val_e/atom_rmse: 0.000274
2025-06-11 22:42:44.635 INFO: val_e/atom_rmse: 0.000274
val_f_mae: 0.016317
2025-06-11 22:42:44.636 INFO: val_f_mae: 0.016317
val_f_rmse: 0.021080
2025-06-11 22:42:44.637 INFO: val_f_rmse: 0.021080
